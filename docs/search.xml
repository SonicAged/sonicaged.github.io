<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Attention Overview</title>
    <url>/2025/07/10/Attention/</url>
    <content><![CDATA[<h1 id="Is-Attention-All-My-Need"><a href="#Is-Attention-All-My-Need" class="headerlink" title="Is Attention All My Need ?"></a>Is Attention All My Need ?</h1><blockquote>
<p>注意力机制在图神经网络中扮演着越来越重要的角色。<del>但鼠鼠现在连正常的 Attention 有哪些都不清楚捏</del>本文鼠鼠将从一般的 Attention 出发，给出 Attention 的总体结构，然后按分类介绍现有的主要的 Attention</p>
</blockquote>
<p>本文主要来自于一篇论文，基本可以看作<a href="/paper/Brauwers%E5%92%8CFrasincar%20-%202023%20-%20A%20General%20Survey%20on%20Attention%20Mechanisms%20in%20Deep%20Learning.pdf">那篇论文</a>的阅读笔记</p>
<span id="more"></span>

<h2 id="🎯-引言"><a href="#🎯-引言" class="headerlink" title="🎯 引言"></a>🎯 引言</h2><p>在深度学习领域，注意力机制已经成为一个革命性的创新，特别是在处理序列数据和图像数据方面取得了巨大成功。而在图神经网络中，注意力机制的引入不仅提高了模型的表现力，还增强了模型的可解释性。</p>
<p>在图结构数据中应用注意力机制主要有以下优势：</p>
<ol>
<li>自适应性：能够根据任务动态调整不同邻居节点的重要性</li>
<li>可解释性：通过注意力权重可以直观理解模型的决策过程</li>
<li>长程依赖：有效缓解了传统 GNN 中的过平滑问题</li>
<li>异质性处理：更好地处理异质图中的不同类型节点和边</li>
</ol>
<h2 id="📚-总览-Attention"><a href="#📚-总览-Attention" class="headerlink" title="📚 总览 Attention"></a>📚 总览 Attention</h2><p>本章节主要参考了论文<a href="/paper/Brauwers%E5%92%8CFrasincar%20-%202023%20-%20A%20General%20Survey%20on%20Attention%20Mechanisms%20in%20Deep%20Learning.pdf">📄 Brauwers 和 Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a>有兴趣的话可以看看原文捏</p>
<embed src="/paper/Brauwers和Frasincar%20-%202023%20-%20A%20General%20Survey%20on%20Attention%20Mechanisms%20in%20Deep%20Learning.pdf" width="45%" height="400" type="application/pdf">

<h3 id="Attention-的一般结构"><a href="#Attention-的一般结构" class="headerlink" title="Attention 的一般结构"></a>Attention 的一般结构</h3><img src="/img/Attention/TotalModel.png" alt="TotalModel" width="60%" height="auto">

<p>上图是从总体上看 Attention 在整个任务模型框架中的位置</p>
<p>框架包含四个核心组件：</p>
<ol>
<li><strong>特征模型</strong>：负责输入数据的特征提取</li>
<li><strong>查询模型</strong>：生成注意力查询向量</li>
<li><strong>注意力模型</strong>：计算注意力权重</li>
<li><strong>输出模型</strong>：生成最终预测结果</li>
</ol>
<p>接下来，我们会从 <em>输入</em> 的角度来看<strong>特征模型</strong>和<strong>查询模型</strong>，从 <em>输出</em> 的角度来看<strong>注意力模型</strong>和<strong>输出模型</strong></p>
<h4 id="输入处理机制"><a href="#输入处理机制" class="headerlink" title="输入处理机制"></a>输入处理机制</h4><ol>
<li><p><strong>特征模型</strong>，即将任务的输入进行 embedding</p>
<p>对于输入矩阵 $ X \in \mathbb{R}^{d_x \times n_x} $ ，特征模型提取特征向量： $\boldsymbol{F} &#x3D; [f_1, \ldots, f_{n_f}] \in \mathbb{R}^{d_f \times n_f}$</p>
</li>
<li><p><strong>查询模型</strong>，查询模型产生查询向量$ \boldsymbol{q} \in \mathbb{R}^{d_q} $，用以告诉注意力模型哪一个特征是重要的</p>
</li>
</ol>
<p>一般情况下，这两个模型可以用 CNN 或 RNN</p>
<h4 id="输出计算机制"><a href="#输出计算机制" class="headerlink" title="输出计算机制"></a>输出计算机制</h4><img src="/img/Attention/GeneralAttentionModule.png" alt="GeneralAttentionModule" width="50%" height="auto">

<p>上图是 Attention 模型总体结构的说明，下面对这张图进行详细的说明</p>
<ol>
<li>特征矩阵$\boldsymbol{F} &#x3D; [\boldsymbol{f}_1, \ldots, \boldsymbol{f}_{n_f}] \in \mathbb{R}^{d_f \times n_f}$，通过<em>某些方法</em>将其分为 Keys 矩阵$\boldsymbol{K} &#x3D; [\boldsymbol{k}_1, \ldots, \boldsymbol{k}_{n_f}] \in \mathbb{R}^{d_k \times n_f}$和 Values 矩阵$\boldsymbol{V} &#x3D; [\boldsymbol{v}_1, \ldots, \boldsymbol{v}_{n_f}] \in \mathbb{R}^{d_v \times n_f}$，这里的<em>某些方法</em>，一般情况下，按以下的方式通过<strong>线性变换</strong>得到：</li>
</ol>
<p>$$<br>\underset{d_{k} \times n_{f}}{\boldsymbol{K}}&#x3D;\underset{d_{k} \times d_{f}}{\boldsymbol{W}_{K}} \times \underset{d_{f} \times n_{f}}{\boldsymbol{F}}, \quad \underset{d_{v} \times n_{f}}{\boldsymbol{V}}&#x3D;\underset{d_{v} \times d_{f}}{\boldsymbol{W}_{V}} \times \underset{d_{f} \times n_{f}}{\boldsymbol{F}} .<br>$$</p>
<ol start="2">
<li><p><code>Attention Scores</code>模块根据 $\boldsymbol{q}$ 计算每一个 key 向量对应的分数$\boldsymbol{e} &#x3D; [e_1, \ldots, e_{n_f}] \in \mathbb{R}^{n_f}$：</p>
<p>$$<br>\underset{1\times 1}{e_l} &#x3D; \text{score}(\underset{d_q \times 1}{\boldsymbol{q}}, \underset{d_k \times 1}{\boldsymbol{k}_l})<br>$$</p>
<p>如前所述，查询象征着对信息的请求。注意力分数$e_l$表示根据查询，关键向量$\boldsymbol{k}_l$中包含的信息的重要性。如果查询和关键向量的维度相同，则得分函数的一个例子是取向量的点积。</p>
</li>
<li><p>由于经过这么一堆操作之后，分数有很大的可能已经飞起来了捏，这个时候就需要<code>Attention Alignment</code>模块对其进行<strong>归一化</strong>之类的操作了捏</p>
<p>$$<br>\underset{1\times 1}{a_l} &#x3D; \text{align}(\underset{d_q \times 1}{\boldsymbol{e_l}}, \underset{n_f \times 1}{\boldsymbol{e}})<br>$$</p>
</li>
</ol>
<p>注意力权重$\boldsymbol{a} &#x3D; [a_1, \ldots, a_{n_f}] \in \mathbb{R}^{n_f}$为注意力模块提供了一个相当直观的解释。每个权重直接表明了每个特征向量相对于其他特征向量对于这个问题的重要性。</p>
<ol start="4">
<li><p>在<code>Weight Average</code>模块完成<strong>上下文生成</strong>：</p>
<p>$$<br>\underset{d_v \times 1}{\boldsymbol{c}} &#x3D; \sum_{l &#x3D; 1}^{n_f} \underset{1 \times 1}{a_l}\times \underset{d_v \times 1}{\boldsymbol{v}_l}<br>$$</p>
</li>
<li><p>输出处理就想怎么搞就怎么搞了捏，例如 用于分类</p>
<p>$$<br>\underset{d_y \times 1}{\hat{\boldsymbol{y}}} &#x3D; \text{softmax}( \underset{d_y \times d_v}{\boldsymbol{W}_c}\times \underset{d_v \times 1}{\boldsymbol{c}} + \underset{d_y \times 1}{\boldsymbol{b}_c})<br>$$</p>
</li>
</ol>
<h3 id="Attention-分类"><a href="#Attention-分类" class="headerlink" title="Attention 分类"></a>Attention 分类</h3><img src="/img/Attention/Taxonomy.png" style="max-width: 100%; height: auto;">

<p>论文按照上图的方式给 Attention 进行了分类</p>
<p>由于篇幅限制，这里决定重开几个博文来分别介绍这些 Attention，链接如下：</p>
<a href="/2025/07/14/Feature-Related-Attention/" title="Fufufu Relashinala">Fufufu Relashinala</a>
<br/>
<a href="/2025/07/16/General-Attention/" title="Gugugu Neralashun">Gugugu Neralashun</a>
<br/>
<a href="/2025/07/17/Query-Related-Attention/" title="Quaqua Rishinala">Quaqua Rishinala</a>

<h3 id="怎样评价-Attention"><a href="#怎样评价-Attention" class="headerlink" title="怎样评价 Attention"></a>怎样评价 Attention</h3><h4 id="外在性能评估"><a href="#外在性能评估" class="headerlink" title="外在性能评估"></a>外在性能评估</h4><ol>
<li><strong>领域特定的评估指标</strong></li>
</ol>
<p>不同领域用于评估注意力模型性能的指标：</p>
<table>
<thead>
<tr>
<th>领域</th>
<th>常用评估指标</th>
<th>典型应用</th>
</tr>
</thead>
<tbody><tr>
<td>自然语言处理</td>
<td>BLEU, METEOR, Perplexity</td>
<td>机器翻译、文本生成</td>
</tr>
<tr>
<td>语音处理</td>
<td>词错误率(WER)、音素错误率(PER)</td>
<td>语音识别</td>
</tr>
<tr>
<td>计算机视觉</td>
<td>PSNR, SSIM, IoU</td>
<td>图像生成、分割</td>
</tr>
<tr>
<td>通用分类</td>
<td>准确率、精确率、召回率、F1</td>
<td>情感分析、文档分类</td>
</tr>
</tbody></table>
<ol start="2">
<li><p><strong>消融研究</strong></p>
<p> 论文强调了消融研究(ablation study)在评估注意力机制重要性方面的价值。典型做法包括：</p>
<ol>
<li>移除或替换注意力机制（如用平均池化代替注意力池化）</li>
<li>比较模型在有无注意力机制时的性能差异</li>
<li>分析不同注意力变体对最终性能的影响</li>
</ol>
<p> 这种评估方法可以明确注意力机制对模型性能的实际贡献，而不仅仅是展示最终结果。</p>
</li>
</ol>
<h4 id="内在特性评估"><a href="#内在特性评估" class="headerlink" title="内在特性评估"></a>内在特性评估</h4><ol>
<li><p><strong>注意力权重分析</strong></p>
<ol>
<li><strong>对齐错误率(AER)</strong>：比较模型生成的注意力权重与人工标注的”黄金”注意力权重之间的差异</li>
<li><strong>监督注意力训练</strong>：将人工标注的注意力权重作为额外监督信号，与任务损失联合训练</li>
<li><strong>注意力可视化</strong>：通过热图等方式直观展示模型关注区域</li>
</ol>
<p>这些方法可以评估注意力权重是否符合人类直觉或领域知识。</p>
</li>
<li><p><strong>基于人类注意力的评估</strong></p>
<p> 论文提出了”注意力正确性”(Attention Correctness)的概念，将模型的注意力模式与真实人类注意力行为进行比较：</p>
<ol>
<li><strong>数据收集</strong>：记录人类在执行相同任务时的注意力模式（如眼动追踪）</li>
<li><strong>度量计算</strong>：定义模型注意力与人类注意力的相似度指标</li>
<li><strong>联合训练</strong>：将人类注意力数据作为监督信号</li>
</ol>
<p> 这种评估方法基于认知科学原理，认为好的注意力模型应该模拟人类的注意力机制。</p>
</li>
</ol>
<h4 id="注意力解释性评估、"><a href="#注意力解释性评估、" class="headerlink" title="注意力解释性评估、"></a>注意力解释性评估、</h4><p>论文讨论了学术界关于”注意力是否提供解释”的争论：</p>
<ol>
<li><p><strong>“Attention is not Explanation”观点</strong></p>
<ul>
<li>注意力权重与模型决策之间缺乏稳定关联</li>
<li>可以构造对抗性注意力分布而不改变模型输出</li>
<li>注意力权重可能反映相关性而非因果性</li>
</ul>
</li>
<li><p><strong>“Attention is not not Explanation”反驳</strong></p>
<ul>
<li>对抗性注意力分布通常性能更差</li>
<li>注意力权重确实反映了输入的相对重要性</li>
<li>在特定架构下注意力可以提供有意义的解释</li>
</ul>
</li>
</ol>
<p><del>这段比较难绷，因此把</del>原文贴在下面了捏</p>
<blockquote>
<p>However, rather than checking if the model focuses on the most important parts of the data, some use the attention weights to determine which parts of the data are most important. This would imply that attention models provide a type of explanation, which is a subject of contention among researchers. Particularly, in [120], extensive experiments are conducted for various natural language processing tasks to investigate the relation between attention weights and important information to determine whether attention can actually provide meaningful explanations. In this paper titled “Attention is not Explanation”, it is found that attention weights do not tend to correlate with important features. Additionally, the authors are able to replace the produced attention weights with completely different values while keeping the model output the same. These so-called “adversarial” attention distributions show that an attention model may focus on completely different information and still come to the same conclusions, which makes interpretation difficult. Yet, in another paper titled “Attention is not not Explanation” [121], the claim that attention is not explanation is questioned by challenging the assumptions of the previous work. It is found that the adversarial attention distributions do not perform as reliably well as the learned attention weights, indicating that it was not proved that attention is not viable for explanation. In general, the conclusion regarding the interpretability of attention models is that researchers must be extremely careful when drawing conclusions based on attention patterns. For example, problems with an attention model can be diagnosed via the attention weights if the model is found to focus on the incorrect parts of the data, if such information is available. Yet, conversely, attention weights may only be used to obtain plausible explanations for why certain parts of the data are focused on, rather than concluding that those parts are significant to the problem [121]. However, one should still be cautious as the viability of such approaches can depend on the model architecture [122].</p>
</blockquote>
<h1 id="📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒"><a href="#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒" class="headerlink" title="📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1><p><a href="/paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf" target="_blank">📄 Brauwers 和 Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a></p>
]]></content>
      <categories>
        <category>model</category>
        <category>attention</category>
      </categories>
      <tags>
        <tag>CDR</tag>
        <tag>model</tag>
        <tag>Basic</tag>
        <tag>deep learning</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>What Is GNN and GCN ?</title>
    <url>/2025/07/10/GNN-and-GCN/</url>
    <content><![CDATA[<h1 id="GNN-与-GCN"><a href="#GNN-与-GCN" class="headerlink" title="GNN 与 GCN"></a>GNN 与 GCN</h1><blockquote>
<p>图神经网络（Graph Neural Networks, GNN）和图卷积网络（Graph Convolutional Networks, GCN）是处理图数据的强大工具。本文将从理论到实践，全面介绍这两种重要的深度学习模型。</p>
</blockquote>
<p>本文主要介绍了<em>GNN 和 GCN 的大致原理</em>，<em>GCN 在 PyG 和 PyTorch 的实现</em> 以及它们在<em>DRP 中的应用</em></p>
<span id="more"></span>

<h2 id="🎯-Intro"><a href="#🎯-Intro" class="headerlink" title="🎯 Intro"></a>🎯 Intro</h2><p>在深度学习领域，处理图结构数据一直是一个具有挑战性的任务。传统的深度学习模型（如 CNN、RNN）在处理欧几里得空间中的数据表现出色，但对于图这种非欧几里得结构的数据却显得力不从心。GNN 和 GCN 的出现，为我们提供了处理图数据的有力工具。</p>
<p>而在 DRP 领域，由于涉及到大量的 Embedding，GCN 现在几乎已经成为了必不可少的模块。</p>
<p>但在开始各种各样的奇形怪状的 GCN 之前，了解 GNN 和 GCN 本身的实现仍然是非常必要的。<del>于鼠鼠而言</del>大致有以下理由：</p>
<ol>
<li>部分抽象的基于 GCN 的模块第三方库不一定支持</li>
<li>由于反应表示数据的不平衡，我们可以构建的模型的层数是非常有限的（因为会过平滑）。因此对层内的改造就显得非常必要了。而这一切的前提便是理解原理捏</li>
</ol>
<p>在这里强烈建议去看一下<a href="https://distill.pub/">Distill</a>的两篇有关图神经网络的博客，非常易懂。</p>
<hr>
<h2 id="📚-理论基础"><a href="#📚-理论基础" class="headerlink" title="📚 理论基础"></a>📚 理论基础</h2><h3 id="图的基本概念"><a href="#图的基本概念" class="headerlink" title="图的基本概念"></a>图的基本概念</h3><p>在开始之前，我们需要理解图的基本表示：</p>
<ul>
<li>图 $G &#x3D; (V, E)$，其中 $V$ 是节点集合，$E$ 是边集合</li>
<li>邻接矩阵 $A \in \mathbb{R}^{n \times n}$</li>
<li>度矩阵 $D &#x3D; diag(d_1,…,d_n)$，其中 $d_i &#x3D; \sum_j A_{ij}$</li>
<li>节点特征矩阵 $X \in \mathbb{R}^{n \times d}$</li>
</ul>
<h3 id="GNN-框架"><a href="#GNN-框架" class="headerlink" title="GNN 框架"></a>GNN 框架</h3><p>GNN 的基本框架遵循消息传递范式（Message Passing Neural Network, MPNN），可以用以下数学公式表示：</p>
<ol>
<li><p><strong>消息传递阶段</strong>（Message Passing）：</p>
<p>对于节点 $v$，从其邻居节点 $u \in \mathcal{N}(v)$ 收集信息：</p>
<p>$$m_v^{(l)} &#x3D; \sum_{u \in \mathcal{N}(v)} M_l(h_v^{(l-1)}, h_u^{(l-1)}, e_{uv})$$</p>
<p>其中：</p>
<ul>
<li>$h_v^{(l-1)}$ 是节点 $v$ 在第 $l-1$ 层的特征</li>
<li>$e_{uv}$ 是边 $(u,v)$ 的特征</li>
<li>$M_l$ 是可学习的消息函数</li>
</ul>
</li>
<li><p><strong>消息聚合阶段</strong>（Aggregation）：</p>
<p>将收集到的消息进行聚合：</p>
<p>$$a_v^{(l)} &#x3D; AGG({m_v^{(l)} | u \in \mathcal{N}(v)})$$</p>
<p>常见的聚合函数包括：</p>
<ul>
<li>求和：$AGG_{sum} &#x3D; \sum_{u \in \mathcal{N}(v)} m_u$</li>
<li>平均：$AGG_{mean} &#x3D; \frac{1}{|\mathcal{N}(v)|} \sum_{u \in \mathcal{N}(v)} m_u$</li>
<li>最大：$AGG_{max} &#x3D; max_{u \in \mathcal{N}(v)} m_u$</li>
</ul>
</li>
<li><p><strong>节点更新阶段</strong>（Update）：</p>
<p>更新节点的表示：</p>
<p>$$h_v^{(l)} &#x3D; U_l(h_v^{(l-1)}, a_v^{(l)})$$</p>
<p>其中 $U_l$ 是可学习的更新函数，通常是 MLP 或其他神经网络。</p>
</li>
</ol>
<h3 id="GCN-实现"><a href="#GCN-实现" class="headerlink" title="GCN 实现"></a>GCN 实现</h3><h4 id="拉普拉斯矩阵-🔍"><a href="#拉普拉斯矩阵-🔍" class="headerlink" title="拉普拉斯矩阵 🔍"></a>拉普拉斯矩阵 🔍</h4><p>拉普拉斯矩阵是图信号处理中的核心概念，有多种形式：</p>
<ol>
<li><p><strong>组合拉普拉斯矩阵</strong>：$L &#x3D; D - A$</p>
</li>
<li><p><strong>标准化拉普拉斯矩阵</strong>：$L_{sym} &#x3D; D^{-\frac{1}{2}}LD^{-\frac{1}{2}} &#x3D; I - D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$</p>
</li>
<li><p><strong>随机游走拉普拉斯矩阵</strong>：$L_{rw} &#x3D; D^{-1}L &#x3D; I - D^{-1}A$</p>
</li>
</ol>
<p>拉普拉斯矩阵的特性：</p>
<ul>
<li>对称性：$L &#x3D; L^T$</li>
<li>半正定性：所有特征值非负</li>
<li>最小特征值为 0，对应的特征向量是常数向量</li>
<li>特征值的重数对应图的连通分量数</li>
</ul>
<h4 id="从传统卷积到图卷积-🔄"><a href="#从传统卷积到图卷积-🔄" class="headerlink" title="从传统卷积到图卷积 🔄"></a>从传统卷积到图卷积 🔄</h4><h5 id="传统卷积回顾"><a href="#传统卷积回顾" class="headerlink" title="传统卷积回顾"></a>传统卷积回顾</h5><p>在欧几里得空间中，卷积操作定义为：</p>
<p>$$(f * g)(p) &#x3D; \sum_{q \in \mathcal{N}(p)} f(q) \cdot g(p-q)$$</p>
<p>这里的关键特点是：</p>
<ul>
<li>平移不变性</li>
<li>局部性</li>
<li>参数共享</li>
</ul>
<h5 id="图上的卷积定义"><a href="#图上的卷积定义" class="headerlink" title="图上的卷积定义"></a>图上的卷积定义</h5><p>在图域中，我们需要重新定义这些特性：</p>
<ol>
<li><p><strong>空间域卷积</strong>：<br>$$h_v &#x3D; \sum_{u \in \mathcal{N}(v)} W(e_{u,v})h_u$$<br>其中 $W(e_{u,v})$ 是边的权重函数</p>
</li>
<li><p><strong>谱域卷积</strong>：<br>$$g_\theta * x &#x3D; Ug_\theta U^T x$$<br>其中 $U$ 是拉普拉斯矩阵的特征向量矩阵</p>
</li>
</ol>
<h4 id="GCN-的数学推导-⚙️"><a href="#GCN-的数学推导-⚙️" class="headerlink" title="GCN 的数学推导 ⚙️"></a>GCN 的数学推导 ⚙️</h4><p>Kipf &amp; Welling 提出的 GCN 模型中，单层传播规则为：</p>
<p>$$H^{(l+1)} &#x3D; \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})$$</p>
<p>其中：</p>
<ul>
<li>$\tilde{A} &#x3D; A + I_N$ 是添加了自环的邻接矩阵</li>
<li>$\tilde{D}_{ii} &#x3D; \sum_{j} \tilde{A}_{ij}$ 是对应的度矩阵</li>
<li>$H^{(l)}$ 是第 $l$ 层的激活值</li>
<li>$W^{(l)}$ 是可学习的权重矩阵</li>
<li>$\sigma$ 是非线性激活函数</li>
</ul>
<p><del>一些自己的理解</del></p>
<ol>
<li>引入$L_{sym} &#x3D; \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$作为聚合（AGG）部分<ul>
<li>添加自环：$\tilde{A} &#x3D; A + I_N$</li>
<li>计算归一化系数：$\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$</li>
</ul>
</li>
<li>特征变换：$H^{(l)}W^{(l)}$</li>
<li>邻域聚合：$\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)}$</li>
<li>非线性变换：$\sigma(\cdot)$</li>
</ol>
<hr>
<h2 id="💻-实现细节"><a href="#💻-实现细节" class="headerlink" title="💻 实现细节"></a>💻 实现细节</h2><p>基于这个理论框架的简单实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">message_passing</span>(<span class="params">nodes, edges</span>):</span><br><span class="line">    messages = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> edge <span class="keyword">in</span> edges:</span><br><span class="line">        src, dst = edge</span><br><span class="line">        msg = compute_message(nodes[src], nodes[dst])</span><br><span class="line">        messages.setdefault(dst, []).append(msg)</span><br><span class="line">    <span class="keyword">return</span> messages</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">aggregate_messages</span>(<span class="params">messages</span>):</span><br><span class="line">    aggregated = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> node, msgs <span class="keyword">in</span> messages.items():</span><br><span class="line">        aggregated[node] = <span class="built_in">sum</span>(msgs) / <span class="built_in">len</span>(msgs)  <span class="comment"># 平均聚合</span></span><br><span class="line">    <span class="keyword">return</span> aggregated</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update_nodes</span>(<span class="params">nodes, aggregated</span>):</span><br><span class="line">    updated = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> node, agg_msg <span class="keyword">in</span> aggregated.items():</span><br><span class="line">        updated[node] = nodes[node] + agg_msg  <span class="comment"># 残差连接</span></span><br><span class="line">    <span class="keyword">return</span> updated</span><br></pre></td></tr></table></figure>

<h3 id="PyTorch-Geometric-实现-🚀"><a href="#PyTorch-Geometric-实现-🚀" class="headerlink" title="PyTorch Geometric 实现 🚀"></a>PyTorch Geometric 实现 🚀</h3><blockquote>
<p>本节代码基于 PyTorch 2.1.0 和 PyTorch Geometric 2.4.0 版本</p>
</blockquote>
<p>使用 PyTorch Geometric 库的 GCN 实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> GCNConv</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GCN</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_features, num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>(GCN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = GCNConv(num_features, <span class="number">16</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = GCNConv(<span class="number">16</span>, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, edge_index</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.conv1(x, edge_index)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = F.dropout(x, training=<span class="variable language_">self</span>.training)</span><br><span class="line">        x = <span class="variable language_">self</span>.conv2(x, edge_index)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h3 id="原生-PyTorch-实现-🔧"><a href="#原生-PyTorch-实现-🔧" class="headerlink" title="原生 PyTorch 实现 🔧"></a>原生 PyTorch 实现 🔧</h3><blockquote>
<p>本节代码基于 PyTorch 2.1.0、NumPy 1.24.0 和 SciPy 1.11.0 版本</p>
</blockquote>
<p>不使用 PyG，手动实现 GCN<del>主要是目前不太清楚主流的 HGCN 的实现方式捏</del>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> scipy.sparse <span class="keyword">as</span> sp</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GCNLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, out_features</span>):</span><br><span class="line">        <span class="built_in">super</span>(GCNLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.W = nn.Parameter(torch.FloatTensor(in_features, out_features))</span><br><span class="line">        <span class="variable language_">self</span>.reset_parameters()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset_parameters</span>(<span class="params">self</span>):</span><br><span class="line">        nn.init.kaiming_uniform_(<span class="variable language_">self</span>.W)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, adj</span>):</span><br><span class="line">        <span class="comment"># adj: 归一化的邻接矩阵</span></span><br><span class="line">        support = torch.mm(x, <span class="variable language_">self</span>.W)</span><br><span class="line">        output = torch.sparse.mm(adj, support)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GCN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, nfeat, nhid, nclass, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(GCN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.gc1 = GCNLayer(nfeat, nhid)</span><br><span class="line">        <span class="variable language_">self</span>.gc2 = GCNLayer(nhid, nclass)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = dropout</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, adj</span>):</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.gc1(x, adj))</span><br><span class="line">        x = F.dropout(x, <span class="variable language_">self</span>.dropout, training=<span class="variable language_">self</span>.training)</span><br><span class="line">        x = <span class="variable language_">self</span>.gc2(x, adj)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normalize_adj</span>(<span class="params">adj</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;归一化邻接矩阵&quot;&quot;&quot;</span></span><br><span class="line">    adj = sp.coo_matrix(adj)</span><br><span class="line">    rowsum = np.array(adj.<span class="built_in">sum</span>(<span class="number">1</span>))</span><br><span class="line">    d_inv_sqrt = np.power(rowsum, -<span class="number">0.5</span>).flatten()</span><br><span class="line">    d_inv_sqrt[np.isinf(d_inv_sqrt)] = <span class="number">0.</span></span><br><span class="line">    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)</span><br><span class="line">    <span class="keyword">return</span> adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="🎮-应用场景"><a href="#🎮-应用场景" class="headerlink" title="🎮 应用场景"></a>🎮 应用场景</h2><p><del>由于鼠鼠就是个臭写 DRP 的捏</del> 这里只给出 GNN 在 DRP 中的应用</p>
<ol>
<li><p><strong>药物表示</strong></p>
<ul>
<li><em>分子图构建</em>：将药物 SMILES 字符串转换为图结构，节点表示原子（含原子类型、电荷等特征），边表示化学键（如键类型、距离）。</li>
<li><em>GNN 编码</em>：使用图卷积网络（GCN）、图注意力网络（GAT）或图同构网络（GIN）等层迭代聚合邻域信息，生成药物嵌入（embedding）。例如，GraTransDRP（2022）结合 GAT 和 Transformer 提升药物表征能力。</li>
</ul>
</li>
<li><p><strong>癌症表示</strong></p>
<ul>
<li><em>生物网络构建</em>：基于基因互作（如 STRING 数据库的蛋白-蛋白互作）、基因共表达或通路信息构建异质图。例如，AGMI（2021）整合多组学数据和 PPI 网络，通过 GNN 学习癌症样本的联合表征。</li>
<li><em>多组学融合</em>：部分模型（如 TGSA）利用 GNN 整合基因组、转录组等数据，通过跨模态注意力机制增强特征交互。</li>
</ul>
</li>
<li><p><strong>异构图与联合建模</strong></p>
<ul>
<li><em>细胞系-药物异构图</em>：如 GraphCDR（2021）将细胞系和药物作为两类节点，通过边连接已知响应对，直接学习跨实体关系。</li>
<li><em>知识增强</em>：预训练 GNN 于大规模生物化学属性预测（如 Zhu et al., 2021），再迁移至 DRP 任务，提升泛化性。</li>
</ul>
</li>
</ol>
<h2 id="🎯-总结与展望"><a href="#🎯-总结与展望" class="headerlink" title="🎯 总结与展望"></a>🎯 总结与展望</h2><ul>
<li><strong>动态图建模</strong>：捕捉治疗过程中动态变化的生物网络。</li>
<li><strong>三维分子图</strong>：结合几何深度学习（如 SchNet）提升立体化学感知。</li>
<li><strong>基准测试</strong>：需统一评估协议（如固定数据集和指标）以公平比较 GNN 与其他方法。</li>
</ul>
<p><del>之后应该会写一些具体模型的博客，有相关的会直接上链接的捏 jrm</del></p>
<h1 id="📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒"><a href="#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒" class="headerlink" title="📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1><a href="https://pytorch-geometric.readthedocs.io/" target="_blank">
  <span style="display: inline-block; vertical-align: middle;">
    <img src="/icon/pyg.svg" alt="pyg" style="height: 1.5em; vertical-align: text-bottom; margin-top: 16px;">
  </span>
  PyTorch Geometric 官方文档</a>

<p><a href="/paper/1609.02907v4.pdf" target="_blank">📄 Thomas - SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</a></p>
<a href="https://distill.pub/2021/gnn-intro/" target="_blank">
  <span style="display: inline-block; vertical-align: middle;">
    <img src="/icon/google.svg" alt="google" style="height: 1.3em; vertical-align: text-bottom; margin-top: 16px;">
  </span>
  Distill: A Gentle Introduction to Graph Neural Networks
</a>

<p><a href="/paper/Feng 等 - 2024 - A Comprehensive Survey of Dynamic Graph Neural Networks Models, Frameworks, Benchmarks, Experiments.pdf" target="_blank">📄 Feng 等 - 2024 - A Comprehensive Survey of Dynamic Graph Neural Networks Models, Frameworks, Benchmarks, Experiments</a></p>
<a href="https://distill.pub/2021/understanding-gnns/" target="_blank">
  <span style="display: inline-block; vertical-align: middle;">
    <img src="/icon/google.svg" alt="google" style="height: 1.3em; vertical-align: text-bottom; margin-top: 16px;">
  </span>
  Distill: Understanding Convolutions on Graphs
</a>

<p><a href="https://arxiv.org/abs/2401.11768" target="_blank">📄 ADA-GNN: Atom-Distance-Angle Graph Neural Network for Crystal Material Property Prediction</a></p>
<a href="https://www.zhihu.com/tardis/zm/art/107162772" target="_blank">
  <span style="display: inline-block; vertical-align: middle;">
    <img src="/icon/zhihu.svg" alt="zhihu" style="height: 1.3em; vertical-align: text-bottom; margin-top: 16px;">
  </span>
  知乎：图卷积网络（GCN）入门详解
</a>

<p><a href="/paper/吴凌飞[等]编. - 2022 - 图神经网络 基础,前沿与应用.pdf" target="_blank">📄 吴凌飞[等]编. - 2022 - 图神经网络 基础,前沿与应用</a></p>
<a href="https://github.com/tkipf/gcn" target="_blank">
  <span style="display: inline-block; vertical-align: middle;">
    <img src="/icon/github.svg" alt="github" style="height: 1.3em; vertical-align: text-bottom; margin-top: 16px;">
  </span>
  GCN 论文官方代码
</a>
]]></content>
      <categories>
        <category>model</category>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>CDR</tag>
        <tag>model</tag>
        <tag>Basic</tag>
        <tag>PyTorch</tag>
        <tag>embedding</tag>
        <tag>graph theory</tag>
      </tags>
  </entry>
  <entry>
    <title>Woc?! GAT? We&#39;re Saved!</title>
    <url>/2025/07/18/GAT/</url>
    <content><![CDATA[<h1 id="GAT（Graph-Attention-Networks）"><a href="#GAT（Graph-Attention-Networks）" class="headerlink" title="GAT（Graph Attention Networks）"></a>GAT（Graph Attention Networks）</h1><p>图神经网络（GNNs）旨在为下游任务学习在低维空间中训练良好的表示，同时保留拓扑结构。近年来，注意力机制在自然语言处理和计算机视觉领域表现出色，被引入到GNNs中，以自适应地选择判别特征并自动过滤噪声信息。本博客将重点介绍GAT </p>
<span id="more"></span>

<h2 id="层内-GAT-Intra-Layer-GAT"><a href="#层内-GAT-Intra-Layer-GAT" class="headerlink" title="层内 GAT (Intra-Layer GAT)"></a>层内 GAT (Intra-Layer GAT)</h2><p>层内GATs是指在单个神经网络层内应用注意力机制的图神经网络，主要特点是：</p>
<ul>
<li>注意力机制作用于局部邻居节点</li>
<li>在特征聚合步骤中为不同节点分配不同权重</li>
<li>能够自适应地关注图中最相关的部分</li>
</ul>
<p>考虑到不同的局部邻域和不同的功能，层内GATs可以进一步分为六个子类，包括邻居注意力、高阶注意力、关系感知注意力、层次注意力、注意力采样&#x2F;池化和超注意力。</p>
<h3 id="邻居注意力-Neighbor-Attention"><a href="#邻居注意力-Neighbor-Attention" class="headerlink" title="邻居注意力 (Neighbor Attention)"></a>邻居注意力 (Neighbor Attention)</h3><h4 id="邻居注意力核心思想"><a href="#邻居注意力核心思想" class="headerlink" title="邻居注意力核心思想"></a>邻居注意力核心思想</h4><p>邻居注意力机制的核心是通过学习的方式动态确定图中每个节点对其邻居的重要性权重，而非传统GNN中采用的固定权重策略。其关键创新点包括：</p>
<ol>
<li><strong>动态权重分配</strong>：根据节点特征相似性动态计算注意力权重</li>
<li><strong>局部聚焦</strong>：仅考虑一阶邻居节点的注意力计算</li>
<li><strong>端到端训练</strong>：注意力权重与网络参数共同优化</li>
</ol>
<h4 id="基础GAT模型"><a href="#基础GAT模型" class="headerlink" title="基础GAT模型"></a>基础GAT模型</h4><p>GAT 可以简单概括为一种将 Attention 机制引入 GCN（图卷积网络）的方法，而 GCN 是一种能够在深度学习中进行图结构分类等任务的方法。Attention 机制是指一种根据输入数据动态决定关注焦点的机制，通过将这种 Attention 机制引入 GCN（图卷积网络），可以提高分类、识别和预测的精度。因此，如果对 GCN 的理解不够深入，可能会难以完全理解 GAT。</p>
<p>GCN 与 GAT 的主要区别在于节点卷积时的系数（这就是所谓的 Attention 系数）存在显著差异。</p>
<p>在常规 GCN 中，当计算某节点的下一层特征（潜在变量）时，会通过将相邻节点的特征与线性权重相乘后的和，再通过激活函数（如 ReLU 等）传递至下一层。在计算相邻节点的特征量的线性组合时，传统方法将所有相邻节点平等对待，而 GAT 则引入了重要性（Attention 系数）的概念，不再将相邻节点视为同等。</p>
<p>其概念可以形象地理解为如下：</p>
<img src="/img/GAT/AttentionInGraphVisualize.png" alt="AttentionInGraphVisualize" width="60%" height="auto">

<blockquote>
<p>上图展示了节点 1 与其相邻的节点 2、3、4 之间，通过粗线表示的边，节点 3 最为重要，其次为节点 4，然后是节点 2，按重要性顺序排列的示意图。</p>
</blockquote>
<p>GAT 正是通过这种方式，将这种概念应用于相邻节点。</p>
<p>GAT通过引入注意力机制来加权邻居节点的特征。对于节点i，其更新公式为：</p>
<p>$$h_i^{(l+1)} &#x3D; \sigma(\sum_{j \in \mathcal{N}_i} \alpha_{ij}W^{(l)}h_j^{(l)})$$</p>
<p>其中注意力系数$\alpha_{ij}$的计算：</p>
<p>$$\alpha_{ij} &#x3D; \frac{exp(LeakyReLU(a^T[Wh_i || Wh_j]))}{\sum_{k \in \mathcal{N}_i} exp(LeakyReLU(a^T[Wh_i || Wh_k]))}$$</p>
<h5 id="注意力计算过程"><a href="#注意力计算过程" class="headerlink" title="注意力计算过程"></a>注意力计算过程</h5><p>基础GAT模型(Velickovic et al. 2018)的注意力计算包含三个关键步骤：</p>
<ol>
<li><strong>线性变换</strong>：对节点特征进行共享线性变换 $h_i’ &#x3D; W h_i$</li>
<li><strong>注意力分数计算</strong>：使用单层前馈网络计算注意力分数 $e_{ij} &#x3D; a^T[Wh_i||Wh_j]$</li>
<li><strong>归一化处理</strong>：通过softmax进行归一化 $\alpha_{ij} &#x3D; \text{softmax}(e_{ij})$</li>
</ol>
<h5 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h5><p>GAT引入多头注意力以稳定学习过程：</p>
<ul>
<li><strong>连接式多头</strong>：各头输出直接拼接 $h_i’ &#x3D; |_{k&#x3D;1}^K \sigma(\sum_{j\in N_i} \alpha_{ij}^k W^k h_j)$</li>
<li><strong>平均式多头</strong>：各头输出取平均 $h_i’ &#x3D; \sigma(\frac{1}{K}\sum_{k&#x3D;1}^K \sum_{j\in N_i} \alpha_{ij}^k W^k h_j)$</li>
</ul>
<h4 id="关键改进模型"><a href="#关键改进模型" class="headerlink" title="关键改进模型"></a>关键改进模型</h4><h5 id="GATv2"><a href="#GATv2" class="headerlink" title="GATv2"></a>GATv2</h5><p>GATv2(Brody et al. 2021)解决了原始GAT的静态注意力问题：</p>
<ul>
<li>调整计算顺序：先非线性变换再线性投影<br>$e_{ij} &#x3D; a^T \text{LeakyReLU}(W[h_i||h_j])$</li>
<li>证明原始GAT的注意力排名与查询无关</li>
<li>在多个基准数据集上表现优于GAT</li>
</ul>
<h5 id="PPRGAT"><a href="#PPRGAT" class="headerlink" title="PPRGAT"></a>PPRGAT</h5><p>PPRGAT(Choi 2022)整合个性化PageRank信息：</p>
<ul>
<li>在注意力计算中加入PPR分数<br>$e_{ij} &#x3D; \text{LeakyReLU}(a^T[Wh_i||Wh_j||\pi_{ij}])$</li>
<li>保留原始GAT结构的同时利用全局图信息</li>
<li>在节点分类任务中表现优异</li>
</ul>
<h5 id="SuperGAT"><a href="#SuperGAT" class="headerlink" title="SuperGAT"></a>SuperGAT</h5><p>SuperGAT(Kim and Oh 2021)提出两种自监督变体：</p>
<ol>
<li>边预测任务：利用注意力分数预测边存在性</li>
<li>标签一致性任务：利用注意力分数预测节点标签一致性</li>
<li>在噪声图上表现更鲁棒</li>
</ol>
<h4 id="💻-实现细节"><a href="#💻-实现细节" class="headerlink" title="💻 实现细节"></a>💻 实现细节</h4><h5 id="PyTorch实现的GAT层"><a href="#PyTorch实现的GAT层" class="headerlink" title="PyTorch实现的GAT层"></a>PyTorch实现的GAT层</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GATLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> \_\_init\_\_(<span class="variable language_">self</span>, <span class="keyword">in</span>\_features, out\_features, dropout, alpha, concat=<span class="literal">True</span>):</span><br><span class="line">        <span class="built_in">super</span>(GATLayer, <span class="variable language_">self</span>).\_\_init\_\_()</span><br><span class="line">        <span class="variable language_">self</span>.<span class="keyword">in</span>\_features = <span class="keyword">in</span>\_features</span><br><span class="line">        <span class="variable language_">self</span>.out\_features = out\_features</span><br><span class="line">        <span class="variable language_">self</span>.dropout = dropout</span><br><span class="line">        <span class="variable language_">self</span>.alpha = alpha</span><br><span class="line">        <span class="variable language_">self</span>.concat = concat</span><br><span class="line"></span><br><span class="line">        <span class="comment">### 变换矩阵</span></span><br><span class="line">        <span class="variable language_">self</span>.W = nn.Parameter(torch.zeros(size=(<span class="keyword">in</span>\_features, out\_features)))</span><br><span class="line">        nn.init.xavier\_uniform\_(<span class="variable language_">self</span>.W.data, gain=<span class="number">1.414</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### 注意力向量</span></span><br><span class="line">        <span class="variable language_">self</span>.a = nn.Parameter(torch.zeros(size=(<span class="number">2</span>*out\_features, <span class="number">1</span>)))</span><br><span class="line">        nn.init.xavier\_uniform\_(<span class="variable language_">self</span>.a.data, gain=<span class="number">1.414</span>)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.leakyrelu = nn.LeakyReLU(<span class="variable language_">self</span>.alpha)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, adj</span>):</span><br><span class="line">        <span class="comment">### x: 节点特征矩阵 [N, in\_features]</span></span><br><span class="line">        <span class="comment">### adj: 邻接矩阵 [N, N]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">### 线性变换</span></span><br><span class="line">        h = torch.mm(x, <span class="variable language_">self</span>.W)  <span class="comment">### [N, out\_features]</span></span><br><span class="line">        N = h.size()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment">### 计算注意力分数</span></span><br><span class="line">        a\_<span class="built_in">input</span> = torch.cat([h.repeat(<span class="number">1</span>, N).view(N * N, -<span class="number">1</span>), h.repeat(N, <span class="number">1</span>)], dim=<span class="number">1</span>)</span><br><span class="line">        a\_<span class="built_in">input</span> = a\_<span class="built_in">input</span>.view(N, N, <span class="number">2</span> * <span class="variable language_">self</span>.out\_features)</span><br><span class="line">        e = <span class="variable language_">self</span>.leakyrelu(torch.matmul(a\_<span class="built_in">input</span>, <span class="variable language_">self</span>.a).squeeze(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">### 掩码机制</span></span><br><span class="line">        zero\_vec = -<span class="number">9e15</span> * torch.ones\_like(e)</span><br><span class="line">        attention = torch.where(adj &gt; <span class="number">0</span>, e, zero\_vec)</span><br><span class="line">        attention = F.softmax(attention, dim=<span class="number">1</span>)</span><br><span class="line">        attention = F.dropout(attention, <span class="variable language_">self</span>.dropout, training=<span class="variable language_">self</span>.training)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### 聚合特征</span></span><br><span class="line">        h\_prime = torch.matmul(attention, h)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.concat:</span><br><span class="line">            <span class="keyword">return</span> F.elu(h\_prime)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> h\_prime</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GAT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> \_\_init\_\_(<span class="variable language_">self</span>, nfeat, nhid, nclass, dropout, alpha, nheads):</span><br><span class="line">        <span class="built_in">super</span>(GAT, <span class="variable language_">self</span>).\_\_init\_\_()</span><br><span class="line">        <span class="variable language_">self</span>.dropout = dropout</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### 多头注意力层</span></span><br><span class="line">        <span class="variable language_">self</span>.attentions = nn.ModuleList([</span><br><span class="line">            GATLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=<span class="literal">True</span>) </span><br><span class="line">            <span class="keyword">for</span> \_ <span class="keyword">in</span> <span class="built_in">range</span>(nheads)</span><br><span class="line">        ])</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### 输出层</span></span><br><span class="line">        <span class="variable language_">self</span>.out\_att = GATLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, adj</span>):</span><br><span class="line">        x = F.dropout(x, <span class="variable language_">self</span>.dropout, training=<span class="variable language_">self</span>.training)</span><br><span class="line">        <span class="comment">### 多头注意力</span></span><br><span class="line">        x = torch.cat([att(x, adj) <span class="keyword">for</span> att <span class="keyword">in</span> <span class="variable language_">self</span>.attentions], dim=<span class="number">1</span>)</span><br><span class="line">        x = F.dropout(x, <span class="variable language_">self</span>.dropout, training=<span class="variable language_">self</span>.training)</span><br><span class="line">        x = <span class="variable language_">self</span>.out\_att(x, adj)</span><br><span class="line">        <span class="keyword">return</span> F.log\_softmax(x, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h5 id="实际应用示例"><a href="#实际应用示例" class="headerlink" title="实际应用示例"></a>实际应用示例</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">### 模型初始化</span></span><br><span class="line">model = GAT(nfeat=<span class="built_in">input</span>\_dim,</span><br><span class="line">           nhid=<span class="number">8</span>,</span><br><span class="line">           nclass=num\_classes,</span><br><span class="line">           dropout=<span class="number">0.6</span>,</span><br><span class="line">           alpha=<span class="number">0.2</span>,</span><br><span class="line">           nheads=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### 优化器</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.005</span>, weight\_decay=<span class="number">5e-4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### 训练循环</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line">    model.train()</span><br><span class="line">    optimizer.zero\_grad()</span><br><span class="line">    output = model(features, adj)</span><br><span class="line">    loss = F.nll\_loss(output[idx\_train], labels[idx\_train])</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="keyword">return</span> loss.item()</span><br></pre></td></tr></table></figure>

<h3 id="高阶注意力-High-Order-Attention"><a href="#高阶注意力-High-Order-Attention" class="headerlink" title="高阶注意力 (High-Order Attention)"></a>高阶注意力 (High-Order Attention)</h3><h4 id="高阶注意力核心思想"><a href="#高阶注意力核心思想" class="headerlink" title="高阶注意力核心思想"></a>高阶注意力核心思想</h4><p>高阶注意力机制突破了传统GAT仅关注直接邻居的限制，通过以下方式扩展了注意力范围：</p>
<ol>
<li>多跳邻居聚合：考虑k-hop范围内的节点（k&gt;1）</li>
<li>路径感知：关注节点间的路径信息而不仅是直接连接</li>
<li>全局感受野：部分模型实现近似全局注意力</li>
</ol>
<img src="\img\GAT\HighOrderAttention.png" alt="HighOrderAttention" width="60%" height="auto">

<h4 id="关键技术路线"><a href="#关键技术路线" class="headerlink" title="关键技术路线"></a>关键技术路线</h4><h5 id="基于路径的注意力"><a href="#基于路径的注意力" class="headerlink" title="基于路径的注意力"></a>基于路径的注意力</h5><p>SPAGAN(Yang et al. 2019)开创性地提出路径注意力机制：</p>
<ul>
<li>计算节点间最短路径作为注意力传播路径</li>
<li>路径特征通过LSTM编码 $p_{ij} &#x3D; \text{LSTM}([h_i, h_{i1}, …, h_{j}])$</li>
<li>注意力分数计算 $\alpha_{ij} &#x3D; \text{softmax}(W_p p_{ij})$</li>
</ul>
<h5 id="基于随机游走的注意力"><a href="#基于随机游走的注意力" class="headerlink" title="基于随机游走的注意力"></a>基于随机游走的注意力</h5><p>PaGNN(Yang et al. 2021d)采用个性化PageRank：</p>
<ul>
<li>定义转移矩阵P&#x3D;AD⁻¹</li>
<li>计算PPR分数矩阵 $\Pi &#x3D; \alpha(I - (1-\alpha)P)^{-1}$</li>
<li>将PPR分数融入注意力计算</li>
</ul>
<h5 id="谱域注意力"><a href="#谱域注意力" class="headerlink" title="谱域注意力"></a>谱域注意力</h5><p>MAGNA(Wang et al. 2021)结合谱理论：</p>
<ul>
<li>使用低通滤波器平滑高频噪声</li>
<li>注意力传播公式 $H^{(l+1)} &#x3D; \sigma(\sum_{k&#x3D;0}^K \beta_k T_k(\tilde{L})H^{(l)}W_k)$ ，其中$T_k$为切比雪夫多项式</li>
</ul>
<h3 id="关系感知注意力-Relation-Aware-Attention"><a href="#关系感知注意力-Relation-Aware-Attention" class="headerlink" title="关系感知注意力 (Relation-Aware Attention)"></a>关系感知注意力 (Relation-Aware Attention)</h3><p>关系感知注意力机制主要解决图中不同类型关系的差异化处理问题，其核心思想包括：</p>
<ol>
<li><strong>关系类型区分</strong>：识别并利用图中不同类型的关系（边）</li>
<li><strong>关系特定参数</strong>：为每种关系类型学习独立的注意力参数</li>
<li><strong>关系特征融合</strong>：结合关系特征与节点特征进行注意力计算</li>
</ol>
<img src="\img\GAT\RelationAwareAttention.png" alt="RelationAwareAttention" width="60%" height="auto">

<h4 id="关键技术方法"><a href="#关键技术方法" class="headerlink" title="关键技术方法"></a>关键技术方法</h4><h5 id="带符号图注意力"><a href="#带符号图注意力" class="headerlink" title="带符号图注意力"></a>带符号图注意力</h5><p>SiGAT(Huang et al. 2019b)针对带符号图提出：</p>
<ul>
<li>平衡理论建模：朋友的朋友是朋友</li>
<li>状态理论建模：敌人的朋友是敌人</li>
<li>注意力分数计算 $α_{ij} &#x3D; \text{softmax}(\text{MLP}([h_i‖h_j‖r_{ij}]))$, 其中$r_{ij}$表示边类型（正&#x2F;负）</li>
</ul>
<h5 id="异构图注意力"><a href="#异构图注意力" class="headerlink" title="异构图注意力"></a>异构图注意力</h5><p>RGAT(Busbridge et al. 2019)处理异构图：</p>
<ul>
<li>关系特定变换矩阵 $h_i^r &#x3D; W_r h_i$</li>
<li>关系感知注意力 $e_{ij}^r &#x3D; a_r^T[h_i^r‖h_j^r]$</li>
</ul>
<h3 id="层次注意力-Hierarchical-Attention"><a href="#层次注意力-Hierarchical-Attention" class="headerlink" title="层次注意力 (Hierarchical Attention)"></a>层次注意力 (Hierarchical Attention)</h3><p>层次注意力机制通过构建多级注意力结构来处理图数据中的复杂关系，其核心思想包括：</p>
<ol>
<li><strong>多层次信息整合</strong>：同时考虑节点级、路径级和图级信息</li>
<li><strong>分层注意力计算</strong>：在不同层次应用不同的注意力机制</li>
<li><strong>信息流动控制</strong>：设计信息从低层向高层的传递方式</li>
</ol>
<img src="\img\GAT\HierachicalAttention.png" alt="HierachicalAttention" width="60%" height="auto">

<h4 id="关键技术方法-1"><a href="#关键技术方法-1" class="headerlink" title="关键技术方法"></a>关键技术方法</h4><h5 id="节点-语义双层次注意力"><a href="#节点-语义双层次注意力" class="headerlink" title="节点-语义双层次注意力"></a>节点-语义双层次注意力</h5><p>HAN(Wang et al. 2019d)提出：</p>
<ul>
<li><strong>节点级注意力</strong>：学习同一元路径下节点的重要性 $h_i’ &#x3D; \sum_{j\in N_i} \alpha_{ij}h_j$</li>
<li><strong>语义级注意力</strong>：学习不同元路径的重要性 $Z &#x3D; \sum_{m&#x3D;1}^M \beta_m \cdot Z_m$</li>
</ul>
<h5 id="多粒度层次注意力"><a href="#多粒度层次注意力" class="headerlink" title="多粒度层次注意力"></a>多粒度层次注意力</h5><p>GraphHAM(Lin et al. 2022)设计：</p>
<ul>
<li>局部粒度注意力：捕捉节点邻域特征</li>
<li>全局粒度注意力：捕捉图结构特征</li>
<li>跨粒度注意力：协调不同粒度信息</li>
</ul>
<h5 id="动态层次注意力"><a href="#动态层次注意力" class="headerlink" title="动态层次注意力"></a>动态层次注意力</h5><p>RGHAT(Zhang et al. 2020c)实现：</p>
<ul>
<li><strong>底层实体注意力</strong>：处理节点特征 $e_{ij} &#x3D; a(h_i,h_j)$</li>
<li><strong>高层关系注意力</strong>：处理边类型特征 $r_k &#x3D; \sum_{i,j} \beta_{ij}^k e_{ij}$</li>
</ul>
<h2 id="层间注意力-Inter-Layer-GAT"><a href="#层间注意力-Inter-Layer-GAT" class="headerlink" title="层间注意力 (Inter-Layer GAT)"></a>层间注意力 (Inter-Layer GAT)</h2><p>在神经网络层之间，跨层GAT通过特征融合方法将不同特征空间的表示结合起来。根据不同的融合方法，我们将这些基于注意力的GNN分为五个子类别，包括多级注意力、多通道注意力、多视图注意力和时空注意力。</p>
<h3 id="多级注意力-Multi-Level-Attention"><a href="#多级注意力-Multi-Level-Attention" class="headerlink" title="多级注意力 (Multi-Level Attention)"></a>多级注意力 (Multi-Level Attention)</h3><p>多级注意力机制通过构建多级注意力结构来处理图数据中的复杂关系，其核心思想包括：</p>
<ol>
<li><strong>层次化信息处理</strong>：将图数据分解为不同层次的抽象表示</li>
<li><strong>跨层次信息交互</strong>：在不同层次间建立注意力连接</li>
<li><strong>自适应权重分配</strong>：自动学习各层次对最终任务的重要性</li>
</ol>
<h4 id="关键技术方法-2"><a href="#关键技术方法-2" class="headerlink" title="关键技术方法"></a>关键技术方法</h4><h5 id="节点-路径双层次注意力"><a href="#节点-路径双层次注意力" class="headerlink" title="节点-路径双层次注意力"></a>节点-路径双层次注意力</h5><p>DAGNN(Liu et al. 2020)提出：</p>
<ul>
<li><strong>节点级注意力</strong>：学习局部邻域内节点的重要性 $h_i^{(l)} &#x3D; \sum_{j\in N_i} \alpha_{ij}^{(l)} h_j^{(l-1)}$</li>
<li><strong>路径级注意力</strong>：学习不同传播深度的重要性 $z_i &#x3D; \sum_{l&#x3D;0}^L \beta_l h_i^{(l)}$</li>
</ul>
<h5 id="自适应深度注意力"><a href="#自适应深度注意力" class="headerlink" title="自适应深度注意力"></a>自适应深度注意力</h5><p>TDGNN(Wang and Derr 2021)设计：</p>
<ul>
<li>动态调整各层的注意力范围</li>
<li>基于树分解的层次化注意力</li>
<li>跨层信息融合机制</li>
</ul>
<h5 id="跳跃知识架构"><a href="#跳跃知识架构" class="headerlink" title="跳跃知识架构"></a>跳跃知识架构</h5><p>GAMLP(Zhang et al. 2022c)实现：</p>
<ul>
<li>底层局部注意力</li>
<li>中层区域注意力</li>
<li>高层全局注意力</li>
<li>跳跃连接整合各层特征</li>
</ul>
<h3 id="多通道注意力-Multi-Channel-Attention"><a href="#多通道注意力-Multi-Channel-Attention" class="headerlink" title="多通道注意力 (Multi-Channel Attention)"></a>多通道注意力 (Multi-Channel Attention)</h3><p>多通道注意力机制通过构建并行注意力通道来处理图数据中的多样化特征，其核心思想包括：</p>
<ol>
<li><strong>通道多样化</strong>：将输入特征分解为多个特征通道</li>
<li><strong>通道特异性</strong>：为每个通道设计独立的注意力机制</li>
<li><strong>通道融合</strong>：自适应地整合不同通道的信息</li>
</ol>
<h4 id="关键技术方法-3"><a href="#关键技术方法-3" class="headerlink" title="关键技术方法"></a>关键技术方法</h4><h5 id="频率自适应注意力"><a href="#频率自适应注意力" class="headerlink" title="频率自适应注意力"></a>频率自适应注意力</h5><p>FAGCN(Bo et al. 2021)提出：</p>
<ul>
<li>低频通道：捕捉节点相似性 $h_{low} &#x3D; \sum_{j\in N_i} \frac{1}{\sqrt{d_i d_j}} h_j$</li>
<li>高频通道：捕捉节点差异性 $h_{high} &#x3D; \sum_{j\in N_i} -\frac{1}{\sqrt{d_i d_j}} h_j$</li>
<li>自适应融合： $h_i &#x3D; \alpha_{low} h_{low} + \alpha_{high} h_{high}$</li>
</ul>
<h5 id="自适应通道混合"><a href="#自适应通道混合" class="headerlink" title="自适应通道混合"></a>自适应通道混合</h5><p>ACM(Luan et al. 2021)设计：</p>
<ul>
<li>多通道特征提取</li>
<li>通道间注意力交互</li>
<li>动态通道权重分配</li>
</ul>
<h5 id="不确定性感知通道"><a href="#不确定性感知通道" class="headerlink" title="不确定性感知通道"></a>不确定性感知通道</h5><p>UAG(Feng et al. 2021)实现：</p>
<ul>
<li>通道不确定性估计</li>
<li>基于不确定性的通道注意力</li>
<li>鲁棒性通道融合</li>
</ul>
<h3 id="多视角注意力-Multi-View-Attention"><a href="#多视角注意力-Multi-View-Attention" class="headerlink" title="多视角注意力 (Multi-View Attention)"></a>多视角注意力 (Multi-View Attention)</h3><p>多视角注意力机制通过构建并行注意力视角来处理图数据中的多样化信息，其核心思想包括：</p>
<ol>
<li><strong>视角多样化</strong>：从不同角度（如拓扑结构、节点特征、时间序列等）构建多个视角</li>
<li><strong>视角特异性</strong>：为每个视角设计独立的注意力机制</li>
<li><strong>视角融合</strong>：自适应地整合不同视角的信息</li>
</ol>
<h4 id="关键技术方法-4"><a href="#关键技术方法-4" class="headerlink" title="关键技术方法"></a>关键技术方法</h4><h5 id="结构-特征双视角注意力"><a href="#结构-特征双视角注意力" class="headerlink" title="结构-特征双视角注意力"></a>结构-特征双视角注意力</h5><p>AM-GCN(Wang et al. 2020b)提出：</p>
<ul>
<li>拓扑视角：基于图结构的邻接矩阵 $A_{topo} &#x3D; A$</li>
<li>特征视角：基于节点特征的相似度矩阵 $A_{feat} &#x3D; \text{sim}(X,X^T)$</li>
<li>注意力融合： $A_{final} &#x3D; \sum_{v\in{topo,feat}} w_v A_v$</li>
</ul>
<h3 id="时空注意力-Spatio-Temporal-Attention"><a href="#时空注意力-Spatio-Temporal-Attention" class="headerlink" title="时空注意力 (Spatio-Temporal Attention)"></a>时空注意力 (Spatio-Temporal Attention)</h3><p>时空注意力机制通过构建并行注意力模块来处理图数据中的空间和时间依赖性，其核心思想包括：</p>
<ol>
<li>空间依赖性：捕捉节点间的拓扑关系</li>
<li>时间依赖性：建模动态图中的时序演化模式</li>
<li>时空交互：联合建模时空维度的相互影响</li>
</ol>
<h4 id="关键技术方法-5"><a href="#关键技术方法-5" class="headerlink" title="关键技术方法"></a>关键技术方法</h4><h5 id="空间-时间双注意力"><a href="#空间-时间双注意力" class="headerlink" title="空间-时间双注意力"></a>空间-时间双注意力</h5><p>DySAT(Sankar et al. 2018)提出：</p>
<ul>
<li>空间注意力：基于当前快照的图结构 $A_{spatial} &#x3D; \text{softmax}(Q_s K_s^T&#x2F;\sqrt{d})$</li>
<li>时间注意力：基于节点的时间序列 $A_{temporal} &#x3D; \text{softmax}(Q_t K_t^T&#x2F;\sqrt{d})$</li>
<li>联合建模： $Z &#x3D; (A_{spatial} \oplus A_{temporal})V$</li>
</ul>
<h3 id="📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒"><a href="#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒" class="headerlink" title="📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h3><p><a href="/paper/Lee 等 - 2018 - Attention Models in Graphs A Survey.pdf" target="_blank">📄 Lee 等 - 2018 - Attention Models in Graphs A Survey</a></p>
<a href="https://github.com/xmu-xiaoma666/External-Attention-pytorch" target="_blank">
  <span style="display: inline-block; vertical-align: middle;">
    <img src="/icon/github.svg" alt="github" style="height: 1.3em; vertical-align: middle; margin-top: 16px;">
  </span>
  External-Attention-pytorch
</a>

<p><a href="/paper/Sun 等 - 2023 - Attention-based graph neural networks a survey.pdf" target="_blank">📄 Sun 等 - 2023 - Attention-based graph neural networks a survey</a></p>
<a href="https://disassemble-channel.com/graph-attention-network-gat/" target="_blank">
  <span style="display: inline-block; vertical-align: middle;">
    <img src="/icon/google.svg" alt="github" style="height: 1.3em; vertical-align: middle; margin-top: 16px;">
  </span>
  【深層学習】Graph Attention Networks(GAT)を理解する
</a> 
]]></content>
      <categories>
        <category>model</category>
        <category>attention</category>
        <category>graph</category>
      </categories>
      <tags>
        <tag>CDR</tag>
        <tag>model</tag>
        <tag>Basic</tag>
        <tag>deep learning</tag>
        <tag>PyTorch</tag>
        <tag>graph theory</tag>
      </tags>
  </entry>
  <entry>
    <title>CDR Input Data Analysis</title>
    <url>/2025/07/09/CDR-data-analysis/</url>
    <content><![CDATA[<h1 id="CDR-数据源分析"><a href="#CDR-数据源分析" class="headerlink" title="CDR 数据源分析"></a>CDR 数据源分析</h1><p>本文主要是介绍一下 <strong>深度学习</strong> 在 <em>药物反应预测</em> 中运用到的数据源。<del>但由于本人比较捞</del> 本文主要从 <strong>深度学习</strong> 角度来看待这些数据源，对其在医学方面的意义<del>（主要是鼠鼠也不会捏）</del>不会有太多的描述</p>
<span id="more"></span>

<h2 id="CDR-Cancer-Drug-Response"><a href="#CDR-Cancer-Drug-Response" class="headerlink" title="CDR &#x3D; Cancer Drug Response"></a>CDR &#x3D; Cancer Drug Response</h2><p>我们的数据源有三种：</p>
<ul>
<li><em>Cancer Representations</em>（癌症特征的表示）</li>
<li><em>Representations of Drug Compounds</em>（药物特征的表示）</li>
<li><em>Representations of Treatment Response</em>（治疗响应的表示）</li>
</ul>
<p>接下来会按顺序进行说明</p>
<hr>
<h3 id="Cancer-Representations"><a href="#Cancer-Representations" class="headerlink" title="Cancer Representations"></a>Cancer Representations</h3><p>癌症的特征是多组学的 <del>这不是理所应当吗</del></p>
<h4 id="多组学类型"><a href="#多组学类型" class="headerlink" title="多组学类型"></a>多组学类型</h4><p>通常基于以下四类组学数据：</p>
<ul>
<li><p>基因组（Genomic）</p>
<ul>
<li>突变（Mutation）：体细胞突变（如单核苷酸变异 SNVs）可能驱动癌症进展，并影响药物靶点。</li>
<li>拷贝数变异（CNV）：基因拷贝数的增加或缺失可能影响药物敏感性（如 HER2 扩增与曲妥珠单抗疗效相关）。</li>
</ul>
</li>
<li><p>转录组（Transcriptomic）</p>
<ul>
<li>基因表达（Gene Expression）：通过微阵列或 RNA 测序（RNA-Seq）量化基因的 mRNA 水平。例如，高表达的耐药基因可能预示治疗失败。</li>
</ul>
</li>
<li><p>表观组（Epigenomic）</p>
<ul>
<li>DNA 甲基化（Methylation）：启动子区域的甲基化可能沉默抑癌基因，影响药物反应。</li>
</ul>
</li>
<li><p>蛋白质组（Proteomic）</p>
<ul>
<li>蛋白质表达（RPPA 等）：直接测量蛋白质丰度（如激酶活性），更接近功能表型。</li>
</ul>
</li>
</ul>
<p>对于同一种组学数据，他们被表示成一组 <strong>维数相同的向量</strong></p>
<h4 id="预处理与整合"><a href="#预处理与整合" class="headerlink" title="预处理与整合"></a>预处理与整合</h4><ol>
<li>数据预处理</li>
</ol>
<ul>
<li>包括标准化（normalization）、批次效应校正（batch effect correction）和质量控制（QC）。例如，RNA-Seq 数据需通过 RPKM 或 TPM 标准化。</li>
</ul>
<ol start="2">
<li>多组学整合方法 ：<ul>
<li>早期整合（Early Integration）：直接拼接不同组学特征为单一向量，但可能因维度灾难（curse of dimensionality）导致过拟合。</li>
<li>晚期整合（Late Integration）：通过独立子网络处理每组学数据（如 CNN 处理突变，GNN 处理表达数据），再融合特征。例如，MOLI 模型通过三重损失函数整合多组学数据，显著提升跨癌症模型的泛化能力。</li>
</ul>
</li>
</ol>
<h4 id="基因特征具有优势及新兴趋势"><a href="#基因特征具有优势及新兴趋势" class="headerlink" title="基因特征具有优势及新兴趋势"></a>基因特征具有优势及新兴趋势</h4><blockquote>
<p>2014 年 NCI-DREAM 挑战赛表明， 基因表达数据在预测乳腺癌细胞系药物敏感性时最具预测力（优于突变或 CNV）。因此，约 90%的 DRP 模型使用基因表达（单独或联合其他组学）<br><img src="/img/CDR-data-analysis/gene.png" alt="gene" width="50%"></p>
</blockquote>
<h5 id="新兴趋势"><a href="#新兴趋势" class="headerlink" title="新兴趋势"></a>新兴趋势</h5><ol>
<li><strong>结构生物学整合</strong>：如利用蛋白质-蛋白质相互作用（PPI）网络（STRING 数据库）或通路信息（GSEA）构建生物网络，增强模型可解释性。</li>
<li><strong>图神经网络（GNN）</strong>：将基因视为节点、相互作用为边，学习拓扑特征（如 GraOmicDRP 模型）。</li>
</ol>
<hr>
<h3 id="Representations-of-Drug-Compounds"><a href="#Representations-of-Drug-Compounds" class="headerlink" title="Representations of Drug Compounds"></a>Representations of Drug Compounds</h3><p>对药物的表示主要分为三种，一般只选取其中的一种 <del>虽然也有选用几种的 <strong>创新</strong> 方式</del>。值得一提的是，在选定药物的表示方式后，之后的特征工程的方式目前来看非常的统一。接下来一一说明每一种表示方式。</p>
<h4 id="SMILES（简化分子输入行条目系统）"><a href="#SMILES（简化分子输入行条目系统）" class="headerlink" title="SMILES（简化分子输入行条目系统）"></a>SMILES（简化分子输入行条目系统）</h4><ol>
<li><em>定义</em>：SMILES 是一种<strong>线性字符串</strong>表示法，通过符号编码分子结构（如<code>CCO</code>表示乙醇）。</li>
<li><em>优势</em>：<ul>
<li>易于存储和处理，广泛用于化学信息学工具（如 RDKit）。</li>
<li>可直接用于序列模型（如 RNN、Transformer）或通过预处理转换为其他表示（如图结构）。</li>
</ul>
</li>
</ol>
<h4 id="分子指纹（Fingerprints-FPs）和描述符（Descriptors）"><a href="#分子指纹（Fingerprints-FPs）和描述符（Descriptors）" class="headerlink" title="分子指纹（Fingerprints, FPs）和描述符（Descriptors）"></a>分子指纹（Fingerprints, FPs）和描述符（Descriptors）</h4><ol>
<li><p>分子指纹</p>
<ul>
<li><em>定义</em>：<strong>二进制向量</strong>，表示分子中是否存在特定子结构（如药效团或官能团）。</li>
<li><em>常用类型</em>：<ul>
<li><strong>Morgan 指纹（ECFP）</strong>：基于原子邻域的圆形拓扑指纹，长度通常为 512 或 1024 位。</li>
<li><strong>RDKit 指纹</strong>：开源工具生成的二进制指纹。</li>
</ul>
</li>
<li><em>优势</em>：固定长度，适合传统机器学习模型（如随机森林）。</li>
</ul>
</li>
<li><p>分子描述符</p>
<ul>
<li><em>定义</em>：<strong>数值向量</strong>，编码物理化学性质（如分子量、疏水性、极性表面积等）。</li>
<li><em>工具</em>：PaDEL、Mordred、Dragon 等软件可自动计算数百至数千个描述符。</li>
</ul>
</li>
</ol>
<h4 id="图结构表示（Graph-based-Representations）"><a href="#图结构表示（Graph-based-Representations）" class="headerlink" title="图结构表示（Graph-based Representations）"></a>图结构表示（Graph-based Representations）</h4><ol>
<li><em>定义</em> ：将分子表示为<strong>图</strong>，其中原子为<strong>节点</strong>，化学键为<strong>边</strong>，节点和边可附加属性（如原子类型、键类型）。</li>
<li><em>优势</em> ：<ul>
<li>更自然地表征分子拓扑结构，适合图神经网络（GNN）。</li>
<li>可捕捉局部和全局分子特征（如官能团相互作用）。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="Representations-of-Treatment-Response"><a href="#Representations-of-Treatment-Response" class="headerlink" title="Representations of Treatment Response"></a>Representations of Treatment Response</h3><p>从构造模型的角度出发，这是 DRP 的核心数据源</p>
<ul>
<li>它决定了模型最后完成的<strong>任务类型</strong>：训练连续值的<strong>回归任务</strong>和训练离散值的<strong>分类任务</strong></li>
<li>他的数据质量很大程度上决定了模型的结果的优劣，即对该数据源对模型的好坏影响很大</li>
</ul>
<p>此外，很少有从数据分析的角度出发分析这个数据源的文献，于是在这里给出简要的说明</p>
<h4 id="连续值表示（Continuous-Measures）"><a href="#连续值表示（Continuous-Measures）" class="headerlink" title="连续值表示（Continuous Measures）"></a>连续值表示（Continuous Measures）</h4><ol>
<li><p><strong>IC50</strong></p>
<ul>
<li>半数抑制浓度，即抑制 50%细胞活力所需的药物浓度。</li>
<li><em>优势</em>：直观反映药物效力，广泛用于回归模型（如预测 IC50 的数值）。</li>
<li><em>局限性</em>：仅反映单一浓度点的效果，可能忽略剂量-反应曲线的整体形状。</li>
</ul>
</li>
<li><p><strong>AUC&#x2F;AAC</strong></p>
<ul>
<li>剂量-反应曲线下面积（Area Under the Curve）或曲线上面积（Activity Area）。</li>
<li><em>优势</em>：全局度量，综合所有浓度点的效果，对噪声更鲁棒。</li>
<li><em>应用</em>：如 DeepCDR 等模型使用 AUC 作为回归目标，实证表明其泛化性优于 IC50。</li>
</ul>
</li>
</ol>
<h4 id="分类表示（Categorical-Measures）"><a href="#分类表示（Categorical-Measures）" class="headerlink" title="分类表示（Categorical Measures）"></a>分类表示（Categorical Measures）</h4><ol>
<li><p><strong>二分类（敏感&#x2F;耐药）</strong></p>
<ul>
<li>通过阈值（如瀑布算法、LOBICO）将连续反应（如 IC50）转化为离散标签。</li>
<li><em>优势</em>：更贴近临床决策需求（如选择敏感药物）。</li>
<li><em>示例</em>：Sharifi-Noghabi et al. (2021) 使用二分类训练深度神经网络，预测患者肿瘤的敏感性。</li>
</ul>
</li>
<li><p><strong>多分类</strong></p>
<ul>
<li>如低&#x2F;中&#x2F;高反应性，适用于更细粒度的临床分级。</li>
</ul>
</li>
</ol>
<h4 id="排序表示（Ranking）"><a href="#排序表示（Ranking）" class="headerlink" title="排序表示（Ranking）"></a>排序表示（Ranking）</h4><ol>
<li><p><em>目标</em></p>
<ul>
<li>为个性化治疗推荐药物排序（如 Top-k 最有效药物）。</li>
</ul>
</li>
<li><p><em>方法</em></p>
<ul>
<li>Prasse et al. (2022)：将 IC50 转化为相关性分数，设计可微排序损失函数。</li>
<li>PPORank：利用强化学习动态优化排序，适应新增数据。</li>
</ul>
</li>
<li><p><em>优势</em></p>
<ul>
<li>直接支持临床优先级排序，优于传统回归或分类。</li>
</ul>
</li>
</ol>
<h4 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h4><p>由于本人大概率会做个分类模型，所以会将主要分析的是<strong>分类表示</strong>的数据在<strong>图神经网络</strong>中比较重视的几个指标，这里分析 <em>CCLE</em> 和 <em>GDSC</em> 两个数据集在选用主流阈值选取方法之后的表示。</p>
<p>直接先看结果捏（这里画了两个小图）</p>
<ul>
<li>CCLE</li>
</ul>
<img src="/img/CDR-data-analysis/comprehensive_bipartite_analysis_ccle.png" alt="CCLE" style="max-width: 100%; height: auto;">

<ul>
<li>GDSC</li>
</ul>
<img src="/img/CDR-data-analysis/comprehensive_bipartite_analysis_gdsc.png" alt="GDSC" style="max-width: 100%; height: auto;">

<p>
  👉 <a href="/code/data_analysis/visualize_graph_analysis.py" target="_blank">查看用于生成上述图表的本地 Python 脚本：visualize_graph_analysis.py</a>
</p>

<h5 id="🔍-关键数据对比"><a href="#🔍-关键数据对比" class="headerlink" title="🔍 关键数据对比"></a>🔍 关键数据对比</h5><table>
    <tr>
        <td>特征</td>
        <td>CCLE</td>
        <td>GDSC</td>
        <td>倍数差异</td>
    </tr>
    <tr>
        <td colspan="4" style="text-align: center;"><b>数据规模</b></td>
    </tr>
    <tr>
        <td>总节点数</td>
        <td>341</td>
        <td>783</td>
        <td>2.3×</td>
    </tr>
    <tr>
        <td>第一类节点</td>
        <td>317</td>
        <td>561</td>
        <td>1.8×</td>
    </tr>
    <tr>
        <td>第二类节点</td>
        <td>24</td>
        <td>222</td>
        <td>9.3×</td>
    </tr>
    <tr>
        <td>总边数</td>
        <td>7,307</td>
        <td>100,572</td>
        <td>13.8×</td>
    </tr>
    <tr>
        <td colspan="4" style="text-align: center;"><b>图结构</b></td>
    </tr>
    <tr>
        <td>密度</td>
        <td>0.9604</td>
        <td>0.8075</td>
        <td>0.84×</td>
    </tr>
    <tr>
        <td>稀疏性</td>
        <td>0.0396</td>
        <td>0.1925</td>
        <td>4.9×</td>
    </tr>
    <tr>
        <td>平均度</td>
        <td>42.86</td>
        <td>256.89</td>
        <td>6.0×</td>
    </tr>
    <tr>
        <td>图直径</td>
        <td>3</td>
        <td>4</td>
        <td>1.3×</td>
    </tr>
    <tr>
        <td colspan="4" style="text-align: center;"><b>边分布</b></td>
    </tr>
    <tr>
        <td>正边数量</td>
        <td>1,375</td>
        <td>11,591</td>
        <td>8.4×</td>
    </tr>
    <tr>
        <td>负边数量</td>
        <td>5,932</td>
        <td>88,981</td>
        <td>15.0×</td>
    </tr>
    <tr>
        <td>正边比例</td>
        <td>18.8%</td>
        <td>11.5%</td>
        <td>0.61×</td>
    </tr>
    <tr>
        <td>正负边比例</td>
        <td>1:4.3</td>
        <td>1:7.7</td>
        <td>1.8× 不平衡</td>
    </tr>
</table>

<h5 id="📊-GNN-训练挑战分析"><a href="#📊-GNN-训练挑战分析" class="headerlink" title="📊 GNN 训练挑战分析"></a>📊 GNN 训练挑战分析</h5><h6 id="过平滑风险评估"><a href="#过平滑风险评估" class="headerlink" title="过平滑风险评估"></a>过平滑风险评估</h6><ul>
<li><strong>CCLE</strong>: ⚠️ 高风险 (平均度 42.86)</li>
<li><strong>GDSC</strong>: 🚨 极高风险 (平均度 256.89)</li>
</ul>
<h6 id="样本不平衡程度"><a href="#样本不平衡程度" class="headerlink" title="样本不平衡程度"></a>样本不平衡程度</h6><ul>
<li><strong>CCLE</strong>: 正负边比例 1:4.3 (中等不平衡)</li>
<li><strong>GDSC</strong>: 正负边比例 1:7.7 (严重不平衡)</li>
</ul>
<h6 id="邻居相似度分析"><a href="#邻居相似度分析" class="headerlink" title="邻居相似度分析"></a>邻居相似度分析</h6><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 邻居重叠度对比</span></span><br><span class="line">CCLE_similarity = &#123;</span><br><span class="line">    <span class="string">&quot;第一类节点&quot;</span>: <span class="number">0.9374</span>,  <span class="comment"># 高度相似</span></span><br><span class="line">    <span class="string">&quot;第二类节点&quot;</span>: <span class="number">0.9274</span>   <span class="comment"># 高度相似</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">GDSC_similarity = &#123;</span><br><span class="line">    <span class="string">&quot;第一类节点&quot;</span>: <span class="number">0.7659</span>,  <span class="comment"># 中等相似</span></span><br><span class="line">    <span class="string">&quot;第二类节点&quot;</span>: <span class="number">0.7143</span>   <span class="comment"># 中等相似</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>结论</strong>: CCLE 结构更均匀但多样性不足，GDSC 结构更复杂但多样性更好</p>
<h5 id="🎯-GNN-架构建议对比"><a href="#🎯-GNN-架构建议对比" class="headerlink" title="🎯 GNN 架构建议对比"></a>🎯 GNN 架构建议对比</h5><h6 id="推荐架构优先级"><a href="#推荐架构优先级" class="headerlink" title="推荐架构优先级"></a>推荐架构优先级</h6><ul>
<li><p>CCLE 推荐架构</p>
<ol>
<li><strong>Bipartite GNN</strong> + Signed GCN</li>
<li><strong>简单异构图 GNN</strong> (HetGNN)</li>
<li><strong>标准 GCN</strong> + 强正则化</li>
</ol>
</li>
<li><p>GDSC 推荐架构</p>
<ol>
<li><strong>采样型 GNN</strong> (GraphSAINT, FastGCN) + SGCN</li>
<li><strong>大规模异构图 GNN</strong> (HGT, RGCN)</li>
<li><strong>图 Transformer</strong> (处理复杂结构)</li>
</ol>
</li>
</ul>
<h6 id="具体参数建议"><a href="#具体参数建议" class="headerlink" title="具体参数建议"></a>具体参数建议</h6><table>
<thead>
<tr>
<th>参数</th>
<th>CCLE</th>
<th>GDSC</th>
<th>原因</th>
</tr>
</thead>
<tbody><tr>
<td><strong>网络深度</strong></td>
<td>2-3 层</td>
<td>严格 2 层</td>
<td>GDSC 过平滑风险更高</td>
</tr>
<tr>
<td><strong>隐藏维度</strong></td>
<td>64-128</td>
<td>128-256</td>
<td>GDSC 需要更大容量</td>
</tr>
<tr>
<td><strong>Dropout 率</strong></td>
<td>0.3-0.5</td>
<td>0.5-0.7</td>
<td>GDSC 需要更强正则化</td>
</tr>
<tr>
<td><strong>学习率</strong></td>
<td>0.001-0.01</td>
<td>0.0001-0.001</td>
<td>GDSC 需要更保守训练</td>
</tr>
<tr>
<td><strong>批次大小</strong></td>
<td>32-64 个子图</td>
<td>16-32 个子图</td>
<td>GDSC 内存限制</td>
</tr>
<tr>
<td><strong>采样策略</strong></td>
<td>可选</td>
<td>必须</td>
<td>GDSC 无法全图训练</td>
</tr>
</tbody></table>
<h1 id="📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒"><a href="#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒" class="headerlink" title="📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1><p><a href="/paper/Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends.pdf" target="_blank">📄 Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends</a></p>
]]></content>
      <categories>
        <category>CDR</category>
        <category>Data Analysis</category>
      </categories>
      <tags>
        <tag>CDR</tag>
        <tag>graph theory</tag>
        <tag>Data Analysis</tag>
        <tag>可能有点用</tag>
      </tags>
  </entry>
  <entry>
    <title>Trans?!and Former?!</title>
    <url>/2025/07/19/Graph-Transformer/</url>
    <content><![CDATA[<h1 id="Graph-Transformer"><a href="#Graph-Transformer" class="headerlink" title="Graph Transformer"></a>Graph Transformer</h1><p>Graph Transformer 是传统Transformer架构在图数据上的泛化，旨在处理任意结构的图数据（如社交网络、分子结构等）。其既保留了Transformer的强表示能力，又继承了GNN对图结构的归纳偏置，成为图表示学习领域的重要基线模型。</p>
<span id="more"></span>

<p>在过去，Transformer（Lin等人，2021年）在许多NLP、CV和GRL任务中取得了卓越的性能。Graph Transformer将Transformer架构推广到图表示学习中，捕捉长距离依赖关系（Ying等人，2021年）。与之前使用局部注意力的方法不同，Graph Transformer通过全局注意力直接学习高阶图属性。Graph Transformer在图深度学习领域发展迅速，特别是在中小型图的图分类任务中。进一步将Graph Transformer分为两个子类别，即标准Transformer（Ying等人，2021年）和GNNTransformer（Nguyen等人，2019年）。标准Transformer通常对输入图的所有节点使用自注意力机制，忽略节点之间的邻接关系，而GNNTransformer则使用GNN层来获取邻接信息。 </p>
<img src="/img/Attention/GraphTransformer.png" alt="GraphTransformer" width="60%" height="auto">

<p>现在介绍Graph Transformer层和具有边特征的Graph Transformer层。该层架构上图所示。第一个模型是为没有显式边属性的图设计的，而第二个模型则保持一个指定的边特征管道，以整合可用的边信息，并在每一层中保持它们的抽象表示。</p>
<h3 id="输入特征线性投影公式"><a href="#输入特征线性投影公式" class="headerlink" title="输入特征线性投影公式"></a>输入特征线性投影公式</h3><p>输入首先，准备将输入节点和边嵌入传递到Graph Transformer层。对于一个具有每个节点i的节点特征 $\alpha_i \in \mathbb{R}^{d_n\times 1}$ 和每个节点$i$和节点$j$之间的边特征$\beta_{ij} \in \mathbb{R}^{d_e\times 1}$的图G，通过线性投影将输入节点特征αi和边特征$\beta_{ij}$传递以嵌入到$d-$维隐藏特征$h^0_i$和$e^0_{ij}$。</p>
<p><strong>节点特征投影</strong>：$\hat{h}_{i}^{0} &#x3D; A^{0}\alpha_{i} + a^{0}\qquad$ <strong>边特征投影</strong>：$e_{ij}^{0} &#x3D; B^{0}\beta_{ij} + b^{0}$</p>
<p>其中，$A^{0} \in \mathbb{R}^{d \times d_{n}}$ 和 $B^{0} \in \mathbb{R}^{d \times d_{e}}$ 是投影矩阵，$a^{0}, b^{0} \in \mathbb{R}^{d}$ 是偏置，$\alpha_i$ 和 $\beta_{ij}$ 分别是原始节点和边特征</p>
<h3 id="2-位置编码融合公式"><a href="#2-位置编码融合公式" class="headerlink" title="2. 位置编码融合公式"></a>2. 位置编码融合公式</h3><p>现在通过线性投影嵌入预先计算的节点位置编码$k$，并将其添加到节点特征$\hat{h}^0_i$。</p>
<p><strong>位置编码投影</strong>$\lambda_{i}^{0} &#x3D; C^{0}\lambda_{i} + c^{0}\qquad$ <strong>节点特征更新</strong>$h_{i}^{0} &#x3D; \hat{h}_{i}^{0} + \lambda_{i}^{0}$</p>
<p>其中，$C^{0} \in \mathbb{R}^{d \times k}$ 是位置编码投影矩阵，$c^{0} \in \mathbb{R}^{d}$ 是偏置项，$\lambda_i$ 是预计算的Laplacian特征向量。请注意，拉普拉斯位置编码仅在输入层添加到节点特征，而不是在中间的Graph Transformer层。</p>
<h3 id="3-基础图Transformer层公式"><a href="#3-基础图Transformer层公式" class="headerlink" title="3. 基础图Transformer层公式"></a>3. 基础图Transformer层公式</h3><p>与最初在(Vaswani等人，2017)中提出的Transformer架构非常相似。现在开始定义一层的节点更新方程。</p>
<p><strong>多头注意力输出</strong>$\hat{h}_{i}^{\ell+1} &#x3D; O_{h}^{\ell} |_{k&#x3D;1}^{H} \left( \sum_{j \in \mathcal{N}_{i}} w_{ij}^{k，\ell} V^{k，\ell} h_{j}^{\ell} \right)$</p>
<p><strong>注意力权重计算</strong>$w_{ij}^{k，\ell} &#x3D; \text{softmax}_{j} \left( \frac{Q^{k，\ell} h_{i}^{\ell} \cdot K^{k，\ell} h_{j}^{\ell}}{\sqrt{d_{k}}} \right)$</p>
<p>其中，$|$ 表示拼接操作，$Q^{k，\ell}, K^{k，\ell}, V^{k，\ell} \in \mathbb{R}^{d_{k} \times d}$ 是各头的查询、键、值矩阵，$O_{h}^{\ell} \in \mathbb{R}^{d \times d}$ 是输出投影矩阵</p>
<h3 id="4-前馈网络与归一化"><a href="#4-前馈网络与归一化" class="headerlink" title="4. 前馈网络与归一化"></a>4. 前馈网络与归一化</h3><p>为了数值稳定性，softmax内部项的指数输出被夹在$−5$到$+5$之间。然后将注意力输出$\hat{h}^{\ell +1}_i$传递给一个前接和后接残差连接和规范化层的前馈网络(FFN)，如下所示:</p>
<p><strong>归一化步骤</strong>$\hat{\hat{h}}_{i}^{\ell+1} &#x3D; \text{Norm}(h_{i}^{\ell} + \hat{h}_{i}^{\ell+1})\quad$ <strong>前馈网络</strong>$\hat{\hat{\hat{h}}}_{i}^{\ell+1} &#x3D; W_{2}^{\ell} \text{ReLU}(W_{1}^{\ell} \hat{\hat{h}}_{i}^{\ell+1})\quad$ <strong>最终输出</strong>$h_{i}^{\ell+1} &#x3D; \text{Norm}(\hat{\hat{h}}_{i}^{\ell+1} + \hat{\hat{\hat{h}}}_{i}^{\ell+1})$</p>
<p>其中，$W_{1}^{\ell} \in \mathbb{R}^{2d \times d}$ 和 $W_{2}^{\ell} \in \mathbb{R}^{d \times 2d}$ 是前馈网络参数，Norm可以是BatchNorm或LayerNorm.为了清晰起见，省略了偏差项。</p>
<h3 id="5-带边特征的图Transformer层"><a href="#5-带边特征的图Transformer层" class="headerlink" title="5. 带边特征的图Transformer层"></a>5. 带边特征的图Transformer层</h3><p>带有边特征的Graph Transformer层带有边特征的Graph Transformer是为更好地利用多种图数据集中的丰富特征信息而设计的，这些信息以边属性的形式存在。由于的目标仍然是更好地利用边特征，这些特征是对应于节点对的成对得分，将这些可用的边特征与通过成对注意力计算的隐式边得分联系起来。</p>
<p>换句话说，当一个节点$i$在 <em>query</em> 和 <em>key</em> 特征投影相乘后关注节点$j$时，中间注意力得分$\hat{w}_{ij}$在softmax之前被计算出来。让将这个得分$\hat{w}_{ij}$视为关于边$&lt;i，j&gt;$的隐式信息。现在尝试注入边$&lt;i，j&gt;$的可用边信息，并改进已经计算出的隐式注意力得分$\hat{w}_{ij}$。</p>
<p>这是通过简单地将两个值$\hat{w}_{ij}$和$e_{ij}$相乘来完成的。这种信息注入在NLPTransformer中并没有被广泛探索或应用，因为在两个单词之间通常没有可用的特征信息。</p>
<p>然而，在分子图或社交媒体图等图数据集中，边交互上往往有一些可用的特征信息，因此设计一种架构来利用这些信息变得自然。对于边，还维护了一个指定的节点对称边特征表示管道，用于从一层到另一层传播边属性。现在继续定义一层的层更新方程。</p>
<p><strong>注意力分数计算</strong>$\hat{w}_{ij}^{k，\ell} &#x3D; \left( \frac{Q^{k，\ell} h_{i}^{\ell} \cdot K^{k，\ell} h_{j}^{\ell}}{\sqrt{d_{k}}} \right) \cdot E^{k，\ell} e_{ij}^{\ell}\quad$ <strong>边特征更新</strong>$\hat{e}_{ij}^{\ell+1} &#x3D; O_{e}^{\ell} \prod_{k&#x3D;1}^{H} (\hat{w}_{ij}^{k，\ell})$</p>
<p>其中，$E^{k，\ell} \in \mathbb{R}^{d_{k} \times d}$ 是边特征投影矩阵，$O_{e}^{\ell} \in \mathbb{R}^{d \times d}$ 是边特征输出投影矩阵</p>
<h3 id="6-边特征的前馈网络"><a href="#6-边特征的前馈网络" class="headerlink" title="6. 边特征的前馈网络"></a>6. 边特征的前馈网络</h3><p><strong>边特征归一化</strong>$\hat{\hat{e}}_{ij}^{\ell+1} &#x3D; \text{Norm}(e_{ij}^{\ell} + \hat{e}_{ij}^{\ell+1})\quad$ <strong>边特征变换</strong>$\hat{\hat{\hat{e}}}_{ij}^{\ell+1} &#x3D; W_{e，2}^{\ell} \text{ReLU}(W_{e，1}^{\ell} \hat{\hat{e}}_{ij}^{\ell+1})\quad$ <strong>最终边输出</strong>$e_{ij}^{\ell+1} &#x3D; \text{Norm}(\hat{\hat{e}}_{ij}^{\ell+1} + \hat{\hat{\hat{e}}}_{ij}^{\ell+1})\quad$</p>
<p>其中，$W_{e，1}^{\ell} \in \mathbb{R}^{2d \times d}$ 和 $W_{e，2}^{\ell} \in \mathbb{R}^{d \times 2d}$ 是边特征前馈网络参数</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>这项工作提出了一种简单而有效的方法，将Transformer网络推广到任意图上，并引入了相应的架构。</p>
<p>实验一致表明，存在：</p>
<ul>
<li>Laplacian特征向量作为节点位置编码</li>
<li>batch normalization，代替层归一化，<br>则在Transformer前馈层周围增强了Transformer在所有实验中的普遍性。</li>
</ul>
<p>鉴于这个架构的简单性和通用性以及与标准GNN相比的竞争性能，提出的模型可以作为进一步改进跨图应用中使用节点注意力的基线。</p>
<h1 id="📚𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒"><a href="#📚𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒" class="headerlink" title="📚𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒"></a>📚𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1>]]></content>
      <categories>
        <category>model</category>
        <category>attention</category>
        <category>graph</category>
      </categories>
      <tags>
        <tag>CDR</tag>
        <tag>model</tag>
        <tag>Basic</tag>
        <tag>PyTorch</tag>
        <tag>deeplearning</tag>
        <tag>graphtheory</tag>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title>Gugugu Neralashun</title>
    <url>/2025/07/16/General-Attention/</url>
    <content><![CDATA[<h1 id="具有一般性的-Attention"><a href="#具有一般性的-Attention" class="headerlink" title="具有一般性的 Attention"></a>具有一般性的 Attention</h1><div style="display: flex; align-items: center;">
  <img src="\img\Attention\Gugugu Neralashun.png" style="width: 200px; margin-right: 20px;">
  <p>详细讨论了可以应用于任何类型注意力模型的基础机制，这些机制不依赖于特定的特征模型或查询模型。这一部分构成了注意力模型的核心计算框架，主要包括三个关键子方面：注意力评分函数(Attention Scoring)、注意力对齐(Attention Alignment)和注意力维度(Attention Dimensionality)。</p>
</div>



<p>在阅读这篇博客前请先阅读 <a href="/2025/07/10/Attention/" title="Attention Overview">Attention Overview</a></p>
<span id="more"></span>

<h2 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h2><img src="/img/Attention/GeneralAttentionModule.png" alt="GeneralAttentionModule" width="50%" height="auto">

<p>如果还记得这张图的话，那便是极好的捏 <del>不记得了就回去看捏</del> 本文主要就是在说明每一个模块常见的具体实现有什么</p>
<h2 id="注意力评分函数-Attention-Scoring"><a href="#注意力评分函数-Attention-Scoring" class="headerlink" title="注意力评分函数(Attention Scoring)"></a>注意力评分函数(Attention Scoring)</h2><p>注意力评分函数是计算查询向量$\mathbf{q}$与键向量$\mathbf{k}_l$之间相关性得分的核心组件：</p>
<ol>
<li><p><strong>加性评分(Additive&#x2F;Concatenate)</strong>：</p>
<p>$$<br>\text{score}(\mathbf{q},\mathbf{k}_l) &#x3D; \mathbf{w}^\top \text{act}(\mathbf{W}_1\mathbf{q} + \mathbf{W}_2\mathbf{k}_l + \mathbf{b})<br>$$</p>
<p>其中$\mathbf{w} \in \mathbb{R}^{d_w}$, $\mathbf{W}_1 \in \mathbb{R}^{d_w \times d_q}$, $\mathbf{W}_2 \in \mathbb{R}^{d_w \times d_k}$和$\mathbf{b} \in \mathbb{R}^{d_w}$是可训练参数。</p>
</li>
<li><p><strong>乘性评分(Multiplicative&#x2F;Dot-Product)</strong>：</p>
<p>$$<br>\text{score}(\mathbf{q},\mathbf{k}_l) &#x3D; \mathbf{q}^\top \mathbf{k}_l<br>$$</p>
</li>
<li><p><strong>缩放乘性评分(Scaled Multiplicative)</strong>：</p>
<p>$$<br>\text{score}(\mathbf{q},\mathbf{k}_l) &#x3D; \frac{\mathbf{q}^\top \mathbf{k}_l}{\sqrt{d_k}}<br>$$</p>
</li>
<li><p><strong>通用评分(General)</strong>：</p>
<p>$$<br>\text{score}(\mathbf{q},\mathbf{k}_l) &#x3D; \mathbf{k}_l^\top \mathbf{W} \mathbf{q}<br>$$</p>
<p>其中$\mathbf{W} \in \mathbb{R}^{d_k \times d_q}$是权重矩阵。</p>
</li>
<li><p><strong>带偏置的通用评分(Biased General)</strong>：</p>
<p>$$<br>\text{score}(\mathbf{q},\mathbf{k}_l) &#x3D; \mathbf{k}_l^\top (\mathbf{W}\mathbf{q} + \mathbf{b})<br>$$</p>
</li>
<li><p><strong>激活通用评分(Activated General)</strong>：<br>$$<br>\text{score}(\mathbf{q},\mathbf{k}_l) &#x3D; \text{act}(\mathbf{k}_l^\top \mathbf{W} \mathbf{q} + b)<br>$$</p>
</li>
</ol>
<h2 id="注意力对齐-Attention-Alignment"><a href="#注意力对齐-Attention-Alignment" class="headerlink" title="注意力对齐(Attention Alignment)"></a>注意力对齐(Attention Alignment)</h2><p>对齐函数将原始注意力分数$\mathbf{e} &#x3D; [e_1, \ldots, e_{n_f}]$转换为标准化权重：</p>
<ol>
<li><p><strong>软对齐&#x2F;全局对齐(Soft&#x2F;Global Alignment)</strong>：</p>
<p>$$<br>a_l &#x3D; \frac{\exp(e_l)}{\sum_{j&#x3D;1}^{n_f} \exp(e_j)}<br>$$</p>
</li>
<li><p><strong>硬对齐(Hard Alignment)</strong>：<br>从多项式分布采样：</p>
<p>$$<br>m \sim \text{Multinomial}(a_1, \ldots, a_{n_f})<br>$$</p>
<p>然后：</p>
<p>$$<br>\mathbf{c} &#x3D; \mathbf{v}_m<br>$$</p>
</li>
<li><p><strong>局部对齐(Local Alignment)</strong>：<br>窗口位置$p$的确定：<br>$$<br>p &#x3D; S \times \text{sigmoid}(\mathbf{w}_p^\top \tanh(\mathbf{W}_p \mathbf{q}))<br>$$<br>然后计算：<br>$$<br>a_l &#x3D; \frac{\exp(e_l)}{\sum_{j&#x3D;p-D}^{p+D} \exp(e_j)} \exp\left(-\frac{(l-p)^2}{2\sigma^2}\right)<br>$$</p>
</li>
</ol>
<h2 id="注意力维度-Attention-Dimensionality"><a href="#注意力维度-Attention-Dimensionality" class="headerlink" title="注意力维度(Attention Dimensionality)"></a>注意力维度(Attention Dimensionality)</h2><ol>
<li><p><strong>单维注意力(Single-Dimensional Attention)</strong>：</p>
<p>$$<br>\mathbf{c} &#x3D; \sum_{l&#x3D;1}^{n_f} a_l \mathbf{v}_l<br>$$</p>
</li>
<li><p><strong>多维注意力(Multi-Dimensional Attention)</strong>：<br>调整评分函数产生向量分数：</p>
<p>$$<br>\mathbf{e}_l &#x3D; \mathbf{W}_d^\top \text{act}(\mathbf{W}_1 \mathbf{q} + \mathbf{W}_2 \mathbf{k}_l + \mathbf{b})<br>$$</p>
<p>然后计算：</p>
<p>$$<br>a_{l,i} &#x3D; \frac{\exp(e_{l,i})}{\sum_{j&#x3D;1}^{n_f} \exp(e_{j,i})}<br>$$</p>
<p>最终上下文向量：</p>
<p>$$<br>\mathbf{c} &#x3D; \sum_{l&#x3D;1}^{n_f} \mathbf{a}_l \circ \mathbf{v}_l<br>$$</p>
<p>其中$\circ$表示逐元素乘法。</p>
</li>
</ol>
<h2 id="4-实际应用与选择建议"><a href="#4-实际应用与选择建议" class="headerlink" title="4. 实际应用与选择建议"></a>4. 实际应用与选择建议</h2><p>论文提供了关于不同机制选择的实用建议：</p>
<ol>
<li><p><strong>评分函数选择</strong>：</p>
<ul>
<li>计算效率优先：乘性或缩放乘性评分</li>
<li>性能优先：加性评分或通用评分</li>
<li>大维度键向量：必须使用缩放乘性评分防止梯度问题</li>
</ul>
</li>
<li><p><strong>对齐方式选择</strong>：</p>
<ul>
<li>标准情况：软对齐</li>
<li>需要严格稀疏性：硬对齐（但要注意训练难度）</li>
<li>序列数据：考虑局部对齐</li>
<li>需要动态选择关注区域：强化对齐</li>
</ul>
</li>
<li><p><strong>维度选择</strong>：</p>
<ul>
<li>大多数情况：单维注意力足够</li>
<li>需要细粒度控制：考虑多维注意力</li>
</ul>
</li>
</ol>
<p>这些通用机制可以自由组合，例如可以设计一个使用加性评分、软对齐和多维注意力的模型。论文特别指出，Transformer 模型成功的关键在于巧妙地组合了缩放乘性评分、软对齐和单维注意力（通过多头机制实现类似多维注意力的效果）。</p>
<h1 id="📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒"><a href="#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒" class="headerlink" title="📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1><p><a href="/paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf" target="_blank">📄 Brauwers 和 Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a></p>
<a href="/2025/07/10/Attention/" title="Attention Overview">Attention Overview</a>
]]></content>
      <categories>
        <category>model</category>
        <category>attention</category>
        <category>category</category>
      </categories>
      <tags>
        <tag>CDR</tag>
        <tag>model</tag>
        <tag>Basic</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Hypergraph</title>
    <url>/2025/07/19/Hypergraph/</url>
    <content><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><span id="more"></span>


<h1 id="📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒"><a href="#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒" class="headerlink" title="📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1>]]></content>
  </entry>
  <entry>
    <title>Fufufu Relashinala</title>
    <url>/2025/07/14/Feature-Related-Attention/</url>
    <content><![CDATA[<h1 id="在输入特征上做文章的-Attention"><a href="#在输入特征上做文章的-Attention" class="headerlink" title="在输入特征上做文章的 Attention"></a>在输入特征上做文章的 Attention</h1><p>本文将接着详细说明一种基于输入特征分类 Attention 的方式，并介绍在这种分类方式下关注到的不同的 Attention 的架构。</p>
<div style="display: flex; align-items: center;">
  <img src="\img\Attention\Fufufu Relashinala.png" style="width: 200px; margin-right: 20px;">
  <p>
  具体来说，本文主要探讨了基于输入特征特性的注意力机制变体。本节根据输入特征的不同特性，将特征相关的注意力机制分为三类：特征多重性(Multiplicity of Features)、特征层级(Levels of Features)和特征表示(Feature Representations)。
  </p>
</div>



<p>在阅读这篇博客前请先阅读 <a href="/2025/07/10/Attention/" title="Attention Overview">Attention Overview</a></p>
<span id="more"></span>

<h2 id="特征多重性-Multiplicity-of-Features"><a href="#特征多重性-Multiplicity-of-Features" class="headerlink" title="特征多重性(Multiplicity of Features)"></a>特征多重性(Multiplicity of Features)</h2><p>这部分讨论了如何处理多个输入源的特征，主要分为单一特征注意力和多特征注意力机制。</p>
<h3 id="单一特征注意力-Singular-Features-Attention"><a href="#单一特征注意力-Singular-Features-Attention" class="headerlink" title="单一特征注意力(Singular Features Attention)"></a>单一特征注意力(Singular Features Attention)</h3><p>大多数任务模型只处理单一输入(如图像、句子或声音序列)，使用单一特征注意力机制。这种机制直接对单个输入的特征向量进行注意力计算。</p>
<h3 id="多特征注意力机制"><a href="#多特征注意力机制" class="headerlink" title="多特征注意力机制"></a>多特征注意力机制</h3><p>当模型需要同时处理多个输入源时，需要特殊的多特征注意力机制：</p>
<p><strong>协同注意力(Co-attention)</strong></p>
<ul>
<li>分为 <strong>粗科粒度(Coarse-grained)</strong> 和 <strong>细颗粒度(Fine-grained)</strong> 两种</li>
<li><strong>粗颗粒度协同</strong>注意力使用一个输入的<em>紧凑表示</em>作为查询来关注另一个输入</li>
<li><strong>细颗粒度协同</strong>注意力使用一个输入的所有特征向量作为查询</li>
</ul>
<h4 id="粗颗粒度协同"><a href="#粗颗粒度协同" class="headerlink" title="粗颗粒度协同"></a>粗颗粒度协同</h4><p>论文给出的粗颗粒度协同的实例是<strong>alternating co-attention</strong></p>
<h5 id="alternating-co-attention"><a href="#alternating-co-attention" class="headerlink" title="alternating co-attention"></a>alternating co-attention</h5><img src="/img/Attention/AlternatingCo-Attention.png" alt="alternating co-attention" width="60%" height="auto">

<p>如上图所示，这是 alternating co-attention 的架构图，该机制交替使用两个输入的特征矩阵，先计算第一个输入的注意力，将其上下文向量作为查询计算第二个输入的注意力，然后再用第二个输入的上下文向量重新计算第一个输入的注意力。</p>
<p>这里现给出他的 score 函数</p>
<p>对于有序列输入的 Attention：</p>
<p>$$<br>\mathrm{score}(\underset{d_{q}\times1}{\boldsymbol{q}},\underset{d_{k}\times1}{\boldsymbol{k}_{l}})&#x3D;\underset{1\times d_{w}}{\boldsymbol{w}^{T}}\times\mathrm{act}(\underset{d_{w}\times d_{q}}{\boldsymbol{W}_{1}}\times\underset{d_{q}\times1}{\boldsymbol{q}}+\underset{d_{w}\times d_{k}}{\boldsymbol{W}_{2}}\times\underset{d_{k}\times1}{\boldsymbol{k}_{l}}+\underset{d_{w}\times1}{\boldsymbol{b}})<br>$$</p>
<p>对于无序列输入的 Attention <del>（这是一种自注意力机制，后面会提到）</del> ：</p>
<p>$$<br>\underset{1\times1}{e_{l}^{(0)}}&#x3D;\underset{1\times d_{w}}{\boldsymbol{w}^{(1)T}}\times\operatorname{act}(\underset{d_{w}\times d_{k}^{(1)}}{\boldsymbol{W}^{(1)}}\times\underset{d_{k}^{(1)}\times1}{\boldsymbol{k}_{l}^{(1)}}+\underset{d_{w}\times1}{\boldsymbol{b}^{(1)}})<br>$$</p>
<p>对于第二层 Attention：</p>
<p>$$<br>\underset{1\times1}{e_{l}^{(2)}}&#x3D;\mathrm{score}(\underset{d_{v}^{(1)}\times 1}{\boldsymbol{c}^{(0)}},\underset{d_{k}^{(2)}\times1}{\boldsymbol{k}_{l}^{(2)}})<br>$$</p>
<p>对于第三层 Attention：</p>
<p>$$<br>\underset{1\times1}{e_{l}^{(1)}}&#x3D;\mathrm{score}(\underset{d_{v}^{(2)}\times 1}{\boldsymbol{c}^{(2)}},\underset{d_{k}^{(1)}\times1}{\boldsymbol{k}_{l}^{(1)}})<br>$$</p>
<p>生成的上下文向量$\boldsymbol{c}^{(1)}$和$\boldsymbol{c}^{(2)}$被连接起来，并在输出模型中用于预测。交替协同注意力由于需要一个接一个地计算上下文向量，因此本质上包含了<em>一种顺序性</em>。这可能会带来计算上的劣势，因为<em>无法并行</em>化。</p>
<h5 id="interactive-co-attention"><a href="#interactive-co-attention" class="headerlink" title="interactive co-attention"></a>interactive co-attention</h5><img src="/img/Attention/InteractiveCo-Attention.png" alt="interactive co-attention" width="60%" height="auto">

<ul>
<li>并行计算两个输入的注意力</li>
<li>使用未加权平均的关键向量作为查询</li>
<li>计算效率更高，可以并行处理</li>
</ul>
<p>$$<br>\underset{d_k^{(i)}\times1}{\bar{\boldsymbol{k}}^{(i)}}&#x3D;\frac{1}{n_f^{(i)}}\sum\limits_{l&#x3D;1}^{n_f^{(i)}}\underset{d_k^{(i)}\times1}{\boldsymbol{k}_l^{(i)}}, \quad \underset{1\times1}{e_{l}^{(i)}}&#x3D;\mathrm{score}(\underset{d_{k}^{(3-i)}\times1}{\bar{\boldsymbol{k}}^{(3-i)}},\underset{d_{k}^{(i)}\times1}{\boldsymbol{k}_{l}^{(i)}}) , \qquad i&#x3D;1,2<br>$$</p>
<h4 id="细颗粒度协同"><a href="#细颗粒度协同" class="headerlink" title="细颗粒度协同"></a>细颗粒度协同</h4><p>虽然粗粒度的共同注意力机制使用一个输入的紧凑表示作为查询，以计算另一个输入的注意力，但细粒度的共同注意力在计算注意力分数时会单独考虑每个输入的每个元素。在这种情况下，查询变成了一个矩阵。</p>
<h5 id="并行协同注意力-Parallel-Co-attention"><a href="#并行协同注意力-Parallel-Co-attention" class="headerlink" title="并行协同注意力(Parallel Co-attention)"></a>并行协同注意力(Parallel Co-attention)</h5><img src="/img/Attention/ParallelCo-Attention.png" alt="parallel co-attention" width="60%" height="auto">

<ul>
<li>同时计算两个输入的注意力</li>
<li>使用亲和矩阵(Affinity Matrix)转换关键向量空间</li>
<li>通过聚合形式计算注意力分数</li>
</ul>
<p>我们有两种方式生成矩阵 $\mathbf{A}$</p>
<p>$$<br>\underset{n_{f}^{(1)}\times n_{f}^{(2)}}{\mathbf{A}}&#x3D;\operatorname{act}(\underset{n_{f}^{(1)}\times d_{k}^{(1)}}{\begin{array}{c}K^{(1)^{T}}\end{array}}\times\underset{d_{k}^{(1)}\times d_{k}^{(2)}}{\begin{array}{c}W_{A}\end{array}}\times\underset{d_{k}^{(2)}\times n_{f}^{(2)}}{\begin{array}{c}K^{(2)}\end{array}})<br>$$</p>
<p>$$<br>\underset{1\times1}{A_{i,j}}&#x3D;\underset{1\times3d_{k}}{\boldsymbol{w}_{A}^{T}}\times\mathrm{concat}(\underset{d_{k}\times1}{\boldsymbol{k}_{i}^{(1)}},\underset{d_{k}\times1}{\boldsymbol{k}_{j}^{(2)}},\underset{d_{k}\times1}{\boldsymbol{k}_{i}^{(1)}}\circ\underset{d_{k}\times1}{\boldsymbol{k}_{j}^{(2)}})<br>$$</p>
<p>其中，$\circ$表示哈德曼积</p>
<p>Affinity Matrix 可以解释为两个键矩阵列的相似性矩阵，并有助于将图像键转换到与句子中单词的键相同的空间，反之亦然。</p>
<p>由于现在查询由向量变成了矩阵，因此 score 函数也发生了变化</p>
<p>$$<br>e^{(1)} &#x3D;\boldsymbol{w}_{1}\times\mathrm{act}(\boldsymbol{W}_{2}\times\boldsymbol{K}^{(2)}\times\boldsymbol{A}^{T}+\boldsymbol{W}_{1}\times\boldsymbol{K}^{(1)})<br>$$</p>
<p>$$<br>e^{(2)} &#x3D;\boldsymbol{w}_{2}\times\mathrm{act}(\boldsymbol{W}_{1}\times\boldsymbol{K}^{(1)}\times\boldsymbol{A}^{::}+\boldsymbol{W}_{2}\times\boldsymbol{K}^{(2)})<br>$$</p>
<p>值得一提的是，之前的 score 函数实际是现在这一形式的特殊表达，也就是说，这个表达更具一般性</p>
<p>如前所述，亲和矩阵本质上是两个注意力模块 1 和 2 的关键向量的相似性矩阵。这个意味着一种不同的确定注意力分数的方法。即，可以将一行或一列中的最大相似度值作为注意力分数。</p>
<p>$$<br>e_{i}^{(1)}&#x3D;\max_{j&#x3D;1,\ldots,n_{f}^{(2)}}A_{i,j},\quad e_{j}^{(2)}&#x3D;\max_{i&#x3D;1,\ldots,n_{f}^{(1)}}A_{i,j}.<br>$$</p>
<h5 id="旋转注意力-Rotatory-Attention"><a href="#旋转注意力-Rotatory-Attention" class="headerlink" title="旋转注意力(Rotatory Attention)"></a>旋转注意力(Rotatory Attention)</h5><p>Rotatory Attention 是一种用于处理多输入数据的注意力机制，特别适用于情感分析任务中同时考虑目标短语、左上下文和右上下文的场景。该机制通过交替关注不同输入来构建更丰富的表示。</p>
<ul>
<li>主要用于情感分析任务</li>
<li>处理三个输入：目标短语 $\boldsymbol{F}^{t} &#x3D; [ \boldsymbol{f}_{1}^{t}, \ldots , \boldsymbol{f}_{n_{f}^{t}}^{t}] \in \mathbb{R} ^{d_{f}^{t}\times n_{f}^{t}}$ 、左上下文 $\boldsymbol{F^{l}} &#x3D; [ \boldsymbol{f_{1}^{l}}, \ldots , \boldsymbol{f_{n_{f}^{l}}^{l}}]\in\mathbb{R} ^{d_{f}^{l}\times n_{f}^{l}}$ 和右上下文 $\boldsymbol{F^{r}} &#x3D; [ \boldsymbol{f_{1}^{r}}, \ldots , \boldsymbol{f_{n_{f}^{r}}^{r}}]\in\mathbb{R}^{d_f^r\times n_f^r}$</li>
<li>通过注意力机制迭代改进表示</li>
</ul>
<p>其大致的过程如下：</p>
<ol>
<li><p><strong>初始特征提取</strong></p>
<p>首先，模型从三个输入中提取特征向量目标短语特征矩阵 $\boldsymbol{F}^{t}$ 左上下文特征矩阵 $\boldsymbol{F^{l}}$ 右上下文特征矩阵 $\boldsymbol{F^{r}}$ </p>
</li>
<li><p><strong>目标短语初始表示</strong></p>
<p>计算目标短语的初始表示向量$r^{t}$，通过对特征向量取平均：</p>
<p>$$<br>r^{t}&#x3D;\frac{1}{n_{f}^{t}}\sum_{i&#x3D;1}^{n_{f}^{t}} f_{i}^{t}<br>$$</p>
</li>
<li><p><strong>左上下文注意力计算</strong></p>
<p>使用$r^{t}$作为查询，计算左上下文的注意力：</p>
<ol>
<li><p>提取键向量 $k_{1}^{l},\ldots,k_{n_{f}^{l}}^{l}\in \mathbb{R}^{d_{k}^{l}}$ 和值向量 $v_{1}^{l},\ldots,v_{n_{f}^{l}}^{l}\in \mathbb{R}^{d_{v}^{l}}$</p>
</li>
<li><p>计算注意力分数 $e_{i}^{l}&#x3D;\operatorname{score}\left(r^{t}, k_{i}^{l}\right)$</p>
</li>
<li><p>使用 softmax 对齐函数计算注意力权重$a_{i}^{l}$</p>
</li>
<li><p>计算左上下文表示向量 $r^{l}&#x3D;\sum_{i&#x3D;1}^{n_{f}^{l}} a_{i}^{l}v_{i}^{l}$</p>
</li>
</ol>
</li>
<li><p><strong>右上下文注意力计算</strong></p>
<p>类似地，计算右上下文的表示向量$r^{r}$：</p>
<ol>
<li><p>提取键向量 $k_{1}^{r},\ldots,k_{n_{f}^{r}}^{r}\in \mathbb{R}^{d_{k}^{r}}$ 和值向量 $v_{1}^{r},\ldots,v_{n_{f}^{r}}^{r}\in \mathbb{R}^{d_{v}^{r}}$</p>
</li>
<li><p>计算注意力分数 $e_{i}^{r}&#x3D;\operatorname{score}\left(r^{t}, k_{i}^{r}\right)$</p>
</li>
<li><p>使用 softmax 对齐函数计算注意力权重$a_{i}^{r}$</p>
</li>
<li><p>计算右上下文表示向量 $r^{r}&#x3D;\sum_{i&#x3D;1}^{n_{f}^{r}} a_{i}^{r}v_{i}^{r}$</p>
</li>
</ol>
</li>
<li><p><strong>目标短语更新表示</strong></p>
<p>使用左上下文表示$r^{l}$和右上下文表示$r^{r}$来更新目标短语的表示：</p>
<ol>
<li><p>提取目标短语的键向量 $k_{1}^{t},\ldots,k_{n_{f}^{t}}^{t}\in \mathbb{R}^{d_{k}^{t}}$ 和值向量 $v_{1}^{t},\ldots,v_{n_{f}^{t}}^{t}\in \mathbb{R}^{d_{v}^{t}}$</p>
</li>
<li><p>计算左感知目标表示 $r^{l_{t}}$：</p>
<ul>
<li>注意力分数：$e_{i}^{l_{t}}&#x3D;\operatorname{score}\left(r^{l}, k_{i}^{t}\right)$</li>
<li>使用 softmax 对齐函数计算注意力权重 $a_{i}^{l_{t}}$</li>
<li>计算表示向量：$r^{l_{t}}&#x3D;\sum_{i&#x3D;1}^{n_{f}^{t}} a_{i}^{l_{t}}v_{i}^{t}$</li>
</ul>
</li>
<li><p>计算右感知目标表示 $r^{r_{t}}$：</p>
<ul>
<li>注意力分数：$e_{i}^{r_{t}}&#x3D;\operatorname{score}\left(r^{r}, k_{i}^{t}\right)$</li>
<li>使用 softmax 对齐函数计算注意力权重 $a_{i}^{r_{t}}$</li>
<li>计算表示向量：$r^{r_{t}}&#x3D;\sum_{i&#x3D;1}^{n_{f}^{t}} a_{i}^{r_{t}}v_{i}^{t}$</li>
</ul>
</li>
</ol>
</li>
<li><p>最终表示为 $r&#x3D;\operatorname{concat}\left(r^{l},r^{r},r^{l_{t}},r^{r_{t}}\right)$</p>
</li>
</ol>
<p>Rotatory Attention 具有以下特点：</p>
<ol>
<li><p><strong>双向信息流动</strong>：通过从目标短语到上下文，再从上下文回到目标短语的信息流动，实现了双向的信息交互。</p>
</li>
<li><p><strong>层次化表示</strong>：构建了多层次的特征表示，从原始特征到上下文感知特征。</p>
</li>
<li><p><strong>特定任务优化</strong>：特别适合情感分析任务，能够捕捉目标短语与上下文之间的复杂关系。</p>
</li>
</ol>
<p>Rotatory Attention 通过这种交替关注的方式，能够更好地理解目标短语在特定上下文中的情感倾向，从而提高了情感分类的准确性。</p>
<h2 id="特征层级-Levels-of-Features"><a href="#特征层级-Levels-of-Features" class="headerlink" title="特征层级(Levels of Features)"></a>特征层级(Levels of Features)</h2><p>这部分讨论了如何处理具有层级结构的特征，主要分为单层级注意力和多层级注意力机制。多层级注意力能够捕捉<strong>不同粒度</strong>上的重要信息。</p>
<h3 id="单层级注意力-Single-Level-Attention"><a href="#单层级注意力-Single-Level-Attention" class="headerlink" title="单层级注意力(Single-Level Attention)"></a>单层级注意力(Single-Level Attention)</h3><p>传统注意力机制通常在单一层级上处理特征，如只关注单词级别或句子级别。</p>
<h3 id="多层级注意力机制"><a href="#多层级注意力机制" class="headerlink" title="多层级注意力机制"></a>多层级注意力机制</h3><ol>
<li><strong>注意力叠加(Attention-via-Attention)</strong></li>
</ol>
<img src="/img/Attention/AttentionViaAttention.png" alt="attention-via-attention" width="60%" height="auto">

<ul>
<li>同时处理字符级和词级特征</li>
<li>先计算词级注意力，用其上下文向量辅助计算字符级注意力</li>
<li>最终拼接两个层级的上下文向量</li>
</ul>
<p>用于机器翻译任务，同时利用字符级和词级信息。其核心思想是在预测字符时，先通过词级注意力确定重要词语，再在这些词语对应的字符上施加注意力。</p>
<p>其大致过程如下：</p>
<ol>
<li><p>输入句子被编码为字符级特征矩阵 $F^{(c)}\in \mathbb{R}^{d_{f}^{(c)}\times n_{f}^{(c)}}$ 和词级特征矩阵 $F^{(w)}\in \mathbb{R}^{d_{f}^{(w)}\times n_{f}^{(w)}}$</p>
</li>
<li><p>字符级查询 $q^{(c)}\in \mathbb{R}^{d_{q}}$ 通过查询模型生成</p>
</li>
<li><p>先计算词级注意力，生成词级上下文向量 $c^{(w)}\in \mathbb{R}^{d_{v}^{(w)}}$</p>
</li>
<li><p>将 $q^{(c)}$ 和 $c^{(w)}$ 拼接作为字符级注意力的查询</p>
</li>
<li><p>最终输出是字符级和词级上下文向量的拼接</p>
</li>
<li><p><strong>层级注意力(Hierarchical Attention)</strong></p>
</li>
</ol>
<img src="/img/Attention/HierarchicalAttention.png" alt="hierarchical attention" width="60%" height="auto">

<ul>
<li>从最低层级开始，逐步构建高层级表示</li>
<li>常用于文档分类：词 → 句 → 文档</li>
<li>每个层级通过注意力机制生成摘要表示</li>
</ul>
<p>用于文档分类。该方法自底向上构建层级表示：从词级表示构建句级表示，再从句级表示构建文档级表示。</p>
<p>其大致过程如下：</p>
<ol>
<li><p>文档包含 $n_S$ 个句子，第 $s$ 个句子包含 $n_s$ 个词</p>
</li>
<li><p>对每个句子计算词级注意力，生成句表示 $c^{(s)}\in \mathbb{R}^{d_{v}^{(S)}}$</p>
</li>
<li><p>将所有句表示 $C&#x3D;[c^{(1)},\ldots,c^{(n_{S})}]\in \mathbb{R}^{d_{v}^{(S)} \times n_{S}}$ 作为文档级注意力的输入</p>
</li>
<li><p>文档级注意力输出 $c^{(D)}\in \mathbb{R}^{d_{v}^{(D)}}$ 用于分类</p>
</li>
</ol>
<h4 id="应用领域"><a href="#应用领域" class="headerlink" title="应用领域"></a>应用领域</h4><p>多层级注意力已成功应用于 <del>懒得做链接了捏，可以去原文找捏</del> ：</p>
<ul>
<li>推荐系统：建模用户长短期偏好(Ying et al., 2018)</li>
<li>视频动作识别：捕捉不同时间尺度的运动信息(Wang et al., 2016)</li>
<li>跨领域情感分类：学习领域共享和特定特征(Li et al., 2018)</li>
<li>聊天机器人：生成更连贯的响应(Xing et al., 2018)</li>
<li>人群计数：处理不同尺度的人群密度(Sindagi &amp; Patel, 2019)</li>
</ul>
<h2 id="特征表示-Feature-Representations"><a href="#特征表示-Feature-Representations" class="headerlink" title="特征表示(Feature Representations)"></a>特征表示(Feature Representations)</h2><p>这部分讨论了特征表示相关的注意力机制（Feature Representations），主要关注如何利用注意力机制来处理和组合不同的特征表示。这部分内容可以分为两类：单表示注意力（Single-representational attention）和多表示注意力（Multi-representational attention）。</p>
<h3 id="单一表示注意力-Single-Representational-Attention"><a href="#单一表示注意力-Single-Representational-Attention" class="headerlink" title="单一表示注意力(Single-Representational Attention)"></a>单一表示注意力(Single-Representational Attention)</h3><p>单表示注意力是最基础的注意力形式，它使用单一的特征表示模型（如词嵌入、CNN特征提取器等）来生成特征向量。这些特征向量随后被送入注意力模块进行处理。</p>
<h3 id="多表示注意力-Multi-Representational-Attention"><a href="#多表示注意力-Multi-Representational-Attention" class="headerlink" title="多表示注意力(Multi-Representational Attention)"></a>多表示注意力(Multi-Representational Attention)</h3><p>多表示注意力是一种更高级的技术，它允许模型同时考虑多种不同的特征表示，并通过注意力机制来学习如何最优地组合这些表示。</p>
<h4 id="元嵌入-Meta-embeddings"><a href="#元嵌入-Meta-embeddings" class="headerlink" title="元嵌入(Meta-embeddings)"></a>元嵌入(Meta-embeddings)</h4><p>这种方法可以创建所谓的”元嵌入”（meta-embeddings）。</p>
<ul>
<li>整合多个嵌入表示</li>
<li>通过注意力机制加权平均不同表示</li>
<li>生成更高质量的特征表示</li>
</ul>
<p>元嵌入的创建过程大致如下：</p>
<ol>
<li><p><strong>输入表示</strong>：对于一个输入 $x$ （如一个词），我们有 $E$ 种不同的嵌入表示： $x^{(e_1)}, \ldots, x^{(e_E)}$ , 其中每种嵌入 $x^{(e_i)}$ 的维度为 $d_{e_i}$（$i&#x3D;1,\ldots,E$ ）。</p>
</li>
<li><p><strong>维度标准化</strong>：由于不同嵌入可能有不同维度，首先通过线性变换将它们映射到统一维度 $d_t$ ： $x^{(t_i)} &#x3D; W_{e_i} \times x^{(e_i)} + b_{e_i}$ ， 其中 $W_{e_i} \in \mathbb{R}^{d_t \times d_{e_i}}$ 和 $b_{e_i} \in \mathbb{R}^{d_t}$ 是可训练的参数。</p>
</li>
<li><p><strong>注意力加权组合</strong>：最终的元嵌入是这些标准化表示的加权和： $x^{(e)} &#x3D; \sum_{i&#x3D;1}^E a_i \times x^{(t_i)}$ 其中权重 $a_i$ 通过注意力机制计算得到。</p>
</li>
</ol>
<h5 id="注意力计算"><a href="#注意力计算" class="headerlink" title="注意力计算"></a>注意力计算</h5><p>在多表示注意力中，注意力权重的计算可以视为一种自注意力机制，其查询可以理解为”哪些表示对当前任务最重要”。具体计算过程如下：</p>
<ol>
<li>将标准化后的表示 $x^{(t_1)}, \ldots, x^{(t_E)}$ 作为特征矩阵F的列向量</li>
<li>由于没有显式查询，这相当于自注意力机制</li>
<li>使用适当的注意力评分函数计算权重</li>
<li>通过对齐函数（如softmax）得到归一化的注意力权重</li>
</ol>
<h3 id="技术优势"><a href="#技术优势" class="headerlink" title="技术优势"></a>技术优势</h3><ol>
<li><strong>灵活性</strong>：可以整合来自不同来源或不同粒度的特征表示</li>
<li><strong>适应性</strong>：通过注意力权重自动学习不同表示的重要性</li>
<li><strong>可解释性</strong>：注意力权重可以提供关于哪些特征表示对任务更重要的见解</li>
</ol>
<h1 id="📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒"><a href="#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒" class="headerlink" title="📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1><p><a href="/paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf" target="_blank">📄 Brauwers 和 Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a></p>
<a href="/2025/07/10/Attention/" title="Attention Overview">Attention Overview</a>
]]></content>
      <categories>
        <category>model</category>
        <category>attention</category>
        <category>category</category>
      </categories>
      <tags>
        <tag>CDR</tag>
        <tag>model</tag>
        <tag>Basic</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title>PEP 8</title>
    <url>/2025/07/08/PEP-8/</url>
    <content><![CDATA[<h1 id="Style-Guide-for-Python-Code"><a href="#Style-Guide-for-Python-Code" class="headerlink" title="Style Guide for Python Code"></a>Style Guide for Python Code</h1><blockquote>
<p><a href="https://peps.python.org/pep-0008/">PEP8</a> 是 Python 社群共通的風格指南，一開始是 Python 之父 Guido van Rossum 自己的撰碼風格，慢慢後來演變至今，目的在於幫助開發者寫出可讀性高且風格一致的程式。許多開源計畫，例如 Django 、 OpenStack 等都是以 PEP8 為基礎再加上自己的風格建議。</p>
</blockquote>
<p>这篇博客主要是为了在搭建自己的模型之前学习一下一些统一的规范是做的记录 <del>主要是目前读到的大多数论文的源码目命名没有规律</del> ，以加强之后搭建模型时代码的可读性</p>
<p>另外，本博客只展示本人不太熟悉的捏</p>
<span id="more"></span>

<h2 id="代码布局"><a href="#代码布局" class="headerlink" title="代码布局"></a>代码布局</h2><h3 id="缩进"><a href="#缩进" class="headerlink" title="缩进"></a>缩进</h3><p><strong>每个缩进级别使用 4 个空格</strong></p>
<p>对于比较臭长的函数，可以使用<em>悬挂缩进</em></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Correct:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Aligned with opening delimiter.</span></span><br><span class="line">foo = long_function_name(var_one, var_two,</span><br><span class="line">                         var_three, var_four)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add 4 spaces (an extra level of indentation) to distinguish arguments from the rest.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">long_function_name</span>(<span class="params"></span></span><br><span class="line"><span class="params">        var_one, var_two, var_three,</span></span><br><span class="line"><span class="params">        var_four</span>):</span><br><span class="line">    <span class="built_in">print</span>(var_one)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hanging indents should add a level.</span></span><br><span class="line">foo = long_function_name(</span><br><span class="line">    var_one, var_two,</span><br><span class="line">    var_three, var_four)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Wrong:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Arguments on first line forbidden when not using vertical alignment.</span></span><br><span class="line">foo = long_function_name(var_one, var_two,</span><br><span class="line">    var_three, var_four)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Further indentation required as indentation is not distinguishable.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">long_function_name</span>(<span class="params"></span></span><br><span class="line"><span class="params">    var_one, var_two, var_three,</span></span><br><span class="line"><span class="params">    var_four</span>):</span><br><span class="line">    <span class="built_in">print</span>(var_one)</span><br></pre></td></tr></table></figure>

<p>优先使用 <em>Tabs</em> 进行缩进， <em>Tabs</em> 和 <em>Spaces</em> 不能混用</p>
<h3 id="每行最多字符数量"><a href="#每行最多字符数量" class="headerlink" title="每行最多字符数量"></a>每行最多字符数量</h3><p><strong>79</strong> 个</p>
<p>合理使用反斜杠</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;/path/to/some/file/you/want/to/read&#x27;</span>) <span class="keyword">as</span> file_1, \</span><br><span class="line">     <span class="built_in">open</span>(<span class="string">&#x27;/path/to/some/file/being/written&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> file_2:</span><br><span class="line">    file_2.write(file_1.read())</span><br></pre></td></tr></table></figure>

<h3 id="二元运算符之前换行"><a href="#二元运算符之前换行" class="headerlink" title="二元运算符之前换行"></a>二元运算符之前换行</h3><p>为了更好的确定该 <code>item</code> 采取的是什么运算</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Wrong:</span></span><br><span class="line"><span class="comment"># operators sit far away from their operands</span></span><br><span class="line">income = (gross_wages +</span><br><span class="line">          taxable_interest +</span><br><span class="line">          (dividends - qualified_dividends) -</span><br><span class="line">          ira_deduction -</span><br><span class="line">          student_loan_interest)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Correct:</span></span><br><span class="line"><span class="comment"># easy to match operators with operands</span></span><br><span class="line">income = (gross_wages</span><br><span class="line">          + taxable_interest</span><br><span class="line">          + (dividends - qualified_dividends)</span><br><span class="line">          - ira_deduction</span><br><span class="line">          - student_loan_interest)</span><br></pre></td></tr></table></figure>

<h3 id="如何空行（Blank-Lines）"><a href="#如何空行（Blank-Lines）" class="headerlink" title="如何空行（Blank Lines）"></a>如何空行（Blank Lines）</h3><p><em>顶级函数</em> 和 <em>类</em> 之间空 <strong>2</strong> 行</p>
<p><em>类中的函数</em> 空 <strong>1</strong> 行</p>
<h3 id="import"><a href="#import" class="headerlink" title="import"></a>import</h3><ul>
<li>通常每一个库 <strong>单独一行</strong>（也有例外）</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> subprocess <span class="keyword">import</span> Popen, PIPE</span><br></pre></td></tr></table></figure>

<ul>
<li>按以下顺序分组，每组间空行<ol>
<li><strong>标准库</strong>导入</li>
<li><strong>相关第三方库</strong>导入</li>
<li><strong>特定的本地库</strong>导入</li>
</ol>
</li>
</ul>
<h2 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h2><blockquote>
<p>Comments that contradict the code are worse than no comments.</p>
</blockquote>
<h2 id="命名约定"><a href="#命名约定" class="headerlink" title="命名约定"></a>命名约定</h2><ol>
<li><p><strong>类名</strong> 用 <strong>大驼峰</strong></p>
</li>
<li><p><strong>函数名</strong> 用 <strong>小写下划线</strong></p>
</li>
<li><p>关于 <em>下划线</em></p>
<ul>
<li><em>单下划线</em> 用于占位</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    <span class="built_in">print</span>(random.randint(<span class="number">1</span>, <span class="number">100</span>))</span><br></pre></td></tr></table></figure>

<ul>
<li><em>单下划线</em> 用于变量前表示该变量为 <strong>弱私有</strong> （语义上的 private），能调用但不能 import</li>
<li><em>双下划线</em> 用于变量前表示该变量为 <strong>强私有</strong> （实际上也不能调用<del>实现方式是重名名</del>）<br>为了更好的说明这两点，给出以下两个测试程序</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">test_private_vars.py</span></span><br><span class="line"><span class="string">This file is used to test the private variables in Python.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TestClass</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.public_var = <span class="string">&quot;这是公有变量&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>._weak_private = <span class="string">&quot;这是弱私有变量&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.__strong_private = <span class="string">&quot;这是强私有变量&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">print_all_vars</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;从内部访问:&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;公有变量: <span class="subst">&#123;self.public_var&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;弱私有变量: <span class="subst">&#123;self._weak_private&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;强私有变量: <span class="subst">&#123;self.__strong_private&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建测试实例</span></span><br><span class="line">test = TestClass()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 测试从类内部访问（通过方法）</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n=== 测试1: 从类内部访问所有变量 ===&quot;</span>)</span><br><span class="line">test.print_all_vars()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 测试从外部直接访问</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n=== 测试2: 从外部访问变量 ===&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;访问公有变量: <span class="subst">&#123;test.public_var&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;访问弱私有变量: <span class="subst">&#123;test._weak_private&#125;</span>&quot;</span>)  <span class="comment"># 能访问，但IDE会警告</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;访问强私有变量: <span class="subst">&#123;test.__strong_private&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">except</span> AttributeError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;访问强私有变量失败: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 测试名称改写机制</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n=== 测试3: 验证强私有变量的名称改写机制 ===&quot;</span>)</span><br><span class="line"><span class="comment"># 实际上Python会将__strong_private改写为_TestClass__strong_private</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;通过改写后的名称访问强私有变量: <span class="subst">&#123;test._TestClass__strong_private&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 测试导入行为</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n=== 测试4: 创建第二个文件并尝试导入 ===&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;请创建 test_import.py 并运行来测试导入行为&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">test_import.py</span></span><br><span class="line"><span class="string">This file is used to test the import of private variables in Python.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> test_private_vars <span class="keyword">import</span> TestClass</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=== 测试导入后的访问行为 ===&quot;</span>)</span><br><span class="line">test = TestClass()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试访问公有变量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;访问公有变量: <span class="subst">&#123;test.public_var&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试访问弱私有变量</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;访问弱私有变量: <span class="subst">&#123;test._weak_private&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;注意：虽然能访问弱私有变量，但这违反了Python的约定&quot;</span>)</span><br><span class="line"><span class="keyword">except</span> AttributeError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;访问弱私有变量失败: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试访问强私有变量</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;访问强私有变量: <span class="subst">&#123;test.__strong_private&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">except</span> AttributeError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;访问强私有变量失败: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试通过名称改写访问强私有变量</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;通过改写后的名称访问强私有变量: <span class="subst">&#123;test._TestClass__strong_private&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;注意：虽然能通过名称改写访问强私有变量，但这是一个不推荐的做法&quot;</span>)</span><br><span class="line"><span class="keyword">except</span> AttributeError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;通过改写名称访问强私有变量失败: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>以下是运行 <code>python test_private_vars.py</code> 的结果</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">=== 测试1: 从类内部访问所有变量 ===</span><br><span class="line">从内部访问:</span><br><span class="line">公有变量: 这是公有变量</span><br><span class="line">弱私有变量: 这是弱私有变量</span><br><span class="line">强私有变量: 这是强私有变量</span><br><span class="line"></span><br><span class="line">=== 测试2: 从外部访问变量 ===</span><br><span class="line">访问公有变量: 这是公有变量</span><br><span class="line">访问弱私有变量: 这是弱私有变量</span><br><span class="line">访问强私有变量失败: <span class="string">&#x27;TestClass&#x27;</span> object has no attribute <span class="string">&#x27;__strong_private&#x27;</span></span><br><span class="line"></span><br><span class="line">=== 测试3: 验证强私有变量的名称改写机制 ===</span><br><span class="line">通过改写后的名称访问强私有变量: 这是强私有变量</span><br><span class="line"></span><br><span class="line">=== 测试4: 创建第二个文件并尝试导入 ===</span><br><span class="line">请创建 test_import.py 并运行来测试导入行为</span><br></pre></td></tr></table></figure>

<p>以下是运行 <code>python test_import.py</code> 的结果</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">=== 测试1: 从类内部访问所有变量 ===</span><br><span class="line">从内部访问:</span><br><span class="line">公有变量: 这是公有变量</span><br><span class="line">弱私有变量: 这是弱私有变量</span><br><span class="line">强私有变量: 这是强私有变量</span><br><span class="line"></span><br><span class="line">=== 测试2: 从外部访问变量 ===</span><br><span class="line">访问公有变量: 这是公有变量</span><br><span class="line">访问弱私有变量: 这是弱私有变量</span><br><span class="line">访问强私有变量失败: <span class="string">&#x27;TestClass&#x27;</span> object has no attribute <span class="string">&#x27;__strong_private&#x27;</span></span><br><span class="line"></span><br><span class="line">=== 测试3: 验证强私有变量的名称改写机制 ===</span><br><span class="line">通过改写后的名称访问强私有变量: 这是强私有变量</span><br><span class="line">=== 测试导入后的访问行为 ===</span><br><span class="line">访问公有变量: 这是公有变量</span><br><span class="line">访问弱私有变量: 这是弱私有变量</span><br><span class="line">注意：虽然能访问弱私有变量，但这违反了Python的约定</span><br><span class="line">访问强私有变量失败: <span class="string">&#x27;TestClass&#x27;</span> object has no attribute <span class="string">&#x27;__strong_private&#x27;</span></span><br><span class="line">通过改写后的名称访问强私有变量: 这是强私有变量</span><br><span class="line">注意：虽然能通过名称改写访问强私有变量，但这是一个不推荐的做法</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>CDR</tag>
        <tag>Python</tag>
        <tag>PEP</tag>
        <tag>闲🉐无聊</tag>
        <tag>大概率没用</tag>
      </tags>
  </entry>
  <entry>
    <title>Quaqua Rishinala</title>
    <url>/2025/07/17/Query-Related-Attention/</url>
    <content><![CDATA[<h1 id="在查询上做文章的-Attention"><a href="#在查询上做文章的-Attention" class="headerlink" title="在查询上做文章的 Attention"></a>在查询上做文章的 Attention</h1><div style="display: flex; align-items: center;">
  <img src="\img\Attention\Quaqua Rishinala.png" style="width: 200px; margin-right: 20px;">
  <p>查询是任何注意力模型的重要组成部分，因为它们直接决定了从特征向量中提取哪些信息。这些查询基于任务模型的期望输出，并可以解释为字面问题。一些查询具有特定的特征，需要特定类型的机制来处理它们。因此，这一类别封装了处理特定类型查询特征的注意力机制。这一类别的机制处理以下两种查询特征之一：查询的类型或查询的多重性。</p>
</div>

<p>在阅读这篇博客前请先阅读 <a href="/2025/07/10/Attention/" title="Attention Overview">Attention Overview</a></p>
<span id="more"></span>

<h2 id="查询类型机制-Query-Type-Mechanisms"><a href="#查询类型机制-Query-Type-Mechanisms" class="headerlink" title="查询类型机制(Query Type Mechanisms)"></a>查询类型机制(Query Type Mechanisms)</h2><h3 id="基本查询与特殊查询"><a href="#基本查询与特殊查询" class="headerlink" title="基本查询与特殊查询"></a>基本查询与特殊查询</h3><p>查询(Query)在注意力机制中扮演着关键角色，它决定了模型关注输入数据的哪些部分。3.3 节首先区分了两种基本查询类型：</p>
<ol>
<li><p><strong>基本查询(Basic Queries)</strong>：这类查询通常直接来源于模型结构或数据特征。例如：</p>
<ul>
<li>RNN 中的隐藏状态作为序列生成过程中的查询</li>
<li>辅助变量（如医疗图像分类中的患者特征）作为查询</li>
<li>图像处理中 CNN 提取的特征向量作为查询</li>
</ul>
</li>
<li><p><strong>特殊查询(Specialized Queries)</strong>：用于特定注意力架构的查询，如：</p>
<ul>
<li>旋转注意力(Rotatory Attention)中使用上下文向量作为查询</li>
<li>交互式协同注意力(Interactive Co-attention)中使用平均键向量作为查询</li>
<li>注意力堆叠(Attention-over-Attention)中的多层查询</li>
</ul>
</li>
</ol>
<h3 id="自注意力机制-Self-Attention"><a href="#自注意力机制-Self-Attention" class="headerlink" title="自注意力机制(Self-Attention)"></a>自注意力机制(Self-Attention)</h3><p>自注意力（或称内部注意力）是查询相关机制中最重要的创新之一，它允许模型通过数据自身生成查询：</p>
<ol>
<li><p><strong>自注意力的两种解释</strong>：</p>
<ul>
<li><strong>恒定查询解释</strong>：将查询视为固定问题（如”文档属于哪类？”）</li>
<li><strong>可学习查询解释</strong>：将查询作为可训练参数，随模型优化</li>
</ul>
</li>
<li><p><strong>技术实现</strong>：<br>自注意力通过线性变换从特征矩阵 F 生成查询矩阵 Q：</p>
<p>$$<br>Q &#x3D; W_Q \times F<br>$$</p>
<p>其中 $W_Q \in \mathbb{R}^{d_q \times d_f}$ 是可训练权重矩阵。</p>
</li>
<li><p><strong>自注意力的应用价值</strong>：</p>
<ul>
<li><p>揭示特征向量间的关系（如词语依赖、图像区域关联）</p>
</li>
<li><p>生成改进的特征表示，可通过两种方式：</p>
<p>$$<br>f^{(new)} &#x3D; c \quad \text{或} \quad f^{(new)} &#x3D; \text{Normalize}(f^{(old)} + c)<br>$$</p>
</li>
<li><p>在 Transformer 架构中作为核心组件</p>
</li>
</ul>
</li>
<li><p><strong>领域应用</strong>：</p>
<ul>
<li>计算机视觉：图像识别、GANs 中的区域聚焦</li>
<li>视频处理：时空关系建模</li>
<li>语音处理：语音识别</li>
<li>自然语言处理：情感分析、机器翻译</li>
<li>图网络：节点关系建模</li>
</ul>
</li>
</ol>
<h2 id="多重查询机制-Multi-Query-Mechanisms"><a href="#多重查询机制-Multi-Query-Mechanisms" class="headerlink" title="多重查询机制(Multi-Query Mechanisms)"></a>多重查询机制(Multi-Query Mechanisms)</h2><h3 id="多头注意力-Multi-Head-Attention"><a href="#多头注意力-Multi-Head-Attention" class="headerlink" title="多头注意力(Multi-Head Attention)"></a>多头注意力(Multi-Head Attention)</h3><img src="/img/Attention/MultiheadAttention.png" alt="Multi-head Attention" width="60%" height="auto">

<p>多头注意力是处理多重查询的核心技术，其关键特点包括：</p>
<ol>
<li><p><strong>并行注意力头</strong>：</p>
<ul>
<li>每个头有独立的$W_Q^{(j)}, W_K^{(j)}, W_V^{(j)}$矩阵</li>
<li>生成不同的查询表示：$q^{(j)} &#x3D; W_Q^{(j)} \times q$</li>
<li>允许模型同时关注不同方面的信息</li>
</ul>
</li>
<li><p><strong>实现细节</strong>：</p>
<ul>
<li>每个头产生独立的上下文向量$c^{(j)}$</li>
<li>最终输出通过线性变换合并 $c &#x3D; W_O \times \text{concat}(c^{(1)},…,c^{(d)})$</li>
</ul>
</li>
<li><p><strong>优势</strong>：</p>
<ul>
<li>增强模型捕捉多样化关系的能力</li>
<li>在 Transformer 中实现并行计算</li>
<li>可解释性强（不同头可学习不同关注模式）</li>
</ul>
</li>
</ol>
<h3 id="多跳注意力-Multi-Hop-Attention"><a href="#多跳注意力-Multi-Hop-Attention" class="headerlink" title="多跳注意力(Multi-Hop Attention)"></a>多跳注意力(Multi-Hop Attention)</h3><img src="/img/Attention/MultihopAttention.png" alt="Multi-hop Attention" width="60%" height="auto">

<p>多跳注意力通过序列化处理逐步精炼查询和上下文：</p>
<ol>
<li><p><strong>工作机制</strong>：</p>
<ul>
<li>迭代更新查询：$q^{(s+1)} &#x3D; \text{transform}(q^{(s)}, c^{(s)})$</li>
<li>逐步积累信息：使用$[q^{(s+1)}, c^{(s)}]$作为新查询</li>
<li>可视为信息传递的”深度”处理</li>
</ul>
</li>
<li><p><strong>与多头注意力的区别</strong>：</p>
<table>
<thead>
<tr>
<th>特性</th>
<th>多头注意力</th>
<th>多跳注意力</th>
</tr>
</thead>
<tbody><tr>
<td>处理方式</td>
<td>并行</td>
<td>串行</td>
</tr>
<tr>
<td>计算效率</td>
<td>高</td>
<td>较低</td>
</tr>
<tr>
<td>信息整合</td>
<td>拼接</td>
<td>迭代精炼</td>
</tr>
<tr>
<td>典型应用</td>
<td>Transformer 编码层</td>
<td>Transformer 解码层</td>
</tr>
</tbody></table>
</li>
<li><p><strong>变体实现</strong>：</p>
<ul>
<li>使用相同权重矩阵的轻量级版本</li>
<li>结合自注意力机制的增强版本</li>
<li>在 LCR-Rot-hop 等模型中的应用</li>
</ul>
</li>
</ol>
<h4 id="胶囊注意力-Capsule-Based-Attention"><a href="#胶囊注意力-Capsule-Based-Attention" class="headerlink" title="胶囊注意力(Capsule-Based Attention)"></a>胶囊注意力(Capsule-Based Attention)</h4><img src="/img/Attention/CapsuleAttention.png" alt="Capsule Attention" width="60%" height="auto">

<p>Capsule-based attention通过将注意力机制与胶囊网络相结合，在多分类任务中展现出独特的优势，特别是在需要可解释性和细粒度分类的场景中。其模块化设计也便于扩展和调整，是注意力机制研究中的一个重要分支。</p>
<h5 id="基本概念与核心思想"><a href="#基本概念与核心思想" class="headerlink" title="基本概念与核心思想"></a>基本概念与核心思想</h5><p>Capsule-based attention是一种特殊的注意力机制，它通过为每个类别创建独立的”胶囊”(capsule)来处理多分类问题。每个胶囊本质上是一个独立的注意力模块，专门负责识别和提取与该类别相关的特征信息。</p>
<h6 id="胶囊网络基础"><a href="#胶囊网络基础" class="headerlink" title="胶囊网络基础"></a>胶囊网络基础</h6><p>Capsule-based attention源自胶囊网络(Capsule Networks)的概念，由Hinton等人于2017年提出。与传统神经网络使用标量神经元不同，胶囊网络使用向量形式的”胶囊”来表示实体及其属性。这种表示方式能够更好地捕捉特征间的空间层次关系。</p>
<h6 id="注意力机制的结合"><a href="#注意力机制的结合" class="headerlink" title="注意力机制的结合"></a>注意力机制的结合</h6><p>将注意力机制与胶囊网络结合，形成了capsule-based attention。这种机制的核心思想是：</p>
<ul>
<li>每个类别对应一个独立的注意力模块(胶囊)</li>
<li>每个胶囊学习自己独特的”查询”(query)，用于从输入特征中提取相关信息</li>
<li>通过注意力权重明确显示哪些特征对分类决策最重要</li>
</ul>
<h5 id="架构与工作流程"><a href="#架构与工作流程" class="headerlink" title="架构与工作流程"></a>架构与工作流程</h5><p>Capsule-based attention模型通常由三个主要组件构成：</p>
<ol>
<li><p><strong>注意力模块</strong></p>
<p> 每个胶囊c的注意力模块计算过程如下：</p>
<ol>
<li><p><strong>键值生成</strong>：将输入特征向量 $f_l$ 转换为键 $k_l$ 和值 $v_l$ ： $k_l &#x3D; W_K^{(c)} f_l, \quad v_l &#x3D; W_V^{(c)} f_l$<br> 其中$W_K^{(c)}$和$W_V^{(c)}$是类别特定的可训练权重矩阵</p>
</li>
<li><p><strong>注意力分数计算</strong>：使用查询向量 $q_l$ 计算注意力分数 $e_{c,l} &#x3D; q_c^T k_l$</p>
</li>
<li><p><strong>注意力权重计算</strong>：通过softmax归一化 $a_{c,l} &#x3D; \frac{\exp(e_{c,l})}{\sum_j \exp(e_{c,j})}$</p>
</li>
<li><p><strong>上下文向量生成</strong>：加权求和得到类别特定的上下文向量 $<br> c_c &#x3D; \sum_l a_{c,l} v_l$</p>
</li>
</ol>
</li>
<li><p><strong>概率模块</strong></p>
<p> 上下文向量经过一个简单的分类层，输出该类别的预测概率 $p_c &#x3D; \sigma(w_c^T c_c + b_c)$ ,其中 $\sigma$ 是sigmoid激活函数， $w_c$ 和 $b_c$ 是可训练参数。</p>
</li>
<li><p><strong>重构模块</strong></p>
<p> 最后将概率与上下文向量结合，生成类别表示向量 $r_c &#x3D; p_c \times c_c$</p>
</li>
</ol>
<h5 id="训练机制与损失函数"><a href="#训练机制与损失函数" class="headerlink" title="训练机制与损失函数"></a>训练机制与损失函数</h5><p>Capsule-based attention采用联合训练策略，优化两个目标：</p>
<ol>
<li><p><strong>分类损失</strong></p>
<p> 使用标准的交叉熵损失函数确保分类准确性 $\mathcal{L}_{cls} &#x3D; -\sum_c y_c \log p_c$ ，其中 $y_c$ 是类别 $c$ 的真实标签。</p>
</li>
<li><p><strong>重构损失</strong></p>
<p> 引入重构损失促使模型学习有意义的表示 $\mathcal{L}_{rec} &#x3D; \sum_c |r_c - \bar{f}|^2$ ，其中$\bar{f}$是所有输入特征向量的平均值。</p>
</li>
<li><p><strong>联合训练</strong></p>
<p> 总损失函数是两者的加权和 $\mathcal{L} &#x3D; \mathcal{L}_{cls} + \lambda \mathcal{L}_{rec}$ ，其中 $\lambda$ 是超参数，控制重构损失的重要性。</p>
</li>
</ol>
<h5 id="技术特点与优势"><a href="#技术特点与优势" class="headerlink" title="技术特点与优势"></a>技术特点与优势</h5><ol>
<li><p><strong>多查询机制</strong>：与传统注意力不同，capsule-based attention为每个类别学习独立的查询向量 $q_c$ ，使得模型能够并行处理多个分类任务，捕捉不同类别关注的不同特征，减少类别间的干扰。</p>
</li>
<li><p><strong>可解释性</strong>：通过分析注意力权重a_{c,l}，可以直观理解哪些输入特征对特定类别最重要、模型做出决策的依据以及不同类别关注的特征差异</p>
</li>
<li><p><strong>鲁棒性</strong>：重构损失项使模型学习到的表示更加鲁棒，迫使胶囊学习有意义的特征组合，减少对噪声特征的依赖，提高对抗样本的抵抗力</p>
</li>
</ol>
<h5 id="变体与扩展"><a href="#变体与扩展" class="headerlink" title="变体与扩展"></a>变体与扩展</h5><ol>
<li><p><strong>动态路由capsule</strong></p>
<p> 引入动态路由机制，迭代调整胶囊间的连接强度：</p>
<p> $$<br> \text{for iteration } t:<br> \quad b_{i,j} \leftarrow b_{i,j} + \hat{v}_j^T u_{j|i}<br> \quad c_{i,j} &#x3D; \text{softmax}(b_{i,j})<br> \quad s_j &#x3D; \sum_i c_{i,j} u_{j|i}<br> \quad v_j &#x3D; \text{squash}(s_j)<br> $$</p>
</li>
<li><p><strong>多头capsule</strong></p>
</li>
</ol>
<p>每个胶囊使用多个注意力头，捕获不同方面的信息 $r_c &#x3D; \text{concat}(r_c^{(1)}, …, r_c^{(h)})$</p>
<ol start="3">
<li><strong>层次化capsule</strong></li>
</ol>
<p>构建多级胶囊结构，从低级特征到高级语义逐步抽象：</p>
<p>$$<br>\text{低级胶囊} \rightarrow \text{中级胶囊} \rightarrow \text{高级胶囊}<br>$$</p>
<h5 id="与其他注意力机制的比较"><a href="#与其他注意力机制的比较" class="headerlink" title="与其他注意力机制的比较"></a>与其他注意力机制的比较</h5><table>
<thead>
<tr>
<th>特性</th>
<th>Capsule-based Attention</th>
<th>传统注意力</th>
<th>多头注意力</th>
</tr>
</thead>
<tbody><tr>
<td>查询数量</td>
<td>每个类别一个</td>
<td>通常一个</td>
<td>固定数量</td>
</tr>
<tr>
<td>参数共享</td>
<td>胶囊间不共享</td>
<td>完全共享</td>
<td>头间共享</td>
</tr>
<tr>
<td>可解释性</td>
<td>高</td>
<td>中等</td>
<td>低</td>
</tr>
<tr>
<td>计算成本</td>
<td>较高</td>
<td>低</td>
<td>中等</td>
</tr>
<tr>
<td>适用任务</td>
<td>多分类&#x2F;多标签</td>
<td>通用</td>
<td>序列处理</td>
</tr>
</tbody></table>
<h2 id="Transformer-架构中的查询机制"><a href="#Transformer-架构中的查询机制" class="headerlink" title="Transformer 架构中的查询机制"></a>Transformer 架构中的查询机制</h2><p>3.3 节特别强调了 Transformer 模型如何整合多种查询机制：</p>
<ol>
<li><p><strong>关键整合</strong>：</p>
<ul>
<li>多头自注意力并行处理</li>
<li>层间多跳式信息传递</li>
<li>查询-键-值分离的灵活设计</li>
</ul>
</li>
<li><p><strong>变体发展</strong>：</p>
<ul>
<li><strong>Transformer-XL</strong>：通过循环机制扩展上下文窗口</li>
<li><strong>Reformer</strong>：通过 LSH 哈希提升计算效率</li>
<li><strong>Linformer</strong>：低秩近似实现线性复杂度</li>
<li><strong>Synthesizer</strong>：探索非成对注意力权重</li>
</ul>
</li>
<li><p><strong>应用领域扩展</strong>：</p>
<ul>
<li>图像描述生成（Image Captioning）</li>
<li>医学图像分割</li>
<li>对话系统响应生成</li>
<li>推荐系统（如 BERT4Rec）</li>
</ul>
</li>
</ol>
<h2 id="查询机制的选择与实践建议"><a href="#查询机制的选择与实践建议" class="headerlink" title="查询机制的选择与实践建议"></a>查询机制的选择与实践建议</h2><ol>
<li><p><strong>机制选择准则</strong>：</p>
<table>
<thead>
<tr>
<th>任务需求</th>
<th>推荐机制</th>
<th>优势</th>
</tr>
</thead>
<tbody><tr>
<td>并行处理</td>
<td>多头注意力</td>
<td>计算效率高</td>
</tr>
<tr>
<td>深度特征交互</td>
<td>多跳注意力</td>
<td>信息整合深入</td>
</tr>
<tr>
<td>细粒度分类</td>
<td>胶囊注意力</td>
<td>可解释性强</td>
</tr>
<tr>
<td>长序列处理</td>
<td>Transformer-XL</td>
<td>上下文扩展</td>
</tr>
<tr>
<td>资源受限</td>
<td>Linformer</td>
<td>线性复杂度</td>
</tr>
</tbody></table>
</li>
<li><p><strong>实现注意事项</strong>：</p>
<ul>
<li>查询维度$d_q$应与键维度$d_k$匹配</li>
<li>多头注意力的头数需平衡效果与计算成本</li>
<li>多跳注意力的深度可能导致梯度消失</li>
<li>胶囊注意力适合中等规模类别数</li>
</ul>
</li>
<li><p><strong>新兴研究方向</strong>：</p>
<ul>
<li>动态查询路由机制</li>
<li>查询生成的条件控制</li>
<li>跨模态查询对齐</li>
<li>基于能量的查询选择</li>
</ul>
</li>
</ol>
<h1 id="📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒"><a href="#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒" class="headerlink" title="📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1><p><a href="/paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf" target="_blank">📄 Brauwers 和 Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a></p>
<a href="/2025/07/10/Attention/" title="Attention Overview">Attention Overview</a>
]]></content>
      <categories>
        <category>model</category>
        <category>attention</category>
        <category>category</category>
      </categories>
      <tags>
        <tag>CDR</tag>
        <tag>model</tag>
        <tag>Basic</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title>K-ON</title>
    <url>/2025/07/19/K-ON/</url>
    <content><![CDATA[<p>不知道写什么，先放个这个捏</p>
<table>
  <tr>
    <td><center><img src="\img\k-on\屏幕截图 2025-07-19 220432.png" width="500"></center></td>
    <td><center><img src="\img\k-on\kingzhenen.jpg" width="500"></center></td>
  </tr>
</table>

<span id="more"></span>


<h1 id="📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒"><a href="#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒" class="headerlink" title="📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1>]]></content>
      <categories>
        <category>Fun</category>
      </categories>
      <tags>
        <tag>乐子</tag>
      </tags>
  </entry>
  <entry>
    <title>hwadee</title>
    <url>/2025/07/13/hwadee/</url>
    <content><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><p>先放在这里，懒得喷，以下是鼠鼠和室友在争论某问题时问的AI</p>
<span id="more"></span>

<h1 id="甲醛有气味吗"><a href="#甲醛有气味吗" class="headerlink" title="甲醛有气味吗"></a>甲醛有气味吗</h1><p>Qus:<br>甲醛有气味吗<br>Ans:</p>
<ul>
<li>引用 7 篇资料作为参考<ol>
<li><a href="https://baike.baidu.com/item/%E7%94%B2%E9%86%9B/167033?fr=aladdin">甲醛（有机化合物）_百度百科</a></li>
<li><a href="http://baike.baidu.com/item/%E6%B0%B4%E9%93%B6/572967?fromModule=lemma_inlink">汞（是常温常压下唯一以液态存在的金属）_百度百科</a></li>
<li><a href="https://www.cndzys.com/ylcore/wendanew_info/11es473224.html">【甲醛有味吗】_徐信群_主任医师 - 大众养生网</a></li>
<li><a href="http://mp.weixin.qq.com/s?__biz=MzA4ODA4MjY4MA==&mid=2650899715&idx=2&sn=19f245dad403038c8fcbd5cc242b82c6&chksm=8a413842f16d27e52c9809f4dc290c194d4bdd131d46559db9eb9591a59757398eea9bb2a724#rd">装修完的新家闻不见味儿，就是没甲醛了？</a></li>
<li><a href="http://mp.weixin.qq.com/s?__biz=MzA5MDY2NTMzNg==&mid=2676835975&idx=3&sn=f0b45599c2c3427a8824a0f99159cc71&chksm=8bbb6131ad4acdd4f1b70ed654da5089241c06a940b56adc749d86160352f04e1d9f6b41e620#rd">甲醛有味道吗，甲醛的气味像什么？</a></li>
<li><a href="https://m.youlai.cn/sjingbian/article/5B3495gV5qk.html">甲醛有味还是无味-有来医生</a></li>
<li><a href="https://m.jia.com/zixun/article/1144566.html">装修的味道是甲醛吗_齐家网</a><br>甲醛的气味特性与其浓度密切相关，具体表现如下：</li>
</ol>
</li>
</ul>
<h3 id="1-低浓度时通常无味"><a href="#1-低浓度时通常无味" class="headerlink" title="1. 低浓度时通常无味"></a>1. 低浓度时通常无味</h3><ul>
<li>甲醛在极低浓度（如低于0.08毫克&#x2F;立方米）时，是一种无色无味的气体，难以通过嗅觉直接察觉。</li>
<li>许多人误以为“没味道&#x3D;没甲醛”，但实际上低浓度甲醛可能仍然存在，只是不易被闻到。</li>
</ul>
<h3 id="2-高浓度时有强烈刺激性气味"><a href="#2-高浓度时有强烈刺激性气味" class="headerlink" title="2. 高浓度时有强烈刺激性气味"></a>2. 高浓度时有强烈刺激性气味</h3><ul>
<li>当甲醛浓度超过0.1毫克&#x2F;立方米时，会与空气中的水、氧气反应，生成具有刺鼻气味的化合物，常被描述为：<ul>
<li>类似燃烧塑料或化学药品的味道；</li>
<li>锯木屑混合酸味（家具中甲醛）；</li>
<li>汽油味或烧焦塑料味（布料、窗帘中甲醛）；</li>
<li>部分人可能感知到淡淡的甜味，但主要仍是刺激性。</li>
</ul>
</li>
</ul>
<h3 id="3-个体差异显著"><a href="#3-个体差异显著" class="headerlink" title="3. 个体差异显著"></a>3. 个体差异显著</h3><ul>
<li>不同人对甲醛气味的敏感度差异较大，有些人可能在较低浓度就能察觉，而另一些人需要更高浓度才会感到不适。</li>
</ul>
<h3 id="4-与其他装修气味的区别"><a href="#4-与其他装修气味的区别" class="headerlink" title="4. 与其他装修气味的区别"></a>4. 与其他装修气味的区别</h3><ul>
<li>装修后的刺鼻气味常被误认为全是甲醛，但实际上可能混合了**苯系物（油漆味）、氨气（厕所味）、TVOC（化学溶剂味）**等。</li>
<li>甲醛的刺激性更偏向黏膜反应（如眼痒、喉咙痛），而苯系物等可能带有更明显的“化学香味”。</li>
</ul>
<h3 id="5-注意事项"><a href="#5-注意事项" class="headerlink" title="5. 注意事项"></a>5. 注意事项</h3><ul>
<li>不能依赖嗅觉判断甲醛安全性：即使闻不到，甲醛仍可能超标（尤其来自人造板材的长期释放）。</li>
<li>科学检测更可靠：家用检测仪易受环境干扰，建议通过专业机构检测。<br>总结：甲醛在低浓度下无味，高浓度时呈现刺鼻气味，但气味并非判断其存在的唯一标准，需结合专业检测和通风措施保障健康。</li>
</ul>
<h1 id="📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒"><a href="#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒" class="headerlink" title="📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1>]]></content>
      <categories>
        <category>sb</category>
      </categories>
      <tags>
        <tag>sb</tag>
        <tag>UESTC</tag>
      </tags>
  </entry>
</search>
