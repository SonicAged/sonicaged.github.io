<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.jpg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.jpg">
  <link rel="mask-icon" href="/images/favicon.jpg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"blog.sonic-aged.site","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.23.2","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"always","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="Is Attention All My Need ? 注意力机制在图神经网络中扮演着越来越重要的角色。但鼠鼠现在连正常的 Attention 有哪些都不清楚捏本文鼠鼠将从一般的 Attention 出发，给出 Attention 的总体结构，然后按分类介绍现有的主要的 Attention  本文主要来自于一篇论文，基本可以看作那篇论文的阅读笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="Attention Overview">
<meta property="og:url" content="https://blog.sonic-aged.site/2025/07/10/Attention/index.html">
<meta property="og:site_name" content="Sonic Aged&#39;s Blog">
<meta property="og:description" content="Is Attention All My Need ? 注意力机制在图神经网络中扮演着越来越重要的角色。但鼠鼠现在连正常的 Attention 有哪些都不清楚捏本文鼠鼠将从一般的 Attention 出发，给出 Attention 的总体结构，然后按分类介绍现有的主要的 Attention  本文主要来自于一篇论文，基本可以看作那篇论文的阅读笔记">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://blog.sonic-aged.site/img/Attention/TotalModel.png">
<meta property="og:image" content="https://blog.sonic-aged.site/img/Attention/GeneralAttentionModule.png">
<meta property="og:image" content="https://blog.sonic-aged.site/img/Attention/Taxonomy.png">
<meta property="article:published_time" content="2025-07-10T11:56:23.000Z">
<meta property="article:modified_time" content="2025-07-17T14:43:44.496Z">
<meta property="article:author" content="Sonic Aged">
<meta property="article:tag" content="CDR">
<meta property="article:tag" content="model">
<meta property="article:tag" content="Basic">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.sonic-aged.site/img/Attention/TotalModel.png">


<link rel="canonical" href="https://blog.sonic-aged.site/2025/07/10/Attention/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://blog.sonic-aged.site/2025/07/10/Attention/","path":"2025/07/10/Attention/","title":"Attention Overview"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Attention Overview | Sonic Aged's Blog</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script><script src="/js/pjax.js" defer></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.5.0/search.js" integrity="sha256-xFC6PJ82SL9b3WkGjFavNiA9gm5z6UBxWPiu4CYjptg=" crossorigin="anonymous" defer></script>
<script src="/js/third-party/search/local-search.js" defer></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.3.1/pdfobject.min.js","integrity":"sha256-jI72I8ZLVflVOisZIOaLvRew3tyvzeu6aZXFm7P7dEo="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js" defer></script>






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js" defer></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Sonic Aged's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">A blog about Shit and more Shit</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Is-Attention-All-My-Need"><span class="nav-number">1.</span> <span class="nav-text">Is Attention All My Need ?</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%F0%9F%8E%AF-%E5%BC%95%E8%A8%80"><span class="nav-number">1.1.</span> <span class="nav-text">🎯 引言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%F0%9F%93%9A-%E6%80%BB%E8%A7%88-Attention"><span class="nav-number">1.2.</span> <span class="nav-text">📚 总览 Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Attention-%E7%9A%84%E4%B8%80%E8%88%AC%E7%BB%93%E6%9E%84"><span class="nav-number">1.2.1.</span> <span class="nav-text">Attention 的一般结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E5%A4%84%E7%90%86%E6%9C%BA%E5%88%B6"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">输入处理机制</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BE%93%E5%87%BA%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%88%B6"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">输出计算机制</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Attention-%E5%88%86%E7%B1%BB"><span class="nav-number">1.2.2.</span> <span class="nav-text">Attention 分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%8E%E6%A0%B7%E8%AF%84%E4%BB%B7-Attention"><span class="nav-number">1.2.3.</span> <span class="nav-text">怎样评价 Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%96%E5%9C%A8%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">外在性能评估</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%85%E5%9C%A8%E7%89%B9%E6%80%A7%E8%AF%84%E4%BC%B0"><span class="nav-number">1.2.3.2.</span> <span class="nav-text">内在特性评估</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%A7%A3%E9%87%8A%E6%80%A7%E8%AF%84%E4%BC%B0%E3%80%81"><span class="nav-number">1.2.3.3.</span> <span class="nav-text">注意力解释性评估、</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%F0%9F%93%9A-%F0%9D%92%A5%F0%9D%91%92%F0%9D%92%BB%F0%9D%91%92%F0%9D%93%87%F0%9D%91%92%F0%9D%93%83%F0%9D%92%B8%F0%9D%91%92"><span class="nav-number">2.</span> <span class="nav-text">📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Sonic Aged"
      src="/images/favicon.jpg">
  <p class="site-author-name" itemprop="name">Sonic Aged</p>
  <div class="site-description" itemprop="description">A blog about Shit and more Shit</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/SonicAged" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;SonicAged" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:sonicaged404@gmail.com" title="E-Mail → mailto:sonicaged404@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blog.sonic-aged.site/2025/07/10/Attention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.jpg">
      <meta itemprop="name" content="Sonic Aged">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sonic Aged's Blog">
      <meta itemprop="description" content="A blog about Shit and more Shit">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Attention Overview | Sonic Aged's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Attention Overview
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-07-10 19:56:23" itemprop="dateCreated datePublished" datetime="2025-07-10T19:56:23+08:00">2025-07-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-07-17 22:43:44" itemprop="dateModified" datetime="2025-07-17T22:43:44+08:00">2025-07-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/model/" itemprop="url" rel="index"><span itemprop="name">model</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/model/attention/" itemprop="url" rel="index"><span itemprop="name">attention</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>2.3k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>9 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="Is-Attention-All-My-Need"><a href="#Is-Attention-All-My-Need" class="headerlink" title="Is Attention All My Need ?"></a>Is Attention All My Need ?</h1><blockquote>
<p>注意力机制在图神经网络中扮演着越来越重要的角色。<del>但鼠鼠现在连正常的 Attention 有哪些都不清楚捏</del>本文鼠鼠将从一般的 Attention 出发，给出 Attention 的总体结构，然后按分类介绍现有的主要的 Attention</p>
</blockquote>
<p>本文主要来自于一篇论文，基本可以看作<a href="/paper/Brauwers%E5%92%8CFrasincar%20-%202023%20-%20A%20General%20Survey%20on%20Attention%20Mechanisms%20in%20Deep%20Learning.pdf">那篇论文</a>的阅读笔记</p>
<span id="more"></span>

<h2 id="🎯-引言"><a href="#🎯-引言" class="headerlink" title="🎯 引言"></a>🎯 引言</h2><p>在深度学习领域，注意力机制已经成为一个革命性的创新，特别是在处理序列数据和图像数据方面取得了巨大成功。而在图神经网络中，注意力机制的引入不仅提高了模型的表现力，还增强了模型的可解释性。</p>
<p>在图结构数据中应用注意力机制主要有以下优势：</p>
<ol>
<li>自适应性：能够根据任务动态调整不同邻居节点的重要性</li>
<li>可解释性：通过注意力权重可以直观理解模型的决策过程</li>
<li>长程依赖：有效缓解了传统 GNN 中的过平滑问题</li>
<li>异质性处理：更好地处理异质图中的不同类型节点和边</li>
</ol>
<h2 id="📚-总览-Attention"><a href="#📚-总览-Attention" class="headerlink" title="📚 总览 Attention"></a>📚 总览 Attention</h2><p>本章节主要参考了论文<a href="/paper/Brauwers%E5%92%8CFrasincar%20-%202023%20-%20A%20General%20Survey%20on%20Attention%20Mechanisms%20in%20Deep%20Learning.pdf">📄 Brauwers 和 Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a>有兴趣的话可以看看原文捏</p>
<embed src="/paper/Brauwers和Frasincar%20-%202023%20-%20A%20General%20Survey%20on%20Attention%20Mechanisms%20in%20Deep%20Learning.pdf" width="45%" height="400" type="application/pdf">

<h3 id="Attention-的一般结构"><a href="#Attention-的一般结构" class="headerlink" title="Attention 的一般结构"></a>Attention 的一般结构</h3><img src="/img/Attention/TotalModel.png" alt="TotalModel" width="60%" height="auto">

<p>上图是从总体上看 Attention 在整个任务模型框架中的位置</p>
<p>框架包含四个核心组件：</p>
<ol>
<li><strong>特征模型</strong>：负责输入数据的特征提取</li>
<li><strong>查询模型</strong>：生成注意力查询向量</li>
<li><strong>注意力模型</strong>：计算注意力权重</li>
<li><strong>输出模型</strong>：生成最终预测结果</li>
</ol>
<p>接下来，我们会从 <em>输入</em> 的角度来看<strong>特征模型</strong>和<strong>查询模型</strong>，从 <em>输出</em> 的角度来看<strong>注意力模型</strong>和<strong>输出模型</strong></p>
<h4 id="输入处理机制"><a href="#输入处理机制" class="headerlink" title="输入处理机制"></a>输入处理机制</h4><ol>
<li><p><strong>特征模型</strong>，即将任务的输入进行 embedding</p>
<p>对于输入矩阵 $ X \in \mathbb{R}^{d_x \times n_x} $ ，特征模型提取特征向量： $\boldsymbol{F} &#x3D; [f_1, \ldots, f_{n_f}] \in \mathbb{R}^{d_f \times n_f}$</p>
</li>
<li><p><strong>查询模型</strong>，查询模型产生查询向量$ \boldsymbol{q} \in \mathbb{R}^{d_q} $，用以告诉注意力模型哪一个特征是重要的</p>
</li>
</ol>
<p>一般情况下，这两个模型可以用 CNN 或 RNN</p>
<h4 id="输出计算机制"><a href="#输出计算机制" class="headerlink" title="输出计算机制"></a>输出计算机制</h4><img src="/img/Attention/GeneralAttentionModule.png" alt="GeneralAttentionModule" width="50%" height="auto">

<p>上图是 Attention 模型总体结构的说明，下面对这张图进行详细的说明</p>
<ol>
<li>特征矩阵$\boldsymbol{F} &#x3D; [\boldsymbol{f}_1, \ldots, \boldsymbol{f}_{n_f}] \in \mathbb{R}^{d_f \times n_f}$，通过<em>某些方法</em>将其分为 Keys 矩阵$\boldsymbol{K} &#x3D; [\boldsymbol{k}_1, \ldots, \boldsymbol{k}_{n_f}] \in \mathbb{R}^{d_k \times n_f}$和 Values 矩阵$\boldsymbol{V} &#x3D; [\boldsymbol{v}_1, \ldots, \boldsymbol{v}_{n_f}] \in \mathbb{R}^{d_v \times n_f}$，这里的<em>某些方法</em>，一般情况下，按以下的方式通过<strong>线性变换</strong>得到：</li>
</ol>
<p>$$<br>\underset{d_{k} \times n_{f}}{\boldsymbol{K}}&#x3D;\underset{d_{k} \times d_{f}}{\boldsymbol{W}_{K}} \times \underset{d_{f} \times n_{f}}{\boldsymbol{F}}, \quad \underset{d_{v} \times n_{f}}{\boldsymbol{V}}&#x3D;\underset{d_{v} \times d_{f}}{\boldsymbol{W}_{V}} \times \underset{d_{f} \times n_{f}}{\boldsymbol{F}} .<br>$$</p>
<ol start="2">
<li><p><code>Attention Scores</code>模块根据 $\boldsymbol{q}$ 计算每一个 key 向量对应的分数$\boldsymbol{e} &#x3D; [e_1, \ldots, e_{n_f}] \in \mathbb{R}^{n_f}$：</p>
<p>$$<br>\underset{1\times 1}{e_l} &#x3D; \text{score}(\underset{d_q \times 1}{\boldsymbol{q}}, \underset{d_k \times 1}{\boldsymbol{k}_l})<br>$$</p>
<p>如前所述，查询象征着对信息的请求。注意力分数$e_l$表示根据查询，关键向量$\boldsymbol{k}_l$中包含的信息的重要性。如果查询和关键向量的维度相同，则得分函数的一个例子是取向量的点积。</p>
</li>
<li><p>由于经过这么一堆操作之后，分数有很大的可能已经飞起来了捏，这个时候就需要<code>Attention Alignment</code>模块对其进行<strong>归一化</strong>之类的操作了捏</p>
<p>$$<br>\underset{1\times 1}{a_l} &#x3D; \text{align}(\underset{d_q \times 1}{\boldsymbol{e_l}}, \underset{n_f \times 1}{\boldsymbol{e}})<br>$$</p>
</li>
</ol>
<p>注意力权重$\boldsymbol{a} &#x3D; [a_1, \ldots, a_{n_f}] \in \mathbb{R}^{n_f}$为注意力模块提供了一个相当直观的解释。每个权重直接表明了每个特征向量相对于其他特征向量对于这个问题的重要性。</p>
<ol start="4">
<li><p>在<code>Weight Average</code>模块完成<strong>上下文生成</strong>：</p>
<p>$$<br>\underset{d_v \times 1}{\boldsymbol{c}} &#x3D; \sum_{l &#x3D; 1}^{n_f} \underset{1 \times 1}{a_l}\times \underset{d_v \times 1}{\boldsymbol{v}_l}<br>$$</p>
</li>
<li><p>输出处理就想怎么搞就怎么搞了捏，例如 用于分类</p>
<p>$$<br>\underset{d_y \times 1}{\hat{\boldsymbol{y}}} &#x3D; \text{softmax}( \underset{d_y \times d_v}{\boldsymbol{W}_c}\times \underset{d_v \times 1}{\boldsymbol{c}} + \underset{d_y \times 1}{\boldsymbol{b}_c})<br>$$</p>
</li>
</ol>
<h3 id="Attention-分类"><a href="#Attention-分类" class="headerlink" title="Attention 分类"></a>Attention 分类</h3><img src="/img/Attention/Taxonomy.png" style="max-width: 100%; height: auto;">

<p>论文按照上图的方式给 Attention 进行了分类</p>
<p>由于篇幅限制，这里决定重开几个博文来分别介绍这些 Attention，链接如下：</p>
<a href="/2025/07/14/Feature-Related-Attention/" title="Fufufu Relashinala">Fufufu Relashinala</a>
<br/>
<a href="/2025/07/16/General-Attention/" title="Gugugu Neralashun">Gugugu Neralashun</a>
<br/>
<a href="/2025/07/17/Query-Related-Attention/" title="Quaqua Rishinala">Quaqua Rishinala</a>

<h3 id="怎样评价-Attention"><a href="#怎样评价-Attention" class="headerlink" title="怎样评价 Attention"></a>怎样评价 Attention</h3><h4 id="外在性能评估"><a href="#外在性能评估" class="headerlink" title="外在性能评估"></a>外在性能评估</h4><ol>
<li><strong>领域特定的评估指标</strong></li>
</ol>
<p>不同领域用于评估注意力模型性能的指标：</p>
<table>
<thead>
<tr>
<th>领域</th>
<th>常用评估指标</th>
<th>典型应用</th>
</tr>
</thead>
<tbody><tr>
<td>自然语言处理</td>
<td>BLEU, METEOR, Perplexity</td>
<td>机器翻译、文本生成</td>
</tr>
<tr>
<td>语音处理</td>
<td>词错误率(WER)、音素错误率(PER)</td>
<td>语音识别</td>
</tr>
<tr>
<td>计算机视觉</td>
<td>PSNR, SSIM, IoU</td>
<td>图像生成、分割</td>
</tr>
<tr>
<td>通用分类</td>
<td>准确率、精确率、召回率、F1</td>
<td>情感分析、文档分类</td>
</tr>
</tbody></table>
<ol start="2">
<li><p><strong>消融研究</strong></p>
<p> 论文强调了消融研究(ablation study)在评估注意力机制重要性方面的价值。典型做法包括：</p>
<ol>
<li>移除或替换注意力机制（如用平均池化代替注意力池化）</li>
<li>比较模型在有无注意力机制时的性能差异</li>
<li>分析不同注意力变体对最终性能的影响</li>
</ol>
<p> 这种评估方法可以明确注意力机制对模型性能的实际贡献，而不仅仅是展示最终结果。</p>
</li>
</ol>
<h4 id="内在特性评估"><a href="#内在特性评估" class="headerlink" title="内在特性评估"></a>内在特性评估</h4><ol>
<li><p><strong>注意力权重分析</strong></p>
<ol>
<li><strong>对齐错误率(AER)</strong>：比较模型生成的注意力权重与人工标注的”黄金”注意力权重之间的差异</li>
<li><strong>监督注意力训练</strong>：将人工标注的注意力权重作为额外监督信号，与任务损失联合训练</li>
<li><strong>注意力可视化</strong>：通过热图等方式直观展示模型关注区域</li>
</ol>
<p>这些方法可以评估注意力权重是否符合人类直觉或领域知识。</p>
</li>
<li><p><strong>基于人类注意力的评估</strong></p>
<p> 论文提出了”注意力正确性”(Attention Correctness)的概念，将模型的注意力模式与真实人类注意力行为进行比较：</p>
<ol>
<li><strong>数据收集</strong>：记录人类在执行相同任务时的注意力模式（如眼动追踪）</li>
<li><strong>度量计算</strong>：定义模型注意力与人类注意力的相似度指标</li>
<li><strong>联合训练</strong>：将人类注意力数据作为监督信号</li>
</ol>
<p> 这种评估方法基于认知科学原理，认为好的注意力模型应该模拟人类的注意力机制。</p>
</li>
</ol>
<h4 id="注意力解释性评估、"><a href="#注意力解释性评估、" class="headerlink" title="注意力解释性评估、"></a>注意力解释性评估、</h4><p>论文讨论了学术界关于”注意力是否提供解释”的争论：</p>
<ol>
<li><p><strong>“Attention is not Explanation”观点</strong></p>
<ul>
<li>注意力权重与模型决策之间缺乏稳定关联</li>
<li>可以构造对抗性注意力分布而不改变模型输出</li>
<li>注意力权重可能反映相关性而非因果性</li>
</ul>
</li>
<li><p><strong>“Attention is not not Explanation”反驳</strong></p>
<ul>
<li>对抗性注意力分布通常性能更差</li>
<li>注意力权重确实反映了输入的相对重要性</li>
<li>在特定架构下注意力可以提供有意义的解释</li>
</ul>
</li>
</ol>
<p><del>这段比较难绷，因此把</del>原文贴在下面了捏</p>
<blockquote>
<p>However, rather than checking if the model focuses on the most important parts of the data, some use the attention weights to determine which parts of the data are most important. This would imply that attention models provide a type of explanation, which is a subject of contention among researchers. Particularly, in [120], extensive experiments are conducted for various natural language processing tasks to investigate the relation between attention weights and important information to determine whether attention can actually provide meaningful explanations. In this paper titled “Attention is not Explanation”, it is found that attention weights do not tend to correlate with important features. Additionally, the authors are able to replace the produced attention weights with completely different values while keeping the model output the same. These so-called “adversarial” attention distributions show that an attention model may focus on completely different information and still come to the same conclusions, which makes interpretation difficult. Yet, in another paper titled “Attention is not not Explanation” [121], the claim that attention is not explanation is questioned by challenging the assumptions of the previous work. It is found that the adversarial attention distributions do not perform as reliably well as the learned attention weights, indicating that it was not proved that attention is not viable for explanation. In general, the conclusion regarding the interpretability of attention models is that researchers must be extremely careful when drawing conclusions based on attention patterns. For example, problems with an attention model can be diagnosed via the attention weights if the model is found to focus on the incorrect parts of the data, if such information is available. Yet, conversely, attention weights may only be used to obtain plausible explanations for why certain parts of the data are focused on, rather than concluding that those parts are significant to the problem [121]. However, one should still be cautious as the viability of such approaches can depend on the model architecture [122].</p>
</blockquote>
<h1 id="📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒"><a href="#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒" class="headerlink" title="📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1><p><a href="/paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf" target="_blank">📄 Brauwers 和 Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/CDR/" rel="tag"># CDR</a>
              <a href="/tags/model/" rel="tag"># model</a>
              <a href="/tags/Basic/" rel="tag"># Basic</a>
              <a href="/tags/deep-learning/" rel="tag"># deep learning</a>
              <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/07/10/GNN-and-GCN/" rel="prev" title="What Is GNN and GCN ?">
                  <i class="fa fa-angle-left"></i> What Is GNN and GCN ?
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/07/13/hwadee/" rel="next" title="hwadee">
                  hwadee <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Sonic Aged</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Word count total">23k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">1:23</span>
  </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
