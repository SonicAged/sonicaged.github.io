{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/css/noscript.styl","path":"css/noscript.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon.jpg","path":"images/favicon.jpg","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo-algolia-nebula-blue-full.svg","path":"images/logo-algolia-nebula-blue-full.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/js/bookmark.js","path":"js/bookmark.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/comments-buttons.js","path":"js/comments-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/comments.js","path":"js/comments.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/config.js","path":"js/config.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/motion.js","path":"js/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/next-boot.js","path":"js/next-boot.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/pjax.js","path":"js/pjax.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/schedule.js","path":"js/schedule.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/sidebar.js","path":"js/sidebar.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/addtoany.js","path":"js/third-party/addtoany.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/fancybox.js","path":"js/third-party/fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/pace.js","path":"js/third-party/pace.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/quicklink.js","path":"js/third-party/quicklink.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/analytics/baidu-analytics.js","path":"js/third-party/analytics/baidu-analytics.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/analytics/google-analytics.js","path":"js/third-party/analytics/google-analytics.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/analytics/growingio.js","path":"js/third-party/analytics/growingio.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/analytics/matomo.js","path":"js/third-party/analytics/matomo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/chat/chatra.js","path":"js/third-party/chat/chatra.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/chat/tidio.js","path":"js/third-party/chat/tidio.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/comments/changyan.js","path":"js/third-party/comments/changyan.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/comments/disqus.js","path":"js/third-party/comments/disqus.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/comments/disqusjs.js","path":"js/third-party/comments/disqusjs.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/comments/gitalk.js","path":"js/third-party/comments/gitalk.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/comments/isso.js","path":"js/third-party/comments/isso.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/comments/livere.js","path":"js/third-party/comments/livere.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/comments/utterances.js","path":"js/third-party/comments/utterances.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/math/katex.js","path":"js/third-party/math/katex.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/math/mathjax.js","path":"js/third-party/math/mathjax.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/search/algolia-search.js","path":"js/third-party/search/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/search/local-search.js","path":"js/third-party/search/local-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/statistics/lean-analytics.js","path":"js/third-party/statistics/lean-analytics.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/tags/mermaid.js","path":"js/third-party/tags/mermaid.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/statistics/firestore.js","path":"js/third-party/statistics/firestore.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/tags/pdf.js","path":"js/third-party/tags/pdf.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/tags/wavedrom.js","path":"js/third-party/tags/wavedrom.js","modified":0,"renderable":1},{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"source/paper/1609.02907v4.pdf","path":"paper/1609.02907v4.pdf","modified":0,"renderable":0},{"_id":"source/paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf","path":"paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf","modified":0,"renderable":0},{"_id":"source/paper/Lee 等 - 2018 - Attention Models in Graphs A Survey.pdf","path":"paper/Lee 等 - 2018 - Attention Models in Graphs A Survey.pdf","modified":0,"renderable":0},{"_id":"source/paper/Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends.pdf","path":"paper/Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends.pdf","modified":0,"renderable":0},{"_id":"source/img/CDR-data-analysis/comprehensive_bipartite_analysis_ccle.png","path":"img/CDR-data-analysis/comprehensive_bipartite_analysis_ccle.png","modified":0,"renderable":0},{"_id":"source/img/CDR-data-analysis/comprehensive_bipartite_analysis_gdsc.png","path":"img/CDR-data-analysis/comprehensive_bipartite_analysis_gdsc.png","modified":0,"renderable":0},{"_id":"source/img/CDR-data-analysis/gene.png","path":"img/CDR-data-analysis/gene.png","modified":0,"renderable":0},{"_id":"source/nodeppt/test/test_nodeppt.html","path":"nodeppt/test/test_nodeppt.html","modified":0,"renderable":0},{"_id":"source/img/Attention/AlternatingCo-Attention.png","path":"img/Attention/AlternatingCo-Attention.png","modified":0,"renderable":0},{"_id":"source/img/Attention/AttentionViaAttention.png","path":"img/Attention/AttentionViaAttention.png","modified":0,"renderable":0},{"_id":"source/img/Attention/CapsuleAttention.png","path":"img/Attention/CapsuleAttention.png","modified":0,"renderable":0},{"_id":"source/img/Attention/GeneralAttentionModule.png","path":"img/Attention/GeneralAttentionModule.png","modified":0,"renderable":0},{"_id":"source/img/Attention/InteractiveCo-Attention.png","path":"img/Attention/InteractiveCo-Attention.png","modified":0,"renderable":0},{"_id":"source/img/Attention/HierarchicalAttention.png","path":"img/Attention/HierarchicalAttention.png","modified":0,"renderable":0},{"_id":"source/img/Attention/MultiheadAttention.png","path":"img/Attention/MultiheadAttention.png","modified":0,"renderable":0},{"_id":"source/img/Attention/MultihopAttention.png","path":"img/Attention/MultihopAttention.png","modified":0,"renderable":0},{"_id":"source/img/Attention/ParallelCo-Attention.png","path":"img/Attention/ParallelCo-Attention.png","modified":0,"renderable":0},{"_id":"source/img/Attention/Taxonomy.png","path":"img/Attention/Taxonomy.png","modified":0,"renderable":0},{"_id":"source/img/Attention/TotalModel.png","path":"img/Attention/TotalModel.png","modified":0,"renderable":0},{"_id":"source/code/data_analysis/visualize_graph_analysis.py","path":"code/data_analysis/visualize_graph_analysis.py","modified":0,"renderable":0},{"_id":"source/nodeppt/test/css/chunk-vendors.4e4765ff.css","path":"nodeppt/test/css/chunk-vendors.4e4765ff.css","modified":0,"renderable":0},{"_id":"source/nodeppt/test/img/swipe.svg","path":"nodeppt/test/img/swipe.svg","modified":0,"renderable":0},{"_id":"source/nodeppt/test/js/chunk-vendors.js","path":"nodeppt/test/js/chunk-vendors.js","modified":0,"renderable":0},{"_id":"source/nodeppt/test/js/chunk-vendors.js.LICENSE.txt","path":"nodeppt/test/js/chunk-vendors.js.LICENSE.txt","modified":0,"renderable":0},{"_id":"source/nodeppt/test/js/test_nodeppt.js.LICENSE.txt","path":"nodeppt/test/js/test_nodeppt.js.LICENSE.txt","modified":0,"renderable":0},{"_id":"source/nodeppt/test/js/test_nodeppt.js","path":"nodeppt/test/js/test_nodeppt.js","modified":0,"renderable":0}],"Cache":[{"_id":"source/CNAME","hash":"9f72a1c93d67b273a3f1b776b7607fca8ecb9800","modified":1752079696958},{"_id":"source/_posts/CDR-data-analysis.md","hash":"8644bad2329b52211ff2cb500e250e8520faba0b","modified":1752203256876},{"_id":"source/_posts/Feature-Related-Attention.md","hash":"e824e3aef7345d4dce5621d411b0b39c6f4b452c","modified":1752507628077},{"_id":"source/_posts/Attention.md","hash":"e185f328a3f65a52c38f3461875b863394a52748","modified":1752503909037},{"_id":"source/_posts/GAT.md","hash":"754ff81781e96a4d44e354220c674049e31db8ab","modified":1752503264356},{"_id":"source/_posts/GNN-and-GCN.md","hash":"ee88582d6b11ede46e8a61b3079d5004248f4c4c","modified":1752303156342},{"_id":"source/_posts/PEP-8.md","hash":"f96ac45169ffe57dd13682067fa26cf947be180c","modified":1752305613250},{"_id":"source/categories/index.md","hash":"893f97551f11e6bc689619cb961451636f68a0f4","modified":1752079696960},{"_id":"source/_posts/hwadee.md","hash":"7d3609b57c066a91fd3ed0e3bcaec8396bd9172b","modified":1752415131190},{"_id":"source/about/index.md","hash":"d59fba8d07a50609ba30828f459552bfa73cc1cb","modified":1752079696959},{"_id":"source/tags/index.md","hash":"99e0b192987cfef15ec25ae1040926c14469828f","modified":1752079696989},{"_id":"source/nodeppt/test/test_nodeppt.html","hash":"861eec2e1a66ee182a5a8ec962c2ad049e5d7908","modified":1752290420910},{"_id":"source/img/CDR-data-analysis/gene.png","hash":"dac1e1481add33e6e471f62deb153719ff929a3a","modified":1752124411429},{"_id":"source/code/data_analysis/visualize_graph_analysis.py","hash":"f7ed9f01a38fe104b17464bf6e2dc7c5093ca7dc","modified":1752203256879},{"_id":"source/nodeppt/test/img/swipe.svg","hash":"47458d9726f189ff19f7f224d24fdc60f6af1c82","modified":1752290420907},{"_id":"source/nodeppt/test/js/chunk-vendors.js","hash":"23c28d1f1615efdfe92ed6c634a9366a4aabb9b1","modified":1752290420908},{"_id":"source/nodeppt/test/js/chunk-vendors.js.LICENSE.txt","hash":"d6fa3279c3fc7ec7562c7dcb614253f886938c32","modified":1752290420909},{"_id":"source/nodeppt/test/js/test_nodeppt.js.LICENSE.txt","hash":"d6fa3279c3fc7ec7562c7dcb614253f886938c32","modified":1752290420910},{"_id":"source/nodeppt/test/js/test_nodeppt.js","hash":"a0d1819335072875ac9197f43d7c09511722a9c8","modified":1752290420909},{"_id":"source/img/Attention/AlternatingCo-Attention.png","hash":"514aa65627d99a19c982ec47a9ebd8a66029d07f","modified":1752505028735},{"_id":"source/img/Attention/AttentionViaAttention.png","hash":"eee60186ed279182176b247f00e6243b707c54a6","modified":1752290420881},{"_id":"source/img/Attention/InteractiveCo-Attention.png","hash":"d66ad9309b6851348104b591f7a265f55ba4aae4","modified":1752290420892},{"_id":"source/img/Attention/MultiheadAttention.png","hash":"1d61e6a59d33b5b70521acf9b1e8b191d04a7278","modified":1752290420894},{"_id":"source/img/Attention/ParallelCo-Attention.png","hash":"1389f902e31640d6ff97a4375804b5b443706c02","modified":1752290420898},{"_id":"source/img/Attention/TotalModel.png","hash":"b9c32980245dcf7bb4e9e54b04b59ccaa14bf265","modified":1752290420905},{"_id":"source/nodeppt/test/css/chunk-vendors.4e4765ff.css","hash":"cb37dab7a01358273812ab41d4325c5c8e3c5af9","modified":1752290420907},{"_id":"source/code/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.md","hash":"2ab6e1f94c8bfa061f5408a2a42354a2e4cd5c33","modified":1752504410000},{"_id":"source/img/Attention/GeneralAttentionModule.png","hash":"9e9b439faf1e0e38564fc8eab3364f8e6938cf6e","modified":1752290420888},{"_id":"source/img/Attention/HierarchicalAttention.png","hash":"ddad89428142f48353d6338f6d69402ec9b0d95e","modified":1752290420890},{"_id":"source/img/Attention/MultihopAttention.png","hash":"9bb72595c6b04f2157d01b069d004eaa94d71efa","modified":1752290420896},{"_id":"source/img/Attention/CapsuleAttention.png","hash":"8d29b9e962f65aea416727e5e0d321cf00e718f1","modified":1752290420886},{"_id":"themes/next/.editorconfig","hash":"731c650ddad6eb0fc7c3d4a91cad1698fe7ad311","modified":1752079696991},{"_id":"themes/next/.gitattributes","hash":"aeeca2f1e987d83232d7870d1435a4e3ed66b648","modified":1752079696991},{"_id":"themes/next/.gitignore","hash":"087b7677078303acb2acb47432165950e4d29b43","modified":1752079697000},{"_id":"themes/next/.stylelintrc","hash":"20f46858e47aba6b3fc47e9b43e9f7531fa66e33","modified":1752079697000},{"_id":"themes/next/README.md","hash":"cfafa81c8df2c64b0db6a14c7f5e0bb014936b36","modified":1752079697000},{"_id":"themes/next/_config.yml","hash":"6eebbc6c763bf07971f201edeaa548701b2c7f73","modified":1752310086668},{"_id":"themes/next/LICENSE.md","hash":"8cfb03967dd4cbaf3b825271ffce0039aa3fc22a","modified":1752079697000},{"_id":"themes/next/_vendors.yml","hash":"0c3c1b4e3e5f89985aca0e24cc8dad496e737634","modified":1752079697000},{"_id":"themes/next/crowdin.yml","hash":"4a53f5985e545c635cb56b2a57ed290cb8cf8942","modified":1752079697000},{"_id":"themes/next/package.json","hash":"761cf8d5dd2a376b18a088d9e908ea75d9270b58","modified":1752079697041},{"_id":"themes/next/renovate.json","hash":"767b077c7b615e20af3cf865813cd64674a9bea6","modified":1752079697041},{"_id":"themes/next/.githooks/install.js","hash":"72757c6827909a5f2c217ddbbdf6034ca6fab74a","modified":1752079696992},{"_id":"themes/next/.githooks/pre-commit","hash":"b69b9d0b51e27d5d4c87c3242f5067c2cda26e44","modified":1752079696992},{"_id":"themes/next/.github/CODE_OF_CONDUCT.md","hash":"82a3d202da8fe6ce765715bd5b7571676e90364d","modified":1752079696992},{"_id":"themes/next/eslint.config.js","hash":"e35570c8e7ef9ea4adad7bafb3558dfd8e928e48","modified":1752079697006},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"2fdca1040427cabfe27cae6754ec5e027ec7092e","modified":1752079696993},{"_id":"themes/next/.github/issue_label_bot.yaml","hash":"533fbe6b2f87d7e7ec6949063bb7ea7eb4fbe52d","modified":1752079696995},{"_id":"themes/next/.github/config.yml","hash":"0956bf71b6f36632b63b14d26580458041a5abd2","modified":1752079696995},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"610675119f83cdbf3f19d7996b17e33062e3b165","modified":1752079696995},{"_id":"themes/next/.github/label-commenter-config.yml","hash":"d403cbbbd0c554563e9e678ff82120ef9451d98a","modified":1752079696996},{"_id":"themes/next/.github/labeler.yml","hash":"35da1a37e9a6bdee1eeae30a8816cd75c9a04b3b","modified":1752079696996},{"_id":"themes/next/.github/release.yml","hash":"83b4dae3f8d76619e208d2110a247b3ccadd64d8","modified":1752079696996},{"_id":"themes/next/docs/AGPL3.md","hash":"f463f95b169d64983f59fa6f3e4b6760290a0e6b","modified":1752079697000},{"_id":"themes/next/docs/LICENSE.txt","hash":"d1cd5a8e83d3bbdb50f902d2b487813da95ddfd3","modified":1752079697000},{"_id":"themes/next/docs/AUTHORS.md","hash":"579014d47f45b27fd1618b9709f0efe9585c7449","modified":1752079697000},{"_id":"themes/next/languages/ar.yml","hash":"c77fc13e0431e71eeb0f767a0a436284fc81df90","modified":1752079697007},{"_id":"themes/next/languages/README.md","hash":"b1c96465b3bc139bf5ba6200974b66581d8ff85a","modified":1752079697007},{"_id":"themes/next/languages/bn.yml","hash":"30ffd43588ddf2cd39432d964087242e1c3e5407","modified":1752079697008},{"_id":"themes/next/languages/de.yml","hash":"5101612f7cac38884206d28319532f1aab32fbe6","modified":1752079697008},{"_id":"themes/next/languages/default.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1752079697009},{"_id":"themes/next/languages/en.yml","hash":"5ca2c0b3c95dd87485f2d2b3477f88810ad1a63b","modified":1752079697009},{"_id":"themes/next/languages/es.yml","hash":"31c2a47d14cd5b804cae0c0b44d43bece069d3aa","modified":1752079697010},{"_id":"themes/next/languages/fa.yml","hash":"cc6ef3144b954daa29ce7d42f08191eafe052fab","modified":1752079697010},{"_id":"themes/next/languages/fr.yml","hash":"3a7469446f77e5d1a12eee828ebc49ecb43b6c35","modified":1752079697011},{"_id":"themes/next/languages/id.yml","hash":"bd2c30def05a37c66653900ca07952d52a142961","modified":1752079697011},{"_id":"themes/next/languages/eo.yml","hash":"7bd0124c2d1dcdbfde350fce335e130556ebcee8","modified":1752079697009},{"_id":"themes/next/languages/it.yml","hash":"4969c4f17d88003135218de72ddf0bf8d894dc07","modified":1752079697011},{"_id":"themes/next/languages/ja.yml","hash":"e660ec12b9eade103492aba7aa9b23ef06d353a9","modified":1752079697011},{"_id":"themes/next/languages/ko.yml","hash":"04927e331652ee13f9c8a110fe12d7f936a6b0e6","modified":1752079697012},{"_id":"themes/next/languages/nl.yml","hash":"4575f93c4ee4383bdfaca01ad6ffff5664cbbbae","modified":1752079697012},{"_id":"themes/next/languages/pt-BR.yml","hash":"ff9aa6a6549d07b7dc667094f34a06945e2c21b4","modified":1752079697013},{"_id":"themes/next/languages/pt.yml","hash":"2c86b3235de9539df2119e52e913d847ecf782c0","modified":1752079697013},{"_id":"themes/next/languages/si.yml","hash":"07f98d6ca99bceef35254c60fc0e4401ff608557","modified":1752079697014},{"_id":"themes/next/languages/ru.yml","hash":"d726265bc5cf8bd4fb64f85b2d07b3326439bb00","modified":1752079697013},{"_id":"themes/next/languages/tr.yml","hash":"822ec400a0a2bb2245abf31e9ed4f3af77b2814e","modified":1752079697015},{"_id":"themes/next/languages/th.yml","hash":"dc434da945d52dc7a8d26acc80b23e6922712f89","modified":1752079697014},{"_id":"themes/next/languages/tk.yml","hash":"7b9115e0520dc81a95a7c8e4f56a01fc84f74017","modified":1752079697015},{"_id":"themes/next/languages/uk.yml","hash":"10a2f2e7183410d1d10fa937e3e0198b46bf4930","modified":1752079697015},{"_id":"themes/next/languages/zh-CN.yml","hash":"c06c95121b3e5bd5abe01848073fbe7f9d6dcda9","modified":1752079697016},{"_id":"themes/next/languages/zh-HK.yml","hash":"293009343786f9b460412b7032e694755e5edc8d","modified":1752079697016},{"_id":"themes/next/languages/vi.yml","hash":"8344cb90ee92a21a398faff3317c025a43446db5","modified":1752079697016},{"_id":"themes/next/languages/zh-TW.yml","hash":"664ebfa09503d29d6f33a5ffadaa16989785e0fe","modified":1752079697017},{"_id":"themes/next/layout/category.njk","hash":"82f541452cae76a94ee15cb8d8a888f44260a0fd","modified":1752079697034},{"_id":"themes/next/layout/index.njk","hash":"fa52c3049871e879980cb6abccdea3792ca4ce70","modified":1752079697034},{"_id":"themes/next/layout/archive.njk","hash":"aa491dba8f746e626c273a920effedf7d0b32170","modified":1752079697034},{"_id":"themes/next/layout/page.njk","hash":"d24238eea91a347050a663c7a7de8cebaab4c595","modified":1752079697034},{"_id":"themes/next/layout/_layout.njk","hash":"e8dd48352cbdb8ade0fb4f9380c5fe3dab0a995e","modified":1752079697017},{"_id":"themes/next/.github/ISSUE_TEMPLATE/config.yml","hash":"daeedc5da2ee74ac31cf71846b766ca6499e9fc6","modified":1752079696994},{"_id":"themes/next/test/index.js","hash":"c0723bd61aed6872f184aa743b782dc0b9a172e1","modified":1752079697104},{"_id":"themes/next/.github/ISSUE_TEMPLATE/feature-request.yml","hash":"fbc3062cd4591c8329fab9db72803746f0d11920","modified":1752079696994},{"_id":"themes/next/layout/post.njk","hash":"6767de9702a07a2a4e16a8a6bc9c5919536c1e3f","modified":1752079697041},{"_id":"themes/next/.github/ISSUE_TEMPLATE/bug-report.yml","hash":"3f19cbf0c2b2fee6bf3788870b842c9ccc1425ca","modified":1752079696993},{"_id":"themes/next/layout/tag.njk","hash":"b6c017d30d08ddd30d66e9c6f3a71aa65d214eac","modified":1752079697041},{"_id":"themes/next/.github/ISSUE_TEMPLATE/other.yml","hash":"10eca518b91a19984f6a5a912d41222042f61d63","modified":1752079696994},{"_id":"themes/next/.github/workflows/labeler.yml","hash":"e8ca602587b3abe08b60fc1bdc1c2d62aeeb85bb","modified":1752079696997},{"_id":"themes/next/.github/workflows/codeql.yml","hash":"82a2df4c676ae45b2768c71c724b188475c88ca5","modified":1752079696997},{"_id":"themes/next/.github/workflows/label-commenter.yml","hash":"dc4809131c54a1d9f8e7392aeaf53b13c6befd90","modified":1752079696997},{"_id":"themes/next/.github/workflows/linter.yml","hash":"294660e1b6275a0fe4db2a44e5b6b679467d29fc","modified":1752079696998},{"_id":"themes/next/.github/workflows/lock.yml","hash":"80dc185dfaeb2927db72a44ff2382e05f47a2df5","modified":1752079696998},{"_id":"themes/next/docs/ru/README.md","hash":"2c22c6f8c77cd22d57b4a556826fa9cbd4034f85","modified":1752079697000},{"_id":"themes/next/.github/workflows/tester.yml","hash":"dba3f1d4bc4d2fab51d73d134ed5c7448a0976dc","modified":1752079697000},{"_id":"themes/next/.github/workflows/npm-publish.yml","hash":"4154828f8fcc37011e636fc1c3448f3b15350d44","modified":1752079696998},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"e077bebf4fb89b159b840b30f0230b95ff7db8a5","modified":1752079697000},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"a09ceb82b45dd8b7da76c227f3d0bb7eebe7d5d1","modified":1752079697006},{"_id":"themes/next/layout/_macro/post.njk","hash":"775f264e8deb66653153117c8bd43854594cb36c","modified":1752079697018},{"_id":"themes/next/layout/_macro/post-collapse.njk","hash":"a60b386c7889f25f811182cae76e6474cb3254d1","modified":1752079697018},{"_id":"themes/next/docs/zh-CN/README.md","hash":"45481be96a710e53498721d7d1db1edb36318606","modified":1752079697006},{"_id":"themes/next/layout/_macro/sidebar.njk","hash":"b9fc0a6b02f0380de939d1ea2a7c7d2e6be9f462","modified":1752079697019},{"_id":"themes/next/layout/_partials/languages.njk","hash":"537026fc120adeef9148c98ebf074207e3810538","modified":1752079697023},{"_id":"themes/next/layout/_partials/footer.njk","hash":"cc51596980560ab60626da76260b7f5e83df8a33","modified":1752310612259},{"_id":"themes/next/layout/_partials/pagination.njk","hash":"c55167bc0dbe7e104d4f4d782e98fcabc7e07a35","modified":1752079697025},{"_id":"themes/next/layout/_scripts/index.njk","hash":"6379fb7776ba2a93eb0220e5fa2adee1e3c5d9b0","modified":1752079697028},{"_id":"themes/next/layout/_scripts/vendors.njk","hash":"71691096a1a034f6af68403730a1589c0d7fabaa","modified":1752079697028},{"_id":"themes/next/layout/_partials/widgets.njk","hash":"bb5d32170b469dc018ceaa10a5b7cb892c9b85a4","modified":1752079697028},{"_id":"themes/next/layout/_third-party/addtoany.njk","hash":"9ef81adb0c35ebdb4499602155e87611aee0762a","modified":1752079697029},{"_id":"themes/next/layout/_partials/comments.njk","hash":"60f4620dd479da6d86d1d493440e9e2a28b6e132","modified":1752079697019},{"_id":"themes/next/layout/_third-party/index.njk","hash":"9bab629e2971b029545511b48f397445858ac7f0","modified":1752079697034},{"_id":"themes/next/layout/_third-party/pace.njk","hash":"13b2a77b4858a127f458ea092b6f713b052befac","modified":1752079697034},{"_id":"themes/next/layout/_third-party/quicklink.njk","hash":"73bc15a9c3c5c239ab90efa19a1e721f41f3cb93","modified":1752079697034},{"_id":"themes/next/scripts/filters/default-injects.js","hash":"0c9a1fe9906672724dbf274154a37bac1915ca2c","modified":1752079697047},{"_id":"themes/next/layout/_third-party/fancybox.njk","hash":"53ad3c31762b74e5d29787b37d5e494cc4fded9b","modified":1752079697034},{"_id":"themes/next/scripts/events/index.js","hash":"8ae618d4436dab49928c2bebc0837e5310dbe7de","modified":1752079697042},{"_id":"themes/next/scripts/filters/locals.js","hash":"87f3bff03fa9fd96cf0787a6442464a7ff57f76b","modified":1752079697048},{"_id":"themes/next/scripts/filters/minify.js","hash":"0f6b9b0eb41f6319e75ff86f9254cf1a9d3333b8","modified":1752079697048},{"_id":"themes/next/scripts/helpers/navigation.js","hash":"82f6e81bacf23d10cac71b09cff8708e47891ef3","modified":1752079697049},{"_id":"themes/next/scripts/helpers/engine.js","hash":"b69e324ae9750cb35a4609c41b73d52177cc94e9","modified":1752079697049},{"_id":"themes/next/scripts/filters/post.js","hash":"e6cf4c94fc2291215a3345134ddbbc74d5091b00","modified":1752079697048},{"_id":"themes/next/scripts/helpers/font.js","hash":"dd76be4927a77c27292a4790bcc659c10518a219","modified":1752079697049},{"_id":"themes/next/scripts/helpers/next-config.js","hash":"2a10b4b8879ccd7ea8b36253fe7a27f4161e6b82","modified":1752079697049},{"_id":"themes/next/scripts/helpers/next-vendors.js","hash":"12814eb1845830204e3a398eb93a2c15e0ff8d94","modified":1752079697051},{"_id":"themes/next/scripts/helpers/next-paginator.js","hash":"2e8dcc12a52517bb4e00ea611720bcd298624112","modified":1752079697050},{"_id":"themes/next/scripts/tags/button.js","hash":"86c71c73a63744efbbbb367612871fede0d69529","modified":1752079697051},{"_id":"themes/next/scripts/helpers/next-url.js","hash":"ceff5e76564a5d6ed3e0222bccf581e106d80f3c","modified":1752079697050},{"_id":"themes/next/scripts/tags/caniuse.js","hash":"8e912c715702addaf0cefe63e580e45b97ae8c3f","modified":1752079697051},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"652140814527db74bcde9110142824dc727be889","modified":1752079697052},{"_id":"themes/next/scripts/tags/label.js","hash":"c18b0e619a779ed40be7f014db92af18f45fbd5c","modified":1752079697053},{"_id":"themes/next/scripts/tags/link-grid.js","hash":"3f358bb78c5c6fdf45de287f3ead553e3a6a93c2","modified":1752079697053},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"b4d12e6fe29089be0f43bafc9eea736602cd16bf","modified":1752079697052},{"_id":"themes/next/scripts/tags/index.js","hash":"1a680bb01e14152905efe3ef624621dd74cf27c3","modified":1752079697052},{"_id":"themes/next/scripts/tags/mermaid.js","hash":"b139224ff2cc668f4d95bde184934833e05b29df","modified":1752079697053},{"_id":"themes/next/scripts/tags/tabs.js","hash":"e0ed5fe1bc9d2957952a1aacdf3252d6ef3f9743","modified":1752079697054},{"_id":"themes/next/scripts/tags/note.js","hash":"a12fd53e421400836a3722ae69130969558d6ac0","modified":1752079697053},{"_id":"themes/next/scripts/tags/video.js","hash":"f6ad3f52779f0636251238d3cbdc5b6f91cc5aba","modified":1752079697054},{"_id":"themes/next/scripts/tags/wavedrom.js","hash":"188c1dd5d7dbc566cac00946da86aa76fff1c682","modified":1752079697055},{"_id":"themes/next/source/css/_colors.styl","hash":"9354013ea89c8e3874a44929c394db39ae04f36a","modified":1752079697055},{"_id":"themes/next/source/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1752079697089},{"_id":"themes/next/source/css/main.styl","hash":"cb3bb1f96ab67e31ce4326159e22ba22cee14c4e","modified":1752310770645},{"_id":"themes/next/scripts/tags/pdf.js","hash":"317ba4611020cc840854386dde098dbbe452777e","modified":1752079697054},{"_id":"themes/next/source/css/_mixins.styl","hash":"bb868086bf28029d2fb9f6d4abe620c382d66568","modified":1752079697078},{"_id":"themes/next/source/css/noscript.styl","hash":"64b378a4d2f0b36bf666fba13b57c59da3a8ac83","modified":1752079697089},{"_id":"themes/next/source/js/comments-buttons.js","hash":"81ea6cbcdf0357094753d7523919c1eafa38e79f","modified":1752079697092},{"_id":"themes/next/source/js/comments.js","hash":"0b4daf0ce610760bd52e95d423f61f3e1c72442a","modified":1752079697092},{"_id":"themes/next/source/images/logo-algolia-nebula-blue-full.svg","hash":"5a81f1c5d66561b3bcb05dae48148088a3fb5c79","modified":1752079697091},{"_id":"themes/next/source/images/logo.svg","hash":"099e11ab995a2c8981427a85476d082609848c77","modified":1752079697091},{"_id":"themes/next/source/js/config.js","hash":"211a9ab35205ccfa6b7c74394bade84da0d00af7","modified":1752079697092},{"_id":"themes/next/source/js/motion.js","hash":"a2abc7279723aee459b4c7a357fc84d5215affcf","modified":1752079697093},{"_id":"themes/next/source/js/bookmark.js","hash":"e2fd71f2876531f4c8bf1f38828aae35dd82cca4","modified":1752079697091},{"_id":"themes/next/source/js/pjax.js","hash":"ca513ae9aa3ab3dfe46668f0717d9d37b571c5b7","modified":1752079697093},{"_id":"themes/next/source/js/schedule.js","hash":"4858233224756e3cadfabd8135f7c4b1e3673b44","modified":1752079697094},{"_id":"themes/next/source/js/utils.js","hash":"381223fa6bba40d973550deb05bc409c3962338b","modified":1752079697104},{"_id":"themes/next/source/js/next-boot.js","hash":"feafaa8cd1305b773f793bd36dee71fd832044a2","modified":1752079697093},{"_id":"themes/next/source/js/sidebar.js","hash":"f3052996408a09233ffaf8d631fd930443ccd781","modified":1752079697094},{"_id":"themes/next/test/helpers/index.js","hash":"2fb58dca3df2fe53116ee2b1232fa26ebe7b2ce5","modified":1752079697104},{"_id":"themes/next/test/helpers/font.js","hash":"f61d3a1de5261728398d2de57e33452cf7d93383","modified":1752079697104},{"_id":"themes/next/test/tags/button.js","hash":"68b2f363286ce08e6b64d5d0d90aad6960e13a93","modified":1752079697104},{"_id":"themes/next/test/helpers/next-url.js","hash":"4f39453286113d3d1e64bcc1934b5f974a33102d","modified":1752079697104},{"_id":"themes/next/test/tags/caniuse.js","hash":"32f1ae9d49fcf0a1ebaa07f1f274aaf6f85b4699","modified":1752079697104},{"_id":"themes/next/test/tags/center-quote.js","hash":"c073b7ee0f72d1c304522030a8eea68878adfcba","modified":1752079697104},{"_id":"themes/next/test/tags/group-pictures.js","hash":"1232c69689eee53b5b5926beb66f402a3bec5581","modified":1752079697104},{"_id":"themes/next/test/tags/index.js","hash":"5cad001936a694bf32d59751cc2b68a66199f976","modified":1752079697104},{"_id":"themes/next/test/tags/label.js","hash":"11c90db85fd373b08fd4241c3adfb19057e34025","modified":1752079697104},{"_id":"themes/next/test/tags/link-grid.js","hash":"8b0ce804b53329c0e76259a94a24c60a66b9f77b","modified":1752079697104},{"_id":"themes/next/test/tags/mermaid.js","hash":"ff6751c3e77db4c80be9c03b896451b1fd1b185f","modified":1752079697104},{"_id":"themes/next/test/tags/pdf.js","hash":"4303175f1702adabd445aec58101d48df8c1a3ba","modified":1752079697104},{"_id":"themes/next/test/tags/note.js","hash":"2c4127a43850d89fe13064ae26e3b45bc72b525d","modified":1752079697104},{"_id":"themes/next/test/tags/tabs.js","hash":"2f809f250a0445671912ae0fd54bbb6c0f5e9880","modified":1752079697104},{"_id":"themes/next/test/tags/video.js","hash":"6ab9deb1d702bda3132e6c45fa5e6b51c79a7151","modified":1752079697110},{"_id":"themes/next/test/validate/index.js","hash":"9d44aff73371402574a8b61dda1037a58e382b52","modified":1752079697110},{"_id":"themes/next/layout/_partials/head/head-unique.njk","hash":"c2ee46d744afa78d95c6bca5dfcbb2fbc38474c2","modified":1752079697021},{"_id":"themes/next/layout/_partials/head/head.njk","hash":"e016c3f80db433f17781caf26e44f2089a71550a","modified":1752079697021},{"_id":"themes/next/layout/_partials/header/brand.njk","hash":"7bf2b6ab499775355bbf8f3ae597ff7bc00e89e4","modified":1752079697022},{"_id":"themes/next/layout/_partials/header/index.njk","hash":"1b2ae17f3c394ce310fe2d9ed5f4d07d8cc74ae7","modified":1752079697022},{"_id":"themes/next/layout/_partials/header/menu.njk","hash":"828700af2a2f273d3e3e0554ffc60706d889d539","modified":1752079697023},{"_id":"themes/next/layout/_partials/header/sub-menu.njk","hash":"20cb9c39bcdcfa3f710df7bc5838d6e62d4dd674","modified":1752079697023},{"_id":"themes/next/layout/_partials/header/menu-item.njk","hash":"62513f08e9e7f4abeaeedca91fd0af0861a2540f","modified":1752079697022},{"_id":"themes/next/layout/_partials/page/breadcrumb.njk","hash":"1fe44a1e156cd30e3e9fd8313e1011ad30970f83","modified":1752079697024},{"_id":"themes/next/layout/_partials/page/categories.njk","hash":"b352346dd2cb42f7eeaec5e39d9a2a353b029775","modified":1752079697024},{"_id":"themes/next/layout/_partials/page/page-header.njk","hash":"92553feb26f30f7fc9147bc4ef122908a9da06be","modified":1752079697024},{"_id":"themes/next/layout/_partials/page/schedule.njk","hash":"6ec9c97e91c793cc2eb5ac0f7c3c36fdaaf637d1","modified":1752079697025},{"_id":"themes/next/layout/_partials/page/tags.njk","hash":"752df7d12360a077c51a25609916a3ecc1763bb3","modified":1752079697025},{"_id":"themes/next/layout/_partials/post/post-copyright.njk","hash":"22c922214982c4bb78ea92fa2ff1de93d98216a3","modified":1752079697025},{"_id":"themes/next/layout/_partials/post/post-followme.njk","hash":"ebe45ab38ace03ea74eed3f676ec1cb5805a3c8a","modified":1752079697026},{"_id":"themes/next/layout/_partials/post/post-meta.njk","hash":"9a9c4fb7e7c4fe4b7d474bdfdb4ed2b0a5423df2","modified":1752079697026},{"_id":"themes/next/layout/_partials/post/post-related.njk","hash":"9a7eda45ad0753d5f624c51a56e00277ba97c93d","modified":1752079697026},{"_id":"themes/next/layout/_partials/post/post-reward.njk","hash":"fd6fde597ea2fb7b80cffd0a3553c8e73acb8ab2","modified":1752079697027},{"_id":"themes/next/layout/_partials/search/index.njk","hash":"10145ae7ef87b502d20bfa08beb2b899228f1419","modified":1752079697027},{"_id":"themes/next/layout/_partials/sidebar/site-overview.njk","hash":"8d76300e31178e790400a37adfaf9bc0e5f8fae7","modified":1752079697027},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.njk","hash":"3e80332f88b101141be69f2a07f54ed8c053eabb","modified":1752079697030},{"_id":"themes/next/layout/_third-party/analytics/growingio.njk","hash":"9ff9ec05c2037beea229a6bb698f9e3546973220","modified":1752079697030},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.njk","hash":"52ad137450f7b3d6a330e16b3ed1c6174290f0eb","modified":1752079697030},{"_id":"themes/next/layout/_third-party/analytics/index.njk","hash":"81abcf790cf97395cde9e3cd6d5d1aa5656bd134","modified":1752079697031},{"_id":"themes/next/layout/_third-party/analytics/umami.njk","hash":"c8b8c10dcf5c4ffa4a8aec5ba5892842ec204ac0","modified":1752079697032},{"_id":"themes/next/layout/_partials/post/post-share.njk","hash":"8a6414987474592e82b3e57eeb0bd526ae42d235","modified":1752079697027},{"_id":"themes/next/layout/_third-party/analytics/matomo.njk","hash":"43238bc1bc2e88d707f8097814ef3ff830cbe641","modified":1752079697031},{"_id":"themes/next/layout/_third-party/analytics/cloudflare.njk","hash":"17173c45f0b740669ef45aaae1fe96eeb0a0ff52","modified":1752079697030},{"_id":"themes/next/layout/_third-party/analytics/microsoft-clarity.njk","hash":"2129a017a1b1ab751aca9c70e76461ad91b2a81e","modified":1752079697031},{"_id":"themes/next/layout/_third-party/analytics/plausible.njk","hash":"64ed6fecae79f9f9c61e956d2bd077792473e069","modified":1752079697031},{"_id":"themes/next/layout/_third-party/chat/tidio.njk","hash":"3fbc72427c1211e5dcfd269af1a74852a7ba5c1a","modified":1752079697032},{"_id":"themes/next/layout/_third-party/comments/changyan.njk","hash":"5f7967bd946060f4102263a552ddfbae9975e7ea","modified":1752079697033},{"_id":"themes/next/layout/_third-party/comments/disqus.njk","hash":"b0828dd1b1fd66ecd612d9e886a08e7579e9a4f7","modified":1752079697033},{"_id":"themes/next/layout/_third-party/comments/disqusjs.njk","hash":"c5086b4c35f730f82c99c4a8317f2f153ebde869","modified":1752079697033},{"_id":"themes/next/layout/_third-party/chat/chatra.njk","hash":"09d2c9487d75894d45a823e3237ae9f90fd6ee01","modified":1752079697032},{"_id":"themes/next/layout/_third-party/comments/isso.njk","hash":"38badcc7624a13961381c2465478056b9602aee5","modified":1752079697034},{"_id":"themes/next/layout/_third-party/comments/livere.njk","hash":"b8e0d5de584cece5e05b03db5b86145aa1e422b4","modified":1752079697034},{"_id":"themes/next/layout/_third-party/comments/gitalk.njk","hash":"6fd4df5c21cfe530dbb0c012bc0b202f2c362b9c","modified":1752079697034},{"_id":"themes/next/layout/_third-party/comments/utterances.njk","hash":"a7921be7328e1509d33b435175f5333a9aada66f","modified":1752079697034},{"_id":"themes/next/layout/_third-party/math/index.njk","hash":"1856c4b035c5b8e64300a11af0461b519dfc4cf4","modified":1752079697034},{"_id":"themes/next/layout/_third-party/math/katex.njk","hash":"1df65b1390add93b86ae3f9423d96a130b0ece04","modified":1752079697034},{"_id":"themes/next/layout/_third-party/math/mathjax.njk","hash":"a62aa1ed4e35b8d0451d83f341bf0a97538bc9a4","modified":1752079697034},{"_id":"themes/next/layout/_third-party/search/algolia-search.njk","hash":"599f3f5e4385501d0010edc02ed9ca547d2467f7","modified":1752079697034},{"_id":"themes/next/layout/_third-party/search/localsearch.njk","hash":"210c32b654adae3d8076c4417d370b42af258cea","modified":1752079697034},{"_id":"themes/next/layout/_third-party/statistics/busuanzi-counter.njk","hash":"442df8a04f1967779cdd7599156496bdac8f1f23","modified":1752079697034},{"_id":"themes/next/layout/_third-party/statistics/index.njk","hash":"866ffa15a3250678eb8a90aa6f609fa965db90fd","modified":1752079697034},{"_id":"themes/next/layout/_third-party/statistics/lean-analytics.njk","hash":"8703d1855bb8d251c9b7c2940b7e3be525e53000","modified":1752079697034},{"_id":"themes/next/layout/_third-party/tags/mermaid.njk","hash":"dd8f963acd5a3685be46fd5319c06df0308d99b2","modified":1752079697034},{"_id":"themes/next/layout/_third-party/tags/wavedrom.njk","hash":"baec18165d767094ecb2dda7141ca3bd8c6f2eb9","modified":1752079697034},{"_id":"themes/next/layout/_third-party/tags/pdf.njk","hash":"0386c708975cc5faea4f782611c5d2c6b8ac2850","modified":1752079697034},{"_id":"themes/next/layout/_third-party/statistics/firestore.njk","hash":"af5336e8bbdc4638435971da115bb7443d374ade","modified":1752079697034},{"_id":"themes/next/scripts/filters/comment/changyan.js","hash":"761760031e49edb77277bd4a8582774959240d6b","modified":1752079697045},{"_id":"themes/next/scripts/filters/comment/common.js","hash":"550cc7f57bc3ab3d093e34741e7860222851d4d4","modified":1752079697045},{"_id":"themes/next/scripts/filters/comment/default-config.js","hash":"1cb58aa6b88f7461c3c3f9605273686adcc30979","modified":1752079697045},{"_id":"themes/next/scripts/filters/comment/disqus.js","hash":"3283bdd6e5ac7d10376df8ddd5faaec5dc1bd667","modified":1752079697046},{"_id":"themes/next/scripts/filters/comment/disqusjs.js","hash":"4dbe9652ae53a181c86eb7e9005a5255f0540055","modified":1752079697046},{"_id":"themes/next/scripts/filters/comment/gitalk.js","hash":"96e58efba0dc76af409cc7d2db225f0fe4526ea8","modified":1752079697046},{"_id":"themes/next/scripts/filters/comment/isso.js","hash":"c22cbccd7d514947e084eeac6a3af1aa41ec857a","modified":1752079697047},{"_id":"themes/next/scripts/filters/comment/utterances.js","hash":"a50718c081685fd35ff8ea9ca13682c284399ed8","modified":1752079697047},{"_id":"themes/next/scripts/events/lib/highlight.js","hash":"df360f546d59b1fee8926806268754433dfa5de7","modified":1752079697043},{"_id":"themes/next/scripts/filters/comment/livere.js","hash":"bb8ebb541c40362c0cbbd8e83d3b777302bb6c40","modified":1752079697047},{"_id":"themes/next/scripts/events/lib/injects.js","hash":"1f1ea7b579a49f17574c31d78d663c54896133eb","modified":1752079697043},{"_id":"themes/next/scripts/events/lib/navigation.js","hash":"0a2df7d40d64c61ce7e730bb8269b848b03526d1","modified":1752079697043},{"_id":"themes/next/scripts/events/lib/config.js","hash":"b4944b3272edecceedc3935f8606e810f9ff237e","modified":1752079697043},{"_id":"themes/next/scripts/events/lib/utils.js","hash":"da22c6008fc8d214ba8561b412233380a06c40f9","modified":1752079697044},{"_id":"themes/next/scripts/events/lib/vendors.js","hash":"af3e7066a7a8d87448aadde3bc9c848077aae418","modified":1752079697044},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"c4537fa2de33d98baff2c87a73801770414e0b69","modified":1752079697087},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"9cd228d5de7c2e49a231be072c9ef13af2dd95ff","modified":1752079697087},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"54c52744898eb5d2073b83f98bb1a61759da92dc","modified":1752079697088},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"9f1424e0b0314fc540c33ff59959fc3b75b18c34","modified":1752311854222},{"_id":"themes/next/source/css/_variables/base.styl","hash":"c4fda1066a429e77828f457aea5d5b06be3eeda6","modified":1752079697088},{"_id":"themes/next/source/js/third-party/addtoany.js","hash":"5cff12a5c9fc55a68f0f57a291494cd738301ba4","modified":1752079697094},{"_id":"themes/next/source/js/third-party/quicklink.js","hash":"5b0197e061f57e00875be3636ba372a67693abe0","modified":1752079697100},{"_id":"themes/next/source/js/third-party/fancybox.js","hash":"e3022c2ea60409a82a3e2a0d3615e4a25c684551","modified":1752079697099},{"_id":"themes/next/source/css/_common/outline/index.styl","hash":"7782dfae7a0f8cd61b936fa8ac980440a7bbd3bb","modified":1752079697067},{"_id":"themes/next/source/css/_common/outline/mobile.styl","hash":"883a4f42badb7b206604eb2805ae582c3a511403","modified":1752079697067},{"_id":"themes/next/source/css/_common/components/index.styl","hash":"49c3063b46d65796003a822deed971abee41675e","modified":1752079697056},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"f882924c7b99afc23106e39164516888b2972c7b","modified":1752079697056},{"_id":"themes/next/source/js/third-party/pace.js","hash":"0ebee77b2307bf4b260afb06c060171ef42b7141","modified":1752079697100},{"_id":"themes/next/source/css/_common/components/reading-progress.styl","hash":"f3defd56be33dba4866a695396d96c767ce63182","modified":1752079697062},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"0951dfc96a5c4aca904142e7f1e57e2ad2dd2e10","modified":1752079697072},{"_id":"themes/next/source/css/_common/scaffolding/buttons.styl","hash":"f768ecb2fe3e9384777c1c115cd7409e9155edd7","modified":1752079697072},{"_id":"themes/next/source/css/_common/scaffolding/index.styl","hash":"43045d115f8fe95732c446aa45bf1c97609ff2a5","modified":1752079697072},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"b9388016f8d9274703e77e306a1feaad1b7b9d6c","modified":1752079697075},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"6d740699fb6a7640647a8fd77c4ea4992d8d6437","modified":1752079697072},{"_id":"themes/next/source/css/_common/scaffolding/toggles.styl","hash":"b9322d644b2090d793521555646673322f2dd5e8","modified":1752079697078},{"_id":"themes/next/source/css/_common/scaffolding/pagination.styl","hash":"fd4a2d2303f107a021837aa6e3d977c2f9d14c42","modified":1752079697072},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"ce25cb35779f29d01f7b7fc51ee09e68987ad318","modified":1752079697079},{"_id":"themes/next/source/css/_common/scaffolding/comments.styl","hash":"cf8446f4378dcab27b55ede1635c608ae6b8a5c8","modified":1752079697072},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"be9d0e9493802c22f16a0645a59d0c2b556c1c3c","modified":1752079697080},{"_id":"themes/next/source/css/_schemes/Mist/_layout.styl","hash":"2db5ada757f7ffb9a36b06400cf30d80ec039b6e","modified":1752079697080},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expand.styl","hash":"68f8d30071516913a7a958e01d010e8a93f7aa24","modified":1752079697081},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"0bf248d61593fbcdfd83b018b92199f4da3699b1","modified":1752079697080},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"89bf3f6b82cb0fafbbd483431df8f450857c5a0b","modified":1752079697081},{"_id":"themes/next/source/css/_schemes/Pisces/_header.styl","hash":"36438f7b47d7ebd250a25f97c743fda484283046","modified":1752079697084},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"3ab87229f9a8357446f77f05a0c6854fb5363fd5","modified":1752079697085},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"7a3b5cd21bc35d92358abb07fc0986722dd10881","modified":1752079697085},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"7125fb0dba920e89c3f7c1c07e2f7cce1fbdb703","modified":1752079697086},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"b5c3dd08c520a16ee49f85fa12b4935e725ef261","modified":1752079697086},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"7905f428b46d100ac5928875cb1e2b99fa86fc0b","modified":1752079697086},{"_id":"themes/next/source/css/_schemes/Muse/_header.styl","hash":"2f19fe2aba8f63fc99641e50bcb96cc9a4feb8a4","modified":1752079697081},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"e6e8892fccb02bff163d9b574984e0440a00d756","modified":1752079697082},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"b26770e09de9b51c851bb90fae19b27860e7fc9f","modified":1752079697082},{"_id":"themes/next/source/css/_schemes/Muse/_sidebar.styl","hash":"2fb4b3f88e34a8544436a05634690217f9483419","modified":1752079697082},{"_id":"themes/next/source/css/_schemes/Muse/_sub-menu.styl","hash":"2d3e05015796a790abd9d68957a5c698c0c9f9b6","modified":1752079697084},{"_id":"themes/next/source/js/third-party/analytics/baidu-analytics.js","hash":"f9579a02599de063ccff336177ba964a2931a6e9","modified":1752079697095},{"_id":"themes/next/source/js/third-party/analytics/google-analytics.js","hash":"27a27de3d4e0d33e9c647f7ae78e1dd6d36d2463","modified":1752079697095},{"_id":"themes/next/source/js/third-party/analytics/growingio.js","hash":"f755e8537ccbbb0bd84c26923f320d4e206e7428","modified":1752079697096},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"25c2a7930da14f023329df20f38df2728057fb4d","modified":1752079697084},{"_id":"themes/next/source/js/third-party/analytics/matomo.js","hash":"f24604710002234f18a7b5cfe9ccdf3ed6e725a8","modified":1752079697096},{"_id":"themes/next/source/js/third-party/chat/chatra.js","hash":"72e0766752b78a723fb30e92d533a8b353104e2d","modified":1752079697096},{"_id":"themes/next/source/js/third-party/chat/tidio.js","hash":"77c231bcd64f1c09bd9989909e9fee703b65f47f","modified":1752079697096},{"_id":"themes/next/source/js/third-party/comments/changyan.js","hash":"b4cb06fdf531292e2361398a98d75a4ca2b8473a","modified":1752079697096},{"_id":"themes/next/source/js/third-party/comments/disqus.js","hash":"07e0038b221b9a4fd8ccac75093de3dee1f8315e","modified":1752079697097},{"_id":"themes/next/source/js/third-party/comments/disqusjs.js","hash":"5673d28509a8e837a512da490a88b7bf5162fc49","modified":1752079697097},{"_id":"themes/next/source/js/third-party/comments/gitalk.js","hash":"1e8509356fb027d948d118ab220d9631f4d482fa","modified":1752079697097},{"_id":"themes/next/source/js/third-party/comments/isso.js","hash":"b9b9fd2f0e098a123b34a4932da912a9485ffe6c","modified":1752079697098},{"_id":"themes/next/source/js/third-party/comments/livere.js","hash":"68892d74ef5fc308c6e7e6b4f190826d79f3055d","modified":1752079697098},{"_id":"themes/next/source/js/third-party/comments/utterances.js","hash":"ec44d7f1c8b51b0aa3cccba099a78f3575ac828c","modified":1752079697098},{"_id":"themes/next/source/js/third-party/math/katex.js","hash":"5c63ec71458b4fe0cd98fd4a04e11c3746764f11","modified":1752079697099},{"_id":"themes/next/source/js/third-party/math/mathjax.js","hash":"d93556184b2c0aa1dbc4a6fb892d2f77b80d7d9f","modified":1752079697099},{"_id":"themes/next/source/js/third-party/search/algolia-search.js","hash":"9486f0ba64182ce1a5b2524c10d02e0b992c7300","modified":1752079697101},{"_id":"themes/next/source/js/third-party/search/local-search.js","hash":"9f07ea758044afea0a293d75e6585ad65888fb71","modified":1752079697102},{"_id":"themes/next/source/js/third-party/statistics/lean-analytics.js","hash":"7db3233f0b33870943ce1547326a67f9e628b411","modified":1752079697103},{"_id":"themes/next/source/js/third-party/statistics/firestore.js","hash":"ce12b5007c9aa997738641c06a1c081c357e27bd","modified":1752079697102},{"_id":"themes/next/source/js/third-party/tags/mermaid.js","hash":"5ef9da034f3ec3b5b66137101ddcff0b2687ed69","modified":1752079697103},{"_id":"themes/next/source/js/third-party/tags/pdf.js","hash":"e109c2d6828f527f0289d5fa3bb02fce63ee6d93","modified":1752079697103},{"_id":"themes/next/source/js/third-party/tags/wavedrom.js","hash":"9cafc8f751581d496f1f3c056b95f3b8e1ebbf4e","modified":1752079697104},{"_id":"themes/next/source/css/_common/outline/footer/index.styl","hash":"39066b4e2e914d85736c9c33fa51f21a3f86c0e4","modified":1752079697065},{"_id":"themes/next/source/css/_common/outline/header/github-banner.styl","hash":"05af22f3edc2383a3d97ec4c05e9ac43b014bead","modified":1752079697066},{"_id":"themes/next/source/css/_common/outline/header/bookmark.styl","hash":"c8648c8ea3105556be0068d9fb2735261d0d94bc","modified":1752079697065},{"_id":"themes/next/source/css/_common/outline/header/site-meta.styl","hash":"dce8ea62d7d7b08a0444fbb2f617be30bfcfc152","modified":1752079697067},{"_id":"themes/next/source/css/_common/outline/header/menu.styl","hash":"f4de9eb94bebdf7790522e103de51205a64cae9e","modified":1752079697066},{"_id":"themes/next/source/css/_common/outline/header/index.styl","hash":"038625515ba4760e4dda6792549bddc0db5a3d20","modified":1752079697066},{"_id":"themes/next/source/css/_common/outline/header/site-nav.styl","hash":"d9bc2b520636b9df7f946295cd430593df4118ff","modified":1752079697067},{"_id":"themes/next/source/css/_common/outline/sidebar/related-posts.styl","hash":"b3689beb90cda64cafecaf6b25981fe8a9525992","modified":1752079697068},{"_id":"themes/next/source/css/_common/outline/sidebar/index.styl","hash":"7a47adf10809dff5cbaa6732cf6aa273e4673fd0","modified":1752079697068},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"d8a028f532d562e6a86bb3b9c7b992e4b6dbbb51","modified":1752079697069},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"63fb6d36d9ea62c7e39274c666e102b12f64ff77","modified":1752079697069},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"69869f1e317d78c03b3ef3a13e553d2c4ad04caf","modified":1752079697069},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"f5c1a6afcd2d460ccd5987ef1af747fc99d3ea72","modified":1752079697069},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-copyright.styl","hash":"a6fbede106afe30a9a7918b52ea8233e020b1382","modified":1752079697069},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"8f82d5141a18f6319e0609a3f6e4fcca6d442203","modified":1752079697069},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"794eabec62d6070255fa10516487cff70e0030bf","modified":1752079697069},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"cbc6b0938a2e60f35a5df32210904fb16e4938da","modified":1752079697069},{"_id":"themes/next/source/css/_common/outline/sidebar/site-state.styl","hash":"2de038def2cb91da143b14696366c14a66e0e569","modified":1752079697071},{"_id":"themes/next/source/css/_common/components/post/index.styl","hash":"aa04527f76de7646573ea327d4f6145200d6070d","modified":1752079697058},{"_id":"themes/next/source/css/_common/components/post/post-body.styl","hash":"9fe9eb46d11cc772e37c067fcffe1b8e7ae10c31","modified":1752079697059},{"_id":"themes/next/source/css/_common/components/post/post-followme.styl","hash":"a87245394f946b607852ddcffb6ef8d5ff9f6561","modified":1752079697059},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"c524d5990a8e7ed9caf17978cf372bb0646f7b9f","modified":1752079697059},{"_id":"themes/next/source/css/_common/components/post/post-footer.styl","hash":"8aff582c5b8e17b99ad61298192a9e6564409211","modified":1752079697060},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"c34936a17c3d8af6c0988ac6746d7509dc0b50eb","modified":1752079697060},{"_id":"themes/next/source/css/_common/components/post/post-header.styl","hash":"8798669728f20843b100dbea65c70247fe978efc","modified":1752079697060},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"69dff7cf231d01f85671758455726dd666664a73","modified":1752079697061},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"6ad0da4a5585508abbe78342a680607face98e19","modified":1752079697061},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"0a698c3adba896a46e26850967cb73295c521940","modified":1752079697062},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"fde10ce94e9ae21a03b60d41d532835b54abdcb1","modified":1752079697057},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"f9fe39bef5fb2565abbffcb26390868c13d18359","modified":1752079697057},{"_id":"themes/next/source/css/_common/components/pages/index.styl","hash":"6cf78a379bb656cc0abb4ab80fcae60152ce41ad","modified":1752079697057},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"091b8c763e43447d087c122a86538f290f83136a","modified":1752079697058},{"_id":"themes/next/source/css/_common/components/pages/tag-cloud.styl","hash":"56d719bcdcba3d725141c55bbd4b168f3942f912","modified":1752079697058},{"_id":"themes/next/source/css/_common/components/third-party/disqusjs.styl","hash":"a2ffaa06c963514a79fda52665e707af6e8e2f5e","modified":1752079697063},{"_id":"themes/next/source/css/_common/components/third-party/gitalk.styl","hash":"406c03be3cf8a8411227bc1fecf942ca368f7e2a","modified":1752079697063},{"_id":"themes/next/source/css/_common/components/third-party/index.styl","hash":"8a7b70ef14c7110aaceb30ea2f7eb4b202f0f0cd","modified":1752079697063},{"_id":"themes/next/source/css/_common/components/third-party/math.styl","hash":"1e5776ad4c5c8bcf7596ac74dcabc30704b3f5a0","modified":1752079697064},{"_id":"themes/next/source/css/_common/components/third-party/utterances.styl","hash":"d28856f365a9373c4ae6fe1e5673d63df2dfd65f","modified":1752079697064},{"_id":"themes/next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"4d674c513300187bd3c08ecaf60358107e91c41d","modified":1752079697072},{"_id":"themes/next/source/css/_common/scaffolding/highlight/fold.styl","hash":"41c0516f76513036e30a70509a6d33e8a79bdba2","modified":1752079697072},{"_id":"themes/next/source/css/_common/scaffolding/highlight/index.styl","hash":"79e246b65cef7e32bfe4b9d32516bad1f1eabda0","modified":1752079697072},{"_id":"themes/next/source/css/_common/components/third-party/search.styl","hash":"9394674b2c1e256ba93235c0dae492a8a3e850bb","modified":1752079697064},{"_id":"themes/next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"6b3680e0dbea8e14c1cec24ef63b7fae5e37f7ef","modified":1752079697075},{"_id":"themes/next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"8d9218980e185210ce034e9769ab639b9630fd88","modified":1752079697075},{"_id":"themes/next/source/css/_common/scaffolding/tags/index.styl","hash":"1772ade171aa6bd806909d69003cfc498c3af019","modified":1752079697075},{"_id":"themes/next/source/css/_common/scaffolding/tags/label.styl","hash":"531daf2612c6217950677a2d03924459ce57c291","modified":1752079697077},{"_id":"themes/next/source/css/_common/scaffolding/tags/mermaid.styl","hash":"c7754dc6c866928b538f0863a05b96ec44b5e986","modified":1752079697077},{"_id":"themes/next/source/css/_common/scaffolding/tags/link-grid.styl","hash":"7efdc07cf0311108c34fb8815eb890954757d351","modified":1752079697077},{"_id":"themes/next/source/css/_common/scaffolding/tags/note.styl","hash":"63910d15fa2b1df112aee0c4629955f157a077a4","modified":1752079697078},{"_id":"themes/next/source/css/_common/scaffolding/tags/tabs.styl","hash":"bcc0d3482f2faa4552d38de51480ea7f92f6c821","modified":1752079697078},{"_id":"themes/next/source/css/_common/scaffolding/tags/pdf.styl","hash":"77122986509a6b4968bae2729417b7016137534c","modified":1752079697078},{"_id":"themes/next/source/css/_common/scaffolding/tags/wavedrom.styl","hash":"2f4c791b5999b2e1a707bd473c1db7c21d206a27","modified":1752079697078},{"_id":"themes/next/source/images/favicon.jpg","hash":"ef2f732cc5072d26347e115d45d3899a9524d1a4","modified":1752079697089},{"_id":"source/img/Attention/Taxonomy.png","hash":"66d9f2b6baff13a690fecf3304ecad76fbfd354c","modified":1752290420903},{"_id":"source/img/CDR-data-analysis/comprehensive_bipartite_analysis_ccle.png","hash":"368e93a9832443fc2cdeedea46465d1ec1c2b94e","modified":1752079696968},{"_id":"source/paper/1609.02907v4.pdf","hash":"1762baa638866a13dcc6d146fd5a49b36cbd9c30","modified":1752203256886},{"_id":"source/paper/Lee 等 - 2018 - Attention Models in Graphs A Survey.pdf","hash":"cc23c580b7d8063415fb6eb512053d1079b849de","modified":1752203256925},{"_id":"source/img/CDR-data-analysis/comprehensive_bipartite_analysis_gdsc.png","hash":"860912fd44247e81e0a8c54b1ff3844d3ef41e0f","modified":1752079696989},{"_id":"source/paper/Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends.pdf","hash":"0c926dd126aa2e32816348ce95767a274e55d8ef","modified":1746022290566},{"_id":"source/paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf","hash":"73f96bdf97da1cef4b43d9ee7e0d08a7900ab2fc","modified":1752203256919},{"_id":"public/search.xml","hash":"7dd8ea772775f3e759d6c82f30a92c04da6fd4ee","modified":1752507699142},{"_id":"public/categories/index.html","hash":"dd6604c97daca74edf1462a5ff698c2726719eed","modified":1752507699142},{"_id":"public/tags/index.html","hash":"7d909890e69df6676c0322861aaf250d7b682004","modified":1752507699142},{"_id":"public/about/index.html","hash":"b755c498427c07b18412e0c5625a71815383a9cb","modified":1752507699142},{"_id":"public/code/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.html","hash":"5d00f01ec4f15cd5898e263cb532586da751ac4b","modified":1752507699142},{"_id":"public/2025/07/14/Feature-Related-Attention/index.html","hash":"806b08a86e5164c21e81e057d6c528065711b842","modified":1752507699142},{"_id":"public/2025/07/14/GAT/index.html","hash":"f1027331de23548ca7f1959ffda224477388e5d3","modified":1752507699142},{"_id":"public/2025/07/13/hwadee/index.html","hash":"5d204bdf7de592fb2c6ea79a7bf89de7f0edc667","modified":1752507699142},{"_id":"public/2025/07/10/Attention/index.html","hash":"d6192dc39b9068e5da49558a0b6baed5b9f2404c","modified":1752507699142},{"_id":"public/2025/07/10/GNN-and-GCN/index.html","hash":"a53ce6832ab52ab1bbf74d7e3578f6144150a7a9","modified":1752507699142},{"_id":"public/2025/07/09/CDR-data-analysis/index.html","hash":"d74a36a899093fc96f5509903c425314e80cb4f3","modified":1752507699142},{"_id":"public/2025/07/08/PEP-8/index.html","hash":"11f1f0a04eed9704e6e071bc497dee7dc8416b43","modified":1752507699142},{"_id":"public/archives/index.html","hash":"f3d04163b7c5b02215dd660ae02085461620c591","modified":1752507699142},{"_id":"public/archives/2025/index.html","hash":"fd93bb34bcb24c006113261716a384b73f5d5333","modified":1752507699142},{"_id":"public/archives/2025/07/index.html","hash":"8bcb4f3374b1ea2c12c8b9fcbcd75474aa8a5b51","modified":1752507699142},{"_id":"public/categories/CDR/index.html","hash":"8090f87b70468a3957e9f2fb899e361adb167b4b","modified":1752507699142},{"_id":"public/categories/CDR/model/index.html","hash":"018f2b9dd10fc48bae90c207a493047f9dfe98b9","modified":1752507699142},{"_id":"public/categories/CDR/model/attention/index.html","hash":"f1302a2a1765304ff7719e0b03ecdad7107270fa","modified":1752507699142},{"_id":"public/categories/CDR/Data-Analysis/index.html","hash":"49ab4b0ee0012539019ba3d3100a4085776b95ab","modified":1752507699142},{"_id":"public/categories/CDR/model/attention/feature-related/index.html","hash":"00b2ae7d13ef228528266a1a09cf7d491a02956a","modified":1752507699142},{"_id":"public/categories/CDR/model/GNN/index.html","hash":"5a2d93638b278b5ff39aca657941f4ed1ccb6796","modified":1752507699142},{"_id":"public/categories/Python/index.html","hash":"d6e46395f6dcd5c6572f99788dc37fe3d9ca604f","modified":1752507699142},{"_id":"public/index.html","hash":"e07537e71205b9a75a6dfc75ae92f0e3c30b40a3","modified":1752507699142},{"_id":"public/tags/CDR/index.html","hash":"e5baa1d8fbca8974195f2411929d005f23c19c1e","modified":1752507699142},{"_id":"public/tags/model/index.html","hash":"e44d5d159df57c8a9f570c4f6426b7033c369d82","modified":1752507699142},{"_id":"public/tags/Basic/index.html","hash":"d9a63f423548a0b936598664fc52bf9d3e7205f3","modified":1752507699142},{"_id":"public/tags/deep-learning/index.html","hash":"c72c7931e7ab3d45aad3a40859825455f3a7d7f2","modified":1752507699142},{"_id":"public/tags/还没写完捏/index.html","hash":"f00851d958a66126069a7c35e699e2473849967d","modified":1752507699142},{"_id":"public/tags/PyTorch/index.html","hash":"3eef4918e0551b5606e6818b3b9e4c449a9d5994","modified":1752507699142},{"_id":"public/tags/Data-Analysis/index.html","hash":"9564bfdbdf086270efd3d22c36d21685d80866ed","modified":1752507699142},{"_id":"public/tags/graph-theory/index.html","hash":"19f01337064505c89697a6c3462ef61771845a78","modified":1752507699142},{"_id":"public/tags/可能有点用/index.html","hash":"79bd6035f43d59bfe3246c29b6e00e85aa8e3c89","modified":1752507699142},{"_id":"public/tags/embedding/index.html","hash":"dafd52162034706261f282373f80b66e376f8256","modified":1752507699142},{"_id":"public/tags/PEP/index.html","hash":"67df19a1679227b652d05f7c640200a6b5ad3c7f","modified":1752507699142},{"_id":"public/tags/Python/index.html","hash":"29cb23f3b27da1527102b49aa09c2a56ce977d6b","modified":1752507699142},{"_id":"public/tags/闲🉐无聊/index.html","hash":"cd41d1dd93e92ddcb2c076e8599bf7cf4b38737c","modified":1752507699142},{"_id":"public/tags/大概率没用/index.html","hash":"4f406bada2fb6b2ecceac815118af9236f68e48a","modified":1752507699142},{"_id":"public/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1752507699142},{"_id":"public/images/logo-algolia-nebula-blue-full.svg","hash":"5a81f1c5d66561b3bcb05dae48148088a3fb5c79","modified":1752507699142},{"_id":"public/images/logo.svg","hash":"099e11ab995a2c8981427a85476d082609848c77","modified":1752507699142},{"_id":"public/CNAME","hash":"9f72a1c93d67b273a3f1b776b7607fca8ecb9800","modified":1752507699142},{"_id":"public/img/CDR-data-analysis/gene.png","hash":"dac1e1481add33e6e471f62deb153719ff929a3a","modified":1752507699142},{"_id":"public/nodeppt/test/test_nodeppt.html","hash":"861eec2e1a66ee182a5a8ec962c2ad049e5d7908","modified":1752507699142},{"_id":"public/code/data_analysis/visualize_graph_analysis.py","hash":"f7ed9f01a38fe104b17464bf6e2dc7c5093ca7dc","modified":1752507699142},{"_id":"public/nodeppt/test/js/test_nodeppt.js","hash":"a0d1819335072875ac9197f43d7c09511722a9c8","modified":1752507699142},{"_id":"public/nodeppt/test/js/chunk-vendors.js.LICENSE.txt","hash":"d6fa3279c3fc7ec7562c7dcb614253f886938c32","modified":1752507699142},{"_id":"public/nodeppt/test/js/chunk-vendors.js","hash":"23c28d1f1615efdfe92ed6c634a9366a4aabb9b1","modified":1752507699142},{"_id":"public/nodeppt/test/js/test_nodeppt.js.LICENSE.txt","hash":"d6fa3279c3fc7ec7562c7dcb614253f886938c32","modified":1752507699142},{"_id":"public/nodeppt/test/img/swipe.svg","hash":"47458d9726f189ff19f7f224d24fdc60f6af1c82","modified":1752507699142},{"_id":"public/css/main.css","hash":"8fb1ec9fd1fd881b3994e3394e54d4316b312308","modified":1752507699142},{"_id":"public/js/bookmark.js","hash":"9ba4cceafd12c6d5ba8a6b986a046ef8319a7811","modified":1752507699142},{"_id":"public/css/noscript.css","hash":"4cd5301e478e0e0d4b176740ec314087ec5cb707","modified":1752507699142},{"_id":"public/images/favicon.jpg","hash":"ef2f732cc5072d26347e115d45d3899a9524d1a4","modified":1752507699142},{"_id":"public/img/Attention/AttentionViaAttention.png","hash":"eee60186ed279182176b247f00e6243b707c54a6","modified":1752507699142},{"_id":"public/img/Attention/AlternatingCo-Attention.png","hash":"514aa65627d99a19c982ec47a9ebd8a66029d07f","modified":1752507699142},{"_id":"public/img/Attention/InteractiveCo-Attention.png","hash":"d66ad9309b6851348104b591f7a265f55ba4aae4","modified":1752507699142},{"_id":"public/img/Attention/ParallelCo-Attention.png","hash":"1389f902e31640d6ff97a4375804b5b443706c02","modified":1752507699142},{"_id":"public/img/Attention/MultiheadAttention.png","hash":"1d61e6a59d33b5b70521acf9b1e8b191d04a7278","modified":1752507699142},{"_id":"public/nodeppt/test/css/chunk-vendors.4e4765ff.css","hash":"cb37dab7a01358273812ab41d4325c5c8e3c5af9","modified":1752507699142},{"_id":"public/img/Attention/TotalModel.png","hash":"b9c32980245dcf7bb4e9e54b04b59ccaa14bf265","modified":1752507699142},{"_id":"public/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1752507699142},{"_id":"public/js/motion.js","hash":"6f751f5c9499a39d7c5e1d323db3260342dd9431","modified":1752507699142},{"_id":"public/js/next-boot.js","hash":"523bbaeda463e82ab0be428cc0005717038ec63e","modified":1752507699142},{"_id":"public/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1752507699142},{"_id":"public/js/schedule.js","hash":"a1333258726caf84f368a8f8454639c7dc1626bb","modified":1752507699142},{"_id":"public/js/sidebar.js","hash":"2ee359ae48273b01ba1e0768704524e08702c7eb","modified":1752507699142},{"_id":"public/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1752507699142},{"_id":"public/js/pjax.js","hash":"694b271819aab37ce473b15db9e6aded971d82e5","modified":1752507699142},{"_id":"public/js/utils.js","hash":"345a8158e6c34e19245a07c778f5699c8673f1b1","modified":1752507699142},{"_id":"public/js/third-party/addtoany.js","hash":"5276c8f78ee562a8965216dc67d762e59cb4a9f2","modified":1752507699142},{"_id":"public/js/third-party/fancybox.js","hash":"819f382c561fe5ec23c67cc5fabd63dd1cc22dc1","modified":1752507699142},{"_id":"public/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1752507699142},{"_id":"public/js/third-party/quicklink.js","hash":"eed02e6fced8e5a653077205d4d4d7834ca71472","modified":1752507699142},{"_id":"public/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1752507699142},{"_id":"public/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1752507699142},{"_id":"public/js/third-party/analytics/matomo.js","hash":"c6a25b26a1443caa70b47fd3dfa282271574deb5","modified":1752507699142},{"_id":"public/js/third-party/analytics/google-analytics.js","hash":"def07bcc7c17d8a0caad177fb1dd2f3a5e5b3536","modified":1752507699142},{"_id":"public/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1752507699142},{"_id":"public/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1752507699142},{"_id":"public/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1752507699142},{"_id":"public/js/third-party/comments/changyan.js","hash":"260d1a77d6a3bb33a579d3e4cca1997003e799b5","modified":1752507699142},{"_id":"public/js/third-party/comments/disqusjs.js","hash":"1e826dea3f684c0515f362dc1352447a1f0eae71","modified":1752507699142},{"_id":"public/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1752507699142},{"_id":"public/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1752507699142},{"_id":"public/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1752507699142},{"_id":"public/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1752507699142},{"_id":"public/js/third-party/comments/disqus.js","hash":"da361917d65e5dca8362f8cdeb6c8cc0e8316cec","modified":1752507699142},{"_id":"public/js/third-party/search/local-search.js","hash":"3968d972f47b79acc6c3fe44028bad77c9c5aab7","modified":1752507699142},{"_id":"public/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1752507699142},{"_id":"public/js/third-party/search/algolia-search.js","hash":"1905978ef587bf08fe088ce4693a4c08db07cfbb","modified":1752507699142},{"_id":"public/js/third-party/tags/mermaid.js","hash":"df01075f52302873f7de36050b5408c8d1afb452","modified":1752507699142},{"_id":"public/js/third-party/statistics/lean-analytics.js","hash":"835cbf54c49ef1327f47df70ff2636ad36b6f57d","modified":1752507699142},{"_id":"public/js/third-party/statistics/firestore.js","hash":"6e0682bb42170d61b13b786295f45f9c785f8b73","modified":1752507699142},{"_id":"public/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1752507699142},{"_id":"public/js/third-party/tags/wavedrom.js","hash":"40dcd10df6edf124088c329346e0cc0bdac74ef1","modified":1752507699142},{"_id":"public/img/Attention/GeneralAttentionModule.png","hash":"9e9b439faf1e0e38564fc8eab3364f8e6938cf6e","modified":1752507699142},{"_id":"public/img/Attention/HierarchicalAttention.png","hash":"ddad89428142f48353d6338f6d69402ec9b0d95e","modified":1752507699142},{"_id":"public/img/Attention/MultihopAttention.png","hash":"9bb72595c6b04f2157d01b069d004eaa94d71efa","modified":1752507699142},{"_id":"public/img/Attention/CapsuleAttention.png","hash":"8d29b9e962f65aea416727e5e0d321cf00e718f1","modified":1752507699142},{"_id":"public/img/Attention/Taxonomy.png","hash":"66d9f2b6baff13a690fecf3304ecad76fbfd354c","modified":1752507699142},{"_id":"public/img/CDR-data-analysis/comprehensive_bipartite_analysis_ccle.png","hash":"368e93a9832443fc2cdeedea46465d1ec1c2b94e","modified":1752507699142},{"_id":"public/paper/1609.02907v4.pdf","hash":"1762baa638866a13dcc6d146fd5a49b36cbd9c30","modified":1752507699142},{"_id":"public/paper/Lee 等 - 2018 - Attention Models in Graphs A Survey.pdf","hash":"cc23c580b7d8063415fb6eb512053d1079b849de","modified":1752507699142},{"_id":"public/img/CDR-data-analysis/comprehensive_bipartite_analysis_gdsc.png","hash":"860912fd44247e81e0a8c54b1ff3844d3ef41e0f","modified":1752507699142},{"_id":"public/paper/Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends.pdf","hash":"0c926dd126aa2e32816348ce95767a274e55d8ef","modified":1752507699142},{"_id":"public/paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf","hash":"73f96bdf97da1cef4b43d9ee7e0d08a7900ab2fc","modified":1752507699142}],"Category":[{"name":"CDR","_id":"cmd39u3je0004qk419vht124d"},{"name":"model","parent":"cmd39u3je0004qk419vht124d","_id":"cmd39u3jk000eqk417i6qd8c3"},{"name":"Data Analysis","parent":"cmd39u3je0004qk419vht124d","_id":"cmd39u3jl000gqk412sq3bjuq"},{"name":"attention","parent":"cmd39u3jk000eqk417i6qd8c3","_id":"cmd39u3jl000kqk41e9obb0lz"},{"name":"feature related","parent":"cmd39u3jl000kqk41e9obb0lz","_id":"cmd39u3jm000wqk41hrb5gubo"},{"name":"GNN","parent":"cmd39u3jk000eqk417i6qd8c3","_id":"cmd39u3jq001oqk411vcr28pt"},{"name":"Python","_id":"cmd39u3jr001qqk41etvledb1"}],"Data":[],"Page":[{"title":"categories","date":"2025-07-05T16:25:00.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2025-07-06 00:25:00\ntype: \"categories\"\n---\n","updated":"2025-07-09T16:48:16.960Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cmd39u3j90000qk41fjls4gcw","content":"","length":0,"excerpt":"","more":""},{"title":"What is it","date":"2025-07-07T08:26:04.000Z","_content":"\n> ——为了纪念自己已经死去的表达能力\n\n这是一个什么都有可能往里面塞的博客，但本人的意愿大致如下：\n\n1. 记录自己的 *CDR奇妙生活*\n2. 发现有 *大病* 的东西记录以绷住\n3. 作为 *UESTCer* 记录一些难绷小事","source":"about/index.md","raw":"---\ntitle: What is it\ndate: 2025-07-07 16:26:04\n---\n\n> ——为了纪念自己已经死去的表达能力\n\n这是一个什么都有可能往里面塞的博客，但本人的意愿大致如下：\n\n1. 记录自己的 *CDR奇妙生活*\n2. 发现有 *大病* 的东西记录以绷住\n3. 作为 *UESTCer* 记录一些难绷小事","updated":"2025-07-09T16:48:16.959Z","path":"about/index.html","comments":1,"layout":"page","_id":"cmd39u3jd0002qk41ebixbioz","content":"<blockquote>\n<p>——为了纪念自己已经死去的表达能力</p>\n</blockquote>\n<p>这是一个什么都有可能往里面塞的博客，但本人的意愿大致如下：</p>\n<ol>\n<li>记录自己的 <em>CDR奇妙生活</em></li>\n<li>发现有 <em>大病</em> 的东西记录以绷住</li>\n<li>作为 <em>UESTCer</em> 记录一些难绷小事</li>\n</ol>\n","length":79,"excerpt":"","more":"<blockquote>\n<p>——为了纪念自己已经死去的表达能力</p>\n</blockquote>\n<p>这是一个什么都有可能往里面塞的博客，但本人的意愿大致如下：</p>\n<ol>\n<li>记录自己的 <em>CDR奇妙生活</em></li>\n<li>发现有 <em>大病</em> 的东西记录以绷住</li>\n<li>作为 <em>UESTCer</em> 记录一些难绷小事</li>\n</ol>\n"},{"title":"tags","date":"2025-07-09T16:33:53.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2025-07-10 00:33:53\ntype: \"tags\"\n---\n","updated":"2025-07-09T16:48:16.989Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cmd39u3jg0006qk41ehlzcnmq","content":"","length":0,"excerpt":"","more":""},{"_content":"# A General Survey on Attention Mechanisms in Deep Learning\n\nGianni Brauwers and Flavius Frasincar\n\nAbstract—Attention is an important mechanism that can be employed for a variety of deep learning models across many different domains and tasks. This survey provides an overview of the most important attention mechanisms proposed in the literature. The various attention mechanisms are explained by means of a framework consisting of a general attention model, uniform notation, and a comprehensive taxonomy of attention mechanisms. Furthermore, the various measures for evaluating attention models are reviewed and methods to characterize the structure of attention models based on the proposed framework are discussed. Last, future work in the field of attention models is considered\n\nIndex Terms-Attention models, deep learning, introductory and survey, neural nets, supervised learning\n\n### 1INTRODUCTION\n\nHdemao model was created for machine translation, but was quickly adopted tobe used for other tasks, such as image processing [15], video processing [16], and recommender systems [17]. The purpose of this survey is to explain the general\n\nthe computational complexity of image processing while improving performance by introducing a model that would only focus on specific regions of images instead of the entire picture. Although, the true starting point of the attention mechanisms we know today is often attributed to originate in the field of natural language processing [3]. Bahdanau et al.[3] implement attention in a machine translation model to address certain issues with the structure of recurrent neural networks. After Bahdanau et al. [3] emphasized the advantages of attention, the attention techniques were refined [4] and quickly became popular for a variety of tasks, such as text classification [5], [6], image captioning [7], [8], sentiment analysis [6], [9], and speech recognition [10], [11], [12] Attention has become a popular technique in deep learn-\n\nform of attention,and provide a comprehensive overview of attention techniques in deep learning. Other surveys have already been published on the subject of attention models For example, in [18], a survey is presented on attention in computer vision, [19] provides an overview of attention in graph models, and [20], [21],[22] are all surveys on attention in natural language processing. This paper partly builds on the information presented in the previously mentioned surveys. Yet, we provide our own significant contributions The main difference between this survey and the previously mentioned ones is that the other surveys generally focus on attention models within a certain domain. This survey however, provides a cross-domain overview of attention techniques. We discuss the attention techniques in a general way, allowing them to be understood and applied in a variety of domains. Furthermore, we found the taxonomies presented in previous surveys to be lacking the depth and structure needed to properly distinguish the various attention mechanisms. Additionally, certain significant attention techniques have not yet been properly discussed in previous surveys, while other presented attention mechanisms seem to be lacking either technical details or intuitive explanations. Therefore, in this paper, we present important attention techniques by means of a single framework using a uniform notation,a combination of both technical and in tuitive explanations for each presented attention technique, and a comprehensive taxonomy of attention mechanisms The structure of this paper is as follows. Section 2 in-\n\ning for several reasons. Firstly, models that incorporate attention mechanisms attain state-of-the-art results for all of the previously mentioned tasks, and many others. Furthermore, most attention mechanisms can be trained jointly with a base model, such as a recurrent neural network or a convolutional neural network using regular backpropa gation [3]. Additionally, attention introduces a certain type of interpretation into neural network models [8] that are generally known to be highly complicated to interpret. Moreover, the popularity of attention mechanisms was additionally boosted after the introduction of the Transformer model [i3] that further proved how effective attention can be.Attention was originally introduced as an extension to recurrent neural networks [14]. However, the Transformer model proposed in [13] poses a major development in attention research as it demonstrates that the attention mechanism is sufficient to build a state-of-the-art model. This means that disadvantages, such as the fact that recurrent neural networks are particularly difficult to parallelize, can\n\ntroduces a general attention model that provides the reader with a basic understanding of the properties of attention and how it can be applied. One of the main contributions of this paper is the taxonomy of attention techniques presented in Section 3. In this section, attention mechanisms are explained and categorized according to the presented\n\n![](./images/fXAE7qMmeZtPgm2VeKxkN3y8GAQXG8CbW.png)\n\nFig. 1. An illustration of the general structure of the task model.\n\ntaxonomy. Section 4 provides an overview of performance measures and methods for evaluating attention models. Furthermore,the taxonomy is used to evaluate the structure of various attention models. Lastly, in Section 5, we give oun conclusions and suggestions for further research\n\n## 2 General Attention Model\n\nThis section presents a general form of attention with corresponding notation. The notation introduced here is based on the notation that was introduced in [23] and popularized in [13]. The framework presented in this section is used throughout the rest of this paper. To implement a general attention model, it is necessary\n\nto first describe the general characteristics of a model that can employ attention. First of all, we will refer to the complete model as the task model, of which the structure is presented in Fig. 1. This model simply takes an input, carries out the specified task, and produces the desired output.For example, the task model can be a language model that takes as input a piece of text, and produces as output a summary of the contents,a classification of the sentiment, or the text translated word for word to another language. Alternatively the task model can take an image, and produce a captionthe general attention module is the query $q\\in\\mathbb{R}^{d_{q}}$, andthe or segmentation for that image. The task model consists of four submodels: the feature model, the query model, the attention model, and the output model. In Subsection 2.1, the feature model and query model are discussed, which are used to prepare the input for the attention calculation. In Subsection 2.2, the attention model and output model are discussed, which are concerned with producing the output\n\n## 2.1Attention Input\n\nSuppose the task model takes as input the matrix $X\\in$ $\\Pi_{\\mathcal{R}}d_{\\mathcal{F}}\\times\\boldsymbol{n}_{\\mathcal{X}}$ where $d_{x}$ represents the size of the input vectors and $n_{x}$ represents the amount of input vectors. The columns in this matrix can represent the words in a sentence the pixels in an image, the characteristics of an acoustic sequence, or any other collection of inputs.The feature model is then employed to extract the $n_{f}$ feature vectors $f_1,\\ldots,f_{n_f}\\in\\mathbb{R}^d_f$ from $X$ where $d_{f}$ represents the size of the feature vectors.The feature model can be a recurrent neural network (RNN), a convolutional neural network (CNN), a simple embedding layer, a linear transformation of the original data, or no transformation at all. Essentially, the feature model consists of all the steps that transform the original input $X$ into the feature vectors $f_{1},\\ldots,f_{n_{f}}$ that the attention modelwill attend to.\n\n![](./images/fxeeSs49eiCx6zygk9gTruATCtkbbN2Om.png)\n\nFig. 2. The inner mechanisms of the general attention module\n\nTo determine which vectors to attend to,the attention model requires the query $q\\in\\mathbb{H}^d_q$ ,where $d_q$ indicates the size of the query vector. This query is extracted by the query model, and is generally designed based on the type of output that is desired of the model. A query tells the attention model which feature vectors to attend to.It can be interpreted literally as a query, or a question. For example, for the task of image captioning, suppose that one uses a decoder RNN model to produce the output caption based on feature vectors obtained from the image by a CNN.At each prediction step, the hidden state of the RNN model can be used as a query to attend to the CNN feature vectors. In each step, the query is a question in the sense that it asks for the necessary information from the feature vectors based on the current prediction context\n\n### 2.2 Attention Output\n\nThe feature vectors and query are used as input for the attention model. This model consists of a single, or a collection of general attention modules.An overview of a general attention module is presented in Fig. 2. The input of matrix of feature vectors $\\boldsymbol{F}=[\\boldsymbol{f}_{1},\\ldots,\\boldsymbol{f}_{n_{f}}]\\in\\mathbb{R}^{d_{f}\\times n_{f}}.$ Two separate matrices are extracted from the matrix $F$ : the keys matrix $K=[k_{1},\\ldots,k_{n_{f}}]\\in\\mathbb{R}^{d_{k}\\times n_{f}}$, and the values matrix $V$ = $[ \\boldsymbol{v}_1, \\ldots , \\boldsymbol{v}_{n_f}]$ $\\in$ $\\mathbb{R} ^{d_v\\times n_f}$, where $d_{k}$ and $d_{v}$ indicate, respectively, the dimensions of the key vectors (columns of $K$ ) and value vectors (columns of $V$ ). The general way of obtaining these matrices is through a linear transformation of $F$ using the weight matrices. $W_{K}\\in\\mathbb{R}^{d_{k}\\times d_{f}}$ and $W_{V}\\in\\mathbb{R}^{d_{v}\\times d_{f}}$ ,for $K$ and $V$ , respectively. The calculations of $K$ and $V$ are presented in (1). Both weight matrices can be learned during training or predefined by the researcher For example, one can choose to define both $W_{K}$ and $W_{V}$ as equal to the identity matrix to retain the original feature vectors. Other ways of defining the keys and the values are also possible, such as using completely separate inputs for the keys and values. The only constraint to be obeyed is that the number of columns in $K$ and $V$ remains the same\n$$K_{d_{k}\\times n_{f}}=W_{K}\\times F_{d_{f}\\times n_{f}},\\quad V_{d_{v}\\times n_{f}}=W_{V}\\times F_{d_{f}\\times n_{f}}.$$\n\nThe goal of the attention module is to produce a weighted average of the value vectors in $V$ .The weights used to produce this output are obtained via an attention scoring and alignment step.The query $q$ and the keys matrix $K$ are used to calculate thevector of attention scores\n\n$\\boldsymbol{e}=\\left[e_{1},\\ldots,e_{n_{f}}\\right]\\in\\mathbb{R}^{n_{f}}.$ This is done via the score function score(), as illustrated in (2)\n\n$$e_{l}\\:=\\:\\mathrm{score}(\\:q\\:,\\:k_{l}\\:).$$\n\nAs discussed before, the query symbolizes a request for information. The attention score $e_{1}$ represents how important the information contained in the key vector $k_{l}$ is according to the query. If the dimensions of the query and key vectors are the same, an example of a score function would be to take the dot-product of the vectors. The different types of score functions are further discussed in Section 3.2.1. Next, the attention scores are processed further through\n\nan alignment layer. The attention scores can generally have a wide range outside of [0,1] . However, since the goal is to produce a weighted average, the scores are redistributed via an alignment function align() as defined in (3).\n\n$$\\begin{matrix}a_l&=\\text{align}(\\begin{array}{c}e_l\\:;&\\boldsymbol{e}\\\\1\\times1&n_f\\times1\\end{array}),\\end{matrix}$$\n\nwhere $a_l\\in\\mathbb{R}^1$ is the attention weight corresponding to the lth value vector.One example of an alignment function wouldbe touse a softmax function,but the various other alignment types are discussed in Section 3.2.2. The attention weights provide a rather intuitive interpretation for the attention module. Each weight is a direct indication of how important each feature vector is relative to the others for this particular problem. This can provide us with a more in-depth understanding of the model behaviour, and the re lations between inputs and outputs. The vector of attention weights $a$ = $[ a_{1}, \\ldots , a_{n_{f}}]$ $\\in$ $\\mathbb{R} ^{n_{f}}$ is used to produce the context vector $c\\in\\mathbb{R}^{d_v}$ by calculating a weighted average of the columns of the values matrix $V$ ,as shown in (4)\n$$\\boldsymbol{c}_{d_v\\times1}=\\sum_{l=1}^{n_f}a_l\\:\\times\\:\\boldsymbol{v}_l\\:.$$\n\nAs illustrated in Fig. 1, the context vector is then used in the output model to create the output $\\ddot{y}$ This output model translates the context vector into an output prediction. For example, it could be a simple softmax layer that takes as input the context vector $C$ , as shown in (5)\n\n$$\\hat{\\boldsymbol{y}}_{d_{y}\\times1}=\\mathrm{softmax}(W_{c}\\:\\times\\:c_{d_{y}\\times1}\\:+\\:b_{c}\\:),$$\n\nwhere $d_{y}$ is the number of output choices or classes, and $W_{c}\\in\\mathbb{R}^{d_{y}\\times d_{v}}$ and $b_{c}\\in\\mathbb{R}^{d}$ are trainable weights\n\n### 2.3 Attention Applications\n\nAttention is a rather general mechanism that can be used in a wide variety of problem domains. Consider the task of machine translation using an RNN model. Also, consider the problem of image classification using a basic CNN model. While an RNN produces a sequence of hidden state vectors, a CNN creates feature maps, where each region in the image is represented by a feature vector. The RNN hidden states are organized sequentially, while the CNN feature maps are organized spatially. Yet, attention can still be applied in both situations, since the attention mechanism does not inherently depend on the organization of thefeature vectors. This characteristic makes attention easy to implement in a wide variety of models in different domains.\n\nAnother domain where attention can be applied is audio processing [24], [25]. Acoustic sequences can be represented by a sequence of feature vectors that relate to certain time periods of the audio sample. These vectors could simply be the raw input audio, or they can be extracted via,for example, an RNN or CNN. Video processing is another domain where attention can be applied intuitively [26], [27] Video data consists of sequences of images, so attention can be applied to the individual images, as well as the entire sequence. Recommender systems often incorporate a users interaction history to produce recommendations. Feature vectors can be extracted based on, for example, the id's or other characteristics of the products the user interacted with, and attention can be applied to them [28]. Attention can generally also be applied to many problems that use a time series as input, be it medical [29], financial [30], or anything else, as long as feature vectors can be extracted The fact that attention does not rely on the organization\n\nof the feature vectors allows it to be applied to various problems that each use data with different structures, as illustrated by the previous domain examples.Yet,this can be taken even further by applying attention to data where there is irregular structure. For example, protein structures, city traffic flows, and communication networks cannot always be represented using neatly structured organizations, such as sequences, like time series, or grids, like images. In such cases, the different aspects of the data are often represented as nodes in a graph. These nodes can be represented by feature vectors, meaning that attention can be applied in domains that use graph-structured data as well [19],[31] In general, attention can be applied to any problem for\n\nwhich a set of feature vectors can be defined or extracted As such, the general attention model presented in Fig. 2 is applicable to a wide range of domains. The problem,how ever, is that there is a large variety of different applications and extensions of the general attention module. As such, in Section 3, a comprehensive overview is provided of a collection of different attention mechanisms.\n\n## 3ATTENTION TAXONOMY\n\nThere are many different types of attention mechanisms and extensions,and a model can use different combinations of these attention techniques. As such, we propose a taxonomy that can be used to classify different types of attention mechanisms. Fig. 3 provides a visual overview of the different categories and subcategories that the attention mechanisms can be organized in. The three major categories are based on whether an attention technique is designed to handle specific types of feature vectors (feature-related), specific types of model queries (query-related), or whether it is simply a general mechanism that is related to neither the feature model, nor the query model (general). Further explanations of these categories and their subcategories are provided in the following subsections. Each mechanism discussedin this section is either amodification to the existing inner mechanisms of the general attention module presented in Section 2, or an extension of it The presented taxonomy can also be used to analyze\n\nthe architecture of attention models.Namely, the major categories and their subcategories can be interpreted as\n\n![](./images/fwzKYgHVze3V4K8ZYBR28uKos0SPY48WT.png)\n\nFig. 3. A taxonomy of attention mechanisms\n\n### TABLE 1 Notation.\n\nattend to these various vectors.These features may have specific structures that require special attention mechanisms to handle them.These mechanisms can be categorized to deal with one of the following feature characteristics: the multiplicity of features, the levels of features, or the representations of features\n\n### 3.1.1Multiplicity of Features\n\nSymbol\tDescription\n$F$\tMatrix of size $d_f \\times n_f$ containing the feature vectors $f_1, \\ldots, f_{n_f} \\in \\mathbb{R}^{d_f}$ as columns. These feature vectors are extracted by the feature model.\n$K$\tMatrix of size $d_k \\times n_j$ containing the key vectors $k_1, \\ldots, k_{n_j} \\in \\mathbb{R}^{d_k}$ as columns. These vectors are used to calculate the attention scores.\n$V$\tMatrix of size $d_v \\times n_j$ containing the value vectors $v_1, \\ldots, v_{n_j} \\in \\mathbb{R}^{d_v}$ as columns. These vectors are used to calculate the context vector.\n$W_K$\tWeights matrix of size $d_k \\times d_f$ used to create the $K$ matrix from the $F$ matrix.\n$W_V$\tWeights matrix of size $d_v \\times d_f$ used to create the $V$ matrix from the $F$ matrix. This vector essentially represents a question, and is used to calculate the attention scores.\n$q$\tQuery vector of size $d_q$. This vector is used to calculate the attention weights.\n$c$\tContext vector of size $d_c$. This vector is the output of the attention model.\n$e$\tScore vector of size $d_{e_i}$ containing the attention scores $e_1, \\ldots, e_{n_e} \\in \\mathbb{R}^1$. These are used to calculate the attention weights.\n$a$\tAttention weights vector of size $d_{a_j}$ containing the attention weights $a_1, \\ldots, a_{n_a} \\in \\mathbb{R}^1$. These are the weights used in the calculation of the context vector.\n\nFor most tasks, a model only processes a single input, such as an image, a sentence, or an acoustic sequence. We refer to such a mechanism as singular features attention. Other models are designed to use attention based on multiple inputs to allow one to introduce more information into the model that can be exploited in various ways.However, this does imply the presence of multiple feature matrices that require special attention mechanisms to be fully used. For example, [32] introduces a concept named co-attention to allow the proposed visual question answering (VQA) model to jointly attend to both an image and a question Co-attention mechanisms can generally be split up\n\ninto two groups [33]: coarse-grained co-attention and fine-grained co-attention. The difference between the two groups is the way attention scores are calculated based on the two feature matrices. Coarse-grained attention mecha-. nisms use a compact representation of one feature matrix as a query when attending to the other feature vectors. Fine-grained co-attention, on the other hand, uses all feature vectors of one input as queries. As such, no information is lost, which is why these mechanisms are called fine-grained As an example of coarse-grained co-attention, [32] pro-\n\northogonal dimensions of an attention model. An attention model can consist of a combination of techniques taken from any or all categories. Some characteristics, such as the scoring and alignment functions, are generally required for any attention model. Other mechanisms, such as multihead attention or co-attention are not necessary in every situation. Lastly, in Table 1, an overview of used notation with corresponding descriptions is provided.\n\nposes an alternating co-attention mechanism that uses the context vector (which is a compact representation) from one attention module as the query for the other module, and vice versa. Alternating co-attention is presented in Fig. 4. Given a set of two input matrices $X^{(1)}$ and $X^{(2)}$ , features are extracted by a feature model to produce the feature extracts feature vectors so that the attention model canmatrices $F^{(1)}\\in\\mathbb{R}^{d_f^{(1)}\\times n_f^{(1)}}$ and $F^{(2)}\\in\\mathbb{R}^{d_{f}^{(2)}\\times n_{f}^{(2)}}$ ,where $d_{f}^{(1)}$\n\n## 3.1 Feature-Related Attention Mechanisms\n\nBased on a particular set of input data, a feature model\n\n![](./images/fTFNeAz730mbNSQS1tZxyhe0r9V8CHLx8.png)\n\nFig. 4. An illustration of alternating co-attention\n\nvectors extracted from the first and second inputs, while co-attention is used for VQA, so the two input matrices are the image data and the question data, for which the feature model for the image consists of a CNN model, and the feature modelfor the question consists of word embeddings a convolutional layer, a pooling layer, and an LSTM model. Firstly, attention is calculated for the first set of features $F^{(1)}$ without the use of a query (Attention Module in Fig. 4). In as a weighted average of the second set of value vectors [32], an adjusted additive attention score function is used for this attention mechanism.The general form of the regularattention module,which will produce the context vector additive score function can be seen in (6).\n$$\\mathrm{score}(\\:\\boldsymbol{q}\\:,\\:\\boldsymbol{k}_{l}\\:)=\\:\\boldsymbol{w}^{T}\\times\\mathrm{act}(\\boldsymbol{W}_{1}\\times\\:\\boldsymbol{q}\\:+\\:\\boldsymbol{W}_{2}\\times\\:\\boldsymbol{k}_{l}+\\boldsymbol{b}\\:),\\\\d_{q}\\times1\\quad d_{k}\\times1\\quad1\\times d_{w}\\quad d_{w}\\times d_{q}\\quad d_{q}\\times1\\quad d_{w}\\times d_{k}\\quad d_{k}\\times1\\quad d_{w}\\times1$$\n\nwhere act() is a non-linear activation function, and $w\\in$ $\\Pi^{\\mathcal{R}}d_{\\mathcal{n}\\mathcal{F}}$ ， $W_{1}$ $\\in$ $\\mathbb{R} ^{d_{w}\\times d_{q}}$ ， $W_{2}\\in$ $\\mathbb{R} ^{d_{w}\\times d_{k}}$, and $b\\in\\mathbb{R}^{d_{w}}$ are trainable weights matrices, for which $d_{w}$ is a predefined dimension of the weight matrices.A variant of this score function adapted to be calculated without a query for the application at hand can be seen in (7).\n\n$$e_{l}^{(0)}=\\boldsymbol{w}^{(1)T}\\times\\operatorname{act}(\\boldsymbol{W}^{(1)}\\times\\boldsymbol{k}_{l}^{(1)}+\\boldsymbol{b}^{(1)}),\\\\1\\times1\\quad1\\times d_{w}\\quad d_{w}\\times d_{k}^{(1)}\\quad d_{k}^{(1)}\\times1\\quad d_{w}\\times1$$\n\nwhere $w^{(1)}\\in\\mathbb{R}^{d_{w}}$ ， $W^{(1)}\\in\\mathbb{R}^{d_{\\omega}\\times d_{k}^{(1)}}$ and $b^{(1)}\\in\\mathbb{R}^{d_{M}}$ areafter another. This may come with a computational distrainable weight matrices for Attention Module,. $k_l^{(1)}\\in$ advantage since it is not possible to parallize. Instead of $\\Pi\\text{砥}^{d_k^{(1)}}$ is the Ith column of the keys matrix $K^{(1)}$ that wasusing a sequential mechanism like alternating co-attention, obtained from $F^{(1)}$ via a linear transformation (see (1), for which $d_{w}$ is a prespecified dimension of the weight matrices and $d_{k}^{(1)}$ is a prespecifiedimension of the key vectors. Perhaps onemay wonder why the query is absent whenqueries,unweighted averages of the keyvectors areused as\n\ncalculating attention in this manner. Essentially, the query in this attention modelis learned alongside the other trainable parameters. As such, the query can be interpreted as a general question: \"Which feature vectors contain the most important information?\".This is also known as a selfattentive mechanism, since attention is calculated based only on the feature vectors themselves. Self-attention is explained in more detail in Subsection 3.3.1 The scores are combined with an alignment function\n\n(see (3)), such as the softmax function, to create attention weights used to calculate the context vector $\\boldsymbol{c}^{(0)}\\in\\mathbb{R}^{\\boldsymbol{d}_{\\mathbb{V}}^{(1)}}$ (see (4). This context vector is not used as the output of the attention model, but rather as a query for calculating the context vector $\\boldsymbol{c}^{(2)}\\in\\mathbb{R}^{d_{w}^{(2)}}$ based on the second feature vectors $c^{(1)}$ and $c^{(2)}$ matrix $F^{(2)}$ where $d_{v}^{\\langle2\\rangle}$ is thedimension of the valuevectors obtained from $F^{(2)}$ via a linear transformation (see (1).For compact representation of one input to use as a query when\n\n![](./images/ftdzr2qwpgGo72yaqTGymWrhgqyyzp9TO.png)\n\nFig. 5. An illustration of interactive co-attention\n\nand $d_{f}^{(2)}$ represent, respectively the dimension of the feature this module (Attention Modulez in Fig. 4), atention scores are calculated using another score function with $C_{0}$ as query $n_f^{(1)}$ Tand $n_{f}^{(2)}$ re esen et ely t ento e ei ut on e n ie ne nc oncie e n \n\n$$e_{l}^{(2)}=\\mathrm{score}(\\begin{array}{c}c^{(0)}\\:,\\:k_{l}^{(2)}\\\\d_{v}^{(1)}\\times1\\:d_{k}^{(2)}\\times1\\end{array}).$$\n\nThese attention scores are then used to calculate attentior weights using, for example, a softmax function as alignment function,after which the context vector $c^{(2)}$ can bederived Finally, the context vector $c^{(2)}$ is used as a query for the first $c^{(1)}$ for the first feature matrix $F^{(1)}$ .Attention scores are calculated according to (9). In [32], the same function and weight matrices as seen in (7) are used, but with an added query making it the same as the general additive score function (see (6)). The rest of the attention calculation is similar as before\n\n$$e_{l}^{(1)}=\\mathrm{score}(\\begin{array}{c}c^{(2)}\\:,\\:k_{l}^{(1)}\\:).\\\\d_{v}^{(2)}\\times1\\:d_{k}^{(1)}\\times1\\end{array}$$\n\nThe produced context vectors $c^{(1)}$ and $c^{(2)}$ are concatenated and used for prediction in the output model.Alternating co-attention inherently contains a form of sequentiality due to the fact that context vectors need to be calculated one [34] proposes the interactive co-attention mechanism that can calculate attention on both feature matrices in parallel, as depicted in Fig. 5. Instead of using the context vectors as queries.The calculation of the average keys are provided in (10), and the calculation of the attention scores are shown in (11). Any score function can be used in this case, but an additive score function is used in [34]\n\n$$\\bar{\\boldsymbol{k}}_{d_k^{(1)}\\times1}^{(1)}=\\frac{1}{n_f^{(1)}}\\sum\\limits_{l=1}^{n_f^{(1)}}\\boldsymbol{k}_l^{(1)},\\quad\\bar{\\boldsymbol{k}}_{d_k^{(2)}\\times1}^{(2)}=\\frac{1}{n_f^{(2)}}\\sum\\limits_{l=1}^{n_f^{(2)}}\\boldsymbol{k}_l^{(2)};$$\n\n$$e_{l}^{(1)}=\\mathrm{score}(\\:\\bar{\\boldsymbol{k}}^{(2)}\\:,\\:\\boldsymbol{k}_{l}^{(1)}\\:),\\:e_{l}^{(2)}=\\mathrm{score}(\\:\\bar{\\boldsymbol{k}}^{(1)}\\:,\\:\\boldsymbol{k}_{l}^{(2)}\\:).\\\\1\\times1\\quad d_{k}^{(2)}\\times1\\quad d_{k}^{(1)}\\times1\\quad1\\times1\\quad d_{k}^{(1)}\\times1\\quad d_{k}^{(2)}\\times1$$\n\nFrom the attention scores, attention weights are created via an alignment function, and are used to produce the context\n\nWhile coarse-grained co-attention mechanisms use a\n\ncalculating attention for another input, fine-grained coattention considers every element of each input individually when calculating attention scores.In this case, the query becomes a matrix. An example of fine-grained co-attention is parallel co-attention [32]. Similarly to interactive coattention, parallel co-attention calculates attention on the two feat ue mariesa hesaemeti e $K^{(1)}\\in\\mathbb{R}^{d_k^{(1)}\\times n_f^{(1)}}$ .d $K^{(2)}\\in\\mathbb{R}_{k}^{d_{k}^{(2)}\\times n_{f}^{(2)}}$ that are obtained by linearly transforming the feature matrices $F^{(1)}$ and $F^{(2)}$ ,where $d_k^{(1)}$ and $d_k^{(2)}$ are $K^{(1)}$ C $K^{(2)}$ Sopdehna tiset dtoedere $A$ $\\in$ $\\mathbb{R} ^{n_f^{( \\dagger ) }\\times n_f^{( 2) }}$ difet die eastons sn afitysaeix atento sore asho na 6) translate one keys matrix to the space of the other keys. In [32], $A$ is calculated as shown in (12).\n$$A_{n_{f}^{(1)}\\times n_{f}^{(2)}}=\\operatorname{act}(\\begin{array}{ccc}K^{(1)^{T}}&\\times&W_{A}&\\times&K^{(2)}\\\\n_{f}^{(1)}\\times d_{k}^{(1)}&&d_{k}^{(1)}\\times d_{k}^{(2)}&&d_{k}^{(2)}\\times n_{f}^{(2)}\\end{array}),$$\n\nweights using an alignment function,so that two context\n\nwhere $W_{A}\\in\\mathbb{R}^{d_{k}^{(1)}\\times d_{k}^{(2)}}$ is a trainable weights matrix and $\\arctan()$ is an activation function for which the tanh() function this matrix,i.e., one can use (13) to calculate each individual element $A_{i,j}$ of the matrix $A$\n\n$$A_{i,j}=\\:\\boldsymbol{w}_{A}^{T}\\:\\times\\mathrm{concat}(\\boldsymbol{k}_{i}^{(1)},\\boldsymbol{k}_{j}^{(2)},\\boldsymbol{k}_{i}^{(1)}\\circ\\boldsymbol{k}_{j}^{(2)}),\\_{1\\times1}\\quad1\\times3d_{k}\\quad d_{k}\\times1\\quad d_{k}\\times1\\quad d_{k}\\times1\\quad d_{k}\\times1$$\n\nwhere $w_{A}\\in\\mathbb{R}^{3d_{k}}$ denotes a trainable vector of weights, concat() denotes vector concatenation, and 0 denotes element-wise multiplication, also known as the Hadamard product. Note that the keys of each keys matrix in this case must have the same dimension $d_{k}$ for the element wise multiplication to work.The affinity matrix can be interpreted as a similarity matrix for the columns of the two keys matrices, and helps translate, for example, image keys to the same space as the keys of the words in a sentence, and vice versa.The vectors of attention scores $e^{(1)}$ and $e^{(2)}$ can be calculated using an altered version of the additive score function as presented in (14) and (15). The previous attention score example i this survey all used a score func- fllowing feature matrices:. $\\boldsymbol{F}^{t}$ = $[ \\boldsymbol{f}_{1}^{t}, \\ldots , \\boldsymbol{f}_{n_{f}^{t}}^{t}]$ $\\in$ $\\mathbb{R} ^{d_{f}^{t}\\times n_{f}^{t}}$ tion to calculate each attention score for each value vector individually. However, (14) and (15) are used to calculate $F^{l}$ = $[ f_{1}^{l}, \\ldots , f_{n_{f}^{l}}^{l}]$ $\\in$ $\\mathbb{R} ^{d_{f}^{l}\\times n_{f}^{l}}.$ and $F^{r}$ = $[ f_{1}^{r}, \\ldots , f_{n_{f}^{r}}^{r}]$ $\\in$ the complete vector of all attention scores. Essentially, the $\\mathbb{R}^{d_f^r\\times n_f^r}$, for the target phrase words, left context words, attention scores are calculated in an aggregated form.\n\n$$\\begin{gathered}\ne^{(1)} =\\:\\boldsymbol{w}_{1}\\times\\mathrm{act}(\\boldsymbol{W}_{2}\\times\\boldsymbol{K}^{(2)}\\times\\:\\boldsymbol{A}^{T}\\:+\\boldsymbol{W}_{1}\\times\\boldsymbol{K}^{(1)}); \\\\\n1\\times n_{f}^{(1)} 1\\times d_{w}\\quad d_{w}\\times d_{k}^{(2)}\\:d_{k}^{(2)}\\times n_{f}^{(2)}\\:n_{f}^{(2)}\\times n_{f}^{(1)}\\:d_{w}\\times d_{h}^{(1)}\\:d_{k}^{(1)}\\times n_{f}^{(1)} \\\\\ne^{(2)} =\\:\\boldsymbol{w}_{2}\\times\\mathrm{act}(\\boldsymbol{W}_{1}\\times\\boldsymbol{K}^{(1)}\\times\\:\\boldsymbol{A}\\:+\\boldsymbol{W}_{2}\\times\\boldsymbol{K}^{(2)}), \\\\\n1\\times r_{e}^{(2)}f 1\\times d_{w}\\quad d_{w}\\times d_{k}^{(1)}\\:d_{k}^{(1)}\\times n_{f}^{(1)}\\:n_{f}^{(1)}\\times n_{f}^{(2)}\\:d_{w}\\times d_{k}^{(2)}\\:d_{k}^{(2)}\\times n_{f}^{(2)} \n\\end{gathered}$$\n\nwhere $w_1\\in\\mathbb{R}^d_u$ ， $w_2\\in\\mathbb{R}^dw$ ， $W_{1}$ $\\in$ $\\mathbb{R} ^{d_{\\omega }\\times d_{h}^{( 1) }}$ and that tanh() is used in [32] for the activation function, and the feature matrices are used as the key matrices. In that case, the affinity matrix $A$ can be seen as a translator between feature spaces. As mentioned before, the affinity matrix is essentially a similarity matrix for the key vectors of the two\n\n![](./images/fcezbCFVVAgYitIXTykvdtgkU8QRrQGOK.png)\n\nFig. 6. An illustration of parallel co-attention\n\nthe maximum similarity value in a row or column as the\n\n$$e_{i}^{(1)}=\\max_{j=1,\\ldots,n_{f}^{(2)}}A_{i,j},\\quad e_{j}^{(2)}=\\max_{i=1,\\ldots,n_{f}^{(1)}}A_{i,j}.$$\n\nNext,the attention scores are used to calculate attention vectors $c^{(1)}$ and $c^{(2)}$ can be derived as weighted averages of the value vectors that are obtained from linearly transform ing the features. For the alignment function, [32] proposes is used in[32].[35] proposes a different way of calculatingtouse a softmaxfunction,and thevalue vectors are simply set equal to the feature vectors. The resulting context vectors can be either concatenated or added together Finally, coarse-grained and fine-grained co-attention can\n\nbe combined to create an even more complex co-attention mechanism. [33] proposes the multi-grained co-attention mechanism that calculates both coarse-grained and finegrained co-attention for two inputs.Each mechanism produces one context vector per input. The four resulting context vectors are concatenated and used in the output model for prediction A mechanism separate from co-attention that still uses\n\nmultiple inputs is the rotatory attention mechanism [36] This technique is typically used in a text sentiment analysis setting where there are three inputs involved: the phrase for which the sentiment needs to be determined (target phrase) the text before the target phrase (left context), and the text after the target phrase (right context). The words in these three inputs are all encoded by the feature model, producing the\n\n$d_{f}^{r}$ d rghtenete ns nesctie hee $d_{f}^{t}$, $d_{f}^{l}$, do $d_{f}^{l}$, rand corresponding inputs, and $n_{f}^{t}$, $n_{f^{\\prime }}^{l}$, n $n_{f}^{l}$, and $n_{f}^{r}$ represent the number of feature vectors for the corresponding inputs. The feature model used in[36]consists of word embeddings and separate Bi-LSTM models for the target phrase, the left context, and the right context. This means that the feature vectors arein fact thehidden statevectors obtained from the $W_{2}\\in$ $\\mathbb{R} ^{d_{w}\\times d_{k}^{( 2) }}$ are trainable weight matrices, for which Bi-LSTM models. Using these features, the idea is to extract $d_{w}$ is a prespecified dimension of the weight matrices. Note a single vector $T^*$ from the inputs such that a softmax layer can be used for classification. As such, we are now faced with two challenges: how to represent the inputs as a single vector, and how to incorporate the information from the left and right context into that vector. [36] proposes to use the rotatory attention mechanism for this purpose.\n\nFirstly, a single target phrase representation is created most importantinformation in the actual target phrase itself by using a pooling layer that takes the average over the Essentially, the mechanism rotates attention between the columns of $F^{t}$ ,as shown in (17).\n\n$$r_{_{d_{f}^{t}\\times1}}^{t}=\\frac{1}{n_{f}^{t}}\\sum_{i=1}^{n_{f}^{t}}f_{i}^{t}\\:.$$\n\n$r^{l}$ is then used as a query to create a context vector out of various scans or documents, that can provide different types the left and right contexts, separately. For example, for the left context,the key vectors $k_{1}^{l},\\ldots,k_{n_{f}^{l}}^{l}\\in\\mathbb{R}^{d_{k}^{l}}$ and value vectors $\\boldsymbol{v}_{1}^{l},\\ldots,\\boldsymbol{v}_{n_{f}^{l}}^{l}\\in\\mathbb{R}^{d_{\\nu}^{l}}$ are extracted rom the let context mages and semantic l gsimuldaneously iel aly n $d_{k}^{l}$ tire $d_{v}^{l}$ cors $f_{1}^{l},\\ldots,f_{n_{f}^{l}}^{l}\\in\\mathbb{R}^{d_{f}^{l}}$ sima ee ee e e respectively. Note that [36] proposes to use the original feature vectors as keys and values, meaning that the linear transformation consists of a multiplication by an identity matrix. Next, the scores are calculated using (18).\n\n$$e_{i}^{l}=\\text{score}(\\begin{array}{c}r^{t}\\\\d_{f}^{t}\\times1\\end{array},\\begin{array}{c}k_{i}^{l}\\\\d_{k}^{l}\\times1\\end{array}).$$\n\nFor the score function, [36] proposes to use an activated general score function [34] with a tanh activation function. The attention scores can be combined with an alignment function and the corresponding value vectors to produce the context vector $r^{l}\\in\\mathbb{R}^{d_{v}^{l}}$ The alignment function used in [36] takes the form of a softmax function. An analogous procedure can be performed to obtain the representation of the right context, $r^{T}$ .These two context representations can then be used to create new representations of the target phrase, $\\Pi\\text{思}^d_{k_0}^r$ and value vectors $\\boldsymbol{v}_{1}^{t}, \\ldots , \\boldsymbol{v}_{n_{f}^{t}}^{t}$ $\\in$ $\\mathbb{R} ^{d_{v}^{t}}$ are extracted from the target phrase feature vectors $f_{1}^{t},\\ldots,f_{n_{f}^{t}}^{t}\\in\\mathbb{R}^{d_{f}^{t}}$ 3.1.2 Feature Levels similarly as before, using a linear transformation, where $d_{k}^{t}$ and $d_{v}^{t}$ are the dimensions of the key and value vectors respectively. Note, again, that the original feature vectors as keys and values in [36]. The attention scores for the leftaware target representation are then calculated using (19).\n$$e_{i}^{l_{t}}=\\mathrm{score}(\\begin{array}{c}r^{l}\\\\d_{v}^{l}\\times1\\end{array},\\begin{array}{c}k_{i}^{t}\\\\d_{k}^{t}\\times1\\end{array}).$$\n\nThe attention scores can be combined with an alignment function and the corresponding value vectors to produce the context vector $r^{l_t}\\in\\mathbb{R}^{d_v^t}$ . For this attention calculation, [34] proposes to use the same score and alignment functions as before. The right-aware target representation $r^{r^{\\prime}t}$ can be calculated in a similar manner. Finally, to obtain the full representation vector 7 that is used to determine the classification, the vectors $r^{l}$, $r^{r}$, $r^{l_{t}}$, $r^{l_{s}}$ p and $r^{r_t}$ are concatenated together, as shown in (20).\n\n$$r_{(d_{v}^{l}+d_{v}^{r}+d_{v}^{t}+d_{v}^{t})\\times1}=\\mathrm{concat}(\\begin{array}{c}r^{l},\\\\d_{v}^{l}\\times1\\end{array},\\begin{array}{c}r^{r},\\\\d_{v}^{t}\\times1\\end{array},\\begin{array}{c}r^{r_{t}}\\\\d_{v}^{t}\\times1\\end{array}).$$\n\nTo summarize, rotatory attention uses the target phrase to compute new representations for the left and right context using attention, and then uses these left and right reprephrase. The first step is designed to capture the words in the left and right contexts that are most important to the target phrase. The second step is there to capture the\n\ntarget and the contexts to improve the representations. There are many applications where combining informa-\n\ntion from different inputs into a single model can be highly beneficial. For example,in the field of medical data,there are often many different types of data available, such as of information. In [37], a co-attention mechanism is used for automatic medical report generation to attend to both study is used in [38] to show that the co-attention part of the model specifically improves performance. A field where multi-feature attention has been extensively explored is the domain of recommender systems.For example,in [39], a coattention network is proposed that attends to both product reviews and the reviews a user has written. In[40], a model is proposed for video recommendation that attends to both user features and video features. Co-attention techniques have also been used in combination with graph networks for the purpose of, for example, reading comprehension across multiple documents [41] and fake news detection [42]. In comparison to co-attention, rotatory attention has typically been explored only in the field of sentiment analysis, which is most likely due to the specific structure of the data that is necessary to use this technique. An implementation of rotatory attention is proposed in [43] for sentiment analysis, where the mechanism is extended by repeating the attention again, using attention. Firstly,the key vectors $k_{1}^{t},\\ldots,k_{n_{f}^{t}}^{t}\\in$ wotationto mectively furthen dep byepe repgese aateons\n\nThe previously discussed attention mechanisms process. data at a single level. We refer to these attention techniques as single-level attention mechanisms. However, some data. types can be analyzed and represented on multiple levels. For example, when analyzing documents, one can analyze the document at the sentence level, word level,or even the character level. When representations or embeddings of all these levels are available, one can exploit the extra levels of information. For example, one could choose to perform translation based on either just the characters, or just the words of the sentence. However, in [44], a technique named attention-via-attention is introduced that allows one to incorporate information from both the character, and the word levels. The idea is to predict the sentence translation character-by-character, while also incorporating information from a word-level attention module. To begin with, a feature model (consisting of, for ex\n\nample,word embeddings and RNNs) is used to encode the input sentence into both a character-level feature matrix $\\dot{F^{( c) }}$ $\\in$ $\\mathbb{R} ^{d_{f}^{( \\mathrm{c} ) }\\times n_{f}^{( \\mathrm{c} ) }}$ and a word-level feature matrix $F^{( w) }\\in$ $\\mathbb{R} ^{d_{f}^{( w) }\\times n_{f}^{( w) }}$ where $d_{f}^{(c)}$ and $n_{f}^{(c)}$ represent, respec tively, the dimension of the embeddings of the characters $d_{\\int}^{(w)}$ and $n_{f}^{(w)}$ reee that each level in the data can be represented or embedded When attempting to predict a character in the translated\n\n![](./images/fSEFcGafUNBvMGze5Vma05K9gNTsnQ3cq.png)\n\nFig. 7. An illustration of attention-via-attention.\n\nsentence, a query $q^{(c)}\\in\\mathbb{R}^dq$ is created by the query model (like a character-level RNN), where $d_{q}$ is the dimension of the query vectors. As illustrated in Fig. 7, the query is used tocaleulate tentononthe corod $c^{(w)}\\in\\mathbb{R}^{d^{(w)}}$ etors $F^{(w)}$ $d_v^{(w)}$ ate ion module ta esentily suemarieste entencec\n\nIee ne eien e e ee et e $d_{f}^{(S)}$ etreate $F^{(s)}\\in\\mathbb{R}^{d_f^{(S)}\\times n_s}$ fore $s=1,\\ldots,n_{S}$, where which words contain the most important information for predicting the next character. If we know which words are most important,then it becomes easier to identify which characters in the input sentence are most important. Thus, the next step is to attend to the character-level features in $F^{(c)}$ , with an additional query input: the word-level context vector $c^{(w)}$ . The actual query input for the attention model will therefore be the concatenation of the query $q^{(c)}$ and the are essentially su-amaries of the words in the sentences. The word contex vector $c^{(w)}$ The ou tput of $c^{(c)}$ character-level matrix of context vectors $C=[\\boldsymbol{c}^{(1)},\\ldots,\\boldsymbol{c}^{(n_S)}]\\in\\mathbb{R}^{d_v^{(S)}\\times n_S}$ context output of the attention model is the concatenation of the word-level, and character-level context vectors The attention-via-attention technique uses representa\n\nnot always be available for each level of the data, or it may be desirable to let the model create the representations during the process by building them from lower level representations. A technique referred to as hierarchical attention [5] can be used in this situation. Hierarchical attention is another technique that allows one to apply attention on different levels of the data.Yet, the exact mechanisms work quite differently compared to attention-via-attention. The idea is to start at the lowest level, and then create representations, or summaries, of the next level using attention. This process is repeated till the highest level is reached. To make this a little clearer, suppose one attempts to create a model for document classification, similarly to the implementation from [5]. We analyze a document containing $7lS$ sentences, with the sth sentence containing $n_{s}$ words, for $s$ = $l, \\ldots , n_{S}$ . One could use attention based on just the collection of words to classify the document. However a significant amount of important context is then left out of the analysis, since the model will consider all words as a single long sentence, and will therefore not consider the context within the separate sentences.Instead, one can use the hierarchical structure of a document (words form sentences, and sentences form the document) Fig. 8 illustrates the structure of hierarchical attention\n\nFor each sentence in the document,a sentence representa tion $\\boldsymbol{c}^{(s)}\\in\\mathbb{R}^{d_{v}^{(S)}}$ is produced, for $s=1,\\ldots,n_{S}$, where $d_v^{(S)}$ is the dimension of the value vectors used in the attention model for sentence representations (Attention Modules in\n\n![](./images/fLQHc0WG3s8qDYeSbBDR2XDFQUQhGQiKx.png)\n\nFig. 8. An illustration of hierarchical attention\n\nFig. 8). The representation is a context vector from an word, and $Tl_{8}$ represents the amount of words in sentence S For extra clarification, the columns of $F^{(s)}$ are feature vectors that correspond to the words in sentence 5 .As shown in Fig. 8, each feature matrix $F^{(s)}$ is used as input for an atten tion model,which produces the context vector $c^{(s)}$ , for each $s=1,\\ldots,n_{S}$ . No queries are used in this step, so it can be considered a self-attentive mechanism. The context vectors is constructed by grouping all the obtained context vectors together as columns. Finally, attention is calculated using $C$ as feature input, producing the representation of the entire tons oitetochis adoent in thonte vector $\\boldsymbol{c}^{(D)}\\in\\mathbb{R}^{d_{v}^{(D)}}$ where $d_{v}^{(D)}$ is the dimension of the value vectors in the attention model for document representation (Attention Modulep in Fig. 8) This context vector can be used to classify the document, since it is essentially a summary of all the sentences (and therefore also the words)in the document Multi-level models can be used in a variety of tasks.\n\nFor example, in [28], hierarchical attention is used in a recommender system to model user preferences at the longterm level and the short-term level. Similarly, [45] proposes a hierarchical model for recommending social media images based on user preferences. Hierarchical attention has also been successfully applied in other domains.For example [46] proposes to use hierarchical attention in a video action recognition model to capture motion information at the the long-term level and the short-term level. Furthermore, [47] proposes a hierarchical attention model for cross-domain. sentiment classification. In [48], a hierarchical attention model for chatbot response generation is proposed. Lastly using image data,[49] proposes a hierarchical attention model for crowdcounting\n\n### 3.1.3Feature Representations\n\nIn a basic attention model, a single embedding or representation model is used to produce feature representa tions for the model to attend to.This is referred to as single-representational attention. Yet, one may also opt to incorporate multiple representations into the model. In. [50], it is argued that allowing a model access to multiple\n\nTABLE 2 Overview of score function (score $(\\boldsymbol{q},\\boldsymbol{k}_l)]$ forms.\n\n| Name | Function | Parameters |\n|------|----------|------------|\n| Additive (Concatenate) [3] | $w \\times act(W_1 q + W_2 k) + b$ | $w \\in \\mathbb{R}^{d \\times d}$, $W_1 \\in \\mathbb{R}^{d \\times d \\times k}$, $W_2 \\in \\mathbb{R}^{d \\times d \\times k}$, $b \\in \\mathbb{R}^d$ |\n| Multiplicative (Dot-Product) [4] | $q \\times k_i$ | - |\n| Scaled Multiplicative [13] | $\\frac{k_i}{d} \\times W \\times q$ | - |\n| Generalized [4] | $k_i / (d \\times W \\times q)$ | $W \\in \\mathbb{R}^{d \\times d \\times k}$ |\n| Biased General [54] | $k_i/(W \\times x \\times q) + b$ | $W \\in \\mathbb{R}^{d \\times d \\times k}$, $x \\in \\mathbb{R}^{d \\times d \\times k}$, $b \\in \\mathbb{R}^d$ |\n| Activated General [34] | $act(k_i/(W \\times x \\times q))$ | $W \\in \\mathbb{R}^{d \\times d \\times k}$, $x \\in \\mathbb{R}^{d \\times d \\times k}$, $b \\in \\mathbb{R}^d$ |\n| Similarity [55] | $q \\times k_i$ | - |\n\nembeddings can allow one to create even higher quality representations. Similarly, [51] incorporates multiple representations of the same book (textual, syntactic, semantic visual etc.) into the feature model. Feature representations are an important part of the attention model, but attention can also be an important part of the feature model. The idea is to create a new representation by taking a weighted average of multiple representations, where the weights are determined via attention.This technique is referred to as multi-representational attention, and allows one to create so-called meta-embeddings. Suppose one wants to create a meta-embedding for a word 2 for which $E$ embeddings $\\boldsymbol{x}^{(e_1)},\\ldots,\\boldsymbol{x}^{(e_E)}$ are available. Each embedding $x^{(e_{i})}$ is of size $d_{e_{i}t}$ for $i=1,\\ldots,E$ Since not all embeddings are of the same size,a transformation is performed to normalize the embedding dimensions. Using embedding-specific weight parameters, each embedding $x^{(e_i)}$ is transformed into the size-normalized embedding $x^{(t_{i})}\\in\\mathbb{R}^{d_{t}}$ ,where $d_{t}$ is the size of every transformed word embedding, as shown in (21).\n$$x^{(t_{i})}=W_{e_{i}}\\times x^{(e_{i})}+b_{e_{i}},\\\\d_{t}\\times1\\quad d_{t}\\times d_{e_{i}}\\quad d_{e_{i}}\\times1\\quad d_{t}\\times1$$\n\nwhere $W_{e_{i}}$ $\\in$ $\\mathbb{R} ^{d_{t}\\times d_{e_{i}}}$ and $b_{e_i}\\in\\mathbb{R}^{d_t}$ are trainable, embedding-specific weights matrices. The final embedding. $x^{(e)}\\in\\mathbb{R}^{d_t}$ is a weighted average of the previously calculated transformed representations, as shown in (22)\n\n$$\\boldsymbol{x}_{d_t\\times1}^{(e)}=\\sum_{i=1}^Ea_i\\times\\boldsymbol{x}_{d_t\\times1}^{(t_i)}.$$\n\nThe final representation $x^{(e)}$ can be interpreted as the context vector from an attention model,meaning that the weights $a_1,\\ldots,a_E\\in\\mathbb{R}^1$ are attention weights. Attention can be calculated as normally, where the columns of the features matrix $F$ are the transformed representations $\\boldsymbol{x}^{(t_1)},\\ldots,\\boldsymbol{x}^{(t_E)}$ Thequery in this case can be ignored since it is constant in all cases. Essentially, the query is \"Which representations are the most important?\"” in every situation. As such, this is a self-attentive mechanism While an interesting idea, applications of multi-\n\nrepresentational attention are limited. One example of the application of this technique is found in[52], where a multirepresentational attention mechanism has been applied to generate multi-lingual meta-embeddings. Another example is [53], where a multi-representational text classification embeddings from part-of-speech tagging, named entity rec ognizers, and character-level and word-level embeddings.\n\n### 3.2 General Attention Mechanisms\n\ncan be applied in any type of attention model. The structure of this component can be broken down into the following sub-aspects: the attention score function, the attention alignment, and attention dimensionality\n\n## 3.2.1Attention Scoring\n\nThe attention score function is a crucial component in how attention is calculated. Various approaches have been developed that each have their own advantages and disadvantages.An overview of these functions is provided in Table 2.\n\nEach row of Table 2 presents a possible form for the function $\\operatorname{score}(\\boldsymbol{q},\\boldsymbol{k}_l)$ , as seen in (23), where $q$ is the query vector, and $k_{l}$ is thelth column of $K$ Note that the score functions presented in this section can be more efficiently calculated in matrix form using $K$ instead of each column separately Nevertheless, the score functions are presented using $k_{I}$ to more clearly illustrate the relation between a key and query.\n\n$$e_l=\\text{score}(\\begin{array}{c}q\\\\d_q\\times1\\end{array},\\begin{array}{c}k_l\\\\d_k\\times1\\end{array}).$$\n\nDue to their simplicity, the most popular choices for the score function are the concatenate score function[3] and the multiplicative score function [4]. The multiplicative score function has the advantage of being computationally inexpensive due to highly optimized vector operations. How. ever, the multiplicative function may produce non-optimal results when the dimension $d_{k}$ is too large [56]. When $d_{k}$ is large, the dot-product between $q$ and $k_{I}$ can grow large in magnitude.To illustrate this, in [13], an example is used where the elements of $q$ and $k_l$ are all normally distributed with a mean equal to zero, and a variance equal to one. Then, the dot-product of the vectors has a variance of $d_{k}$ A higher variance means a higher chance of numbers that are large in magnitude. When the softmax function of the alignment step is then applied using these large numbers, the gradient will become very small, meaning the model will have trouble converging [13]. To adjust for this, [13] proposes to scale the multiplicative function by the factor mode is eaepesed t at nerpeate dferent eesene $\\frac{1}{\\sqrt{d_{k_{k}}}}$ prod ucing the caled utiplictiescore nc en \n\nintroducing a weights matrix $W$ .This form, referred to as the general score function, allows for an extra transformation of $k_l$ .The biased general score function [54]is a further extension of the general function that introduces This major category consists of attention mechanisms that a bias weight vector $b$ A final extension on this function named the activated general score function is introduced in [34], and includes the use of both a bias weight $b$, and an activation function act(). The previously presented score functions are all based on\n\ndetermining a type of similarity between the key vector and the query vector. As such, more typical similarity measures such as the Euclidean $(\\mathrm{L}_{2})$ distance and cosine similarity can also be implemented [55]. These scoring methods are summarized under the similarity score function which is represented by the similarity() function\n\nThere typically is no common usage across domains regarding score functions. The choice of score function for a particular task is most often based on empirical experiments. However, there are exceptions when, for example efficiency is vital. In models where this is the case, the multiplicative or scaled multiplicative score functions are typi-However, a compromise can be made in certain situations cally the best choice.An example of this is the TransformerLocal alignment [4] is a method that implements a softmax model, which is generally computationally expensive.\n\n### 3.2.2Attention Alignment\n\nThe attention alignment is the step after the attention scoring. This alignment process directly determines which parts of the input data the model will attend to. The alignment function is denoted as align() and has various forms. The align() function takes as input the previously calculated attention score vector $e$ and calculates for each element $el$ of e the attention weight $u_{l}$ .These attention weights can then average of the value vectors $v_{1},\\cdots,v_{n_{f}}$\n\n$$\\boldsymbol{c}_{d_v\\times1}=\\sum_{l=1}^{n_f}a_l\\:\\times\\:\\boldsymbol{v}_l\\:.$$\n\nThe most popular alignment method to calculate these weights is a simple softmax function, as depicted in (25)\n\n$$\\underset{1\\times1}{a_l}=\\underset{1\\times1}{\\text{align}}(\\underset{1\\times1}{e_l}\\:;\\:\\boldsymbol{e}\\:)=\\frac{\\exp(e_l)}{\\sum_{j=1}^{n_f}\\exp(e_j)}.$$\n\nThis alignment method is often referred to as soft alignment in computervision settings [8],or global alignment for sequence data [4]. Nevertheless, both these terms represent the same function and can be interpreted similarly. Soft/global alignment can be interpreted as the model attending to all feature vectors.For example, the model attends to all regions in an image, or all words in a sentence. Even though the attention model generally does focus more on specific parts of the input, every part of the input will receive at least some amount of attention due to the nature of the softmax function. Furthermore, an advantage of the softmax function. is that it introduces a probabilistic interpretation to the input vectors. This allows one to easily analyze which parts of the input are important to the output predictions In contrast to soft/global alignment, other methods aim\n\nto achieve a more focused form of alignment. For example hard alignment [8], also known as hard attention or nondeterministic attention, is an alignment type that forces the attention model to focus on exactly one feature vector. Firstly, this method implements the softmax function in the exact same way as global alignment. However, the outputs $a_{1},\\cdots,a_{n_{f}}$ are not used as weights for the context vector calculation. Instead, these values are used as probabilities to draw the choice of the one value vector from.A value $m\\in\\mathbb{R}^1$ is drawn from a multinomial distribution with $a_{1},\\cdots,a_{n_{f}}$ as parameters for the probabilities. Then, the context vector is simply defined as follows:\n\n$$\\begin{array}{c}c\\\\d_v\\times1\\end{array}=\\begin{array}{c}v_m\\\\d_v\\times1\\end{array}.$$\n\nHard alignment is typically more efficient at inference compared to soft alignment.On the other hand, the main disadvantage of hard attention is that, due to the stochastic\n\nalignment of attention, the training of the model cannot be done via the regular backpropagation method. Instead simulation and sampling, or reinforcement learning [57] are required to calculate the gradient at the hard attention layer. As such, soft/global attention is generally preferred. distribution, similarly to soft/global alignment. But, the. softmax distribution is calculated based only on a subset of the inputs. This method is generally used in combination with sequence data.One has to specify a variable $p\\in\\mathbb{R}^{1}$ that determines the position of the region.Feature vectors close to $p$ will be attended to by the model, and vectors too far from $P$ will be ignored. The size of the subset will be determined by the variable $D\\in\\mathbb{R}^{1}$ . Summarizing the attention model will apply a softmax function on the attention scores in the subset $[p-D,p+D]$ . In other words be used to create the context vector C by taking a weighteda window is placed on the input and soft/global attention is calculated within that window：\n$$a_{l}=\\text{align}(\\begin{smallmatrix}e_{l}&;&e\\\\1\\times1&n_{f}\\times1\\end{smallmatrix})=\\frac{\\exp(e_{l})}{\\sum_{j=p-D}^{p+D}\\exp(e_{j})}.$$\n\nThe question that remains is how to determine the location parameter $P$ The first method is referred to as monotonic alignment. This straightforward method entails simply setting the location parameter equal to the location of the prediction in the output sequence.Another method of deter mining the position of the region is referred to as predictive alignment. As the name entails, the model attempts to actually predict the location of interest in the sequence\n\n$$p\\:=\\:S\\:\\times\\mathrm{sigmoid}(\\boldsymbol{w}_{1\\times d_{p}}^{T}\\:\\times\\mathrm{tanh}(\\boldsymbol{W}_{p}\\:\\times\\:\\boldsymbol{q}\\:)),$$\n\nwhere $S\\in\\mathbb{R}^{1}$ is the length of the input sequence, and $w_{p}\\in\\mathbb{R}^{d_{P}}$ and $W_{p}$ $\\in$ $\\mathbb{R} ^{d_{p}\\times d_{q}}$ are both trainable weights parameters. The sigmoid function multiplied by $S$ makes sure that $p$ is in the range $[0,S]$ .Additionally, in [4], it is recommended to add an additional term to the alignment function to favor alignment around $P$\n\n$$a_{l}=\\operatorname{align}(\\begin{smallmatrix}e_{l}&;&e\\\\1\\times1&n_{f}\\times1\\end{smallmatrix})\\exp(-\\frac{(l-p)^{2})}{2\\sigma^{2}}),$$\n\nwhere $\\sigma\\in\\mathbb{R}^{1}$ is empirically set equal to $\\frac{D}{2}$ according to [4]. Another proposed method for compromising between soft and hard alignment is reinforced alignment[58]. Similarly to local alignment, a subset of the feature vectors is determined, for which soft alignment is calculated. However, instead of using a window to determine the subset, reinforced alignment uses a reinforcement learning agent. [57], similarly to hard alignment, to choose the subset of feature vectors. The attention calculation based on these chosen feature vectors is the same as regular soft alignment Soft alignment is often regarded as the standard align\n\nment function for attention models in practically every domain.Yet, the other alignment methods have also seen inter esting uses in various domains. For example, hard attention is used in [59] for the task of visual question answering In [60], both soft and hard attention are used in a graph attention model for multi-agent game abstraction. Similarly. in [61], both global and local alignment are used for review rating predictions. Reinforced alignment has been employed\n\nin combination with a co-attention structure in [62] for the extended with the multi-dimensional attention mechanisn task of aspect sentiment classification. In [63], reinforced alignment is used for the task of person re-identification using surveillance images\n\n### 3.2.3 Attention Dimensionality\n\nAll previous model specifications of attention use a scalar weight $a_l$ for each value vector $v_l$ . This technique is referred to as single-dimensional attention. However, instead of determining a single attention score and weight for the entire vector, [64] proposes to calculate weights for every single feature in those vectors separately. This technique is referred to as multi-dimensional attention,since the attention weights now become higher dimensional vectors. The idea is that the model no longer has to attend to entire vectors, but it can instead pick and choose specific elements from those vectors. More specifically, attention is calculated for each dimension.As such,the model must create a vecto of attention weights $a_l\\in\\mathbb{H}^d_v$ for each value vector $v_l\\in\\mathbb{R}^dv$ The context vector can then be calculated by summing the element-wise multiplications (o) of the value vectors $\\boldsymbol{v}_1,\\ldots,\\boldsymbol{v}_{n_f}\\in\\mathbb{R}^d$, and the corresponding attention weight vectors $a_{1},\\ldots,a_{n_{f}}\\in\\mathbb{R}^{d_{\\upsilon}}$ as follows:\n\n$$\\boldsymbol{c}_{d_v\\times1}=\\sum_{l=1}^{n_f}\\boldsymbol{a}_l\\circ\\boldsymbol{v}_l.$$\n\nHowever, since one needs to create attention weight vectors, this technique requires adjusted attention score and weight calculations. For example, the concatenate score function found in Table 2 can be adjusted by changing the $w\\in\\mathbb{R}^{d_{\\cos}}$ weights vector to the weight matrix $W_{d}\\in\\mathbb{R}^{d_{w}\\times d_{v}}$\n\n$$e_{l}\\:=\\:W_{d}^{T}\\:\\times\\mathrm{act}(\\:W_{1}\\:\\times\\:q\\:+\\:W_{2}\\:\\times\\:k_{l}\\:+\\:b\\:).$$\n\nThis new score function produces the attention score vectors $\\boldsymbol{e}_1,\\ldots,\\boldsymbol{e}_{n_f}\\in\\mathbb{R}^dv$ . These score vectors can be combined into a matrix of scores $e$ = $[ \\boldsymbol{e}_{1}, \\ldots , \\boldsymbol{e}_{n_{f}}]$ $\\in$ $\\mathbb{R} ^{d_{v}\\times n_{f}}$ To produce multi-dimensional attention weights, the alignment function stays the same, but it is applied for each feature across the attention score columns. To illustrate, when implementing soft attention, the attention weight produced from the ith element of score vector $e_{l}$ is defined as follows:\n\n$$\\underset{1\\times1}{a_{l,i}}=\\underset{1\\times1}{\\text{align}}(e_{l,i};\\quad\\boldsymbol{e}_{d_{v}\\times n_{f}})=\\frac{\\exp(e_{l,i})}{\\sum_{j=1}^{n_{f}}\\exp(e_{j,i})},$$\n\nwhere $el,i$ represents the ith element of score vector $e_{l,}$ and $ul,i$ is the ith element of the attention weights vector $a_{l}$ .Finally, these attention weight vectors can be used to compute the context vector as presented in (30). Multi-dimensional attention is a very general mecha\n\nnism that can be applied in practically every attention model, but actual applications of the technique have been relatively sparse. One application example is [65], where multi-dimensional attention is used in a model for named entity recognition based on text and visual context from multimedia posts. In [66], multi-dimensional attention is used in a model for answer selection in community question answering. In [67], the U-net model for medical image segmentation is extended with a multi-dimensional attention mechanism. Similarly, in [68], the Transformer model is\n\nfor the task of dialogue response generation. In [69], multidimensional attention is used to extend graph attention networks for dialogue state tracking. Lastly, for the task of next-item recommendation, [70] proposes a model that incorporates multi-dimensional attention\n\n### 3.3 Query-Related Attention Mechanisms\n\nQueries are an important part of any attention model, since they directly determine which information is extracted from the feature vectors. These queries are based on the desired output of the task model, and can be interpreted as literal questions. Some queries have specific characteristics that require specific types of mechanisms to process them. As such, this category encapsulates the attention mechanisms that deal with specific types of query characteristics. The mechanisms in this category dealwith one of the two following query characteristics: the type of queries or the multiplicity of queries\n\n## 3.3.1Type of Queries\n\nDifferent attention models employ attention for different purposes, meaning that distinct query types are necessary There are basic queries, which are queries that are typically straightforward to definebased on the data and model.For example, the hidden state for one prediction in an RNN is often used as the query for the next prediction. One could also use a vector of auxiliary variables as query. For example, when doing medical image classification, general patient characteristics can be incorporated into a query Some attention mechanisms, such as co-attention, rota-\n\ntory attention, and attention-over-attention, use specialized queries. For example, rotatory attention uses the context vector from another attention module as query, while interactive co-attention uses an averaged keys vector based on another input. Another case one can consider is when attention is calculated based purely on the feature vectors This concept has been mentioned before and is referred to as self-attention or intra-attention [71]. We say that the models use self-attentive queries. There are two ways of interpreting such queries. Firstly, one can say that the query is constant. For example, document classification requires only a single classification as the output of the model. As such, the query is always the same, namely: \"What is the class of the document?\". The query can be ignored and attention can be calculated based only on the features themselves Score functions can be adjusted for this by making the query vector a vector of constants or removing it entirely:\n$$\\mathrm{score}(\\boldsymbol{k}_{l})=\\:\\boldsymbol{w}^{T}\\times\\mathrm{act}(\\boldsymbol{W}\\times\\:\\boldsymbol{k}_{l}+\\boldsymbol{b}\\:).\\\\d_{k}\\times1\\quad1\\times d_{w}\\quad d_{w}\\times d_{k}\\quad d_{k}\\times1\\quad d_{w}\\times1$$\n\nAdditionally, one can also interpret self-attention as learning the query along the way, meaning that the query can be defined as a trainable vector of weights.For example,the dot-product score function may take the following form\n\n$$\\begin{array}{rcl}\\mathrm{score}(\\boldsymbol{k}_l)&=&\\boldsymbol{q}^T\\times\\boldsymbol{k}_l\\\\d_k\\times1&&1\\times d_k&d_k\\times1\\end{array},$$\n\nwhere $q\\in\\mathbb{R}^{d_k}$ is a trainable vector of weights. One could also interpret vector $b\\in\\mathbb{R}^{d_{w}}$ as the query in (33).Another\n\nuse of self-attention istouncover the relations between the feature vectors $f_{1},\\ldots,f_{n_{f}}$ . These relations can then be used as additional information to incorporate into new representations of the feature vectors. With basic attention mechanisms, the keys matrix $K$, and the values matrix $V$ are extracted from the features matrix $F_{\\prime}$ while the query $q$ is produced separately. For this type of self-attention, the query vectors are extracted in a similar process as the keys and values, via a transformation matrix of trainable weights $W_{Q}\\in\\mathbb{R}^{d_{q}\\times d_{f}}$ . We define the matrix $Q=\\left[\\boldsymbol{q}_{1},\\ldots,\\boldsymbol{q}_{n_{f}}\\right]\\in$ $\\Pi$玻 $d_q\\times nf$ which canbe obtained asfollows:\n\n$$Q_{d_{q}\\times n_{f}}=W_{Q}\\times F_{d_{f}\\times n_{f}}.$$\n\nEach column of $Q$ can be used as the query for the attention model. When attention is calculated using a query $q.$ the resulting context vector C will summarize the information the query, or a column of $Q$ , is now also a feature vector representation, the context vector contains the information of allfeature vectors that are important to that specific feature vector. In other words, the context vectors capture the relations between the feature vectors. For example, selfattention allows one to extract the relations between words: which verbs refer to which nouns, which pronouns refer to which nouns, etc. For images, self-attention can be used to determine which image regions relate to each other While self-attention is placed in the query-related cat-\n\negory, it is also very much related to the feature model. Namely, self-attention is a technique that is often used in the feature model to create improved representations of the feature vectors. For example, the Transformer model for language processing [13], and the Transformer model for image processing [15], both use multiple rounds of (multihead) self-attention to improve the representation of the feature vectors. The relations captured by the self-attention mechanism are incorporated into new representations. A. simple method of determining such a new representation is to simply set the feature vectors equal to the acquired self-attention context vectors [71], as presented in (36).\n\n$$f_{_{d_f\\times1}}^{(\\mathrm{new})}=\\underset{d_f\\times1}{\\operatorname*{c}},$$\n\nwhere $f^{(\\mathrm{new})}$ is the updated feature vector. Another possibility is to add the context vectors to the previous feature vectors with an additional normalization layer [13]:\n\n$$f_{d_f\\times1}^{(\\mathrm{new})}=\\mathrm{Normalize}(f_{d_f\\times1}^{(\\mathrm{old})}+\\sum_{d_f\\times1}),$$\n\nwhere $f^{(\\mathrm{old})}$ is the previous feature vector, and Normalize( is a normalization layer[72]. Using such techniques, self embeddings that enhance model accuracy [71]. Self-attention is arguably one of the more important\n\ntypes of attention, partly due to its vital role in the highly popular Transformer model. Self-attention is a very general mechanism and can be applied to practically any problem. As such, self-attention has been extensively explored in many different fields in both Transformer-based architec tures and other types of models. For example, in [73], selfattention is explored for image recognition tasks, and results\n\nindicate that the technique may have substantial advantages with regards to robustness and generalization. In [74], selfattention is used in a generative adversarial network (GAN) [75] to determine which regions of the input image to focus on when generating the regions of a new image. In [76], selfattention is used to design a state-of-the-art medical image segmentation model. Naturally, self-attention can also be used for video processing. In [77], a self-attention model is proposed for the purpose of video summarization that reaches state-of-the-art results. In other fields, like audio processing, self-attention has been explored as well. In [78], self-attention is used to create a speech recognition model Self-attention has also been explored in overlapping domains. For example, in [79], the self-attention Transformer architecture is used to create a model that can recognize phrases from audio and by lip-reading from a video. For the problem of next item recommendation, [80] proposes in the feature vectors that is important to the query. Sincea Transformer model that explicitly captures item-item re lations using self-attention. Self-attention also has applica-. tions in any natural language processing fields. For example, in [81], self-attention is used for sentiment analysis. Selfattention is also highly popular for graph models. For example, self-attention is explored in [82] for the purpose of representation learning in communication networks and rating networks. Additionally, the first attention model for graph networks was based on self-attention [83]\n\n### 3.3.2Multiplicity ot Queries\n\nIn previous examples, the attention model generally used a single query for a prediction.We say that such models use singular query attention. However, there are attention architectures that allow the model to compute attention using multiple queries. Note that this is different from, for example, an RNN that may involve multiple queries to produce a sequence of predictions. Namely, such a model still requires only a single query per prediction One example of a technique that incorporates multiple\n\nqueries is multi-head attention [13], as presented in Fig. 9. Multi-head attention works by implementing multiple attention modules in parallel by utilizing multiple different versions of the same query. The idea is to linearly transform the query $q$ using different weight matrices. Each newly formed query essentially asks fora different type of relevant information, allowing the attention model to introduce more information into the context vector calculation. An attention model implements $d\\geq1$ heads with each attention head having its own query vector, keys matrix, and values matrix $q^{(j)}$ ， $K^{(j)}$ and $V^{(j)}$ ，for $j$ = $1, \\ldots , d$ The query $q^{(j)}$ is obtained by linearly transforming the original query $\\Psi$, while the matrices $K^{(j)}$ and $V^{(j)}$ are obtained through linear transformations of $F$ .As such,each attention head attention has been used to create improved word or sentence has its own learnable weights matrices $W_{q}^{(j)}$ ” $W_{K}^{(j)}$ and $W_{V}^{(j)}$ for these transformations.The calculation of the query keys, and values for the $j$ th head are defined as follows:\n$$\\boldsymbol{q}_{d_{q}\\times1}^{(j)}=\\boldsymbol{W}_{q}^{(j)}\\times\\boldsymbol{q}_{d_{q}\\times1},\\quad\\boldsymbol{K}_{d_{k}\\times n_{f}}^{(j)}=\\boldsymbol{W}_{K}^{(j)}\\times\\boldsymbol{F}_{d_{f}\\times n_{f}},\\\\\\boldsymbol{V}_{d_{v}\\times n_{f}}^{(j)}=\\boldsymbol{W}_{V}^{(j)}\\times\\boldsymbol{F}_{d_{f}\\times n_{f}}.$$\n\nThus,each head creates its own representations of the query $q$, and the input matrix $F$ Each head can therefore\n\n![](./images/fLxYvsz8GoGucwnBRCXegWEcG76geRpkm.png)\n\nFig. 9. An illustration of multi-head attention\n\nlearn to focus on different parts of the inputs, allowing the model to attend to more information.For example when training a machine translation model, one attention head can learn to focus on which nouns (e.g., student, car, apple) do certain verbs (e.g., walking, driving, buying) refer to,while another attention head learns to focus on which $\\boldsymbol{e}^{( j) }$ = $[ e_{1}^{( j) }, \\ldots , e_{n_{f}}^{( j) }]$ $\\in$ $\\mathbb{R} ^{n_{f}}$ ae $q^{(s)}$ Pr $q^{(0)}$ A $S$, Cat $q^{(s+1)}$ of ttention weights $\\boldsymbol{a}^{( j) }$ = $[ a_{1}^{( j) }, \\ldots , a_{n_{f}}^{( j) }]$ $\\in$ $\\mathbb{R} ^{n_{f}}$ As canpossibly using the current context vector $c^{(s)}$ as another be expected, each attention model produces its own context vector $\\boldsymbol{c}^{(j)}\\in\\mathbb{R}^{dv}$ ,as follows:\n\n$$\\boldsymbol{c}_{d_{v}\\times1}^{(j)}=\\sum_{l=1}^{n_{f}}a_{l}^{(j)}\\times\\boldsymbol{v}_{l}^{(j)}.$$\n\nThe goal is still to create a single context vector as output of the attention model.As such, the context vectors produced by the individual attention heads are concatenated into a single vector. Afterwards, a linear transformation is applied using the weight matrix $W_{O}\\in\\mathbb{R}^{d_{c}\\times d_{v}d}$ tomake sure the resulting context vector $c\\in\\mathbb{R}^{d_{c}}$ has the desired dimension. This calculation is presented in (40). The dimension $d_{c}$ can be pre-specified by, for example, setting it equal to $d_{v,}$ so that the context vector dimension is unchanged.\n\n$$c\\:=\\:W_{O}\\:\\times\\mathrm{concat}(\\boldsymbol{c}^{(1)},...,\\boldsymbol{c}^{(d)}).$$\n\nMulti-head attention processes multiple attention modmented sequentially to iteratively adjust the context vec tors.Each of these attention modules are referred to as \"repetitions” or “rounds” of attention. Such attention architectures are referred to as multi-hop attention models also known as multi-step attention models.An important note to consider is the fact that multi-hop attention tant mote to con sider is be ap thas m unt hap ustention she that the actunl o $c^{(s)}$ ind $q^{(s+1)}$ the atentoen noedelin throughout various works. While the mechanism always involves multiple rounds of attention, the multi-hop implementation proposed in [84] differs from the mechanism proposed in[85]or [86]. Another interesting example is [87], where a “multi-hop”attention model is proposed that would actually be considered alternating co-attention in this survey, as explained in Subsection 3.1.1. We present a general form of multi-hop attention that\n\nis largely a generalization of the techniquesintroduced into produce the next context vector $c^{(s+1)}$ .Onemust note [85] and [88]. Fig. 10 provides an example implementation of a multi-hop attention mechanism. The general idea isweights,meaning that the number of parameters do not to iteratively transform the query, and use the query to transform the context vector,such that the model can extract different information in each step. Remember that a query\n\n![](./images/fC2yeUZnAYXZanLazGfbKKQkNuntGh8I1.png)\n\nFig. 10.An example illustration of multi-hop attention. Solid arrows represent the base multi-hop model structure, while dotted arrows represent optional connections.\n\nis similar to a literal question. As such, one can interpret the transformed queries as asking the same question in a different manner or from a different perspective, similarly to the queries in multi-head attention. The query that was previously denoted by $\\Psi$ is now referred to as the initial input, and some transformation function transform()\n\n$$\\boldsymbol{q}_{d_{q}\\times1}^{(s+1)}=\\mathrm{transform}(\\boldsymbol{q}^{(s)},\\:\\boldsymbol{c}^{(s)}).$$\n\nFor the specific form of the transformation function transform(),[85]proposes to use a mechanism similar to self-attention. Essentially, the queries used by the question answer matching model proposed in [85] were originally based on a set of feature vectors extracted from a question [85] also defines the original query $q^{(0)}$ as the unweighted average of these feature vectors.At each hop $S$, attention can be calculated on these feature vectors using the previous query $q^{(s)}$ as the query in this process.The resulting context vector of this calculation is the next query vector. Using the context vector $c^{(s)}$ instead of $q^{(s)}$ as the query for this process is also a possibility, which is similar to the LCRRot-hop model proposed in [43] and the multi-step model proposed in [88]. Such a connection is represented by the dotted arrows in Fig. 10. The transformation mechanism ules in parallel, but attention modules can also be imple-uses either the $q^{(s)}$ or the context vector $c^{(s)}$ as query, but a combination via concatenation is also possible. Each query representation is used as input for the at\n\ntention module to compute attention on the columns of the feature matrix $F$ ,as seen previously. One main difference. however, is that the context vector $c^{(s)}$ is also used as input, score function is presented in (42). Note that the initial context vector $c^{(0)}$ is predefined.One way of doing thisis by setting it equal to the unweighted average of the value vectors $\\boldsymbol{v}_1,\\ldots,\\boldsymbol{v}_{n_f}\\in\\mathbb{R}^{d_v}$ extracted from $F$\n\n$$e_{l}^{(s)}=\\text{score}(\\text{concat}(\\boldsymbol{q}_{d_{q}\\times1}^{(s+1)},\\boldsymbol{c}^{(s)}),\\boldsymbol{k}_{l}).$$\n\nAn alignment function and the value vectors are then used that in [85], the weights used in each iteration are the same scale with the number of repetitions. Yet,using multiple hops with different weight matrices can also be viable, as shown by the Transformer model [13] and in [88]. It may be\n\ndifficult to grasp why $c^{(s)}$ is part of the query input for the attention model. Essentially, this technique is closely related to self-attention in the sense that,in each iteration,a new context representation is created from the feature vectors and the context vector. The essence of this mechanism is that one wants to iteratively alter the query and the context vector,while attending to the feature vectors.In the process, the new representations of the context vector absorb more differentkinds of information.This is also the main difference between this type of attention and multi-head attention. Multi-head attention creates multiple context vectors from multiple queries and combines them to create a final context vector as output. Multi-hop attention iteratively. refines the context vector by incorporating information from the different queries. This does have the disadvantage of having to calculate attention sequentially Interestingly, due to the variations in which multi-hop\n\nattention has been proposed, some consider the Transformer model’s encoder and decoder to consist of several singlehop attention mechanisms [84] instead of being a multihop model. However, in the context of this survey, we consider the Transformer model to be an alternative form of the multi-hop mechanism, as the features matrix $F$ is not directly reused in each step. Instead, $F$ is only used as an input for the first hop, and is transformed via self-attention into a new representation. The self-attention mechanism uses each feature vector in $F$ as a query, resulting in a matrix of context vectors as output of each attention hop. The $p_{1},\\cdots,p_{d_{[0]}}$ as accurately as possible compared to the true intermediate context vectors are turned into matrices and represent iterative transformations of the matrix $F$ which are used in the consecutive steps. Thus, the Transformer model iteratively refines the features matrix $F$ by extracting and incorporating new information When dealing with a classification task, another idea is\n\nto use a different query for each class. This is the basic principle behind capsule-based attention [89], as inspired by the capsule networks [90]. Suppose we have the feature classes that the model can predict. Then, a capsule-based attention model defines a capsule for each of the $d_{y}$ classes that each take as input the feature vectors.Each capsule consists of, in order, an attention module, a probability module, and a reconstruction module, which are depicted in. Fig. 11. The attention modules all use self-attentive queries so each module learns its own query: \"Which feature vectors are important to identify this class?\". In [89], a self-attentive multiplicative score function is used for this purpose\n\n$$e_{c,l}=\\begin{array}{ccc}\\boldsymbol{q}_c^T&\\times&\\boldsymbol{k}_l\\\\1\\times1&1\\times d_k&d_k\\times1\\end{array},$$\n\nwhere $e_{c,l}\\in\\mathbb{R}^1$ is the attention score for vector $l$ in capsule $U_{f}$ and $q_{c}\\in\\mathbb{R}^{d_k}$ is a trainable query for capsule $C_{f}$ for $c=1,\\ldots,d_{y}$ . Each attention module then uses an alignment function, and uses the produced attention weights to determine a context vector $c_{c}\\in\\mathbb{R}^{d_{v}}$ .Next, the context vector $C_{t}$ is fed through a probability layer consisting of a linear transformation with a sigmoid activation function:\n\n$$\\begin{aligned}p_{c}&=\\mathrm{sigmoid}(\\boldsymbol{w}_{c}^{T}\\times\\boldsymbol{c}_{c}\\:+\\:b_{c}\\:),\\\\&_{1\\times1}\\times d_{v}\\quad d_{v}\\times1\\quad1\\times1\\end{aligned}$$\n\n![](./images/fFOVLaKiKKX8YVu44wZt6GwmcOzskqWOp.png)\n\nFig. 11. An illustration of capsule-based attention\n\nwhere $w_c\\in\\mathbb{R}^dv$ and $b_{c}\\in\\mathbb{R}^{1}$ are trainable capsule-specific weights parameters, and $p_{c}\\in\\mathbb{R}^{1}$ is the predicted probability that the correct class is class $t$ . The final layer is the reconstruction module that creates a class vector representation This representation $r_{c}\\in\\mathbb{R}^{dv}$ is determined by simply multiplying the context vector $C_{c}$ by the probability $P_{c}$\n\n$$\\begin{array}{rcl}r_c&=&p_c\\times&c_c\\\\d_v\\times1&&1\\times1&&d_v\\times1\\end{array}.$$\n\nThe capsule representation is used when training the model. First of all, the model is trained to predict the probabilities values. Secondly, via a joint loss function, the model is also trained to accurately construct the capsule representations ${\\boldsymbol{r}}1,\\cdots,{\\boldsymbol{r}}d_{\\psi}$ . A features representation $f\\in\\mathbb{R}^{d_f}$ is defined which is simply the unweighted average of the original feature vectors. The idea is to train the model such that vector representations from capsules that are not the correct class differ significantly from $f$ while the representation from the correct capsule is very similar to $f$ A dot-product between the capsule representations and the features representation vectors $f_{1}, \\ldots , f_{n_{f}}$ $\\in$ $\\mathbb{R} ^{d_{f}}$ , and suppose there are are $d_{y}$ is used in [89] as a measure of the distance between the vectors. Note that $d_v$ must equal $d_{f}$ in this case, otherwise the vectors would have incompatible dimensions. Interestingly, since attention is calculated for each class individually, one can track which specific feature vectors are important for which specific class. In [89], this idea is used to discover which words correspond to which sentiment class. The number of tasks that can make use of multiple\n\nqueries is substantial, due to how general the mechanisms are.As such, the techniques described in this section have been extensively explored in various domains.For example, multi-head attention has been used for speaker recog nition based on audio spectrograms [91]. In [92], multihead attention is used for recommendation of news articles. Additionally, multi-head attention can be beneficial for graph attention models as well [83]. As for multi-hop attention, quite a few papers have been mentioned before but there are still many other interesting examples.For example, in [93], a multi-hop attention model is proposed for medication recommendation. Furthermore, practically every Transformer model makes use of both multi-head and multi-hop attention. The Transformer model has been extensively explored in various domains. For example, in [94], a Transformer model is implemented for image cap-\n\ntioning. In [95], Transformers are explored for medical image segmentation. In [96], a Transformer model is used for emotion recognition in text messages. A last example of an application of Transformers is [17], which proposes a Transformer model for recommender systems. In comparison with multi-head and multi-hop attention, capsule-based attention is arguably the least popular of the mechanisms discussed for the multiplicity of queries. One example is [97], where an attention-based capsule network is proposed that also includes a multi-hop attention mechanism for the purpose of visual question answering. Another example is [98], where capsule-based attention is used for aspect-level sentiment analysis of restaurant reviews. The multiplicity of queries is a particularly interesting ang Parlprioyesong ar th nthe fieldof audipor 108\n\ncategory due to the Transformer model [13], which com provements and iterations of the model have been produced that typically aim to improve the predictive performance, the computational efficiency, or both.For example, the Transformer-XL [99] is an extension of the original Trans former that uses a recurrence mechanism to not be limited by a context window when processing the outputs.This allows the model to learn significantly longer dependencies while also being computationally more efficient during the evaluation phase. Another extension of the Transformer is known as the Reformer model [100]. This model is signifi cantly more efficient computationally, by means of localitysensitive hashing, and memory-wise, by means of reversible residual layers. Such computational improvements are vital, since one of the main disadvantages of the Transformer model is the sheer computational cost due to the complexity of the model scaling quadratically with the amount of input feature vectors. The Linformer model [101] manages to reduce the complexity of the model to scale linearly, while achieving similar performance as the Transformer model This is achieved by approximating the attention weights using a low-rank matrix. The Lite-Transformer model proposed in [102] achieves similar results by implementing two branches within the Transformer block that specialize in capturing global and local information. Another interesting Transformer architecture is the Synthesizer [103] This model replaces the pairwise self-attention mechanism with \"synthetic attention weights. Interestingly, the per formance of this model is relatively close to the original Transformer, meaning that the necessity of the pairwise self-attention mechanism of the Transformer model may be questionable.For a more comprehensive overview of Transformer architectures, we refer to [104].\n\n## 4EVALUATIOn OF ATTenTIOn MOdeLs\n\nIn this section, we present various types of evaluation for attention models.Firstly, one can evaluate the structure of attention models using the taxonomy presented in Section3 For such an analysis, we consider the attention mechanism categories (see Fig. 3) as orthogonal dimensions of a model. The structure of a model can be analyzed by determining which mechanism a model uses for each category. Table 3 provides an overview of attention models found in the liter-\n\nature with a corresponding analysis based on the attention mechanisms the models implement Secondly, we discuss various techniques for evaluating\n\nthe performance of attention models. The performance of attention models can be evaluated using extrinsic or intrinsic performance measures, which are discussed in Subsections. 4.1 and 4.2, respectively\n\n### 4.1 Extrinsic Evaluation\n\nIn general, the performance of an attention model is measured using extrinsic performance measures. For example,. performance measures typically used in the field of natural language processing are the BLEU [107], METEOR [108] the Word Error Rate[110] and Phoneme Error Rate[111] are bins fom of mult-hop ad multi-hd attntion Due gerally mpoyed For genal assfiation tasks, eo rates, precision, and recall are generally used.For computer vision tasks, the PSNR[112], SSIM [113], or IoU [114] metrics are used. Using these performance measures, an attention model can either be compared to other state-of-the-art models, or an ablation study can be performed. If possible, the importance of the attention mechanism can be tested by replacing it with another mechanism and observing whether the overall performance of the model decreases [105], [115] An example of this is replacing the weighted average used to produce the context vector with a simple unweighted average and observing whether there is a decrease in overall model performance [35]. This ablation method can be used to evaluate whether the attention weights can actually distinguish important from irrelevant informationd\n\n### 4.2Intrinsic Evaluation\n\nAttention models can also be evaluated using attentionspecific intrinsic performance measures.In [4], the attention weights are formally evaluatedvia the Alignment Error Rate (AER) to measure the accuracy of the attention weights with respect to annotated attention vectors. [116] incorporates this idea into an attention model by supervising the attention mechanism using gold attention vectors A joint loss function consisting of the regular task-specific loss and the attention weights loss function is constructed for this purpose. The gold attention vectors are based on annotated text data sets where keywords are hand-labelled. However, since attention is inspired by human attention, one could evaluate attention models by comparing them to the attention behaviour of humans.\n\n### 4.2.1 Evaluation via Human Attention\n\nIn [117], the concept of attention correctness is proposed which is a quantitative intrinsic performance metric that evaluates the quality of the attention mechanism based on actual human attention behaviour. Firstly, the calculation of this metric requires data that includes the attention behaviour of a human.For example, a data set containing images with the corresponding regions that a human focuses on when performing a certain task, such as image captioning.The collection of regions focused on by the human is referred to as the ground truth region. Suppose an attention model attends to the $n_{f}$ feature vectors $f_1,\\ldots,\\boldsymbol{f}_{n_f}\\in\\mathbb{R}^{d_f}$ Feature vector $f_i$ corresponds to region $R_{i}$ of the given\n\nTABLE 3 Attention models analyzed based on the proposed taxonomy. A plus sign (+) between two mechanisms indicates that both techniques were combined in the same model, while a comma (,) indicates that both mechanisms were tested in the same paper, but not necessarily as a combination in the same model\n\nBuhleran et al. [3]\nMultiplicity\nLevels\nRepresentations\nScoring\nGeneralization\nDimensionality\nType\nQuery-Related Multiplicity\nSingular\nSingle-Level\nSingle-Representational\nAdditive\nGlobal\nSingle-Dimensional\nBasic\nSingular\n\nLuong et al. [4]\nSingular\nSingle-Level\nSingle-Representational\nMultiplicative\nGlobal, Local\nSingle-Dimensional\nBasic\nSingular\n\nXu et al. [8]\nSingular\nSingle-Level\nSingle-Representational\nAdditive\nSoft, Hard\nSingle-Dimensional\nBasic\nSingular\n\nTu et al. [32]\nParallel / Co-attention\nHierarchical\nSingle-Representational\nAdditive\nGlobal\nSingle-Dimensional\nSpecialized\nSingular\n\nYang et al. [5]\nSingular\nHierarchical\nSingle-Representational\nAdditive\nGlobal\nSingle-Dimensional\nSelf-Attentive\nSingular\n\nLi et al. [14]\nSingular\nHierarchical\nSingle-Representational\nAdditive\nGlobal\nSingle-Dimensional\nSelf-Attentive\nSingular\n\nKoivisto et al. [13]\nSingular\nParallel / Co-attention\nSingle-Representational\nMultiplicative\nGlobal\nSingle-Dimensional\nMulti-Hop\nSingular\n\nWaalstra and Frasincar [15]\nRotatory\nSingle-Level\nSingle-Representational\nActivated\nGlobal\nSingle-Dimensional\nSpecialized\nMulti-Hop\n\nKiela et al. [50]\nSingular\nSingle-Level\nMulti-Representational\nAdditive\nGlobal\nMulti-Dimensional\nSingular\n\nShen et al. [64]\nSingular\nSingle-Level\nSingle-Representational\nAdditive\nGlobal\nMulti-Dimensional\nSingular\n\nZhang et al. [74]\nSingle-Level\nSingle-Representational\nMultiplicative\nGlobal\nSingle-Dimensional\nSelf-Attentive\nSingular\n\nLi et al. [115]\nParallel / Co-attention\nHierarchical\nSingle-Representational\nAdditive\nGlobal\nSingle-Dimensional\nSpecialized + Multi-Hop\n\nYu et al. [106]\nParallel / Co-attention\nSingle-Level\nSingle-Representational\nMultiplicative\nGlobal\nSingle-Dimensional\nSelf-Attentive + Specialized\n\nWang et al. [62]\nParallel / Co-attention\nHierarchical\nSingle-Representational\nAdditive\nReinforced\nSingle-Dimensional\nSingular\n\nOktay et al. [67]\nSingular\nSingle-Level\nSingle-Representational\nAdditive\nGlobal\nMulti-Dimensional\nSelf-Attentive + Specialized\nSingular\n\nWinata et al. [52]\nSingular\nSingle-Level\nMulti-Representational\nAdditive\nGlobal\nSingle-Dimensional\nSelf-Attentive\nMulti-Hop\n\nWang et al. [89]\nSingular\nSingle-Level\nSingle-Representational\nMultiplicative\nGlobal\nSingle-Dimensional\nSelf-Attentive\nCapsule-Based\n\nimage, for $i$ = $1, \\ldots , n_{f}$ .We define the set $G$ as the set of regions that belong to the ground truth region, such that $R_{i}\\in G$ if $R_{i}$ is part of the ground truth region. The attentionmodel is behaving correctly. This allows for some interpremodel calculates the attention weights $a_1,\\ldots,a_{n_f}\\in\\mathbb{R}^1$ via the usual attention process. The Attention Correctness $(AC)$ metric can then be calculated using (46).\n\n$$\\begin{matrix}AC=\\sum_{i:R_i\\in G}a_i\\\\\\end{matrix}.$$\n\nThus, this metric is equal to the sum of the attention weights for the ground truth regions.Since the attention weights sum up to 1 due to, for example, a softmax alignment function, the $AC$ value will be a value between O and 1. If the model attends to only the ground truth regions, then $AC$ is equal to 1, and if the attention model does not attend to any of the ground truth regions, $AC$ will be equal to 0. In [118], a rank correlation metric is used to compare\n\nimportant. Therefore, observing which parts of the inputs the model focuses on can be helpful in determining if the tation of the behaviour of models that are typically known to be black boxes.However, rather than checking if the model focuses onthemost important parts of thedatasome use the attention weights to determine which parts of the data are most important. This would imply that attention models provide a type of explanation, which is a subject of contention among researchers. Particularly, in [120], extensive experiments are conducted for various natural language processing tasks to investigate the relation between attention weights and important information to determine whether attention can actually provide meaningful explanations. In this paper titled \"Attention is not Explanation\" it is found that attention weights do not tend to correlate with important features. Additionally, the authors are able to replace the produced attention weights with completely different values while keeping the model output the same. These so-called \"adversarial attention distributions show that an attention model may focus on completely different information and still come to the same conclusions, which makes interpretation difficult. Yet, in another paper titled \"Attention is not not Explanation\"[121], the claim that attention is not explanation is questioned by challenging the assumptions of the previous work.It is found that the adversarial attention distributions do not perform as reliably well as the learned attention weights, indicating that it was not proved that attention is not viable for explanation. In general, the conclusion regarding the interpretability\n\nthe generated attention weights to the attention behaviour of humans.The conclusion of this work is that attention maps generated by standard attention models generally do not correspond to human attention. Attention models often focus on much larger regions or multiple small non-adjacent regions.As such,a technique to improve attention models is to allow the model to learn from human attention patterns via a joint loss of the regular loss function and an attention weight loss function based on the human gaze behaviour, similarly to how annotated attention vectors are used in [116] to supervise the attention mechanism. [117] proposes to use human attention data to supervise the attention mechanism in such a manner. Similarly, a state-of-the-art video captioning model is proposed in [119] that learns fromof attention models is that researchers must be extremely human gaze data to improve the attention mechanism\n\n### 4.2.2Manual Evaluation\n\ncareful when drawing conclusions based on attention patterns.For example,problems with an attention model can be diagnosed via the attention weights if the model is found to focus on the incorrect parts of the data, if such information the manual inspection of attention weights.As previouslyis available. Yet, conversely, attention weights may only be used to obtain plausible explanations for why certain parts of the data are focused on,rather than concluding that those\n\nA method that is often used to evaluate attention models is mentioned, the attention weights are a direct indication of which parts of the data the attention model finds most\n\nparts are significant to the problem [121]. However, one should still be cautious as the viability of such approaches can depend on the model architecture [122]\n\n## 5CONCLUSION\n\nIn this survey, we have provided an overview of recent research on attention models in deep learning. Attention mechanisms have been a prominent development for deep learning models as they have shown to improve model performance significantly, producing state-of-the-art results for various tasks in several fields of research. We have presented a comprehensive taxonomy that can be used to categorize and explain the diverse number of attention mechanisms proposed in the literature. The organization of the taxonomy wasmotivated based on the structure ofa taskmodel that consists of a feature model, an attention model, a query model, and an output model. Furthermore, the attention mechanisms have been discussed using a framework based on queries, keys, and values. Last, we have shown how one can use extrinsic and intrinsic measures to evaluate the performance of attention models, and how one can use the taxonomy to analyze the structure of attention models The attention mechanism is typically relatively simple\n\nto understand and implement and can led to signficant25. 2015, 412-1 He, Amola, and E Hovy improvements in performance.As such, it is no surprise that this is a highly active field of research with new attention mechanisms and models being developed constantly. Not only are new mechanisms consistently being developed but there is also still ample opportunity for the exploration of existing mechanisms for new tasks. For example, multidimensional attention[64] is a technique that shows promis dienqaM almost any attention model. However, it has not seen much application in current works. Similarly, multi-head atten-. tion [13] is a technique that can be efficiently parallelized and implemented in practically any attention model.Yet it is mostly seen only in Transformer-based architectures. Lastly, similarly to how [43] combines rotatory attention with multi-hop attention, combining multi-dimensional at-. tention, multi-head attention, capsule-based attention, or any of the other mechanisms presented in this survey may produce new state-of-the-art results for the various fields of research mentioned in this survey. This survey has mainly focused on attention mech\n\nanisms for supervised models,since these comprise the largest proportion of the attention models in the literature. In comparison to the total amount of research that has been done on attention models,research on attention models for semi-supervised learning [123], [124] or unsupervised learning [125], [126] has received limited attention and has only become active recently. Attention may play a more significant role for such tasks in the future as obtaining research on attention models can advance even further. For example, we mentioned the fact that attention weights can be trained directly based on hand-annotated data[116] or actual human attention behaviour[117], [119]. As new data sets are released,future research may focus on developing attention models that can incorporate those types of data.\n\nWhile attention is intuitively easy to understand,there still is a substantial lack of theoretical support for attention As such, we expect more theoretical studies to additionally contribute to the understanding of the attention mechanisms in complex deep learning systems. Nevertheless, the practical advantages of attention models are clear. Since attention models provide significant performance improve-. ments in a variety of fields, and as there are ample opportunities for more advancements, we foresee that these models will still receive significant attention in the time to come.\n\n### REFERENCES\n\n[1]H. Larochelle and G. E. Hinton, \"Learning to combine foveal glimpses with a third-order Boltzmann machine,\" in 24th Annuai Conference in Neural Information Processing Systems (NIPS 2010) Curran Associates, Inc., 2010, pp. 12431251. [2]V. Mnih, N. Heess, A. Graves, and k. kavukcuoglu, “Recurrenf models of visual attention,”in 27th Armual Conference on Neurai Information Processing Systems (NIPs 2014).Curran Associates Inc., 2014, pp. 22042212 [3]D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” in 3rd Internationai Conference on Learning Representation (ICLR 2015), 2015 [4]T. Luong, H. Pham, and C. D. Manning, “Effective approaches to attention-based neural machine translation,\" in 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP Hierarchical attention networks for document classification, in 2016 Conference of the NorthAmerican Chapter of the Association fon Computational Linguistics: Human Language Technologies (NAACL [6]YW2016 ACL, 206, 148012890,\"Attention-based LSTM for aspect-level sentiment classification, in 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP 2016). and L. Zhang, “Bottom-up and top-down attention for image captioning and visual question answering,” in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2018), 2018, pp. 60776086 [8]K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov R. Zemel, and Y. Bengio, \"Show, attend and tell: Neural image caption generation with visual attention,\" in 32nd Internationa Conference on Machine Learning (ICML 2015),vol. 37.PMLR 2015, pp. 20482057 [9]Y. Ma, H. Peng, and E. Cambria, “Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive LSTM, in 32nd AAAI Conference orn Artificial Intelligence (AAAI 2018).AAAI Press, 2018, Pp. 58765883 [10] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio \"Attention-based models for speech recognition,” in 28th Annuai Conference on Neural Information Processing Systems (NIPS 2015) [1 D.Brandasociates lnc 2015D pe575,PB Brakl and Y Bengio End-to-end attention-based large vocabulary speech recogni tion, in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2016). IEEE Signal Processing Society, 2016, pp. 49454949 [12] S. Kim, T. Hori, and S. Watanabe, “Joint CTC-attention based end-to-end speech recognition using multi-task learning, in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2017).IEEE Signal Processing Society, 2017, lag m a d sk ma Jo Gomez, L. u. Kaiser, and I Polosukhin, \"Attention is all you need,\" in 31st Annual Conference on Neural Information Processing Systems (NIPS 2017).Curran Associates, Inc., 2017, Pp. 5998- 6008 [14] K. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio, $“On$ the properties of neural machine translation: Encoder-decoder approaches,” in Sth Workshop on Syntax, Semarntics and Structure in Statistical Translation (SSST 2014).ACL, 2014, pp. 103111\n\n[15] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, [36] S. Zheng and R. Xia, “Leff-center-right separated neural network and D. Tran, “Image Transformer,\" in 35th Internafional Conference on Machine Learning (ICML 2018), vol. 80. PMLR, 2018, pp. 40554064. [16] L. Zhou, Y. Zhou, J. J. Corso, R. Socher, and C. Xiong, “End-toend dense video captioning with masked transformer,\" in 2018 IEEE/CVF Conference on Computer Vision and Patfern Recognifion [38]J. Gao, X. Wang, Y. Wang, Z. Yang, J. Gao, J. Wang, W. Tang [17] EVP 01,1ECompufer ocity,2018, p, 8739 874 iang BERT4Rec: Sequential recommendation with bidirectional en coder representations from transformer,” in 28th ACM Interna-[39]Y. Tay, A. T. Luu, and S. C. Hui, “Multi-pointer co-attention nettional Conference on Information and Knowledge Management (CIKM 2019).ACM, 2019, p. 14411450 [18] Wangnand D,MiT.Tas \"Ssrvey on theatention based N23, [40 S LMu,2018,bpe,239-31 and X Hu, User-video co-ttention 2016 [19] J. B. Lee, R. A. Rossi, S. Kim, N. K. Ahmed, and E. Koh, \"Attention models in graphs: A survey,” ACM Transitions on Knowledge Discovery fror Data, vol. 13, pp. 62:162:25, 2019. [20] S. Chaudhari, V. Mithal, G. Polatkan, and R.Ramanath, \"An attentive survey of attention models, ACM Transactions on Intelligent Systems and Technology, vol. 12, no. 5, pp. 132, 2021. [1]D.ue\"An introductory survey onatention mechanisms in NLe 42] mnd tol incanec ah w. 270-27enton network (IntelliSys 2019), ser. AISC, vol. 1038. Springer, 2020, pp. 432448 [22]A. Galassi, M. Lippi, and P. Torroni,\"Attention, please! a critical review of neuralatention modes in natural language pocess 43] O. Wallat and E raincar, A hybrid approach for aspet [23].Danilek in Rockt schl l Weab, and . iedel \" rustratinel national Conference on Learning Representations (ICLR 2017), 2017 [24] Xionan onga Hongs Wng ed D. Pumbeuat 4] and han ttentionvia-atentin neural machine model for weakly supervised audio tagging,\" in Proceedings of fhe 18th Annual Conference of the International Speech Communication Association (Interspeech 2017).ISCA, 2017, pp. 30833087 [25]C. Yu, K. S. Barsim, Q. Kong, and B. Yang, *Multi-level attention model for weakly supervised audio classification, in Proceedings fohe Detetionase Clas, cotio pf.Acoustic ces and Eents 018[46] . Wang S. Wang J. Tang N. OHare, Y. Chang, and B Li, [26] S. Sharma, R. Kiros, and R. Salakhutdinov, \"Action recognition using visual atention\" in Procedings ohth (4h zorn, one.47 Li, Y Wei, Zhang and Q Yang. \"Hierarchical attention [27] L. Gao, Z. Guo, H. Zhang, X. Xu, and H. T. Shen, *Video captioning with attention-based LSTM and semantic consistency,” IEEE Trarsacfions on Meltimedia, vol. 19, no. 9, pp. 20452055, 2017 [28]H. Ying, F. Zhuang, F. Zhang, Y. Liu, G. Xu, X. Xie, H. Xiong, and J. Wu, “Sequential recommender system based on hierarchical attention networks, in 27th International Joint Conference on Artificial Intelligence (IJCAI 2018).IJCAI, 2018, pp. 39263932 [29] H. Song, D. Rajan, J. Thiagarajan, and A. Spanias, \"Attend and diagnose: Clinical time series analysis using attention models,\" in 32nd AAAI Conference on Artificial Intelligence (AAAI 2018). AAAI[50]D. Kiela, C. Wang, and K. Cho, Dynamic meta-embeddingg Press, 2018, pp. 40914098 [30] D.T. Tran, A. Iosifidis, J. Kanniainen, and M. Gabbouj, “Temporal attention-augmented bilinear network for financial time-series data analysis,” IEEE Transactions on Neural Netuorks and Learning [51] S. Maharjan, M. Montes, F. A. Gonzalez, and T. Solorio, \"A Systems, vol. 30, no. 5, pp. 14071418, 2019 [31] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio, \"Graph attention networks,” in 6th Internationai Conference on Learning Representations (ICLR 2018), 2018. [32]J. Lu, J. Yang, D. Batra, and D. Parikh, “Hierarchical question image co-attention for visual question answering,\" in 30th Anrual Conference on Neural Information Processing Systems (NIPs 2016). Curran Associates, Inc., 2016, pp. 289297 [33] F. Fan, Y. Feng, and D. Zhao, “Multi-grained attention network for aspect-level sentiment classification, in 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP 2018).[54] A. Sordoni, P. Bachman, A. Trischler, and Y. Bengio,It [34 D.CLM,2018. P, 3-34g, and H. Wang, Interativ atenton networks for aspect-level sentiment classification,\" in 26th In- [55] A. Graves, G. Wayne, and I. Danihelka, “Neural Turing maternational Joint Conference on Artificial Intelligence (IJCAI 2017). IJCAI, 2017, pp. 40684074 [35] M. Seo, A. Kembhavi, A. Farhadi, and H. Hajishirzi, “Bidirectional attention flow for machine comprehension,” in 4th Irterna tional Conference on Learning Represertations (ICLR 2016), 2016\n\nfor aspect-based sentiment analysis with rotatory attention, arXiv:1802.00892, 2018 [37] B. Jing, P. Xie, and E. Xing, “On the automatic generation of med ical imaging reports,” in 56th Anrual Meeting of the Association for Computational Linguistics (ACL 2018).ACL, 2018, pp. 25772586 and X. Xie, \"CAMP: Co-attention memory networks for diagnosis prediction in healthcare,” in 2019 IEEE International Conference o Data Minting (ICDM 2019).IEEE, 2019, pp. 10361041. works for recommendation, in 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD 2018) network for personalized micro-video recommendation, in 2019 World Wide Web Conference (WWW 2019).ACM, 2019, pp.30203026 [41] M. Tu, G. Wang, J. Huang, Y. Tang, X. He, and B. Zhou, “Multihop reading comprehension across multiple documents by reasoning over heterogeneous graphs in 57thAnmual Meeting of the Association for Computational Linguistics (ACL 2019).Association for Computational Linguistics, 2019, pp. 27042713 for explainable fake news detection on social media,” in 58fh Annual Meeting of fhe Association for Computational Linguistics (ACL 2020).ACL, 2020, Pp. 505514. based sentiment aalysusing a leicalied domain ontology Corference (ESWC 2019), ser. LNCS, vol. 11503.Springer, 2019) Pp. 363378 translation, in 32nd AAAI Conference on Artificial Intelligence (AAAI 2018).AAAI Press, 2018, pp. 563570. [45]L. Wu, L. Chen, R. Hong, Y. Fu, X. Xie, and M. Wang, \"A hierarchical attention model for social contextual image recom mendation, IEEE Transactions on Knowledge and Data Engineering, 2019 Hierarchical attention network for action recognition in videos,\" arXiv:1607.06416, 2016. transfer network for cross-domain sentiment classification, in 32nd AAAI Conference on Artificial Intelligence (AAAI 2018). AAAI Press, 2018, p. 58525859 [48] C. Xing, Y. Wu, W. Wu, Y. Huang, and M. Zhou, *Hierarchical recurrent attention network for response generation,” in 32nd AAAI Conference on Artificial Intelligence (AAAI 2018). AAAI Press, 2018, pp. 56105617 [49] V. A. Sindagi and V. M. Patel, “HA-CCN: Hierarchical attention based crowd counting network,\" IEEE Transactions on Image Processing, vol. 29, pp. 323335, 2019 for improved sentence representations,” in 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP 2018) ACL, 2018, pp. 14661477. genre-aware attention model to improve the likability prediction of books,”in 2018 Conference onEmpirical Methods in Naturai Language Processing (EMNLP 2018).ACL, 2018, pp. 33813391. [52] G. I. Winata, Z. Lin, and P. Fung, “Learming multilingual metaembeddings for code-switching named entity recognition,\" in 4fh Workshop on Representation Learning for NLP (RepL4NLP 2019) ACL, 2019, pp. 181186 [53] R. Jin, L. Lu, J. Lee, and A. Usman, “Multi-representational con volutional neural networks for text classification, Computationa Intelligence, vol.35, no. 3, pp. 599609, 2019 erative alternating neural attention for machine reading, arXiv:1606.02245, 2016. chines, arXiv:1410.5401, 2014 [56] D. Britz, A. Goldie, M.-T. Luong, and Q. Le, “Massive exploration of neural machine translation architectures,” in 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017). ACL, 2017, pp. 14421451.\n\n[57]R. J. Williams, Simple statistical gradient-following algorithms for connectionist reinforcement learning,\" Machine Learning vol. 8, no. 3, pp. 229256, 1992. [58] T.Shen, T. Zhou, G. Long, J. Jiang, S. Wang, and C. Zhang, *Reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling,” in 27th International Joint Cornference on Artificial Intelligence (IJCAI 2018).IJCAI, 2018, pp. 43454352 [59] M. Malinowski, C. Doersch, A. Santoro, and P. Battaglia, “Learning visual question answering by bootstrapping hard attention,\" in 2018 European Conference on Computer Vision (ECCV 2018), 2018. [60] Y. Liu, W. Wang, Y. Hu, J. Hao, X. Chen, and Y. Gao, “Multiagent game abstraction via graph attention neural network,”\" in 34th AAAI Conference on Artificial Intelligence (AAAI 2020), vol. 34, no. 05.AAAI Press, 2020, pp. 72117218. [61] S.Seo,J.Huang, H. Yang, and Y. Liu, “Interpretable convolutional neural networks with dual local and global attention for review rating prediction,in 11th ACM Conference on Recommender Systems (RecSys 2017).ACM, 2017, pp. 297305 [62] J. Wang, C. Sun, S. Li, X. Liu, L. Si, M. Zhang, and G. Zhou, \"Aspect sentiment classification towards question-answering with reinforced bidirectional attention network, in 57th Ammual Meeting of fhe Association for Computational Linguistics (ACL 2019). ACL, 2019, pp. 35483557. [63] M. Jiang, C. Li, J. Kong, Z. Teng, and D. Zhuang, \"Crosslevel reinforced attention network for person re-identification, Journal of Visual Communication and Image Representation, vol. 69 p. 102775, 2020. [64] T. Shen, T. Zhou, G. Long, J. Jiang, S. Pan, and C. Zhang, “DiSAN: Directional self-attention network for RNN/CNN-free language understanding,\" in 32nd AAAI Conference on Artificial Intelligence. (AAAI 2018).AAAI Press, 2018, Pp. 54465455 [65] O. Arshad, I Gallo, S. Nawaz, and A. Calefati, \"Aiding intra-text representations with visual context for multimodal named entity recognition,\" in 2019 International Conference on Document Analysis. and Recognition (ICDAR 2019).IEEE, 2019, pp. 337342 [66] W. Wu, X. Sun, and H. Wang, \"Question condensing networks for answer selection in community question answering,\" in Proceed. ings of the 56th Annual Meeting of the Association for Coputational Linguistics (ACL 2018).ACL, 2018, pp. 17461755 [67] . Okta.a, Schlem se MDonaFol,go MH aemeMa, Beiric, B. Glocker, and D. Rueckert, \"Attention U-Net: Learming where to look for the pancreas in 1st Medical Imaging with Deep Learning Conference (MIDL 2018), 2018 [68] R. Tan, J. Sun, B. Su, and G. Liu, “Extending the transformer with context and multi-dimensional mechanism for dialogue response generation, in 8th International Conference on Natural Language Processing and Chinese Computing (NLPCC 2019), ser. LNCS, J. Tang, M.-Y. Kan, D. Zhao, S. Li, and H. Zan, Eds., vol. 11839. Springer, 2019, pp. 189199 [69] L. Chen, B. Lv, C. Wang, S. Zhu, B. Tan, and K. Yu, “\"Schemaguided multi-domain dialogue state tracking with graph attention neural networks,in 34th AAAI Conference on Artificial Intelligence (AAAI 2020), vol. 34, no. 05.AAAI Press, 2020, pp. 75217528 [70] H. Wang, G. Liu, A. Liu, Z. Li, and K. Zheng, \"Dmran: A hierarchical fine-grained attention-based network for recommenda-. tion, in 28th International Joint Conference on Artificial Intelligence (IJCAI 2019). [71] Z. Lin, M. Feng, C. N. d. Santos, M. Yu, B. Xiang, B. Zhou, and Y. Bengio, \"A structured self-attentive sentence embedding in 5th International Conference on Learning Representations (ICLR 2017), 2017. [72]J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization, arXiv:1607.06450, 2016. [73] H. Zhao, J. Jia, and V. Koltun,“Exploring self-attention for image recognition,\" in 2020 IEEE/CVF Conference on Computer Vision and Patfern Recognition (CVPR 2020), 2020, pp. 10 07610 085. [74] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, \"Selfattention generative adversarial networks, in 36th International Conference on Machine Learning (ICML 2019), ser. Proceedings of Machine Learning Research, K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97. PMLR, 2019, pp. 73547363 [75] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. WardeFarley, S. Ozair, A. Courville, and Y. Bengio, \"Generative ad versarial nets,in 27th Anmual Conference on Neural Information\n\nProcessing Systems (NIPs 2014).Curran Associates, Inc., 2014 Pp. 2672-2680 [76] A. Sinha and J. Dolz, \"Multi-scale self-guided attention for medical image segmentation,” IEEE Journal of Biomedical and Health Informafics, vol. 25, no. 1, pp. 121-130, 2021 [77] J. Fajtl, H. S. Sokeh, V. Argyriou, D. Monekosso, and P. Re magnino, Summarizing videos with attention, in 2018 Asian Corferernce on Computer Vision (ACCV 2018), ser. LNCS, vol. 11367 Springer, 2018, pp. 3954. [78] J. Salazar, K. Kirchhoff, and Z. Huang, “\"Self-attention networks for connectionist temporal classification in speech recognition, in 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2019).IEEE, 2019, pp. 71157119 [79]T. Afouras, J. S. Chung, A.Senior, O. Vinyals, and A.Zisserman, Deep audio-visual speech recognition, IEEE Transactions or Pattern Analysis and Machine Intelligence, pp. 11, 2018 [80]S. Zhang, Y. Tay, L. Yao, and A. Sun, *Next item recommendation with self-attention, arXiv preprint arXiv:1808.06414, 2018 [81]G. Letarte, F. Paradis, P. Giguere, and F. Laviolette, “Impor tance of self-attention for sentiment analysis,\" in 2018 Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP (BlackboxNLP 2018).ACL, 2018, pp. 267275 [82] A. Sankar, Y. Wu, L. Gou, W. Zhang, and H. Yang, “Dysat Deep neural representation learning on dynamic graphs via selfattention networks, in 13th International Conference on Web Search and Data Minting (WSDM 2020), 2020, pp. 519527 [83] P. Velickovic, G.Cucurull, A. Casanova, A.Romero, P. Lio and Y. Bengio, Graph attention networks, in 5th International Conference on Learning Representations (ICLR 2017), 2017 [84] S. Iida, R Kimura, H. Cui, P-H. Hung, T. Utsuro, and M. Nagata, \"Attention over heads: A multi-hop attention for neural machine translation, in 57th Anmual Meeting of the Association for Com putational Linguistics: Student Research Workshop (ACL-SRW 2019). ACL, 2019, pp. 217222 [85] N. K. Tran and C. Niedereee, “Multihop attention networks for question answer matching, in 41st ACM SIGIR International Conference on Research & Development in Information Retrieval (SIGIR 2018).ACM, 2018, Pp. 325334 [86]Y. Gong and S. R. Bowman, \"Ruminating reader: Reasoning with gated multi-hop attention, in 5th International Conference on Learning Representation (ICLR 2017), 2017 [87]S. Yoon, S. Byun, S. Dey, and K. Jung, “Speech emotion recognition using multi-hop attention mechanism, in 2019 IEEE Interna fional Conference on Acoustics, Speech and Signal Processing (ICASSP 2019).IEEE, 2019, pp. 28222826. [88]Z. Yang, X. He, J. Gao, L. Deng, and A. Smola, “Stacked attention networks for image question answering,” in 2016 IEEE/CVF Con ference on Compufer Vision and Pattern Recognition (CVPR 2016) 2016, pp.2129 [89]Y. Wang, A. Sun, J. Han, Y. Liu, and X. Zhu, \"Sentiment analysis by capsules,” in 2018 World Wide Web Conference (WWW 2018). ACM, 2018, p. 11651174 [90]S. Sabour, Ni. Frosst, and G. E. Hinton, “Dynamic routing between capsules,in 31st Anmual Conference on Neural Information Prosng Systems NIPs 2017). Curran Associates Inc, 2017, p. [91] M. India, P. Safari, and J. Hernando, “Self multi-head attention for speaker recognition,\" in Proceedings of fhe 20th Anmual Conference of the International Speech Communication Association (Interspeecl 2019). ISCA, 2019, pp. 28222826 [92] C. Wu, F. Wu, S. Ge, T. Qi, Y. Huang, and X. Xie, “Neural news recommendation with multi-head self-attention, in 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP 2019).ACL, 2019, pp. 63896394. [93]Y. Wang, W. Chen, D. Pi, and L. Yue,\"Adversarially regularized medication recommendation model with multi-hop memory net-. work,Knowledge and Information Systems, vol.63, no. 1, pp. 125142, 2021 [94] M. Cornia, M. Stefanini, L. Baraldi, and R. Cucchiara, “Meshedmemory transformer for image captioning,\" in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2020), 2020, pp. 10 57810 587 [95] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille and Y. Zhou, \"TransUnet: Transformers make strong encoders for medical image segmentation, arXiv preprint arXiv:2102.043062021\n\n[96]P. Zhong, D. Wang, and C. Miao, “Knowledge-enriched transformer for emotion detection in textual conversations,”in 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. (EMNLP-IJCNLP 2019). ACL, 2019, Pp. 165176 [97]Y. Zhou, R. Ji, J. Su, X. Sun, and W. Chen, “Dynamic capsule 2019, pp. 93249331 [98] Y. Wang, A. Sun, M. Huang, and X. Zhu, \"Aspect-level sentiment analysis using AS-capsules,\" in The World Wide Web Conference, [9 21Da,2 233-20.Yang . Carboell . Le, and R slakhtdinovg Transformer-XL: Attentive language models beyond a fixed length context,”in 57th Annual Meeting of the Association for Computational Linguistics (ACL 2019).ACL, 2019, pp. 29782988. [100] N. Kitaev, L. Kaiser, and A. Levskaya, Reformer: The efficient Transformer, in 8th International Conference on Learning Represen. attention with linear complexity,\" rXiv:2006.04768, 2020. [102] Z. Wu, Z. Liu, J. Lin, Y. Lin, and S. Han, “Lite transformer with long-short range attention, in 8th International Conference on Learning Representations (ICLR 2020), 2020 [103] Y. Tay, D. Bahri, D. Metzler, D.-C. Juan, Z. Zhao, and C. Zheng, Synthesizer: Rethinking self-attention for transformer models, in Proceedings of the38th International Cornference on Machine Learn ing (ICML 2021), vol. 139.PMLR, 2021, pp. 10 18310192 [104] Y. Tay, M. Dehghani, D. Bahri, and D. Metzler, “Efficient transformers: A survey, arXiv:2009.06732, 2020. [105] X. Li, J. Song, L. Gao, X. Liu, W. Huang, X. He, and C. Gan, Beyond RNNs: Positional self-attention with co-attention for video question answering,\" in 33rd AAAI Conference on Artificial Intelligence (AAAI 2019), vol. 33.AAAI Press, 2019, Pp. 86588665 [106] A. W. Yu, D. Dohan, M.-T. Luong, R. Zhao, K. Chen, M. Norouzi and Q. V. Le, \"QANet: Combining local convolution with global self-attention for reading comprehension,\" in 6th International Conference on Learning Representations (ICLR 2018), 2018 [107] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, *BLEU: a method for automatic evaluation of machine translation,” in 40th Annual Meeting of fhe Association for Compufational Linguistics (ACL 2002). ACL, 2002, pp. 311318. [108] S. Banerjee and A. Lavie, “METEOR: An automatic metric for MT evaluation with improved correlation with human judgments, in 2005 Workshop oni Intrinsic and Extrinsic Eoaluation Measures for Machine Translation and/or Summarization. ACL, 2005, pp. 6572 [109] R. Sennrich, “Perplexity minimization for translation model domain adaptation in statistical machine translation,” in 13th Conference of fhe European Chuapter of the Association for Computational Linguistics (EACL 2012).ACL, 2012, pp. 539549 [110] M. Popovic and H. Ney, \"Word error rates: Decomposition over POS classes and applications for error analysis,” in 2nd Workshop on Statistical Machtine Translation (WMT 2007).ACL, 2007, Pp 48-55 [111] P. Schwarz, P. Matejka, and J. Cernocky, “Towards lower error rates in phoneme recognition, in 7th International Conference. on Text, Speech and Diaiogue (TSD 2004), ser. LNCS, vol. 3206 Springer, 2004, pp. 465472. [112] D. S. Turaga, Y. Chen, and J. Caviedes, “No reference PSNR estimation for compressed pictures,\" Signal Processing: Image Commticatfion, vol. 19, no. 2, pp. 173184, 2004 [113] P. Ndajah, H. Kikuchi, M. Yukawa, H. Watanabe, and S. Muramatsu, “SSIM image quality metric for denoised images,” in 3rd WSEAS International Conference on Visualization, Imaging and Simulation (VIS 2010).WSEAS, 2010, pp. 5358 [114] M. A. Rahman and Y. Wang, “Optimizing intersection-over-union in deep neural networks for image segmentation,\" in 12th International Symposium on Visual Computing (ISVC 2016), ser. LNCS, vol. 10072.Springer, 2016, pp. 234244. [115] X. Chen, L. Yao, and Y. Zhang, “Residual attention U-net for auto mated multi-class segmentation of COVID-19 chest CT images, arXiv:2004.05645, 2020 [116] S. Liu, Y. Chen, K. Liu, and J. Zhao, “Exploiting argument information to improve event detection via supervised attention of the Journal of Web Engineering. Dr. Frasincar is a member of the mechanisms,\" in 55th Arnual Meeting of the Association for Coputational Linguistics (ACL 2017).ACL, 2017, pp. 17891798\n\n[117] C. Liu, J. Mao, F. Sha, and A. Yuille, \"Attention correctness in neural image captioning,” in 31st AAAl Conference on Artificial Intelligence (AAAI 2017). AAAI Press, 2017, Pp. 41764182 [118] A. Das, H. Agrawal, L. Zitnick, D. Parikh, and D. Batra, “Human atention novisual question anowering: Do hu mans and dee neural attention models for video captioning by human gaze data,\"in 2017 IEEE Conference on Computer Vision and Paftern Recognition (CVPR 2017).IEEE Computer Society, 2017 [120] S. Jain and B. C. Wallace, \"Attention is not explanation,” in 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2019).ACL, 2019, pp. 35433556 [121] S. Wiegreffe and Y. Pinter, \"Attention is not not explanation,” in 2019 Conference on Empirical Methods in Natural Language Process ing and fhe9th International Joint Conference on Nafural Language 1 B. V. Srinivasan, and B. Ravindran, Towards transparent and explainable attention models,” in 58th Anmual Meeting of the Association for Computational Linguistics (ACL 2020).ACL, 2020 pp. 4206-4216. [123] K. K. Thekumparampil, C. Wang, S. Oh, and L-J. Li, \"Attention based graph neural network for semi-supervised learning, arXiv:1803.03735, 2018 [124]D. Nie, Y. Gao, L. Wang, and D. Shen, \"ASDNet: Attention based semi-supervised deep networks for medical image segmenta tion, in 21st International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI 2018), ser. LNCS, vol 11073.Springer, 2018,pp.370-378 [125]Y. Alami Mejati, C. Richardt, J. Tompkin, D. Cosker, and K. I. Kim, Unsupervised attention-guided image-to-image transla tion, in 32nd Anmual Conference on Neural Information Processing Systems (NIPS 2018).Curran Associates, Inc., 2018, Pp. 36933703 [126] R. He, W. S. Lee, H. T. Ng, and D. Dahlmeier, \"An unsupervised neural attention model for aspect extraction,” in 55thi Annua Meeting of the Association for Computational Linguistics (ACL 2017) ACL, 2017, pp. 388397.\n\nGianni Brauwers was born in Spijkenisse,the Netherlands.in 1998. He received the B.S. de gree in econometrics and operations research from Erasmus University Rotterdam, Rotterdam the Netherlands,in 2019,and is currently pursu ing the M.S. degree in econometrics and man agement science at Erasmus University Rotter dam. He is a Research Assistant at Erasmus Uni-\n\n![](./images/fwKubs9NrrsE4E6IOIuyNmz1254MnctEC.png)\n\nversity Rotterdam, focusing his research on neural attention models and sentiment analysis\n\nFlavius Frasincar was born in Bucharest.Ro mania, in 1971. He received the M.S. degree in computer science,in 1996,and the M.Phil degree in computer science, in 1997, from Politehnica University of Bucharest, Bucharest, Romania, and the P.D.Eng. degree in computer sci ence, in 2000, and the Ph.D. degree in computer science, in 2005, from Eindhoven University of Technology, Eindhoven, the Netherlands Since 2005,he has been an Assistant Profes\n\n![](./images/fRt7WcnS0GB13Asmx3FTox8eFYu3ccbMN.png)\n\nsor in computer science at Erasmus University Rotterdam, Rotterdam, the Netherlands. He has published in numerous\n\nconferences and journals in the areas of databases,Web information systems, personalization, machine learning, and the Semantic Web. He is a member of the editorial boards of Decision Support Systems. International Journal of Web Engineering and Technology, and Com putational Linguistics in the Netherlands Journal, and co-editor-in-chief Association for Computing Machinery","source":"code/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.md","raw":"# A General Survey on Attention Mechanisms in Deep Learning\n\nGianni Brauwers and Flavius Frasincar\n\nAbstract—Attention is an important mechanism that can be employed for a variety of deep learning models across many different domains and tasks. This survey provides an overview of the most important attention mechanisms proposed in the literature. The various attention mechanisms are explained by means of a framework consisting of a general attention model, uniform notation, and a comprehensive taxonomy of attention mechanisms. Furthermore, the various measures for evaluating attention models are reviewed and methods to characterize the structure of attention models based on the proposed framework are discussed. Last, future work in the field of attention models is considered\n\nIndex Terms-Attention models, deep learning, introductory and survey, neural nets, supervised learning\n\n### 1INTRODUCTION\n\nHdemao model was created for machine translation, but was quickly adopted tobe used for other tasks, such as image processing [15], video processing [16], and recommender systems [17]. The purpose of this survey is to explain the general\n\nthe computational complexity of image processing while improving performance by introducing a model that would only focus on specific regions of images instead of the entire picture. Although, the true starting point of the attention mechanisms we know today is often attributed to originate in the field of natural language processing [3]. Bahdanau et al.[3] implement attention in a machine translation model to address certain issues with the structure of recurrent neural networks. After Bahdanau et al. [3] emphasized the advantages of attention, the attention techniques were refined [4] and quickly became popular for a variety of tasks, such as text classification [5], [6], image captioning [7], [8], sentiment analysis [6], [9], and speech recognition [10], [11], [12] Attention has become a popular technique in deep learn-\n\nform of attention,and provide a comprehensive overview of attention techniques in deep learning. Other surveys have already been published on the subject of attention models For example, in [18], a survey is presented on attention in computer vision, [19] provides an overview of attention in graph models, and [20], [21],[22] are all surveys on attention in natural language processing. This paper partly builds on the information presented in the previously mentioned surveys. Yet, we provide our own significant contributions The main difference between this survey and the previously mentioned ones is that the other surveys generally focus on attention models within a certain domain. This survey however, provides a cross-domain overview of attention techniques. We discuss the attention techniques in a general way, allowing them to be understood and applied in a variety of domains. Furthermore, we found the taxonomies presented in previous surveys to be lacking the depth and structure needed to properly distinguish the various attention mechanisms. Additionally, certain significant attention techniques have not yet been properly discussed in previous surveys, while other presented attention mechanisms seem to be lacking either technical details or intuitive explanations. Therefore, in this paper, we present important attention techniques by means of a single framework using a uniform notation,a combination of both technical and in tuitive explanations for each presented attention technique, and a comprehensive taxonomy of attention mechanisms The structure of this paper is as follows. Section 2 in-\n\ning for several reasons. Firstly, models that incorporate attention mechanisms attain state-of-the-art results for all of the previously mentioned tasks, and many others. Furthermore, most attention mechanisms can be trained jointly with a base model, such as a recurrent neural network or a convolutional neural network using regular backpropa gation [3]. Additionally, attention introduces a certain type of interpretation into neural network models [8] that are generally known to be highly complicated to interpret. Moreover, the popularity of attention mechanisms was additionally boosted after the introduction of the Transformer model [i3] that further proved how effective attention can be.Attention was originally introduced as an extension to recurrent neural networks [14]. However, the Transformer model proposed in [13] poses a major development in attention research as it demonstrates that the attention mechanism is sufficient to build a state-of-the-art model. This means that disadvantages, such as the fact that recurrent neural networks are particularly difficult to parallelize, can\n\ntroduces a general attention model that provides the reader with a basic understanding of the properties of attention and how it can be applied. One of the main contributions of this paper is the taxonomy of attention techniques presented in Section 3. In this section, attention mechanisms are explained and categorized according to the presented\n\n![](./images/fXAE7qMmeZtPgm2VeKxkN3y8GAQXG8CbW.png)\n\nFig. 1. An illustration of the general structure of the task model.\n\ntaxonomy. Section 4 provides an overview of performance measures and methods for evaluating attention models. Furthermore,the taxonomy is used to evaluate the structure of various attention models. Lastly, in Section 5, we give oun conclusions and suggestions for further research\n\n## 2 General Attention Model\n\nThis section presents a general form of attention with corresponding notation. The notation introduced here is based on the notation that was introduced in [23] and popularized in [13]. The framework presented in this section is used throughout the rest of this paper. To implement a general attention model, it is necessary\n\nto first describe the general characteristics of a model that can employ attention. First of all, we will refer to the complete model as the task model, of which the structure is presented in Fig. 1. This model simply takes an input, carries out the specified task, and produces the desired output.For example, the task model can be a language model that takes as input a piece of text, and produces as output a summary of the contents,a classification of the sentiment, or the text translated word for word to another language. Alternatively the task model can take an image, and produce a captionthe general attention module is the query $q\\in\\mathbb{R}^{d_{q}}$, andthe or segmentation for that image. The task model consists of four submodels: the feature model, the query model, the attention model, and the output model. In Subsection 2.1, the feature model and query model are discussed, which are used to prepare the input for the attention calculation. In Subsection 2.2, the attention model and output model are discussed, which are concerned with producing the output\n\n## 2.1Attention Input\n\nSuppose the task model takes as input the matrix $X\\in$ $\\Pi_{\\mathcal{R}}d_{\\mathcal{F}}\\times\\boldsymbol{n}_{\\mathcal{X}}$ where $d_{x}$ represents the size of the input vectors and $n_{x}$ represents the amount of input vectors. The columns in this matrix can represent the words in a sentence the pixels in an image, the characteristics of an acoustic sequence, or any other collection of inputs.The feature model is then employed to extract the $n_{f}$ feature vectors $f_1,\\ldots,f_{n_f}\\in\\mathbb{R}^d_f$ from $X$ where $d_{f}$ represents the size of the feature vectors.The feature model can be a recurrent neural network (RNN), a convolutional neural network (CNN), a simple embedding layer, a linear transformation of the original data, or no transformation at all. Essentially, the feature model consists of all the steps that transform the original input $X$ into the feature vectors $f_{1},\\ldots,f_{n_{f}}$ that the attention modelwill attend to.\n\n![](./images/fxeeSs49eiCx6zygk9gTruATCtkbbN2Om.png)\n\nFig. 2. The inner mechanisms of the general attention module\n\nTo determine which vectors to attend to,the attention model requires the query $q\\in\\mathbb{H}^d_q$ ,where $d_q$ indicates the size of the query vector. This query is extracted by the query model, and is generally designed based on the type of output that is desired of the model. A query tells the attention model which feature vectors to attend to.It can be interpreted literally as a query, or a question. For example, for the task of image captioning, suppose that one uses a decoder RNN model to produce the output caption based on feature vectors obtained from the image by a CNN.At each prediction step, the hidden state of the RNN model can be used as a query to attend to the CNN feature vectors. In each step, the query is a question in the sense that it asks for the necessary information from the feature vectors based on the current prediction context\n\n### 2.2 Attention Output\n\nThe feature vectors and query are used as input for the attention model. This model consists of a single, or a collection of general attention modules.An overview of a general attention module is presented in Fig. 2. The input of matrix of feature vectors $\\boldsymbol{F}=[\\boldsymbol{f}_{1},\\ldots,\\boldsymbol{f}_{n_{f}}]\\in\\mathbb{R}^{d_{f}\\times n_{f}}.$ Two separate matrices are extracted from the matrix $F$ : the keys matrix $K=[k_{1},\\ldots,k_{n_{f}}]\\in\\mathbb{R}^{d_{k}\\times n_{f}}$, and the values matrix $V$ = $[ \\boldsymbol{v}_1, \\ldots , \\boldsymbol{v}_{n_f}]$ $\\in$ $\\mathbb{R} ^{d_v\\times n_f}$, where $d_{k}$ and $d_{v}$ indicate, respectively, the dimensions of the key vectors (columns of $K$ ) and value vectors (columns of $V$ ). The general way of obtaining these matrices is through a linear transformation of $F$ using the weight matrices. $W_{K}\\in\\mathbb{R}^{d_{k}\\times d_{f}}$ and $W_{V}\\in\\mathbb{R}^{d_{v}\\times d_{f}}$ ,for $K$ and $V$ , respectively. The calculations of $K$ and $V$ are presented in (1). Both weight matrices can be learned during training or predefined by the researcher For example, one can choose to define both $W_{K}$ and $W_{V}$ as equal to the identity matrix to retain the original feature vectors. Other ways of defining the keys and the values are also possible, such as using completely separate inputs for the keys and values. The only constraint to be obeyed is that the number of columns in $K$ and $V$ remains the same\n$$K_{d_{k}\\times n_{f}}=W_{K}\\times F_{d_{f}\\times n_{f}},\\quad V_{d_{v}\\times n_{f}}=W_{V}\\times F_{d_{f}\\times n_{f}}.$$\n\nThe goal of the attention module is to produce a weighted average of the value vectors in $V$ .The weights used to produce this output are obtained via an attention scoring and alignment step.The query $q$ and the keys matrix $K$ are used to calculate thevector of attention scores\n\n$\\boldsymbol{e}=\\left[e_{1},\\ldots,e_{n_{f}}\\right]\\in\\mathbb{R}^{n_{f}}.$ This is done via the score function score(), as illustrated in (2)\n\n$$e_{l}\\:=\\:\\mathrm{score}(\\:q\\:,\\:k_{l}\\:).$$\n\nAs discussed before, the query symbolizes a request for information. The attention score $e_{1}$ represents how important the information contained in the key vector $k_{l}$ is according to the query. If the dimensions of the query and key vectors are the same, an example of a score function would be to take the dot-product of the vectors. The different types of score functions are further discussed in Section 3.2.1. Next, the attention scores are processed further through\n\nan alignment layer. The attention scores can generally have a wide range outside of [0,1] . However, since the goal is to produce a weighted average, the scores are redistributed via an alignment function align() as defined in (3).\n\n$$\\begin{matrix}a_l&=\\text{align}(\\begin{array}{c}e_l\\:;&\\boldsymbol{e}\\\\1\\times1&n_f\\times1\\end{array}),\\end{matrix}$$\n\nwhere $a_l\\in\\mathbb{R}^1$ is the attention weight corresponding to the lth value vector.One example of an alignment function wouldbe touse a softmax function,but the various other alignment types are discussed in Section 3.2.2. The attention weights provide a rather intuitive interpretation for the attention module. Each weight is a direct indication of how important each feature vector is relative to the others for this particular problem. This can provide us with a more in-depth understanding of the model behaviour, and the re lations between inputs and outputs. The vector of attention weights $a$ = $[ a_{1}, \\ldots , a_{n_{f}}]$ $\\in$ $\\mathbb{R} ^{n_{f}}$ is used to produce the context vector $c\\in\\mathbb{R}^{d_v}$ by calculating a weighted average of the columns of the values matrix $V$ ,as shown in (4)\n$$\\boldsymbol{c}_{d_v\\times1}=\\sum_{l=1}^{n_f}a_l\\:\\times\\:\\boldsymbol{v}_l\\:.$$\n\nAs illustrated in Fig. 1, the context vector is then used in the output model to create the output $\\ddot{y}$ This output model translates the context vector into an output prediction. For example, it could be a simple softmax layer that takes as input the context vector $C$ , as shown in (5)\n\n$$\\hat{\\boldsymbol{y}}_{d_{y}\\times1}=\\mathrm{softmax}(W_{c}\\:\\times\\:c_{d_{y}\\times1}\\:+\\:b_{c}\\:),$$\n\nwhere $d_{y}$ is the number of output choices or classes, and $W_{c}\\in\\mathbb{R}^{d_{y}\\times d_{v}}$ and $b_{c}\\in\\mathbb{R}^{d}$ are trainable weights\n\n### 2.3 Attention Applications\n\nAttention is a rather general mechanism that can be used in a wide variety of problem domains. Consider the task of machine translation using an RNN model. Also, consider the problem of image classification using a basic CNN model. While an RNN produces a sequence of hidden state vectors, a CNN creates feature maps, where each region in the image is represented by a feature vector. The RNN hidden states are organized sequentially, while the CNN feature maps are organized spatially. Yet, attention can still be applied in both situations, since the attention mechanism does not inherently depend on the organization of thefeature vectors. This characteristic makes attention easy to implement in a wide variety of models in different domains.\n\nAnother domain where attention can be applied is audio processing [24], [25]. Acoustic sequences can be represented by a sequence of feature vectors that relate to certain time periods of the audio sample. These vectors could simply be the raw input audio, or they can be extracted via,for example, an RNN or CNN. Video processing is another domain where attention can be applied intuitively [26], [27] Video data consists of sequences of images, so attention can be applied to the individual images, as well as the entire sequence. Recommender systems often incorporate a users interaction history to produce recommendations. Feature vectors can be extracted based on, for example, the id's or other characteristics of the products the user interacted with, and attention can be applied to them [28]. Attention can generally also be applied to many problems that use a time series as input, be it medical [29], financial [30], or anything else, as long as feature vectors can be extracted The fact that attention does not rely on the organization\n\nof the feature vectors allows it to be applied to various problems that each use data with different structures, as illustrated by the previous domain examples.Yet,this can be taken even further by applying attention to data where there is irregular structure. For example, protein structures, city traffic flows, and communication networks cannot always be represented using neatly structured organizations, such as sequences, like time series, or grids, like images. In such cases, the different aspects of the data are often represented as nodes in a graph. These nodes can be represented by feature vectors, meaning that attention can be applied in domains that use graph-structured data as well [19],[31] In general, attention can be applied to any problem for\n\nwhich a set of feature vectors can be defined or extracted As such, the general attention model presented in Fig. 2 is applicable to a wide range of domains. The problem,how ever, is that there is a large variety of different applications and extensions of the general attention module. As such, in Section 3, a comprehensive overview is provided of a collection of different attention mechanisms.\n\n## 3ATTENTION TAXONOMY\n\nThere are many different types of attention mechanisms and extensions,and a model can use different combinations of these attention techniques. As such, we propose a taxonomy that can be used to classify different types of attention mechanisms. Fig. 3 provides a visual overview of the different categories and subcategories that the attention mechanisms can be organized in. The three major categories are based on whether an attention technique is designed to handle specific types of feature vectors (feature-related), specific types of model queries (query-related), or whether it is simply a general mechanism that is related to neither the feature model, nor the query model (general). Further explanations of these categories and their subcategories are provided in the following subsections. Each mechanism discussedin this section is either amodification to the existing inner mechanisms of the general attention module presented in Section 2, or an extension of it The presented taxonomy can also be used to analyze\n\nthe architecture of attention models.Namely, the major categories and their subcategories can be interpreted as\n\n![](./images/fwzKYgHVze3V4K8ZYBR28uKos0SPY48WT.png)\n\nFig. 3. A taxonomy of attention mechanisms\n\n### TABLE 1 Notation.\n\nattend to these various vectors.These features may have specific structures that require special attention mechanisms to handle them.These mechanisms can be categorized to deal with one of the following feature characteristics: the multiplicity of features, the levels of features, or the representations of features\n\n### 3.1.1Multiplicity of Features\n\nSymbol\tDescription\n$F$\tMatrix of size $d_f \\times n_f$ containing the feature vectors $f_1, \\ldots, f_{n_f} \\in \\mathbb{R}^{d_f}$ as columns. These feature vectors are extracted by the feature model.\n$K$\tMatrix of size $d_k \\times n_j$ containing the key vectors $k_1, \\ldots, k_{n_j} \\in \\mathbb{R}^{d_k}$ as columns. These vectors are used to calculate the attention scores.\n$V$\tMatrix of size $d_v \\times n_j$ containing the value vectors $v_1, \\ldots, v_{n_j} \\in \\mathbb{R}^{d_v}$ as columns. These vectors are used to calculate the context vector.\n$W_K$\tWeights matrix of size $d_k \\times d_f$ used to create the $K$ matrix from the $F$ matrix.\n$W_V$\tWeights matrix of size $d_v \\times d_f$ used to create the $V$ matrix from the $F$ matrix. This vector essentially represents a question, and is used to calculate the attention scores.\n$q$\tQuery vector of size $d_q$. This vector is used to calculate the attention weights.\n$c$\tContext vector of size $d_c$. This vector is the output of the attention model.\n$e$\tScore vector of size $d_{e_i}$ containing the attention scores $e_1, \\ldots, e_{n_e} \\in \\mathbb{R}^1$. These are used to calculate the attention weights.\n$a$\tAttention weights vector of size $d_{a_j}$ containing the attention weights $a_1, \\ldots, a_{n_a} \\in \\mathbb{R}^1$. These are the weights used in the calculation of the context vector.\n\nFor most tasks, a model only processes a single input, such as an image, a sentence, or an acoustic sequence. We refer to such a mechanism as singular features attention. Other models are designed to use attention based on multiple inputs to allow one to introduce more information into the model that can be exploited in various ways.However, this does imply the presence of multiple feature matrices that require special attention mechanisms to be fully used. For example, [32] introduces a concept named co-attention to allow the proposed visual question answering (VQA) model to jointly attend to both an image and a question Co-attention mechanisms can generally be split up\n\ninto two groups [33]: coarse-grained co-attention and fine-grained co-attention. The difference between the two groups is the way attention scores are calculated based on the two feature matrices. Coarse-grained attention mecha-. nisms use a compact representation of one feature matrix as a query when attending to the other feature vectors. Fine-grained co-attention, on the other hand, uses all feature vectors of one input as queries. As such, no information is lost, which is why these mechanisms are called fine-grained As an example of coarse-grained co-attention, [32] pro-\n\northogonal dimensions of an attention model. An attention model can consist of a combination of techniques taken from any or all categories. Some characteristics, such as the scoring and alignment functions, are generally required for any attention model. Other mechanisms, such as multihead attention or co-attention are not necessary in every situation. Lastly, in Table 1, an overview of used notation with corresponding descriptions is provided.\n\nposes an alternating co-attention mechanism that uses the context vector (which is a compact representation) from one attention module as the query for the other module, and vice versa. Alternating co-attention is presented in Fig. 4. Given a set of two input matrices $X^{(1)}$ and $X^{(2)}$ , features are extracted by a feature model to produce the feature extracts feature vectors so that the attention model canmatrices $F^{(1)}\\in\\mathbb{R}^{d_f^{(1)}\\times n_f^{(1)}}$ and $F^{(2)}\\in\\mathbb{R}^{d_{f}^{(2)}\\times n_{f}^{(2)}}$ ,where $d_{f}^{(1)}$\n\n## 3.1 Feature-Related Attention Mechanisms\n\nBased on a particular set of input data, a feature model\n\n![](./images/fTFNeAz730mbNSQS1tZxyhe0r9V8CHLx8.png)\n\nFig. 4. An illustration of alternating co-attention\n\nvectors extracted from the first and second inputs, while co-attention is used for VQA, so the two input matrices are the image data and the question data, for which the feature model for the image consists of a CNN model, and the feature modelfor the question consists of word embeddings a convolutional layer, a pooling layer, and an LSTM model. Firstly, attention is calculated for the first set of features $F^{(1)}$ without the use of a query (Attention Module in Fig. 4). In as a weighted average of the second set of value vectors [32], an adjusted additive attention score function is used for this attention mechanism.The general form of the regularattention module,which will produce the context vector additive score function can be seen in (6).\n$$\\mathrm{score}(\\:\\boldsymbol{q}\\:,\\:\\boldsymbol{k}_{l}\\:)=\\:\\boldsymbol{w}^{T}\\times\\mathrm{act}(\\boldsymbol{W}_{1}\\times\\:\\boldsymbol{q}\\:+\\:\\boldsymbol{W}_{2}\\times\\:\\boldsymbol{k}_{l}+\\boldsymbol{b}\\:),\\\\d_{q}\\times1\\quad d_{k}\\times1\\quad1\\times d_{w}\\quad d_{w}\\times d_{q}\\quad d_{q}\\times1\\quad d_{w}\\times d_{k}\\quad d_{k}\\times1\\quad d_{w}\\times1$$\n\nwhere act() is a non-linear activation function, and $w\\in$ $\\Pi^{\\mathcal{R}}d_{\\mathcal{n}\\mathcal{F}}$ ， $W_{1}$ $\\in$ $\\mathbb{R} ^{d_{w}\\times d_{q}}$ ， $W_{2}\\in$ $\\mathbb{R} ^{d_{w}\\times d_{k}}$, and $b\\in\\mathbb{R}^{d_{w}}$ are trainable weights matrices, for which $d_{w}$ is a predefined dimension of the weight matrices.A variant of this score function adapted to be calculated without a query for the application at hand can be seen in (7).\n\n$$e_{l}^{(0)}=\\boldsymbol{w}^{(1)T}\\times\\operatorname{act}(\\boldsymbol{W}^{(1)}\\times\\boldsymbol{k}_{l}^{(1)}+\\boldsymbol{b}^{(1)}),\\\\1\\times1\\quad1\\times d_{w}\\quad d_{w}\\times d_{k}^{(1)}\\quad d_{k}^{(1)}\\times1\\quad d_{w}\\times1$$\n\nwhere $w^{(1)}\\in\\mathbb{R}^{d_{w}}$ ， $W^{(1)}\\in\\mathbb{R}^{d_{\\omega}\\times d_{k}^{(1)}}$ and $b^{(1)}\\in\\mathbb{R}^{d_{M}}$ areafter another. This may come with a computational distrainable weight matrices for Attention Module,. $k_l^{(1)}\\in$ advantage since it is not possible to parallize. Instead of $\\Pi\\text{砥}^{d_k^{(1)}}$ is the Ith column of the keys matrix $K^{(1)}$ that wasusing a sequential mechanism like alternating co-attention, obtained from $F^{(1)}$ via a linear transformation (see (1), for which $d_{w}$ is a prespecified dimension of the weight matrices and $d_{k}^{(1)}$ is a prespecifiedimension of the key vectors. Perhaps onemay wonder why the query is absent whenqueries,unweighted averages of the keyvectors areused as\n\ncalculating attention in this manner. Essentially, the query in this attention modelis learned alongside the other trainable parameters. As such, the query can be interpreted as a general question: \"Which feature vectors contain the most important information?\".This is also known as a selfattentive mechanism, since attention is calculated based only on the feature vectors themselves. Self-attention is explained in more detail in Subsection 3.3.1 The scores are combined with an alignment function\n\n(see (3)), such as the softmax function, to create attention weights used to calculate the context vector $\\boldsymbol{c}^{(0)}\\in\\mathbb{R}^{\\boldsymbol{d}_{\\mathbb{V}}^{(1)}}$ (see (4). This context vector is not used as the output of the attention model, but rather as a query for calculating the context vector $\\boldsymbol{c}^{(2)}\\in\\mathbb{R}^{d_{w}^{(2)}}$ based on the second feature vectors $c^{(1)}$ and $c^{(2)}$ matrix $F^{(2)}$ where $d_{v}^{\\langle2\\rangle}$ is thedimension of the valuevectors obtained from $F^{(2)}$ via a linear transformation (see (1).For compact representation of one input to use as a query when\n\n![](./images/ftdzr2qwpgGo72yaqTGymWrhgqyyzp9TO.png)\n\nFig. 5. An illustration of interactive co-attention\n\nand $d_{f}^{(2)}$ represent, respectively the dimension of the feature this module (Attention Modulez in Fig. 4), atention scores are calculated using another score function with $C_{0}$ as query $n_f^{(1)}$ Tand $n_{f}^{(2)}$ re esen et ely t ento e ei ut on e n ie ne nc oncie e n \n\n$$e_{l}^{(2)}=\\mathrm{score}(\\begin{array}{c}c^{(0)}\\:,\\:k_{l}^{(2)}\\\\d_{v}^{(1)}\\times1\\:d_{k}^{(2)}\\times1\\end{array}).$$\n\nThese attention scores are then used to calculate attentior weights using, for example, a softmax function as alignment function,after which the context vector $c^{(2)}$ can bederived Finally, the context vector $c^{(2)}$ is used as a query for the first $c^{(1)}$ for the first feature matrix $F^{(1)}$ .Attention scores are calculated according to (9). In [32], the same function and weight matrices as seen in (7) are used, but with an added query making it the same as the general additive score function (see (6)). The rest of the attention calculation is similar as before\n\n$$e_{l}^{(1)}=\\mathrm{score}(\\begin{array}{c}c^{(2)}\\:,\\:k_{l}^{(1)}\\:).\\\\d_{v}^{(2)}\\times1\\:d_{k}^{(1)}\\times1\\end{array}$$\n\nThe produced context vectors $c^{(1)}$ and $c^{(2)}$ are concatenated and used for prediction in the output model.Alternating co-attention inherently contains a form of sequentiality due to the fact that context vectors need to be calculated one [34] proposes the interactive co-attention mechanism that can calculate attention on both feature matrices in parallel, as depicted in Fig. 5. Instead of using the context vectors as queries.The calculation of the average keys are provided in (10), and the calculation of the attention scores are shown in (11). Any score function can be used in this case, but an additive score function is used in [34]\n\n$$\\bar{\\boldsymbol{k}}_{d_k^{(1)}\\times1}^{(1)}=\\frac{1}{n_f^{(1)}}\\sum\\limits_{l=1}^{n_f^{(1)}}\\boldsymbol{k}_l^{(1)},\\quad\\bar{\\boldsymbol{k}}_{d_k^{(2)}\\times1}^{(2)}=\\frac{1}{n_f^{(2)}}\\sum\\limits_{l=1}^{n_f^{(2)}}\\boldsymbol{k}_l^{(2)};$$\n\n$$e_{l}^{(1)}=\\mathrm{score}(\\:\\bar{\\boldsymbol{k}}^{(2)}\\:,\\:\\boldsymbol{k}_{l}^{(1)}\\:),\\:e_{l}^{(2)}=\\mathrm{score}(\\:\\bar{\\boldsymbol{k}}^{(1)}\\:,\\:\\boldsymbol{k}_{l}^{(2)}\\:).\\\\1\\times1\\quad d_{k}^{(2)}\\times1\\quad d_{k}^{(1)}\\times1\\quad1\\times1\\quad d_{k}^{(1)}\\times1\\quad d_{k}^{(2)}\\times1$$\n\nFrom the attention scores, attention weights are created via an alignment function, and are used to produce the context\n\nWhile coarse-grained co-attention mechanisms use a\n\ncalculating attention for another input, fine-grained coattention considers every element of each input individually when calculating attention scores.In this case, the query becomes a matrix. An example of fine-grained co-attention is parallel co-attention [32]. Similarly to interactive coattention, parallel co-attention calculates attention on the two feat ue mariesa hesaemeti e $K^{(1)}\\in\\mathbb{R}^{d_k^{(1)}\\times n_f^{(1)}}$ .d $K^{(2)}\\in\\mathbb{R}_{k}^{d_{k}^{(2)}\\times n_{f}^{(2)}}$ that are obtained by linearly transforming the feature matrices $F^{(1)}$ and $F^{(2)}$ ,where $d_k^{(1)}$ and $d_k^{(2)}$ are $K^{(1)}$ C $K^{(2)}$ Sopdehna tiset dtoedere $A$ $\\in$ $\\mathbb{R} ^{n_f^{( \\dagger ) }\\times n_f^{( 2) }}$ difet die eastons sn afitysaeix atento sore asho na 6) translate one keys matrix to the space of the other keys. In [32], $A$ is calculated as shown in (12).\n$$A_{n_{f}^{(1)}\\times n_{f}^{(2)}}=\\operatorname{act}(\\begin{array}{ccc}K^{(1)^{T}}&\\times&W_{A}&\\times&K^{(2)}\\\\n_{f}^{(1)}\\times d_{k}^{(1)}&&d_{k}^{(1)}\\times d_{k}^{(2)}&&d_{k}^{(2)}\\times n_{f}^{(2)}\\end{array}),$$\n\nweights using an alignment function,so that two context\n\nwhere $W_{A}\\in\\mathbb{R}^{d_{k}^{(1)}\\times d_{k}^{(2)}}$ is a trainable weights matrix and $\\arctan()$ is an activation function for which the tanh() function this matrix,i.e., one can use (13) to calculate each individual element $A_{i,j}$ of the matrix $A$\n\n$$A_{i,j}=\\:\\boldsymbol{w}_{A}^{T}\\:\\times\\mathrm{concat}(\\boldsymbol{k}_{i}^{(1)},\\boldsymbol{k}_{j}^{(2)},\\boldsymbol{k}_{i}^{(1)}\\circ\\boldsymbol{k}_{j}^{(2)}),\\_{1\\times1}\\quad1\\times3d_{k}\\quad d_{k}\\times1\\quad d_{k}\\times1\\quad d_{k}\\times1\\quad d_{k}\\times1$$\n\nwhere $w_{A}\\in\\mathbb{R}^{3d_{k}}$ denotes a trainable vector of weights, concat() denotes vector concatenation, and 0 denotes element-wise multiplication, also known as the Hadamard product. Note that the keys of each keys matrix in this case must have the same dimension $d_{k}$ for the element wise multiplication to work.The affinity matrix can be interpreted as a similarity matrix for the columns of the two keys matrices, and helps translate, for example, image keys to the same space as the keys of the words in a sentence, and vice versa.The vectors of attention scores $e^{(1)}$ and $e^{(2)}$ can be calculated using an altered version of the additive score function as presented in (14) and (15). The previous attention score example i this survey all used a score func- fllowing feature matrices:. $\\boldsymbol{F}^{t}$ = $[ \\boldsymbol{f}_{1}^{t}, \\ldots , \\boldsymbol{f}_{n_{f}^{t}}^{t}]$ $\\in$ $\\mathbb{R} ^{d_{f}^{t}\\times n_{f}^{t}}$ tion to calculate each attention score for each value vector individually. However, (14) and (15) are used to calculate $F^{l}$ = $[ f_{1}^{l}, \\ldots , f_{n_{f}^{l}}^{l}]$ $\\in$ $\\mathbb{R} ^{d_{f}^{l}\\times n_{f}^{l}}.$ and $F^{r}$ = $[ f_{1}^{r}, \\ldots , f_{n_{f}^{r}}^{r}]$ $\\in$ the complete vector of all attention scores. Essentially, the $\\mathbb{R}^{d_f^r\\times n_f^r}$, for the target phrase words, left context words, attention scores are calculated in an aggregated form.\n\n$$\\begin{gathered}\ne^{(1)} =\\:\\boldsymbol{w}_{1}\\times\\mathrm{act}(\\boldsymbol{W}_{2}\\times\\boldsymbol{K}^{(2)}\\times\\:\\boldsymbol{A}^{T}\\:+\\boldsymbol{W}_{1}\\times\\boldsymbol{K}^{(1)}); \\\\\n1\\times n_{f}^{(1)} 1\\times d_{w}\\quad d_{w}\\times d_{k}^{(2)}\\:d_{k}^{(2)}\\times n_{f}^{(2)}\\:n_{f}^{(2)}\\times n_{f}^{(1)}\\:d_{w}\\times d_{h}^{(1)}\\:d_{k}^{(1)}\\times n_{f}^{(1)} \\\\\ne^{(2)} =\\:\\boldsymbol{w}_{2}\\times\\mathrm{act}(\\boldsymbol{W}_{1}\\times\\boldsymbol{K}^{(1)}\\times\\:\\boldsymbol{A}\\:+\\boldsymbol{W}_{2}\\times\\boldsymbol{K}^{(2)}), \\\\\n1\\times r_{e}^{(2)}f 1\\times d_{w}\\quad d_{w}\\times d_{k}^{(1)}\\:d_{k}^{(1)}\\times n_{f}^{(1)}\\:n_{f}^{(1)}\\times n_{f}^{(2)}\\:d_{w}\\times d_{k}^{(2)}\\:d_{k}^{(2)}\\times n_{f}^{(2)} \n\\end{gathered}$$\n\nwhere $w_1\\in\\mathbb{R}^d_u$ ， $w_2\\in\\mathbb{R}^dw$ ， $W_{1}$ $\\in$ $\\mathbb{R} ^{d_{\\omega }\\times d_{h}^{( 1) }}$ and that tanh() is used in [32] for the activation function, and the feature matrices are used as the key matrices. In that case, the affinity matrix $A$ can be seen as a translator between feature spaces. As mentioned before, the affinity matrix is essentially a similarity matrix for the key vectors of the two\n\n![](./images/fcezbCFVVAgYitIXTykvdtgkU8QRrQGOK.png)\n\nFig. 6. An illustration of parallel co-attention\n\nthe maximum similarity value in a row or column as the\n\n$$e_{i}^{(1)}=\\max_{j=1,\\ldots,n_{f}^{(2)}}A_{i,j},\\quad e_{j}^{(2)}=\\max_{i=1,\\ldots,n_{f}^{(1)}}A_{i,j}.$$\n\nNext,the attention scores are used to calculate attention vectors $c^{(1)}$ and $c^{(2)}$ can be derived as weighted averages of the value vectors that are obtained from linearly transform ing the features. For the alignment function, [32] proposes is used in[32].[35] proposes a different way of calculatingtouse a softmaxfunction,and thevalue vectors are simply set equal to the feature vectors. The resulting context vectors can be either concatenated or added together Finally, coarse-grained and fine-grained co-attention can\n\nbe combined to create an even more complex co-attention mechanism. [33] proposes the multi-grained co-attention mechanism that calculates both coarse-grained and finegrained co-attention for two inputs.Each mechanism produces one context vector per input. The four resulting context vectors are concatenated and used in the output model for prediction A mechanism separate from co-attention that still uses\n\nmultiple inputs is the rotatory attention mechanism [36] This technique is typically used in a text sentiment analysis setting where there are three inputs involved: the phrase for which the sentiment needs to be determined (target phrase) the text before the target phrase (left context), and the text after the target phrase (right context). The words in these three inputs are all encoded by the feature model, producing the\n\n$d_{f}^{r}$ d rghtenete ns nesctie hee $d_{f}^{t}$, $d_{f}^{l}$, do $d_{f}^{l}$, rand corresponding inputs, and $n_{f}^{t}$, $n_{f^{\\prime }}^{l}$, n $n_{f}^{l}$, and $n_{f}^{r}$ represent the number of feature vectors for the corresponding inputs. The feature model used in[36]consists of word embeddings and separate Bi-LSTM models for the target phrase, the left context, and the right context. This means that the feature vectors arein fact thehidden statevectors obtained from the $W_{2}\\in$ $\\mathbb{R} ^{d_{w}\\times d_{k}^{( 2) }}$ are trainable weight matrices, for which Bi-LSTM models. Using these features, the idea is to extract $d_{w}$ is a prespecified dimension of the weight matrices. Note a single vector $T^*$ from the inputs such that a softmax layer can be used for classification. As such, we are now faced with two challenges: how to represent the inputs as a single vector, and how to incorporate the information from the left and right context into that vector. [36] proposes to use the rotatory attention mechanism for this purpose.\n\nFirstly, a single target phrase representation is created most importantinformation in the actual target phrase itself by using a pooling layer that takes the average over the Essentially, the mechanism rotates attention between the columns of $F^{t}$ ,as shown in (17).\n\n$$r_{_{d_{f}^{t}\\times1}}^{t}=\\frac{1}{n_{f}^{t}}\\sum_{i=1}^{n_{f}^{t}}f_{i}^{t}\\:.$$\n\n$r^{l}$ is then used as a query to create a context vector out of various scans or documents, that can provide different types the left and right contexts, separately. For example, for the left context,the key vectors $k_{1}^{l},\\ldots,k_{n_{f}^{l}}^{l}\\in\\mathbb{R}^{d_{k}^{l}}$ and value vectors $\\boldsymbol{v}_{1}^{l},\\ldots,\\boldsymbol{v}_{n_{f}^{l}}^{l}\\in\\mathbb{R}^{d_{\\nu}^{l}}$ are extracted rom the let context mages and semantic l gsimuldaneously iel aly n $d_{k}^{l}$ tire $d_{v}^{l}$ cors $f_{1}^{l},\\ldots,f_{n_{f}^{l}}^{l}\\in\\mathbb{R}^{d_{f}^{l}}$ sima ee ee e e respectively. Note that [36] proposes to use the original feature vectors as keys and values, meaning that the linear transformation consists of a multiplication by an identity matrix. Next, the scores are calculated using (18).\n\n$$e_{i}^{l}=\\text{score}(\\begin{array}{c}r^{t}\\\\d_{f}^{t}\\times1\\end{array},\\begin{array}{c}k_{i}^{l}\\\\d_{k}^{l}\\times1\\end{array}).$$\n\nFor the score function, [36] proposes to use an activated general score function [34] with a tanh activation function. The attention scores can be combined with an alignment function and the corresponding value vectors to produce the context vector $r^{l}\\in\\mathbb{R}^{d_{v}^{l}}$ The alignment function used in [36] takes the form of a softmax function. An analogous procedure can be performed to obtain the representation of the right context, $r^{T}$ .These two context representations can then be used to create new representations of the target phrase, $\\Pi\\text{思}^d_{k_0}^r$ and value vectors $\\boldsymbol{v}_{1}^{t}, \\ldots , \\boldsymbol{v}_{n_{f}^{t}}^{t}$ $\\in$ $\\mathbb{R} ^{d_{v}^{t}}$ are extracted from the target phrase feature vectors $f_{1}^{t},\\ldots,f_{n_{f}^{t}}^{t}\\in\\mathbb{R}^{d_{f}^{t}}$ 3.1.2 Feature Levels similarly as before, using a linear transformation, where $d_{k}^{t}$ and $d_{v}^{t}$ are the dimensions of the key and value vectors respectively. Note, again, that the original feature vectors as keys and values in [36]. The attention scores for the leftaware target representation are then calculated using (19).\n$$e_{i}^{l_{t}}=\\mathrm{score}(\\begin{array}{c}r^{l}\\\\d_{v}^{l}\\times1\\end{array},\\begin{array}{c}k_{i}^{t}\\\\d_{k}^{t}\\times1\\end{array}).$$\n\nThe attention scores can be combined with an alignment function and the corresponding value vectors to produce the context vector $r^{l_t}\\in\\mathbb{R}^{d_v^t}$ . For this attention calculation, [34] proposes to use the same score and alignment functions as before. The right-aware target representation $r^{r^{\\prime}t}$ can be calculated in a similar manner. Finally, to obtain the full representation vector 7 that is used to determine the classification, the vectors $r^{l}$, $r^{r}$, $r^{l_{t}}$, $r^{l_{s}}$ p and $r^{r_t}$ are concatenated together, as shown in (20).\n\n$$r_{(d_{v}^{l}+d_{v}^{r}+d_{v}^{t}+d_{v}^{t})\\times1}=\\mathrm{concat}(\\begin{array}{c}r^{l},\\\\d_{v}^{l}\\times1\\end{array},\\begin{array}{c}r^{r},\\\\d_{v}^{t}\\times1\\end{array},\\begin{array}{c}r^{r_{t}}\\\\d_{v}^{t}\\times1\\end{array}).$$\n\nTo summarize, rotatory attention uses the target phrase to compute new representations for the left and right context using attention, and then uses these left and right reprephrase. The first step is designed to capture the words in the left and right contexts that are most important to the target phrase. The second step is there to capture the\n\ntarget and the contexts to improve the representations. There are many applications where combining informa-\n\ntion from different inputs into a single model can be highly beneficial. For example,in the field of medical data,there are often many different types of data available, such as of information. In [37], a co-attention mechanism is used for automatic medical report generation to attend to both study is used in [38] to show that the co-attention part of the model specifically improves performance. A field where multi-feature attention has been extensively explored is the domain of recommender systems.For example,in [39], a coattention network is proposed that attends to both product reviews and the reviews a user has written. In[40], a model is proposed for video recommendation that attends to both user features and video features. Co-attention techniques have also been used in combination with graph networks for the purpose of, for example, reading comprehension across multiple documents [41] and fake news detection [42]. In comparison to co-attention, rotatory attention has typically been explored only in the field of sentiment analysis, which is most likely due to the specific structure of the data that is necessary to use this technique. An implementation of rotatory attention is proposed in [43] for sentiment analysis, where the mechanism is extended by repeating the attention again, using attention. Firstly,the key vectors $k_{1}^{t},\\ldots,k_{n_{f}^{t}}^{t}\\in$ wotationto mectively furthen dep byepe repgese aateons\n\nThe previously discussed attention mechanisms process. data at a single level. We refer to these attention techniques as single-level attention mechanisms. However, some data. types can be analyzed and represented on multiple levels. For example, when analyzing documents, one can analyze the document at the sentence level, word level,or even the character level. When representations or embeddings of all these levels are available, one can exploit the extra levels of information. For example, one could choose to perform translation based on either just the characters, or just the words of the sentence. However, in [44], a technique named attention-via-attention is introduced that allows one to incorporate information from both the character, and the word levels. The idea is to predict the sentence translation character-by-character, while also incorporating information from a word-level attention module. To begin with, a feature model (consisting of, for ex\n\nample,word embeddings and RNNs) is used to encode the input sentence into both a character-level feature matrix $\\dot{F^{( c) }}$ $\\in$ $\\mathbb{R} ^{d_{f}^{( \\mathrm{c} ) }\\times n_{f}^{( \\mathrm{c} ) }}$ and a word-level feature matrix $F^{( w) }\\in$ $\\mathbb{R} ^{d_{f}^{( w) }\\times n_{f}^{( w) }}$ where $d_{f}^{(c)}$ and $n_{f}^{(c)}$ represent, respec tively, the dimension of the embeddings of the characters $d_{\\int}^{(w)}$ and $n_{f}^{(w)}$ reee that each level in the data can be represented or embedded When attempting to predict a character in the translated\n\n![](./images/fSEFcGafUNBvMGze5Vma05K9gNTsnQ3cq.png)\n\nFig. 7. An illustration of attention-via-attention.\n\nsentence, a query $q^{(c)}\\in\\mathbb{R}^dq$ is created by the query model (like a character-level RNN), where $d_{q}$ is the dimension of the query vectors. As illustrated in Fig. 7, the query is used tocaleulate tentononthe corod $c^{(w)}\\in\\mathbb{R}^{d^{(w)}}$ etors $F^{(w)}$ $d_v^{(w)}$ ate ion module ta esentily suemarieste entencec\n\nIee ne eien e e ee et e $d_{f}^{(S)}$ etreate $F^{(s)}\\in\\mathbb{R}^{d_f^{(S)}\\times n_s}$ fore $s=1,\\ldots,n_{S}$, where which words contain the most important information for predicting the next character. If we know which words are most important,then it becomes easier to identify which characters in the input sentence are most important. Thus, the next step is to attend to the character-level features in $F^{(c)}$ , with an additional query input: the word-level context vector $c^{(w)}$ . The actual query input for the attention model will therefore be the concatenation of the query $q^{(c)}$ and the are essentially su-amaries of the words in the sentences. The word contex vector $c^{(w)}$ The ou tput of $c^{(c)}$ character-level matrix of context vectors $C=[\\boldsymbol{c}^{(1)},\\ldots,\\boldsymbol{c}^{(n_S)}]\\in\\mathbb{R}^{d_v^{(S)}\\times n_S}$ context output of the attention model is the concatenation of the word-level, and character-level context vectors The attention-via-attention technique uses representa\n\nnot always be available for each level of the data, or it may be desirable to let the model create the representations during the process by building them from lower level representations. A technique referred to as hierarchical attention [5] can be used in this situation. Hierarchical attention is another technique that allows one to apply attention on different levels of the data.Yet, the exact mechanisms work quite differently compared to attention-via-attention. The idea is to start at the lowest level, and then create representations, or summaries, of the next level using attention. This process is repeated till the highest level is reached. To make this a little clearer, suppose one attempts to create a model for document classification, similarly to the implementation from [5]. We analyze a document containing $7lS$ sentences, with the sth sentence containing $n_{s}$ words, for $s$ = $l, \\ldots , n_{S}$ . One could use attention based on just the collection of words to classify the document. However a significant amount of important context is then left out of the analysis, since the model will consider all words as a single long sentence, and will therefore not consider the context within the separate sentences.Instead, one can use the hierarchical structure of a document (words form sentences, and sentences form the document) Fig. 8 illustrates the structure of hierarchical attention\n\nFor each sentence in the document,a sentence representa tion $\\boldsymbol{c}^{(s)}\\in\\mathbb{R}^{d_{v}^{(S)}}$ is produced, for $s=1,\\ldots,n_{S}$, where $d_v^{(S)}$ is the dimension of the value vectors used in the attention model for sentence representations (Attention Modules in\n\n![](./images/fLQHc0WG3s8qDYeSbBDR2XDFQUQhGQiKx.png)\n\nFig. 8. An illustration of hierarchical attention\n\nFig. 8). The representation is a context vector from an word, and $Tl_{8}$ represents the amount of words in sentence S For extra clarification, the columns of $F^{(s)}$ are feature vectors that correspond to the words in sentence 5 .As shown in Fig. 8, each feature matrix $F^{(s)}$ is used as input for an atten tion model,which produces the context vector $c^{(s)}$ , for each $s=1,\\ldots,n_{S}$ . No queries are used in this step, so it can be considered a self-attentive mechanism. The context vectors is constructed by grouping all the obtained context vectors together as columns. Finally, attention is calculated using $C$ as feature input, producing the representation of the entire tons oitetochis adoent in thonte vector $\\boldsymbol{c}^{(D)}\\in\\mathbb{R}^{d_{v}^{(D)}}$ where $d_{v}^{(D)}$ is the dimension of the value vectors in the attention model for document representation (Attention Modulep in Fig. 8) This context vector can be used to classify the document, since it is essentially a summary of all the sentences (and therefore also the words)in the document Multi-level models can be used in a variety of tasks.\n\nFor example, in [28], hierarchical attention is used in a recommender system to model user preferences at the longterm level and the short-term level. Similarly, [45] proposes a hierarchical model for recommending social media images based on user preferences. Hierarchical attention has also been successfully applied in other domains.For example [46] proposes to use hierarchical attention in a video action recognition model to capture motion information at the the long-term level and the short-term level. Furthermore, [47] proposes a hierarchical attention model for cross-domain. sentiment classification. In [48], a hierarchical attention model for chatbot response generation is proposed. Lastly using image data,[49] proposes a hierarchical attention model for crowdcounting\n\n### 3.1.3Feature Representations\n\nIn a basic attention model, a single embedding or representation model is used to produce feature representa tions for the model to attend to.This is referred to as single-representational attention. Yet, one may also opt to incorporate multiple representations into the model. In. [50], it is argued that allowing a model access to multiple\n\nTABLE 2 Overview of score function (score $(\\boldsymbol{q},\\boldsymbol{k}_l)]$ forms.\n\n| Name | Function | Parameters |\n|------|----------|------------|\n| Additive (Concatenate) [3] | $w \\times act(W_1 q + W_2 k) + b$ | $w \\in \\mathbb{R}^{d \\times d}$, $W_1 \\in \\mathbb{R}^{d \\times d \\times k}$, $W_2 \\in \\mathbb{R}^{d \\times d \\times k}$, $b \\in \\mathbb{R}^d$ |\n| Multiplicative (Dot-Product) [4] | $q \\times k_i$ | - |\n| Scaled Multiplicative [13] | $\\frac{k_i}{d} \\times W \\times q$ | - |\n| Generalized [4] | $k_i / (d \\times W \\times q)$ | $W \\in \\mathbb{R}^{d \\times d \\times k}$ |\n| Biased General [54] | $k_i/(W \\times x \\times q) + b$ | $W \\in \\mathbb{R}^{d \\times d \\times k}$, $x \\in \\mathbb{R}^{d \\times d \\times k}$, $b \\in \\mathbb{R}^d$ |\n| Activated General [34] | $act(k_i/(W \\times x \\times q))$ | $W \\in \\mathbb{R}^{d \\times d \\times k}$, $x \\in \\mathbb{R}^{d \\times d \\times k}$, $b \\in \\mathbb{R}^d$ |\n| Similarity [55] | $q \\times k_i$ | - |\n\nembeddings can allow one to create even higher quality representations. Similarly, [51] incorporates multiple representations of the same book (textual, syntactic, semantic visual etc.) into the feature model. Feature representations are an important part of the attention model, but attention can also be an important part of the feature model. The idea is to create a new representation by taking a weighted average of multiple representations, where the weights are determined via attention.This technique is referred to as multi-representational attention, and allows one to create so-called meta-embeddings. Suppose one wants to create a meta-embedding for a word 2 for which $E$ embeddings $\\boldsymbol{x}^{(e_1)},\\ldots,\\boldsymbol{x}^{(e_E)}$ are available. Each embedding $x^{(e_{i})}$ is of size $d_{e_{i}t}$ for $i=1,\\ldots,E$ Since not all embeddings are of the same size,a transformation is performed to normalize the embedding dimensions. Using embedding-specific weight parameters, each embedding $x^{(e_i)}$ is transformed into the size-normalized embedding $x^{(t_{i})}\\in\\mathbb{R}^{d_{t}}$ ,where $d_{t}$ is the size of every transformed word embedding, as shown in (21).\n$$x^{(t_{i})}=W_{e_{i}}\\times x^{(e_{i})}+b_{e_{i}},\\\\d_{t}\\times1\\quad d_{t}\\times d_{e_{i}}\\quad d_{e_{i}}\\times1\\quad d_{t}\\times1$$\n\nwhere $W_{e_{i}}$ $\\in$ $\\mathbb{R} ^{d_{t}\\times d_{e_{i}}}$ and $b_{e_i}\\in\\mathbb{R}^{d_t}$ are trainable, embedding-specific weights matrices. The final embedding. $x^{(e)}\\in\\mathbb{R}^{d_t}$ is a weighted average of the previously calculated transformed representations, as shown in (22)\n\n$$\\boldsymbol{x}_{d_t\\times1}^{(e)}=\\sum_{i=1}^Ea_i\\times\\boldsymbol{x}_{d_t\\times1}^{(t_i)}.$$\n\nThe final representation $x^{(e)}$ can be interpreted as the context vector from an attention model,meaning that the weights $a_1,\\ldots,a_E\\in\\mathbb{R}^1$ are attention weights. Attention can be calculated as normally, where the columns of the features matrix $F$ are the transformed representations $\\boldsymbol{x}^{(t_1)},\\ldots,\\boldsymbol{x}^{(t_E)}$ Thequery in this case can be ignored since it is constant in all cases. Essentially, the query is \"Which representations are the most important?\"” in every situation. As such, this is a self-attentive mechanism While an interesting idea, applications of multi-\n\nrepresentational attention are limited. One example of the application of this technique is found in[52], where a multirepresentational attention mechanism has been applied to generate multi-lingual meta-embeddings. Another example is [53], where a multi-representational text classification embeddings from part-of-speech tagging, named entity rec ognizers, and character-level and word-level embeddings.\n\n### 3.2 General Attention Mechanisms\n\ncan be applied in any type of attention model. The structure of this component can be broken down into the following sub-aspects: the attention score function, the attention alignment, and attention dimensionality\n\n## 3.2.1Attention Scoring\n\nThe attention score function is a crucial component in how attention is calculated. Various approaches have been developed that each have their own advantages and disadvantages.An overview of these functions is provided in Table 2.\n\nEach row of Table 2 presents a possible form for the function $\\operatorname{score}(\\boldsymbol{q},\\boldsymbol{k}_l)$ , as seen in (23), where $q$ is the query vector, and $k_{l}$ is thelth column of $K$ Note that the score functions presented in this section can be more efficiently calculated in matrix form using $K$ instead of each column separately Nevertheless, the score functions are presented using $k_{I}$ to more clearly illustrate the relation between a key and query.\n\n$$e_l=\\text{score}(\\begin{array}{c}q\\\\d_q\\times1\\end{array},\\begin{array}{c}k_l\\\\d_k\\times1\\end{array}).$$\n\nDue to their simplicity, the most popular choices for the score function are the concatenate score function[3] and the multiplicative score function [4]. The multiplicative score function has the advantage of being computationally inexpensive due to highly optimized vector operations. How. ever, the multiplicative function may produce non-optimal results when the dimension $d_{k}$ is too large [56]. When $d_{k}$ is large, the dot-product between $q$ and $k_{I}$ can grow large in magnitude.To illustrate this, in [13], an example is used where the elements of $q$ and $k_l$ are all normally distributed with a mean equal to zero, and a variance equal to one. Then, the dot-product of the vectors has a variance of $d_{k}$ A higher variance means a higher chance of numbers that are large in magnitude. When the softmax function of the alignment step is then applied using these large numbers, the gradient will become very small, meaning the model will have trouble converging [13]. To adjust for this, [13] proposes to scale the multiplicative function by the factor mode is eaepesed t at nerpeate dferent eesene $\\frac{1}{\\sqrt{d_{k_{k}}}}$ prod ucing the caled utiplictiescore nc en \n\nintroducing a weights matrix $W$ .This form, referred to as the general score function, allows for an extra transformation of $k_l$ .The biased general score function [54]is a further extension of the general function that introduces This major category consists of attention mechanisms that a bias weight vector $b$ A final extension on this function named the activated general score function is introduced in [34], and includes the use of both a bias weight $b$, and an activation function act(). The previously presented score functions are all based on\n\ndetermining a type of similarity between the key vector and the query vector. As such, more typical similarity measures such as the Euclidean $(\\mathrm{L}_{2})$ distance and cosine similarity can also be implemented [55]. These scoring methods are summarized under the similarity score function which is represented by the similarity() function\n\nThere typically is no common usage across domains regarding score functions. The choice of score function for a particular task is most often based on empirical experiments. However, there are exceptions when, for example efficiency is vital. In models where this is the case, the multiplicative or scaled multiplicative score functions are typi-However, a compromise can be made in certain situations cally the best choice.An example of this is the TransformerLocal alignment [4] is a method that implements a softmax model, which is generally computationally expensive.\n\n### 3.2.2Attention Alignment\n\nThe attention alignment is the step after the attention scoring. This alignment process directly determines which parts of the input data the model will attend to. The alignment function is denoted as align() and has various forms. The align() function takes as input the previously calculated attention score vector $e$ and calculates for each element $el$ of e the attention weight $u_{l}$ .These attention weights can then average of the value vectors $v_{1},\\cdots,v_{n_{f}}$\n\n$$\\boldsymbol{c}_{d_v\\times1}=\\sum_{l=1}^{n_f}a_l\\:\\times\\:\\boldsymbol{v}_l\\:.$$\n\nThe most popular alignment method to calculate these weights is a simple softmax function, as depicted in (25)\n\n$$\\underset{1\\times1}{a_l}=\\underset{1\\times1}{\\text{align}}(\\underset{1\\times1}{e_l}\\:;\\:\\boldsymbol{e}\\:)=\\frac{\\exp(e_l)}{\\sum_{j=1}^{n_f}\\exp(e_j)}.$$\n\nThis alignment method is often referred to as soft alignment in computervision settings [8],or global alignment for sequence data [4]. Nevertheless, both these terms represent the same function and can be interpreted similarly. Soft/global alignment can be interpreted as the model attending to all feature vectors.For example, the model attends to all regions in an image, or all words in a sentence. Even though the attention model generally does focus more on specific parts of the input, every part of the input will receive at least some amount of attention due to the nature of the softmax function. Furthermore, an advantage of the softmax function. is that it introduces a probabilistic interpretation to the input vectors. This allows one to easily analyze which parts of the input are important to the output predictions In contrast to soft/global alignment, other methods aim\n\nto achieve a more focused form of alignment. For example hard alignment [8], also known as hard attention or nondeterministic attention, is an alignment type that forces the attention model to focus on exactly one feature vector. Firstly, this method implements the softmax function in the exact same way as global alignment. However, the outputs $a_{1},\\cdots,a_{n_{f}}$ are not used as weights for the context vector calculation. Instead, these values are used as probabilities to draw the choice of the one value vector from.A value $m\\in\\mathbb{R}^1$ is drawn from a multinomial distribution with $a_{1},\\cdots,a_{n_{f}}$ as parameters for the probabilities. Then, the context vector is simply defined as follows:\n\n$$\\begin{array}{c}c\\\\d_v\\times1\\end{array}=\\begin{array}{c}v_m\\\\d_v\\times1\\end{array}.$$\n\nHard alignment is typically more efficient at inference compared to soft alignment.On the other hand, the main disadvantage of hard attention is that, due to the stochastic\n\nalignment of attention, the training of the model cannot be done via the regular backpropagation method. Instead simulation and sampling, or reinforcement learning [57] are required to calculate the gradient at the hard attention layer. As such, soft/global attention is generally preferred. distribution, similarly to soft/global alignment. But, the. softmax distribution is calculated based only on a subset of the inputs. This method is generally used in combination with sequence data.One has to specify a variable $p\\in\\mathbb{R}^{1}$ that determines the position of the region.Feature vectors close to $p$ will be attended to by the model, and vectors too far from $P$ will be ignored. The size of the subset will be determined by the variable $D\\in\\mathbb{R}^{1}$ . Summarizing the attention model will apply a softmax function on the attention scores in the subset $[p-D,p+D]$ . In other words be used to create the context vector C by taking a weighteda window is placed on the input and soft/global attention is calculated within that window：\n$$a_{l}=\\text{align}(\\begin{smallmatrix}e_{l}&;&e\\\\1\\times1&n_{f}\\times1\\end{smallmatrix})=\\frac{\\exp(e_{l})}{\\sum_{j=p-D}^{p+D}\\exp(e_{j})}.$$\n\nThe question that remains is how to determine the location parameter $P$ The first method is referred to as monotonic alignment. This straightforward method entails simply setting the location parameter equal to the location of the prediction in the output sequence.Another method of deter mining the position of the region is referred to as predictive alignment. As the name entails, the model attempts to actually predict the location of interest in the sequence\n\n$$p\\:=\\:S\\:\\times\\mathrm{sigmoid}(\\boldsymbol{w}_{1\\times d_{p}}^{T}\\:\\times\\mathrm{tanh}(\\boldsymbol{W}_{p}\\:\\times\\:\\boldsymbol{q}\\:)),$$\n\nwhere $S\\in\\mathbb{R}^{1}$ is the length of the input sequence, and $w_{p}\\in\\mathbb{R}^{d_{P}}$ and $W_{p}$ $\\in$ $\\mathbb{R} ^{d_{p}\\times d_{q}}$ are both trainable weights parameters. The sigmoid function multiplied by $S$ makes sure that $p$ is in the range $[0,S]$ .Additionally, in [4], it is recommended to add an additional term to the alignment function to favor alignment around $P$\n\n$$a_{l}=\\operatorname{align}(\\begin{smallmatrix}e_{l}&;&e\\\\1\\times1&n_{f}\\times1\\end{smallmatrix})\\exp(-\\frac{(l-p)^{2})}{2\\sigma^{2}}),$$\n\nwhere $\\sigma\\in\\mathbb{R}^{1}$ is empirically set equal to $\\frac{D}{2}$ according to [4]. Another proposed method for compromising between soft and hard alignment is reinforced alignment[58]. Similarly to local alignment, a subset of the feature vectors is determined, for which soft alignment is calculated. However, instead of using a window to determine the subset, reinforced alignment uses a reinforcement learning agent. [57], similarly to hard alignment, to choose the subset of feature vectors. The attention calculation based on these chosen feature vectors is the same as regular soft alignment Soft alignment is often regarded as the standard align\n\nment function for attention models in practically every domain.Yet, the other alignment methods have also seen inter esting uses in various domains. For example, hard attention is used in [59] for the task of visual question answering In [60], both soft and hard attention are used in a graph attention model for multi-agent game abstraction. Similarly. in [61], both global and local alignment are used for review rating predictions. Reinforced alignment has been employed\n\nin combination with a co-attention structure in [62] for the extended with the multi-dimensional attention mechanisn task of aspect sentiment classification. In [63], reinforced alignment is used for the task of person re-identification using surveillance images\n\n### 3.2.3 Attention Dimensionality\n\nAll previous model specifications of attention use a scalar weight $a_l$ for each value vector $v_l$ . This technique is referred to as single-dimensional attention. However, instead of determining a single attention score and weight for the entire vector, [64] proposes to calculate weights for every single feature in those vectors separately. This technique is referred to as multi-dimensional attention,since the attention weights now become higher dimensional vectors. The idea is that the model no longer has to attend to entire vectors, but it can instead pick and choose specific elements from those vectors. More specifically, attention is calculated for each dimension.As such,the model must create a vecto of attention weights $a_l\\in\\mathbb{H}^d_v$ for each value vector $v_l\\in\\mathbb{R}^dv$ The context vector can then be calculated by summing the element-wise multiplications (o) of the value vectors $\\boldsymbol{v}_1,\\ldots,\\boldsymbol{v}_{n_f}\\in\\mathbb{R}^d$, and the corresponding attention weight vectors $a_{1},\\ldots,a_{n_{f}}\\in\\mathbb{R}^{d_{\\upsilon}}$ as follows:\n\n$$\\boldsymbol{c}_{d_v\\times1}=\\sum_{l=1}^{n_f}\\boldsymbol{a}_l\\circ\\boldsymbol{v}_l.$$\n\nHowever, since one needs to create attention weight vectors, this technique requires adjusted attention score and weight calculations. For example, the concatenate score function found in Table 2 can be adjusted by changing the $w\\in\\mathbb{R}^{d_{\\cos}}$ weights vector to the weight matrix $W_{d}\\in\\mathbb{R}^{d_{w}\\times d_{v}}$\n\n$$e_{l}\\:=\\:W_{d}^{T}\\:\\times\\mathrm{act}(\\:W_{1}\\:\\times\\:q\\:+\\:W_{2}\\:\\times\\:k_{l}\\:+\\:b\\:).$$\n\nThis new score function produces the attention score vectors $\\boldsymbol{e}_1,\\ldots,\\boldsymbol{e}_{n_f}\\in\\mathbb{R}^dv$ . These score vectors can be combined into a matrix of scores $e$ = $[ \\boldsymbol{e}_{1}, \\ldots , \\boldsymbol{e}_{n_{f}}]$ $\\in$ $\\mathbb{R} ^{d_{v}\\times n_{f}}$ To produce multi-dimensional attention weights, the alignment function stays the same, but it is applied for each feature across the attention score columns. To illustrate, when implementing soft attention, the attention weight produced from the ith element of score vector $e_{l}$ is defined as follows:\n\n$$\\underset{1\\times1}{a_{l,i}}=\\underset{1\\times1}{\\text{align}}(e_{l,i};\\quad\\boldsymbol{e}_{d_{v}\\times n_{f}})=\\frac{\\exp(e_{l,i})}{\\sum_{j=1}^{n_{f}}\\exp(e_{j,i})},$$\n\nwhere $el,i$ represents the ith element of score vector $e_{l,}$ and $ul,i$ is the ith element of the attention weights vector $a_{l}$ .Finally, these attention weight vectors can be used to compute the context vector as presented in (30). Multi-dimensional attention is a very general mecha\n\nnism that can be applied in practically every attention model, but actual applications of the technique have been relatively sparse. One application example is [65], where multi-dimensional attention is used in a model for named entity recognition based on text and visual context from multimedia posts. In [66], multi-dimensional attention is used in a model for answer selection in community question answering. In [67], the U-net model for medical image segmentation is extended with a multi-dimensional attention mechanism. Similarly, in [68], the Transformer model is\n\nfor the task of dialogue response generation. In [69], multidimensional attention is used to extend graph attention networks for dialogue state tracking. Lastly, for the task of next-item recommendation, [70] proposes a model that incorporates multi-dimensional attention\n\n### 3.3 Query-Related Attention Mechanisms\n\nQueries are an important part of any attention model, since they directly determine which information is extracted from the feature vectors. These queries are based on the desired output of the task model, and can be interpreted as literal questions. Some queries have specific characteristics that require specific types of mechanisms to process them. As such, this category encapsulates the attention mechanisms that deal with specific types of query characteristics. The mechanisms in this category dealwith one of the two following query characteristics: the type of queries or the multiplicity of queries\n\n## 3.3.1Type of Queries\n\nDifferent attention models employ attention for different purposes, meaning that distinct query types are necessary There are basic queries, which are queries that are typically straightforward to definebased on the data and model.For example, the hidden state for one prediction in an RNN is often used as the query for the next prediction. One could also use a vector of auxiliary variables as query. For example, when doing medical image classification, general patient characteristics can be incorporated into a query Some attention mechanisms, such as co-attention, rota-\n\ntory attention, and attention-over-attention, use specialized queries. For example, rotatory attention uses the context vector from another attention module as query, while interactive co-attention uses an averaged keys vector based on another input. Another case one can consider is when attention is calculated based purely on the feature vectors This concept has been mentioned before and is referred to as self-attention or intra-attention [71]. We say that the models use self-attentive queries. There are two ways of interpreting such queries. Firstly, one can say that the query is constant. For example, document classification requires only a single classification as the output of the model. As such, the query is always the same, namely: \"What is the class of the document?\". The query can be ignored and attention can be calculated based only on the features themselves Score functions can be adjusted for this by making the query vector a vector of constants or removing it entirely:\n$$\\mathrm{score}(\\boldsymbol{k}_{l})=\\:\\boldsymbol{w}^{T}\\times\\mathrm{act}(\\boldsymbol{W}\\times\\:\\boldsymbol{k}_{l}+\\boldsymbol{b}\\:).\\\\d_{k}\\times1\\quad1\\times d_{w}\\quad d_{w}\\times d_{k}\\quad d_{k}\\times1\\quad d_{w}\\times1$$\n\nAdditionally, one can also interpret self-attention as learning the query along the way, meaning that the query can be defined as a trainable vector of weights.For example,the dot-product score function may take the following form\n\n$$\\begin{array}{rcl}\\mathrm{score}(\\boldsymbol{k}_l)&=&\\boldsymbol{q}^T\\times\\boldsymbol{k}_l\\\\d_k\\times1&&1\\times d_k&d_k\\times1\\end{array},$$\n\nwhere $q\\in\\mathbb{R}^{d_k}$ is a trainable vector of weights. One could also interpret vector $b\\in\\mathbb{R}^{d_{w}}$ as the query in (33).Another\n\nuse of self-attention istouncover the relations between the feature vectors $f_{1},\\ldots,f_{n_{f}}$ . These relations can then be used as additional information to incorporate into new representations of the feature vectors. With basic attention mechanisms, the keys matrix $K$, and the values matrix $V$ are extracted from the features matrix $F_{\\prime}$ while the query $q$ is produced separately. For this type of self-attention, the query vectors are extracted in a similar process as the keys and values, via a transformation matrix of trainable weights $W_{Q}\\in\\mathbb{R}^{d_{q}\\times d_{f}}$ . We define the matrix $Q=\\left[\\boldsymbol{q}_{1},\\ldots,\\boldsymbol{q}_{n_{f}}\\right]\\in$ $\\Pi$玻 $d_q\\times nf$ which canbe obtained asfollows:\n\n$$Q_{d_{q}\\times n_{f}}=W_{Q}\\times F_{d_{f}\\times n_{f}}.$$\n\nEach column of $Q$ can be used as the query for the attention model. When attention is calculated using a query $q.$ the resulting context vector C will summarize the information the query, or a column of $Q$ , is now also a feature vector representation, the context vector contains the information of allfeature vectors that are important to that specific feature vector. In other words, the context vectors capture the relations between the feature vectors. For example, selfattention allows one to extract the relations between words: which verbs refer to which nouns, which pronouns refer to which nouns, etc. For images, self-attention can be used to determine which image regions relate to each other While self-attention is placed in the query-related cat-\n\negory, it is also very much related to the feature model. Namely, self-attention is a technique that is often used in the feature model to create improved representations of the feature vectors. For example, the Transformer model for language processing [13], and the Transformer model for image processing [15], both use multiple rounds of (multihead) self-attention to improve the representation of the feature vectors. The relations captured by the self-attention mechanism are incorporated into new representations. A. simple method of determining such a new representation is to simply set the feature vectors equal to the acquired self-attention context vectors [71], as presented in (36).\n\n$$f_{_{d_f\\times1}}^{(\\mathrm{new})}=\\underset{d_f\\times1}{\\operatorname*{c}},$$\n\nwhere $f^{(\\mathrm{new})}$ is the updated feature vector. Another possibility is to add the context vectors to the previous feature vectors with an additional normalization layer [13]:\n\n$$f_{d_f\\times1}^{(\\mathrm{new})}=\\mathrm{Normalize}(f_{d_f\\times1}^{(\\mathrm{old})}+\\sum_{d_f\\times1}),$$\n\nwhere $f^{(\\mathrm{old})}$ is the previous feature vector, and Normalize( is a normalization layer[72]. Using such techniques, self embeddings that enhance model accuracy [71]. Self-attention is arguably one of the more important\n\ntypes of attention, partly due to its vital role in the highly popular Transformer model. Self-attention is a very general mechanism and can be applied to practically any problem. As such, self-attention has been extensively explored in many different fields in both Transformer-based architec tures and other types of models. For example, in [73], selfattention is explored for image recognition tasks, and results\n\nindicate that the technique may have substantial advantages with regards to robustness and generalization. In [74], selfattention is used in a generative adversarial network (GAN) [75] to determine which regions of the input image to focus on when generating the regions of a new image. In [76], selfattention is used to design a state-of-the-art medical image segmentation model. Naturally, self-attention can also be used for video processing. In [77], a self-attention model is proposed for the purpose of video summarization that reaches state-of-the-art results. In other fields, like audio processing, self-attention has been explored as well. In [78], self-attention is used to create a speech recognition model Self-attention has also been explored in overlapping domains. For example, in [79], the self-attention Transformer architecture is used to create a model that can recognize phrases from audio and by lip-reading from a video. For the problem of next item recommendation, [80] proposes in the feature vectors that is important to the query. Sincea Transformer model that explicitly captures item-item re lations using self-attention. Self-attention also has applica-. tions in any natural language processing fields. For example, in [81], self-attention is used for sentiment analysis. Selfattention is also highly popular for graph models. For example, self-attention is explored in [82] for the purpose of representation learning in communication networks and rating networks. Additionally, the first attention model for graph networks was based on self-attention [83]\n\n### 3.3.2Multiplicity ot Queries\n\nIn previous examples, the attention model generally used a single query for a prediction.We say that such models use singular query attention. However, there are attention architectures that allow the model to compute attention using multiple queries. Note that this is different from, for example, an RNN that may involve multiple queries to produce a sequence of predictions. Namely, such a model still requires only a single query per prediction One example of a technique that incorporates multiple\n\nqueries is multi-head attention [13], as presented in Fig. 9. Multi-head attention works by implementing multiple attention modules in parallel by utilizing multiple different versions of the same query. The idea is to linearly transform the query $q$ using different weight matrices. Each newly formed query essentially asks fora different type of relevant information, allowing the attention model to introduce more information into the context vector calculation. An attention model implements $d\\geq1$ heads with each attention head having its own query vector, keys matrix, and values matrix $q^{(j)}$ ， $K^{(j)}$ and $V^{(j)}$ ，for $j$ = $1, \\ldots , d$ The query $q^{(j)}$ is obtained by linearly transforming the original query $\\Psi$, while the matrices $K^{(j)}$ and $V^{(j)}$ are obtained through linear transformations of $F$ .As such,each attention head attention has been used to create improved word or sentence has its own learnable weights matrices $W_{q}^{(j)}$ ” $W_{K}^{(j)}$ and $W_{V}^{(j)}$ for these transformations.The calculation of the query keys, and values for the $j$ th head are defined as follows:\n$$\\boldsymbol{q}_{d_{q}\\times1}^{(j)}=\\boldsymbol{W}_{q}^{(j)}\\times\\boldsymbol{q}_{d_{q}\\times1},\\quad\\boldsymbol{K}_{d_{k}\\times n_{f}}^{(j)}=\\boldsymbol{W}_{K}^{(j)}\\times\\boldsymbol{F}_{d_{f}\\times n_{f}},\\\\\\boldsymbol{V}_{d_{v}\\times n_{f}}^{(j)}=\\boldsymbol{W}_{V}^{(j)}\\times\\boldsymbol{F}_{d_{f}\\times n_{f}}.$$\n\nThus,each head creates its own representations of the query $q$, and the input matrix $F$ Each head can therefore\n\n![](./images/fLxYvsz8GoGucwnBRCXegWEcG76geRpkm.png)\n\nFig. 9. An illustration of multi-head attention\n\nlearn to focus on different parts of the inputs, allowing the model to attend to more information.For example when training a machine translation model, one attention head can learn to focus on which nouns (e.g., student, car, apple) do certain verbs (e.g., walking, driving, buying) refer to,while another attention head learns to focus on which $\\boldsymbol{e}^{( j) }$ = $[ e_{1}^{( j) }, \\ldots , e_{n_{f}}^{( j) }]$ $\\in$ $\\mathbb{R} ^{n_{f}}$ ae $q^{(s)}$ Pr $q^{(0)}$ A $S$, Cat $q^{(s+1)}$ of ttention weights $\\boldsymbol{a}^{( j) }$ = $[ a_{1}^{( j) }, \\ldots , a_{n_{f}}^{( j) }]$ $\\in$ $\\mathbb{R} ^{n_{f}}$ As canpossibly using the current context vector $c^{(s)}$ as another be expected, each attention model produces its own context vector $\\boldsymbol{c}^{(j)}\\in\\mathbb{R}^{dv}$ ,as follows:\n\n$$\\boldsymbol{c}_{d_{v}\\times1}^{(j)}=\\sum_{l=1}^{n_{f}}a_{l}^{(j)}\\times\\boldsymbol{v}_{l}^{(j)}.$$\n\nThe goal is still to create a single context vector as output of the attention model.As such, the context vectors produced by the individual attention heads are concatenated into a single vector. Afterwards, a linear transformation is applied using the weight matrix $W_{O}\\in\\mathbb{R}^{d_{c}\\times d_{v}d}$ tomake sure the resulting context vector $c\\in\\mathbb{R}^{d_{c}}$ has the desired dimension. This calculation is presented in (40). The dimension $d_{c}$ can be pre-specified by, for example, setting it equal to $d_{v,}$ so that the context vector dimension is unchanged.\n\n$$c\\:=\\:W_{O}\\:\\times\\mathrm{concat}(\\boldsymbol{c}^{(1)},...,\\boldsymbol{c}^{(d)}).$$\n\nMulti-head attention processes multiple attention modmented sequentially to iteratively adjust the context vec tors.Each of these attention modules are referred to as \"repetitions” or “rounds” of attention. Such attention architectures are referred to as multi-hop attention models also known as multi-step attention models.An important note to consider is the fact that multi-hop attention tant mote to con sider is be ap thas m unt hap ustention she that the actunl o $c^{(s)}$ ind $q^{(s+1)}$ the atentoen noedelin throughout various works. While the mechanism always involves multiple rounds of attention, the multi-hop implementation proposed in [84] differs from the mechanism proposed in[85]or [86]. Another interesting example is [87], where a “multi-hop”attention model is proposed that would actually be considered alternating co-attention in this survey, as explained in Subsection 3.1.1. We present a general form of multi-hop attention that\n\nis largely a generalization of the techniquesintroduced into produce the next context vector $c^{(s+1)}$ .Onemust note [85] and [88]. Fig. 10 provides an example implementation of a multi-hop attention mechanism. The general idea isweights,meaning that the number of parameters do not to iteratively transform the query, and use the query to transform the context vector,such that the model can extract different information in each step. Remember that a query\n\n![](./images/fC2yeUZnAYXZanLazGfbKKQkNuntGh8I1.png)\n\nFig. 10.An example illustration of multi-hop attention. Solid arrows represent the base multi-hop model structure, while dotted arrows represent optional connections.\n\nis similar to a literal question. As such, one can interpret the transformed queries as asking the same question in a different manner or from a different perspective, similarly to the queries in multi-head attention. The query that was previously denoted by $\\Psi$ is now referred to as the initial input, and some transformation function transform()\n\n$$\\boldsymbol{q}_{d_{q}\\times1}^{(s+1)}=\\mathrm{transform}(\\boldsymbol{q}^{(s)},\\:\\boldsymbol{c}^{(s)}).$$\n\nFor the specific form of the transformation function transform(),[85]proposes to use a mechanism similar to self-attention. Essentially, the queries used by the question answer matching model proposed in [85] were originally based on a set of feature vectors extracted from a question [85] also defines the original query $q^{(0)}$ as the unweighted average of these feature vectors.At each hop $S$, attention can be calculated on these feature vectors using the previous query $q^{(s)}$ as the query in this process.The resulting context vector of this calculation is the next query vector. Using the context vector $c^{(s)}$ instead of $q^{(s)}$ as the query for this process is also a possibility, which is similar to the LCRRot-hop model proposed in [43] and the multi-step model proposed in [88]. Such a connection is represented by the dotted arrows in Fig. 10. The transformation mechanism ules in parallel, but attention modules can also be imple-uses either the $q^{(s)}$ or the context vector $c^{(s)}$ as query, but a combination via concatenation is also possible. Each query representation is used as input for the at\n\ntention module to compute attention on the columns of the feature matrix $F$ ,as seen previously. One main difference. however, is that the context vector $c^{(s)}$ is also used as input, score function is presented in (42). Note that the initial context vector $c^{(0)}$ is predefined.One way of doing thisis by setting it equal to the unweighted average of the value vectors $\\boldsymbol{v}_1,\\ldots,\\boldsymbol{v}_{n_f}\\in\\mathbb{R}^{d_v}$ extracted from $F$\n\n$$e_{l}^{(s)}=\\text{score}(\\text{concat}(\\boldsymbol{q}_{d_{q}\\times1}^{(s+1)},\\boldsymbol{c}^{(s)}),\\boldsymbol{k}_{l}).$$\n\nAn alignment function and the value vectors are then used that in [85], the weights used in each iteration are the same scale with the number of repetitions. Yet,using multiple hops with different weight matrices can also be viable, as shown by the Transformer model [13] and in [88]. It may be\n\ndifficult to grasp why $c^{(s)}$ is part of the query input for the attention model. Essentially, this technique is closely related to self-attention in the sense that,in each iteration,a new context representation is created from the feature vectors and the context vector. The essence of this mechanism is that one wants to iteratively alter the query and the context vector,while attending to the feature vectors.In the process, the new representations of the context vector absorb more differentkinds of information.This is also the main difference between this type of attention and multi-head attention. Multi-head attention creates multiple context vectors from multiple queries and combines them to create a final context vector as output. Multi-hop attention iteratively. refines the context vector by incorporating information from the different queries. This does have the disadvantage of having to calculate attention sequentially Interestingly, due to the variations in which multi-hop\n\nattention has been proposed, some consider the Transformer model’s encoder and decoder to consist of several singlehop attention mechanisms [84] instead of being a multihop model. However, in the context of this survey, we consider the Transformer model to be an alternative form of the multi-hop mechanism, as the features matrix $F$ is not directly reused in each step. Instead, $F$ is only used as an input for the first hop, and is transformed via self-attention into a new representation. The self-attention mechanism uses each feature vector in $F$ as a query, resulting in a matrix of context vectors as output of each attention hop. The $p_{1},\\cdots,p_{d_{[0]}}$ as accurately as possible compared to the true intermediate context vectors are turned into matrices and represent iterative transformations of the matrix $F$ which are used in the consecutive steps. Thus, the Transformer model iteratively refines the features matrix $F$ by extracting and incorporating new information When dealing with a classification task, another idea is\n\nto use a different query for each class. This is the basic principle behind capsule-based attention [89], as inspired by the capsule networks [90]. Suppose we have the feature classes that the model can predict. Then, a capsule-based attention model defines a capsule for each of the $d_{y}$ classes that each take as input the feature vectors.Each capsule consists of, in order, an attention module, a probability module, and a reconstruction module, which are depicted in. Fig. 11. The attention modules all use self-attentive queries so each module learns its own query: \"Which feature vectors are important to identify this class?\". In [89], a self-attentive multiplicative score function is used for this purpose\n\n$$e_{c,l}=\\begin{array}{ccc}\\boldsymbol{q}_c^T&\\times&\\boldsymbol{k}_l\\\\1\\times1&1\\times d_k&d_k\\times1\\end{array},$$\n\nwhere $e_{c,l}\\in\\mathbb{R}^1$ is the attention score for vector $l$ in capsule $U_{f}$ and $q_{c}\\in\\mathbb{R}^{d_k}$ is a trainable query for capsule $C_{f}$ for $c=1,\\ldots,d_{y}$ . Each attention module then uses an alignment function, and uses the produced attention weights to determine a context vector $c_{c}\\in\\mathbb{R}^{d_{v}}$ .Next, the context vector $C_{t}$ is fed through a probability layer consisting of a linear transformation with a sigmoid activation function:\n\n$$\\begin{aligned}p_{c}&=\\mathrm{sigmoid}(\\boldsymbol{w}_{c}^{T}\\times\\boldsymbol{c}_{c}\\:+\\:b_{c}\\:),\\\\&_{1\\times1}\\times d_{v}\\quad d_{v}\\times1\\quad1\\times1\\end{aligned}$$\n\n![](./images/fFOVLaKiKKX8YVu44wZt6GwmcOzskqWOp.png)\n\nFig. 11. An illustration of capsule-based attention\n\nwhere $w_c\\in\\mathbb{R}^dv$ and $b_{c}\\in\\mathbb{R}^{1}$ are trainable capsule-specific weights parameters, and $p_{c}\\in\\mathbb{R}^{1}$ is the predicted probability that the correct class is class $t$ . The final layer is the reconstruction module that creates a class vector representation This representation $r_{c}\\in\\mathbb{R}^{dv}$ is determined by simply multiplying the context vector $C_{c}$ by the probability $P_{c}$\n\n$$\\begin{array}{rcl}r_c&=&p_c\\times&c_c\\\\d_v\\times1&&1\\times1&&d_v\\times1\\end{array}.$$\n\nThe capsule representation is used when training the model. First of all, the model is trained to predict the probabilities values. Secondly, via a joint loss function, the model is also trained to accurately construct the capsule representations ${\\boldsymbol{r}}1,\\cdots,{\\boldsymbol{r}}d_{\\psi}$ . A features representation $f\\in\\mathbb{R}^{d_f}$ is defined which is simply the unweighted average of the original feature vectors. The idea is to train the model such that vector representations from capsules that are not the correct class differ significantly from $f$ while the representation from the correct capsule is very similar to $f$ A dot-product between the capsule representations and the features representation vectors $f_{1}, \\ldots , f_{n_{f}}$ $\\in$ $\\mathbb{R} ^{d_{f}}$ , and suppose there are are $d_{y}$ is used in [89] as a measure of the distance between the vectors. Note that $d_v$ must equal $d_{f}$ in this case, otherwise the vectors would have incompatible dimensions. Interestingly, since attention is calculated for each class individually, one can track which specific feature vectors are important for which specific class. In [89], this idea is used to discover which words correspond to which sentiment class. The number of tasks that can make use of multiple\n\nqueries is substantial, due to how general the mechanisms are.As such, the techniques described in this section have been extensively explored in various domains.For example, multi-head attention has been used for speaker recog nition based on audio spectrograms [91]. In [92], multihead attention is used for recommendation of news articles. Additionally, multi-head attention can be beneficial for graph attention models as well [83]. As for multi-hop attention, quite a few papers have been mentioned before but there are still many other interesting examples.For example, in [93], a multi-hop attention model is proposed for medication recommendation. Furthermore, practically every Transformer model makes use of both multi-head and multi-hop attention. The Transformer model has been extensively explored in various domains. For example, in [94], a Transformer model is implemented for image cap-\n\ntioning. In [95], Transformers are explored for medical image segmentation. In [96], a Transformer model is used for emotion recognition in text messages. A last example of an application of Transformers is [17], which proposes a Transformer model for recommender systems. In comparison with multi-head and multi-hop attention, capsule-based attention is arguably the least popular of the mechanisms discussed for the multiplicity of queries. One example is [97], where an attention-based capsule network is proposed that also includes a multi-hop attention mechanism for the purpose of visual question answering. Another example is [98], where capsule-based attention is used for aspect-level sentiment analysis of restaurant reviews. The multiplicity of queries is a particularly interesting ang Parlprioyesong ar th nthe fieldof audipor 108\n\ncategory due to the Transformer model [13], which com provements and iterations of the model have been produced that typically aim to improve the predictive performance, the computational efficiency, or both.For example, the Transformer-XL [99] is an extension of the original Trans former that uses a recurrence mechanism to not be limited by a context window when processing the outputs.This allows the model to learn significantly longer dependencies while also being computationally more efficient during the evaluation phase. Another extension of the Transformer is known as the Reformer model [100]. This model is signifi cantly more efficient computationally, by means of localitysensitive hashing, and memory-wise, by means of reversible residual layers. Such computational improvements are vital, since one of the main disadvantages of the Transformer model is the sheer computational cost due to the complexity of the model scaling quadratically with the amount of input feature vectors. The Linformer model [101] manages to reduce the complexity of the model to scale linearly, while achieving similar performance as the Transformer model This is achieved by approximating the attention weights using a low-rank matrix. The Lite-Transformer model proposed in [102] achieves similar results by implementing two branches within the Transformer block that specialize in capturing global and local information. Another interesting Transformer architecture is the Synthesizer [103] This model replaces the pairwise self-attention mechanism with \"synthetic attention weights. Interestingly, the per formance of this model is relatively close to the original Transformer, meaning that the necessity of the pairwise self-attention mechanism of the Transformer model may be questionable.For a more comprehensive overview of Transformer architectures, we refer to [104].\n\n## 4EVALUATIOn OF ATTenTIOn MOdeLs\n\nIn this section, we present various types of evaluation for attention models.Firstly, one can evaluate the structure of attention models using the taxonomy presented in Section3 For such an analysis, we consider the attention mechanism categories (see Fig. 3) as orthogonal dimensions of a model. The structure of a model can be analyzed by determining which mechanism a model uses for each category. Table 3 provides an overview of attention models found in the liter-\n\nature with a corresponding analysis based on the attention mechanisms the models implement Secondly, we discuss various techniques for evaluating\n\nthe performance of attention models. The performance of attention models can be evaluated using extrinsic or intrinsic performance measures, which are discussed in Subsections. 4.1 and 4.2, respectively\n\n### 4.1 Extrinsic Evaluation\n\nIn general, the performance of an attention model is measured using extrinsic performance measures. For example,. performance measures typically used in the field of natural language processing are the BLEU [107], METEOR [108] the Word Error Rate[110] and Phoneme Error Rate[111] are bins fom of mult-hop ad multi-hd attntion Due gerally mpoyed For genal assfiation tasks, eo rates, precision, and recall are generally used.For computer vision tasks, the PSNR[112], SSIM [113], or IoU [114] metrics are used. Using these performance measures, an attention model can either be compared to other state-of-the-art models, or an ablation study can be performed. If possible, the importance of the attention mechanism can be tested by replacing it with another mechanism and observing whether the overall performance of the model decreases [105], [115] An example of this is replacing the weighted average used to produce the context vector with a simple unweighted average and observing whether there is a decrease in overall model performance [35]. This ablation method can be used to evaluate whether the attention weights can actually distinguish important from irrelevant informationd\n\n### 4.2Intrinsic Evaluation\n\nAttention models can also be evaluated using attentionspecific intrinsic performance measures.In [4], the attention weights are formally evaluatedvia the Alignment Error Rate (AER) to measure the accuracy of the attention weights with respect to annotated attention vectors. [116] incorporates this idea into an attention model by supervising the attention mechanism using gold attention vectors A joint loss function consisting of the regular task-specific loss and the attention weights loss function is constructed for this purpose. The gold attention vectors are based on annotated text data sets where keywords are hand-labelled. However, since attention is inspired by human attention, one could evaluate attention models by comparing them to the attention behaviour of humans.\n\n### 4.2.1 Evaluation via Human Attention\n\nIn [117], the concept of attention correctness is proposed which is a quantitative intrinsic performance metric that evaluates the quality of the attention mechanism based on actual human attention behaviour. Firstly, the calculation of this metric requires data that includes the attention behaviour of a human.For example, a data set containing images with the corresponding regions that a human focuses on when performing a certain task, such as image captioning.The collection of regions focused on by the human is referred to as the ground truth region. Suppose an attention model attends to the $n_{f}$ feature vectors $f_1,\\ldots,\\boldsymbol{f}_{n_f}\\in\\mathbb{R}^{d_f}$ Feature vector $f_i$ corresponds to region $R_{i}$ of the given\n\nTABLE 3 Attention models analyzed based on the proposed taxonomy. A plus sign (+) between two mechanisms indicates that both techniques were combined in the same model, while a comma (,) indicates that both mechanisms were tested in the same paper, but not necessarily as a combination in the same model\n\nBuhleran et al. [3]\nMultiplicity\nLevels\nRepresentations\nScoring\nGeneralization\nDimensionality\nType\nQuery-Related Multiplicity\nSingular\nSingle-Level\nSingle-Representational\nAdditive\nGlobal\nSingle-Dimensional\nBasic\nSingular\n\nLuong et al. [4]\nSingular\nSingle-Level\nSingle-Representational\nMultiplicative\nGlobal, Local\nSingle-Dimensional\nBasic\nSingular\n\nXu et al. [8]\nSingular\nSingle-Level\nSingle-Representational\nAdditive\nSoft, Hard\nSingle-Dimensional\nBasic\nSingular\n\nTu et al. [32]\nParallel / Co-attention\nHierarchical\nSingle-Representational\nAdditive\nGlobal\nSingle-Dimensional\nSpecialized\nSingular\n\nYang et al. [5]\nSingular\nHierarchical\nSingle-Representational\nAdditive\nGlobal\nSingle-Dimensional\nSelf-Attentive\nSingular\n\nLi et al. [14]\nSingular\nHierarchical\nSingle-Representational\nAdditive\nGlobal\nSingle-Dimensional\nSelf-Attentive\nSingular\n\nKoivisto et al. [13]\nSingular\nParallel / Co-attention\nSingle-Representational\nMultiplicative\nGlobal\nSingle-Dimensional\nMulti-Hop\nSingular\n\nWaalstra and Frasincar [15]\nRotatory\nSingle-Level\nSingle-Representational\nActivated\nGlobal\nSingle-Dimensional\nSpecialized\nMulti-Hop\n\nKiela et al. [50]\nSingular\nSingle-Level\nMulti-Representational\nAdditive\nGlobal\nMulti-Dimensional\nSingular\n\nShen et al. [64]\nSingular\nSingle-Level\nSingle-Representational\nAdditive\nGlobal\nMulti-Dimensional\nSingular\n\nZhang et al. [74]\nSingle-Level\nSingle-Representational\nMultiplicative\nGlobal\nSingle-Dimensional\nSelf-Attentive\nSingular\n\nLi et al. [115]\nParallel / Co-attention\nHierarchical\nSingle-Representational\nAdditive\nGlobal\nSingle-Dimensional\nSpecialized + Multi-Hop\n\nYu et al. [106]\nParallel / Co-attention\nSingle-Level\nSingle-Representational\nMultiplicative\nGlobal\nSingle-Dimensional\nSelf-Attentive + Specialized\n\nWang et al. [62]\nParallel / Co-attention\nHierarchical\nSingle-Representational\nAdditive\nReinforced\nSingle-Dimensional\nSingular\n\nOktay et al. [67]\nSingular\nSingle-Level\nSingle-Representational\nAdditive\nGlobal\nMulti-Dimensional\nSelf-Attentive + Specialized\nSingular\n\nWinata et al. [52]\nSingular\nSingle-Level\nMulti-Representational\nAdditive\nGlobal\nSingle-Dimensional\nSelf-Attentive\nMulti-Hop\n\nWang et al. [89]\nSingular\nSingle-Level\nSingle-Representational\nMultiplicative\nGlobal\nSingle-Dimensional\nSelf-Attentive\nCapsule-Based\n\nimage, for $i$ = $1, \\ldots , n_{f}$ .We define the set $G$ as the set of regions that belong to the ground truth region, such that $R_{i}\\in G$ if $R_{i}$ is part of the ground truth region. The attentionmodel is behaving correctly. This allows for some interpremodel calculates the attention weights $a_1,\\ldots,a_{n_f}\\in\\mathbb{R}^1$ via the usual attention process. The Attention Correctness $(AC)$ metric can then be calculated using (46).\n\n$$\\begin{matrix}AC=\\sum_{i:R_i\\in G}a_i\\\\\\end{matrix}.$$\n\nThus, this metric is equal to the sum of the attention weights for the ground truth regions.Since the attention weights sum up to 1 due to, for example, a softmax alignment function, the $AC$ value will be a value between O and 1. If the model attends to only the ground truth regions, then $AC$ is equal to 1, and if the attention model does not attend to any of the ground truth regions, $AC$ will be equal to 0. In [118], a rank correlation metric is used to compare\n\nimportant. Therefore, observing which parts of the inputs the model focuses on can be helpful in determining if the tation of the behaviour of models that are typically known to be black boxes.However, rather than checking if the model focuses onthemost important parts of thedatasome use the attention weights to determine which parts of the data are most important. This would imply that attention models provide a type of explanation, which is a subject of contention among researchers. Particularly, in [120], extensive experiments are conducted for various natural language processing tasks to investigate the relation between attention weights and important information to determine whether attention can actually provide meaningful explanations. In this paper titled \"Attention is not Explanation\" it is found that attention weights do not tend to correlate with important features. Additionally, the authors are able to replace the produced attention weights with completely different values while keeping the model output the same. These so-called \"adversarial attention distributions show that an attention model may focus on completely different information and still come to the same conclusions, which makes interpretation difficult. Yet, in another paper titled \"Attention is not not Explanation\"[121], the claim that attention is not explanation is questioned by challenging the assumptions of the previous work.It is found that the adversarial attention distributions do not perform as reliably well as the learned attention weights, indicating that it was not proved that attention is not viable for explanation. In general, the conclusion regarding the interpretability\n\nthe generated attention weights to the attention behaviour of humans.The conclusion of this work is that attention maps generated by standard attention models generally do not correspond to human attention. Attention models often focus on much larger regions or multiple small non-adjacent regions.As such,a technique to improve attention models is to allow the model to learn from human attention patterns via a joint loss of the regular loss function and an attention weight loss function based on the human gaze behaviour, similarly to how annotated attention vectors are used in [116] to supervise the attention mechanism. [117] proposes to use human attention data to supervise the attention mechanism in such a manner. Similarly, a state-of-the-art video captioning model is proposed in [119] that learns fromof attention models is that researchers must be extremely human gaze data to improve the attention mechanism\n\n### 4.2.2Manual Evaluation\n\ncareful when drawing conclusions based on attention patterns.For example,problems with an attention model can be diagnosed via the attention weights if the model is found to focus on the incorrect parts of the data, if such information the manual inspection of attention weights.As previouslyis available. Yet, conversely, attention weights may only be used to obtain plausible explanations for why certain parts of the data are focused on,rather than concluding that those\n\nA method that is often used to evaluate attention models is mentioned, the attention weights are a direct indication of which parts of the data the attention model finds most\n\nparts are significant to the problem [121]. However, one should still be cautious as the viability of such approaches can depend on the model architecture [122]\n\n## 5CONCLUSION\n\nIn this survey, we have provided an overview of recent research on attention models in deep learning. Attention mechanisms have been a prominent development for deep learning models as they have shown to improve model performance significantly, producing state-of-the-art results for various tasks in several fields of research. We have presented a comprehensive taxonomy that can be used to categorize and explain the diverse number of attention mechanisms proposed in the literature. The organization of the taxonomy wasmotivated based on the structure ofa taskmodel that consists of a feature model, an attention model, a query model, and an output model. Furthermore, the attention mechanisms have been discussed using a framework based on queries, keys, and values. Last, we have shown how one can use extrinsic and intrinsic measures to evaluate the performance of attention models, and how one can use the taxonomy to analyze the structure of attention models The attention mechanism is typically relatively simple\n\nto understand and implement and can led to signficant25. 2015, 412-1 He, Amola, and E Hovy improvements in performance.As such, it is no surprise that this is a highly active field of research with new attention mechanisms and models being developed constantly. Not only are new mechanisms consistently being developed but there is also still ample opportunity for the exploration of existing mechanisms for new tasks. For example, multidimensional attention[64] is a technique that shows promis dienqaM almost any attention model. However, it has not seen much application in current works. Similarly, multi-head atten-. tion [13] is a technique that can be efficiently parallelized and implemented in practically any attention model.Yet it is mostly seen only in Transformer-based architectures. Lastly, similarly to how [43] combines rotatory attention with multi-hop attention, combining multi-dimensional at-. tention, multi-head attention, capsule-based attention, or any of the other mechanisms presented in this survey may produce new state-of-the-art results for the various fields of research mentioned in this survey. This survey has mainly focused on attention mech\n\nanisms for supervised models,since these comprise the largest proportion of the attention models in the literature. In comparison to the total amount of research that has been done on attention models,research on attention models for semi-supervised learning [123], [124] or unsupervised learning [125], [126] has received limited attention and has only become active recently. Attention may play a more significant role for such tasks in the future as obtaining research on attention models can advance even further. For example, we mentioned the fact that attention weights can be trained directly based on hand-annotated data[116] or actual human attention behaviour[117], [119]. As new data sets are released,future research may focus on developing attention models that can incorporate those types of data.\n\nWhile attention is intuitively easy to understand,there still is a substantial lack of theoretical support for attention As such, we expect more theoretical studies to additionally contribute to the understanding of the attention mechanisms in complex deep learning systems. Nevertheless, the practical advantages of attention models are clear. Since attention models provide significant performance improve-. ments in a variety of fields, and as there are ample opportunities for more advancements, we foresee that these models will still receive significant attention in the time to come.\n\n### REFERENCES\n\n[1]H. Larochelle and G. E. Hinton, \"Learning to combine foveal glimpses with a third-order Boltzmann machine,\" in 24th Annuai Conference in Neural Information Processing Systems (NIPS 2010) Curran Associates, Inc., 2010, pp. 12431251. [2]V. Mnih, N. Heess, A. Graves, and k. kavukcuoglu, “Recurrenf models of visual attention,”in 27th Armual Conference on Neurai Information Processing Systems (NIPs 2014).Curran Associates Inc., 2014, pp. 22042212 [3]D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” in 3rd Internationai Conference on Learning Representation (ICLR 2015), 2015 [4]T. Luong, H. Pham, and C. D. Manning, “Effective approaches to attention-based neural machine translation,\" in 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP Hierarchical attention networks for document classification, in 2016 Conference of the NorthAmerican Chapter of the Association fon Computational Linguistics: Human Language Technologies (NAACL [6]YW2016 ACL, 206, 148012890,\"Attention-based LSTM for aspect-level sentiment classification, in 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP 2016). and L. Zhang, “Bottom-up and top-down attention for image captioning and visual question answering,” in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2018), 2018, pp. 60776086 [8]K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov R. Zemel, and Y. Bengio, \"Show, attend and tell: Neural image caption generation with visual attention,\" in 32nd Internationa Conference on Machine Learning (ICML 2015),vol. 37.PMLR 2015, pp. 20482057 [9]Y. Ma, H. Peng, and E. Cambria, “Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive LSTM, in 32nd AAAI Conference orn Artificial Intelligence (AAAI 2018).AAAI Press, 2018, Pp. 58765883 [10] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio \"Attention-based models for speech recognition,” in 28th Annuai Conference on Neural Information Processing Systems (NIPS 2015) [1 D.Brandasociates lnc 2015D pe575,PB Brakl and Y Bengio End-to-end attention-based large vocabulary speech recogni tion, in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2016). IEEE Signal Processing Society, 2016, pp. 49454949 [12] S. Kim, T. Hori, and S. Watanabe, “Joint CTC-attention based end-to-end speech recognition using multi-task learning, in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2017).IEEE Signal Processing Society, 2017, lag m a d sk ma Jo Gomez, L. u. Kaiser, and I Polosukhin, \"Attention is all you need,\" in 31st Annual Conference on Neural Information Processing Systems (NIPS 2017).Curran Associates, Inc., 2017, Pp. 5998- 6008 [14] K. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio, $“On$ the properties of neural machine translation: Encoder-decoder approaches,” in Sth Workshop on Syntax, Semarntics and Structure in Statistical Translation (SSST 2014).ACL, 2014, pp. 103111\n\n[15] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, [36] S. Zheng and R. Xia, “Leff-center-right separated neural network and D. Tran, “Image Transformer,\" in 35th Internafional Conference on Machine Learning (ICML 2018), vol. 80. PMLR, 2018, pp. 40554064. [16] L. Zhou, Y. Zhou, J. J. Corso, R. Socher, and C. Xiong, “End-toend dense video captioning with masked transformer,\" in 2018 IEEE/CVF Conference on Computer Vision and Patfern Recognifion [38]J. Gao, X. Wang, Y. Wang, Z. Yang, J. Gao, J. Wang, W. Tang [17] EVP 01,1ECompufer ocity,2018, p, 8739 874 iang BERT4Rec: Sequential recommendation with bidirectional en coder representations from transformer,” in 28th ACM Interna-[39]Y. Tay, A. T. Luu, and S. C. Hui, “Multi-pointer co-attention nettional Conference on Information and Knowledge Management (CIKM 2019).ACM, 2019, p. 14411450 [18] Wangnand D,MiT.Tas \"Ssrvey on theatention based N23, [40 S LMu,2018,bpe,239-31 and X Hu, User-video co-ttention 2016 [19] J. B. Lee, R. A. Rossi, S. Kim, N. K. Ahmed, and E. Koh, \"Attention models in graphs: A survey,” ACM Transitions on Knowledge Discovery fror Data, vol. 13, pp. 62:162:25, 2019. [20] S. Chaudhari, V. Mithal, G. Polatkan, and R.Ramanath, \"An attentive survey of attention models, ACM Transactions on Intelligent Systems and Technology, vol. 12, no. 5, pp. 132, 2021. [1]D.ue\"An introductory survey onatention mechanisms in NLe 42] mnd tol incanec ah w. 270-27enton network (IntelliSys 2019), ser. AISC, vol. 1038. Springer, 2020, pp. 432448 [22]A. Galassi, M. Lippi, and P. Torroni,\"Attention, please! a critical review of neuralatention modes in natural language pocess 43] O. Wallat and E raincar, A hybrid approach for aspet [23].Danilek in Rockt schl l Weab, and . iedel \" rustratinel national Conference on Learning Representations (ICLR 2017), 2017 [24] Xionan onga Hongs Wng ed D. Pumbeuat 4] and han ttentionvia-atentin neural machine model for weakly supervised audio tagging,\" in Proceedings of fhe 18th Annual Conference of the International Speech Communication Association (Interspeech 2017).ISCA, 2017, pp. 30833087 [25]C. Yu, K. S. Barsim, Q. Kong, and B. Yang, *Multi-level attention model for weakly supervised audio classification, in Proceedings fohe Detetionase Clas, cotio pf.Acoustic ces and Eents 018[46] . Wang S. Wang J. Tang N. OHare, Y. Chang, and B Li, [26] S. Sharma, R. Kiros, and R. Salakhutdinov, \"Action recognition using visual atention\" in Procedings ohth (4h zorn, one.47 Li, Y Wei, Zhang and Q Yang. \"Hierarchical attention [27] L. Gao, Z. Guo, H. Zhang, X. Xu, and H. T. Shen, *Video captioning with attention-based LSTM and semantic consistency,” IEEE Trarsacfions on Meltimedia, vol. 19, no. 9, pp. 20452055, 2017 [28]H. Ying, F. Zhuang, F. Zhang, Y. Liu, G. Xu, X. Xie, H. Xiong, and J. Wu, “Sequential recommender system based on hierarchical attention networks, in 27th International Joint Conference on Artificial Intelligence (IJCAI 2018).IJCAI, 2018, pp. 39263932 [29] H. Song, D. Rajan, J. Thiagarajan, and A. Spanias, \"Attend and diagnose: Clinical time series analysis using attention models,\" in 32nd AAAI Conference on Artificial Intelligence (AAAI 2018). AAAI[50]D. Kiela, C. Wang, and K. Cho, Dynamic meta-embeddingg Press, 2018, pp. 40914098 [30] D.T. Tran, A. Iosifidis, J. Kanniainen, and M. Gabbouj, “Temporal attention-augmented bilinear network for financial time-series data analysis,” IEEE Transactions on Neural Netuorks and Learning [51] S. Maharjan, M. Montes, F. A. Gonzalez, and T. Solorio, \"A Systems, vol. 30, no. 5, pp. 14071418, 2019 [31] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio, \"Graph attention networks,” in 6th Internationai Conference on Learning Representations (ICLR 2018), 2018. [32]J. Lu, J. Yang, D. Batra, and D. Parikh, “Hierarchical question image co-attention for visual question answering,\" in 30th Anrual Conference on Neural Information Processing Systems (NIPs 2016). Curran Associates, Inc., 2016, pp. 289297 [33] F. Fan, Y. Feng, and D. Zhao, “Multi-grained attention network for aspect-level sentiment classification, in 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP 2018).[54] A. Sordoni, P. Bachman, A. Trischler, and Y. Bengio,It [34 D.CLM,2018. P, 3-34g, and H. Wang, Interativ atenton networks for aspect-level sentiment classification,\" in 26th In- [55] A. Graves, G. Wayne, and I. Danihelka, “Neural Turing maternational Joint Conference on Artificial Intelligence (IJCAI 2017). IJCAI, 2017, pp. 40684074 [35] M. Seo, A. Kembhavi, A. Farhadi, and H. Hajishirzi, “Bidirectional attention flow for machine comprehension,” in 4th Irterna tional Conference on Learning Represertations (ICLR 2016), 2016\n\nfor aspect-based sentiment analysis with rotatory attention, arXiv:1802.00892, 2018 [37] B. Jing, P. Xie, and E. Xing, “On the automatic generation of med ical imaging reports,” in 56th Anrual Meeting of the Association for Computational Linguistics (ACL 2018).ACL, 2018, pp. 25772586 and X. Xie, \"CAMP: Co-attention memory networks for diagnosis prediction in healthcare,” in 2019 IEEE International Conference o Data Minting (ICDM 2019).IEEE, 2019, pp. 10361041. works for recommendation, in 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD 2018) network for personalized micro-video recommendation, in 2019 World Wide Web Conference (WWW 2019).ACM, 2019, pp.30203026 [41] M. Tu, G. Wang, J. Huang, Y. Tang, X. He, and B. Zhou, “Multihop reading comprehension across multiple documents by reasoning over heterogeneous graphs in 57thAnmual Meeting of the Association for Computational Linguistics (ACL 2019).Association for Computational Linguistics, 2019, pp. 27042713 for explainable fake news detection on social media,” in 58fh Annual Meeting of fhe Association for Computational Linguistics (ACL 2020).ACL, 2020, Pp. 505514. based sentiment aalysusing a leicalied domain ontology Corference (ESWC 2019), ser. LNCS, vol. 11503.Springer, 2019) Pp. 363378 translation, in 32nd AAAI Conference on Artificial Intelligence (AAAI 2018).AAAI Press, 2018, pp. 563570. [45]L. Wu, L. Chen, R. Hong, Y. Fu, X. Xie, and M. Wang, \"A hierarchical attention model for social contextual image recom mendation, IEEE Transactions on Knowledge and Data Engineering, 2019 Hierarchical attention network for action recognition in videos,\" arXiv:1607.06416, 2016. transfer network for cross-domain sentiment classification, in 32nd AAAI Conference on Artificial Intelligence (AAAI 2018). AAAI Press, 2018, p. 58525859 [48] C. Xing, Y. Wu, W. Wu, Y. Huang, and M. Zhou, *Hierarchical recurrent attention network for response generation,” in 32nd AAAI Conference on Artificial Intelligence (AAAI 2018). AAAI Press, 2018, pp. 56105617 [49] V. A. Sindagi and V. M. Patel, “HA-CCN: Hierarchical attention based crowd counting network,\" IEEE Transactions on Image Processing, vol. 29, pp. 323335, 2019 for improved sentence representations,” in 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP 2018) ACL, 2018, pp. 14661477. genre-aware attention model to improve the likability prediction of books,”in 2018 Conference onEmpirical Methods in Naturai Language Processing (EMNLP 2018).ACL, 2018, pp. 33813391. [52] G. I. Winata, Z. Lin, and P. Fung, “Learming multilingual metaembeddings for code-switching named entity recognition,\" in 4fh Workshop on Representation Learning for NLP (RepL4NLP 2019) ACL, 2019, pp. 181186 [53] R. Jin, L. Lu, J. Lee, and A. Usman, “Multi-representational con volutional neural networks for text classification, Computationa Intelligence, vol.35, no. 3, pp. 599609, 2019 erative alternating neural attention for machine reading, arXiv:1606.02245, 2016. chines, arXiv:1410.5401, 2014 [56] D. Britz, A. Goldie, M.-T. Luong, and Q. Le, “Massive exploration of neural machine translation architectures,” in 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017). ACL, 2017, pp. 14421451.\n\n[57]R. J. Williams, Simple statistical gradient-following algorithms for connectionist reinforcement learning,\" Machine Learning vol. 8, no. 3, pp. 229256, 1992. [58] T.Shen, T. Zhou, G. Long, J. Jiang, S. Wang, and C. Zhang, *Reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling,” in 27th International Joint Cornference on Artificial Intelligence (IJCAI 2018).IJCAI, 2018, pp. 43454352 [59] M. Malinowski, C. Doersch, A. Santoro, and P. Battaglia, “Learning visual question answering by bootstrapping hard attention,\" in 2018 European Conference on Computer Vision (ECCV 2018), 2018. [60] Y. Liu, W. Wang, Y. Hu, J. Hao, X. Chen, and Y. Gao, “Multiagent game abstraction via graph attention neural network,”\" in 34th AAAI Conference on Artificial Intelligence (AAAI 2020), vol. 34, no. 05.AAAI Press, 2020, pp. 72117218. [61] S.Seo,J.Huang, H. Yang, and Y. Liu, “Interpretable convolutional neural networks with dual local and global attention for review rating prediction,in 11th ACM Conference on Recommender Systems (RecSys 2017).ACM, 2017, pp. 297305 [62] J. Wang, C. Sun, S. Li, X. Liu, L. Si, M. Zhang, and G. Zhou, \"Aspect sentiment classification towards question-answering with reinforced bidirectional attention network, in 57th Ammual Meeting of fhe Association for Computational Linguistics (ACL 2019). ACL, 2019, pp. 35483557. [63] M. Jiang, C. Li, J. Kong, Z. Teng, and D. Zhuang, \"Crosslevel reinforced attention network for person re-identification, Journal of Visual Communication and Image Representation, vol. 69 p. 102775, 2020. [64] T. Shen, T. Zhou, G. Long, J. Jiang, S. Pan, and C. Zhang, “DiSAN: Directional self-attention network for RNN/CNN-free language understanding,\" in 32nd AAAI Conference on Artificial Intelligence. (AAAI 2018).AAAI Press, 2018, Pp. 54465455 [65] O. Arshad, I Gallo, S. Nawaz, and A. Calefati, \"Aiding intra-text representations with visual context for multimodal named entity recognition,\" in 2019 International Conference on Document Analysis. and Recognition (ICDAR 2019).IEEE, 2019, pp. 337342 [66] W. Wu, X. Sun, and H. Wang, \"Question condensing networks for answer selection in community question answering,\" in Proceed. ings of the 56th Annual Meeting of the Association for Coputational Linguistics (ACL 2018).ACL, 2018, pp. 17461755 [67] . Okta.a, Schlem se MDonaFol,go MH aemeMa, Beiric, B. Glocker, and D. Rueckert, \"Attention U-Net: Learming where to look for the pancreas in 1st Medical Imaging with Deep Learning Conference (MIDL 2018), 2018 [68] R. Tan, J. Sun, B. Su, and G. Liu, “Extending the transformer with context and multi-dimensional mechanism for dialogue response generation, in 8th International Conference on Natural Language Processing and Chinese Computing (NLPCC 2019), ser. LNCS, J. Tang, M.-Y. Kan, D. Zhao, S. Li, and H. Zan, Eds., vol. 11839. Springer, 2019, pp. 189199 [69] L. Chen, B. Lv, C. Wang, S. Zhu, B. Tan, and K. Yu, “\"Schemaguided multi-domain dialogue state tracking with graph attention neural networks,in 34th AAAI Conference on Artificial Intelligence (AAAI 2020), vol. 34, no. 05.AAAI Press, 2020, pp. 75217528 [70] H. Wang, G. Liu, A. Liu, Z. Li, and K. Zheng, \"Dmran: A hierarchical fine-grained attention-based network for recommenda-. tion, in 28th International Joint Conference on Artificial Intelligence (IJCAI 2019). [71] Z. Lin, M. Feng, C. N. d. Santos, M. Yu, B. Xiang, B. Zhou, and Y. Bengio, \"A structured self-attentive sentence embedding in 5th International Conference on Learning Representations (ICLR 2017), 2017. [72]J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization, arXiv:1607.06450, 2016. [73] H. Zhao, J. Jia, and V. Koltun,“Exploring self-attention for image recognition,\" in 2020 IEEE/CVF Conference on Computer Vision and Patfern Recognition (CVPR 2020), 2020, pp. 10 07610 085. [74] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, \"Selfattention generative adversarial networks, in 36th International Conference on Machine Learning (ICML 2019), ser. Proceedings of Machine Learning Research, K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97. PMLR, 2019, pp. 73547363 [75] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. WardeFarley, S. Ozair, A. Courville, and Y. Bengio, \"Generative ad versarial nets,in 27th Anmual Conference on Neural Information\n\nProcessing Systems (NIPs 2014).Curran Associates, Inc., 2014 Pp. 2672-2680 [76] A. Sinha and J. Dolz, \"Multi-scale self-guided attention for medical image segmentation,” IEEE Journal of Biomedical and Health Informafics, vol. 25, no. 1, pp. 121-130, 2021 [77] J. Fajtl, H. S. Sokeh, V. Argyriou, D. Monekosso, and P. Re magnino, Summarizing videos with attention, in 2018 Asian Corferernce on Computer Vision (ACCV 2018), ser. LNCS, vol. 11367 Springer, 2018, pp. 3954. [78] J. Salazar, K. Kirchhoff, and Z. Huang, “\"Self-attention networks for connectionist temporal classification in speech recognition, in 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2019).IEEE, 2019, pp. 71157119 [79]T. Afouras, J. S. Chung, A.Senior, O. Vinyals, and A.Zisserman, Deep audio-visual speech recognition, IEEE Transactions or Pattern Analysis and Machine Intelligence, pp. 11, 2018 [80]S. Zhang, Y. Tay, L. Yao, and A. Sun, *Next item recommendation with self-attention, arXiv preprint arXiv:1808.06414, 2018 [81]G. Letarte, F. Paradis, P. Giguere, and F. Laviolette, “Impor tance of self-attention for sentiment analysis,\" in 2018 Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP (BlackboxNLP 2018).ACL, 2018, pp. 267275 [82] A. Sankar, Y. Wu, L. Gou, W. Zhang, and H. Yang, “Dysat Deep neural representation learning on dynamic graphs via selfattention networks, in 13th International Conference on Web Search and Data Minting (WSDM 2020), 2020, pp. 519527 [83] P. Velickovic, G.Cucurull, A. Casanova, A.Romero, P. Lio and Y. Bengio, Graph attention networks, in 5th International Conference on Learning Representations (ICLR 2017), 2017 [84] S. Iida, R Kimura, H. Cui, P-H. Hung, T. Utsuro, and M. Nagata, \"Attention over heads: A multi-hop attention for neural machine translation, in 57th Anmual Meeting of the Association for Com putational Linguistics: Student Research Workshop (ACL-SRW 2019). ACL, 2019, pp. 217222 [85] N. K. Tran and C. Niedereee, “Multihop attention networks for question answer matching, in 41st ACM SIGIR International Conference on Research & Development in Information Retrieval (SIGIR 2018).ACM, 2018, Pp. 325334 [86]Y. Gong and S. R. Bowman, \"Ruminating reader: Reasoning with gated multi-hop attention, in 5th International Conference on Learning Representation (ICLR 2017), 2017 [87]S. Yoon, S. Byun, S. Dey, and K. Jung, “Speech emotion recognition using multi-hop attention mechanism, in 2019 IEEE Interna fional Conference on Acoustics, Speech and Signal Processing (ICASSP 2019).IEEE, 2019, pp. 28222826. [88]Z. Yang, X. He, J. Gao, L. Deng, and A. Smola, “Stacked attention networks for image question answering,” in 2016 IEEE/CVF Con ference on Compufer Vision and Pattern Recognition (CVPR 2016) 2016, pp.2129 [89]Y. Wang, A. Sun, J. Han, Y. Liu, and X. Zhu, \"Sentiment analysis by capsules,” in 2018 World Wide Web Conference (WWW 2018). ACM, 2018, p. 11651174 [90]S. Sabour, Ni. Frosst, and G. E. Hinton, “Dynamic routing between capsules,in 31st Anmual Conference on Neural Information Prosng Systems NIPs 2017). Curran Associates Inc, 2017, p. [91] M. India, P. Safari, and J. Hernando, “Self multi-head attention for speaker recognition,\" in Proceedings of fhe 20th Anmual Conference of the International Speech Communication Association (Interspeecl 2019). ISCA, 2019, pp. 28222826 [92] C. Wu, F. Wu, S. Ge, T. Qi, Y. Huang, and X. Xie, “Neural news recommendation with multi-head self-attention, in 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP 2019).ACL, 2019, pp. 63896394. [93]Y. Wang, W. Chen, D. Pi, and L. Yue,\"Adversarially regularized medication recommendation model with multi-hop memory net-. work,Knowledge and Information Systems, vol.63, no. 1, pp. 125142, 2021 [94] M. Cornia, M. Stefanini, L. Baraldi, and R. Cucchiara, “Meshedmemory transformer for image captioning,\" in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2020), 2020, pp. 10 57810 587 [95] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille and Y. Zhou, \"TransUnet: Transformers make strong encoders for medical image segmentation, arXiv preprint arXiv:2102.043062021\n\n[96]P. Zhong, D. Wang, and C. Miao, “Knowledge-enriched transformer for emotion detection in textual conversations,”in 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. (EMNLP-IJCNLP 2019). ACL, 2019, Pp. 165176 [97]Y. Zhou, R. Ji, J. Su, X. Sun, and W. Chen, “Dynamic capsule 2019, pp. 93249331 [98] Y. Wang, A. Sun, M. Huang, and X. Zhu, \"Aspect-level sentiment analysis using AS-capsules,\" in The World Wide Web Conference, [9 21Da,2 233-20.Yang . Carboell . Le, and R slakhtdinovg Transformer-XL: Attentive language models beyond a fixed length context,”in 57th Annual Meeting of the Association for Computational Linguistics (ACL 2019).ACL, 2019, pp. 29782988. [100] N. Kitaev, L. Kaiser, and A. Levskaya, Reformer: The efficient Transformer, in 8th International Conference on Learning Represen. attention with linear complexity,\" rXiv:2006.04768, 2020. [102] Z. Wu, Z. Liu, J. Lin, Y. Lin, and S. Han, “Lite transformer with long-short range attention, in 8th International Conference on Learning Representations (ICLR 2020), 2020 [103] Y. Tay, D. Bahri, D. Metzler, D.-C. Juan, Z. Zhao, and C. Zheng, Synthesizer: Rethinking self-attention for transformer models, in Proceedings of the38th International Cornference on Machine Learn ing (ICML 2021), vol. 139.PMLR, 2021, pp. 10 18310192 [104] Y. Tay, M. Dehghani, D. Bahri, and D. Metzler, “Efficient transformers: A survey, arXiv:2009.06732, 2020. [105] X. Li, J. Song, L. Gao, X. Liu, W. Huang, X. He, and C. Gan, Beyond RNNs: Positional self-attention with co-attention for video question answering,\" in 33rd AAAI Conference on Artificial Intelligence (AAAI 2019), vol. 33.AAAI Press, 2019, Pp. 86588665 [106] A. W. Yu, D. Dohan, M.-T. Luong, R. Zhao, K. Chen, M. Norouzi and Q. V. Le, \"QANet: Combining local convolution with global self-attention for reading comprehension,\" in 6th International Conference on Learning Representations (ICLR 2018), 2018 [107] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, *BLEU: a method for automatic evaluation of machine translation,” in 40th Annual Meeting of fhe Association for Compufational Linguistics (ACL 2002). ACL, 2002, pp. 311318. [108] S. Banerjee and A. Lavie, “METEOR: An automatic metric for MT evaluation with improved correlation with human judgments, in 2005 Workshop oni Intrinsic and Extrinsic Eoaluation Measures for Machine Translation and/or Summarization. ACL, 2005, pp. 6572 [109] R. Sennrich, “Perplexity minimization for translation model domain adaptation in statistical machine translation,” in 13th Conference of fhe European Chuapter of the Association for Computational Linguistics (EACL 2012).ACL, 2012, pp. 539549 [110] M. Popovic and H. Ney, \"Word error rates: Decomposition over POS classes and applications for error analysis,” in 2nd Workshop on Statistical Machtine Translation (WMT 2007).ACL, 2007, Pp 48-55 [111] P. Schwarz, P. Matejka, and J. Cernocky, “Towards lower error rates in phoneme recognition, in 7th International Conference. on Text, Speech and Diaiogue (TSD 2004), ser. LNCS, vol. 3206 Springer, 2004, pp. 465472. [112] D. S. Turaga, Y. Chen, and J. Caviedes, “No reference PSNR estimation for compressed pictures,\" Signal Processing: Image Commticatfion, vol. 19, no. 2, pp. 173184, 2004 [113] P. Ndajah, H. Kikuchi, M. Yukawa, H. Watanabe, and S. Muramatsu, “SSIM image quality metric for denoised images,” in 3rd WSEAS International Conference on Visualization, Imaging and Simulation (VIS 2010).WSEAS, 2010, pp. 5358 [114] M. A. Rahman and Y. Wang, “Optimizing intersection-over-union in deep neural networks for image segmentation,\" in 12th International Symposium on Visual Computing (ISVC 2016), ser. LNCS, vol. 10072.Springer, 2016, pp. 234244. [115] X. Chen, L. Yao, and Y. Zhang, “Residual attention U-net for auto mated multi-class segmentation of COVID-19 chest CT images, arXiv:2004.05645, 2020 [116] S. Liu, Y. Chen, K. Liu, and J. Zhao, “Exploiting argument information to improve event detection via supervised attention of the Journal of Web Engineering. Dr. Frasincar is a member of the mechanisms,\" in 55th Arnual Meeting of the Association for Coputational Linguistics (ACL 2017).ACL, 2017, pp. 17891798\n\n[117] C. Liu, J. Mao, F. Sha, and A. Yuille, \"Attention correctness in neural image captioning,” in 31st AAAl Conference on Artificial Intelligence (AAAI 2017). AAAI Press, 2017, Pp. 41764182 [118] A. Das, H. Agrawal, L. Zitnick, D. Parikh, and D. Batra, “Human atention novisual question anowering: Do hu mans and dee neural attention models for video captioning by human gaze data,\"in 2017 IEEE Conference on Computer Vision and Paftern Recognition (CVPR 2017).IEEE Computer Society, 2017 [120] S. Jain and B. C. Wallace, \"Attention is not explanation,” in 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2019).ACL, 2019, pp. 35433556 [121] S. Wiegreffe and Y. Pinter, \"Attention is not not explanation,” in 2019 Conference on Empirical Methods in Natural Language Process ing and fhe9th International Joint Conference on Nafural Language 1 B. V. Srinivasan, and B. Ravindran, Towards transparent and explainable attention models,” in 58th Anmual Meeting of the Association for Computational Linguistics (ACL 2020).ACL, 2020 pp. 4206-4216. [123] K. K. Thekumparampil, C. Wang, S. Oh, and L-J. Li, \"Attention based graph neural network for semi-supervised learning, arXiv:1803.03735, 2018 [124]D. Nie, Y. Gao, L. Wang, and D. Shen, \"ASDNet: Attention based semi-supervised deep networks for medical image segmenta tion, in 21st International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI 2018), ser. LNCS, vol 11073.Springer, 2018,pp.370-378 [125]Y. Alami Mejati, C. Richardt, J. Tompkin, D. Cosker, and K. I. Kim, Unsupervised attention-guided image-to-image transla tion, in 32nd Anmual Conference on Neural Information Processing Systems (NIPS 2018).Curran Associates, Inc., 2018, Pp. 36933703 [126] R. He, W. S. Lee, H. T. Ng, and D. Dahlmeier, \"An unsupervised neural attention model for aspect extraction,” in 55thi Annua Meeting of the Association for Computational Linguistics (ACL 2017) ACL, 2017, pp. 388397.\n\nGianni Brauwers was born in Spijkenisse,the Netherlands.in 1998. He received the B.S. de gree in econometrics and operations research from Erasmus University Rotterdam, Rotterdam the Netherlands,in 2019,and is currently pursu ing the M.S. degree in econometrics and man agement science at Erasmus University Rotter dam. He is a Research Assistant at Erasmus Uni-\n\n![](./images/fwKubs9NrrsE4E6IOIuyNmz1254MnctEC.png)\n\nversity Rotterdam, focusing his research on neural attention models and sentiment analysis\n\nFlavius Frasincar was born in Bucharest.Ro mania, in 1971. He received the M.S. degree in computer science,in 1996,and the M.Phil degree in computer science, in 1997, from Politehnica University of Bucharest, Bucharest, Romania, and the P.D.Eng. degree in computer sci ence, in 2000, and the Ph.D. degree in computer science, in 2005, from Eindhoven University of Technology, Eindhoven, the Netherlands Since 2005,he has been an Assistant Profes\n\n![](./images/fRt7WcnS0GB13Asmx3FTox8eFYu3ccbMN.png)\n\nsor in computer science at Erasmus University Rotterdam, Rotterdam, the Netherlands. He has published in numerous\n\nconferences and journals in the areas of databases,Web information systems, personalization, machine learning, and the Semantic Web. He is a member of the editorial boards of Decision Support Systems. International Journal of Web Engineering and Technology, and Com putational Linguistics in the Netherlands Journal, and co-editor-in-chief Association for Computing Machinery","date":"2025-07-14T15:37:06.726Z","updated":"2025-07-14T14:46:50.000Z","path":"code/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.html","title":"","comments":1,"layout":"page","_id":"cmd39u3jh0008qk417elxe8i9","content":"<h1 id=\"A-General-Survey-on-Attention-Mechanisms-in-Deep-Learning\"><a href=\"#A-General-Survey-on-Attention-Mechanisms-in-Deep-Learning\" class=\"headerlink\" title=\"A General Survey on Attention Mechanisms in Deep Learning\"></a>A General Survey on Attention Mechanisms in Deep Learning</h1><p>Gianni Brauwers and Flavius Frasincar</p>\n<p>Abstract—Attention is an important mechanism that can be employed for a variety of deep learning models across many different domains and tasks. This survey provides an overview of the most important attention mechanisms proposed in the literature. The various attention mechanisms are explained by means of a framework consisting of a general attention model, uniform notation, and a comprehensive taxonomy of attention mechanisms. Furthermore, the various measures for evaluating attention models are reviewed and methods to characterize the structure of attention models based on the proposed framework are discussed. Last, future work in the field of attention models is considered</p>\n<p>Index Terms-Attention models, deep learning, introductory and survey, neural nets, supervised learning</p>\n<h3 id=\"1INTRODUCTION\"><a href=\"#1INTRODUCTION\" class=\"headerlink\" title=\"1INTRODUCTION\"></a>1INTRODUCTION</h3><p>Hdemao model was created for machine translation, but was quickly adopted tobe used for other tasks, such as image processing [15], video processing [16], and recommender systems [17]. The purpose of this survey is to explain the general</p>\n<p>the computational complexity of image processing while improving performance by introducing a model that would only focus on specific regions of images instead of the entire picture. Although, the true starting point of the attention mechanisms we know today is often attributed to originate in the field of natural language processing [3]. Bahdanau et al.[3] implement attention in a machine translation model to address certain issues with the structure of recurrent neural networks. After Bahdanau et al. [3] emphasized the advantages of attention, the attention techniques were refined [4] and quickly became popular for a variety of tasks, such as text classification [5], [6], image captioning [7], [8], sentiment analysis [6], [9], and speech recognition [10], [11], [12] Attention has become a popular technique in deep learn-</p>\n<p>form of attention,and provide a comprehensive overview of attention techniques in deep learning. Other surveys have already been published on the subject of attention models For example, in [18], a survey is presented on attention in computer vision, [19] provides an overview of attention in graph models, and [20], [21],[22] are all surveys on attention in natural language processing. This paper partly builds on the information presented in the previously mentioned surveys. Yet, we provide our own significant contributions The main difference between this survey and the previously mentioned ones is that the other surveys generally focus on attention models within a certain domain. This survey however, provides a cross-domain overview of attention techniques. We discuss the attention techniques in a general way, allowing them to be understood and applied in a variety of domains. Furthermore, we found the taxonomies presented in previous surveys to be lacking the depth and structure needed to properly distinguish the various attention mechanisms. Additionally, certain significant attention techniques have not yet been properly discussed in previous surveys, while other presented attention mechanisms seem to be lacking either technical details or intuitive explanations. Therefore, in this paper, we present important attention techniques by means of a single framework using a uniform notation,a combination of both technical and in tuitive explanations for each presented attention technique, and a comprehensive taxonomy of attention mechanisms The structure of this paper is as follows. Section 2 in-</p>\n<p>ing for several reasons. Firstly, models that incorporate attention mechanisms attain state-of-the-art results for all of the previously mentioned tasks, and many others. Furthermore, most attention mechanisms can be trained jointly with a base model, such as a recurrent neural network or a convolutional neural network using regular backpropa gation [3]. Additionally, attention introduces a certain type of interpretation into neural network models [8] that are generally known to be highly complicated to interpret. Moreover, the popularity of attention mechanisms was additionally boosted after the introduction of the Transformer model [i3] that further proved how effective attention can be.Attention was originally introduced as an extension to recurrent neural networks [14]. However, the Transformer model proposed in [13] poses a major development in attention research as it demonstrates that the attention mechanism is sufficient to build a state-of-the-art model. This means that disadvantages, such as the fact that recurrent neural networks are particularly difficult to parallelize, can</p>\n<p>troduces a general attention model that provides the reader with a basic understanding of the properties of attention and how it can be applied. One of the main contributions of this paper is the taxonomy of attention techniques presented in Section 3. In this section, attention mechanisms are explained and categorized according to the presented</p>\n<p><img src=\"/./images/fXAE7qMmeZtPgm2VeKxkN3y8GAQXG8CbW.png\"></p>\n<p>Fig. 1. An illustration of the general structure of the task model.</p>\n<p>taxonomy. Section 4 provides an overview of performance measures and methods for evaluating attention models. Furthermore,the taxonomy is used to evaluate the structure of various attention models. Lastly, in Section 5, we give oun conclusions and suggestions for further research</p>\n<h2 id=\"2-General-Attention-Model\"><a href=\"#2-General-Attention-Model\" class=\"headerlink\" title=\"2 General Attention Model\"></a>2 General Attention Model</h2><p>This section presents a general form of attention with corresponding notation. The notation introduced here is based on the notation that was introduced in [23] and popularized in [13]. The framework presented in this section is used throughout the rest of this paper. To implement a general attention model, it is necessary</p>\n<p>to first describe the general characteristics of a model that can employ attention. First of all, we will refer to the complete model as the task model, of which the structure is presented in Fig. 1. This model simply takes an input, carries out the specified task, and produces the desired output.For example, the task model can be a language model that takes as input a piece of text, and produces as output a summary of the contents,a classification of the sentiment, or the text translated word for word to another language. Alternatively the task model can take an image, and produce a captionthe general attention module is the query $q\\in\\mathbb{R}^{d_{q}}$, andthe or segmentation for that image. The task model consists of four submodels: the feature model, the query model, the attention model, and the output model. In Subsection 2.1, the feature model and query model are discussed, which are used to prepare the input for the attention calculation. In Subsection 2.2, the attention model and output model are discussed, which are concerned with producing the output</p>\n<h2 id=\"2-1Attention-Input\"><a href=\"#2-1Attention-Input\" class=\"headerlink\" title=\"2.1Attention Input\"></a>2.1Attention Input</h2><p>Suppose the task model takes as input the matrix $X\\in$ $\\Pi_{\\mathcal{R}}d_{\\mathcal{F}}\\times\\boldsymbol{n}<em>{\\mathcal{X}}$ where $d</em>{x}$ represents the size of the input vectors and $n_{x}$ represents the amount of input vectors. The columns in this matrix can represent the words in a sentence the pixels in an image, the characteristics of an acoustic sequence, or any other collection of inputs.The feature model is then employed to extract the $n_{f}$ feature vectors $f_1,\\ldots,f_{n_f}\\in\\mathbb{R}^d_f$ from $X$ where $d_{f}$ represents the size of the feature vectors.The feature model can be a recurrent neural network (RNN), a convolutional neural network (CNN), a simple embedding layer, a linear transformation of the original data, or no transformation at all. Essentially, the feature model consists of all the steps that transform the original input $X$ into the feature vectors $f_{1},\\ldots,f_{n_{f}}$ that the attention modelwill attend to.</p>\n<p><img src=\"/./images/fxeeSs49eiCx6zygk9gTruATCtkbbN2Om.png\"></p>\n<p>Fig. 2. The inner mechanisms of the general attention module</p>\n<p>To determine which vectors to attend to,the attention model requires the query $q\\in\\mathbb{H}^d_q$ ,where $d_q$ indicates the size of the query vector. This query is extracted by the query model, and is generally designed based on the type of output that is desired of the model. A query tells the attention model which feature vectors to attend to.It can be interpreted literally as a query, or a question. For example, for the task of image captioning, suppose that one uses a decoder RNN model to produce the output caption based on feature vectors obtained from the image by a CNN.At each prediction step, the hidden state of the RNN model can be used as a query to attend to the CNN feature vectors. In each step, the query is a question in the sense that it asks for the necessary information from the feature vectors based on the current prediction context</p>\n<h3 id=\"2-2-Attention-Output\"><a href=\"#2-2-Attention-Output\" class=\"headerlink\" title=\"2.2 Attention Output\"></a>2.2 Attention Output</h3><p>The feature vectors and query are used as input for the attention model. This model consists of a single, or a collection of general attention modules.An overview of a general attention module is presented in Fig. 2. The input of matrix of feature vectors $\\boldsymbol{F}&#x3D;[\\boldsymbol{f}<em>{1},\\ldots,\\boldsymbol{f}</em>{n_{f}}]\\in\\mathbb{R}^{d_{f}\\times n_{f}}.$ Two separate matrices are extracted from the matrix $F$ : the keys matrix $K&#x3D;[k_{1},\\ldots,k_{n_{f}}]\\in\\mathbb{R}^{d_{k}\\times n_{f}}$, and the values matrix $V$ &#x3D; $[ \\boldsymbol{v}<em>1, \\ldots , \\boldsymbol{v}</em>{n_f}]$ $\\in$ $\\mathbb{R} ^{d_v\\times n_f}$, where $d_{k}$ and $d_{v}$ indicate, respectively, the dimensions of the key vectors (columns of $K$ ) and value vectors (columns of $V$ ). The general way of obtaining these matrices is through a linear transformation of $F$ using the weight matrices. $W_{K}\\in\\mathbb{R}^{d_{k}\\times d_{f}}$ and $W_{V}\\in\\mathbb{R}^{d_{v}\\times d_{f}}$ ,for $K$ and $V$ , respectively. The calculations of $K$ and $V$ are presented in (1). Both weight matrices can be learned during training or predefined by the researcher For example, one can choose to define both $W_{K}$ and $W_{V}$ as equal to the identity matrix to retain the original feature vectors. Other ways of defining the keys and the values are also possible, such as using completely separate inputs for the keys and values. The only constraint to be obeyed is that the number of columns in $K$ and $V$ remains the same<br>$$K_{d_{k}\\times n_{f}}&#x3D;W_{K}\\times F_{d_{f}\\times n_{f}},\\quad V_{d_{v}\\times n_{f}}&#x3D;W_{V}\\times F_{d_{f}\\times n_{f}}.$$</p>\n<p>The goal of the attention module is to produce a weighted average of the value vectors in $V$ .The weights used to produce this output are obtained via an attention scoring and alignment step.The query $q$ and the keys matrix $K$ are used to calculate thevector of attention scores</p>\n<p>$\\boldsymbol{e}&#x3D;\\left[e_{1},\\ldots,e_{n_{f}}\\right]\\in\\mathbb{R}^{n_{f}}.$ This is done via the score function score(), as illustrated in (2)</p>\n<p>$$e_{l}:&#x3D;:\\mathrm{score}(:q:,:k_{l}:).$$</p>\n<p>As discussed before, the query symbolizes a request for information. The attention score $e_{1}$ represents how important the information contained in the key vector $k_{l}$ is according to the query. If the dimensions of the query and key vectors are the same, an example of a score function would be to take the dot-product of the vectors. The different types of score functions are further discussed in Section 3.2.1. Next, the attention scores are processed further through</p>\n<p>an alignment layer. The attention scores can generally have a wide range outside of [0,1] . However, since the goal is to produce a weighted average, the scores are redistributed via an alignment function align() as defined in (3).</p>\n<p>$$\\begin{matrix}a_l&amp;&#x3D;\\text{align}(\\begin{array}{c}e_l:;&amp;\\boldsymbol{e}\\1\\times1&amp;n_f\\times1\\end{array}),\\end{matrix}$$</p>\n<p>where $a_l\\in\\mathbb{R}^1$ is the attention weight corresponding to the lth value vector.One example of an alignment function wouldbe touse a softmax function,but the various other alignment types are discussed in Section 3.2.2. The attention weights provide a rather intuitive interpretation for the attention module. Each weight is a direct indication of how important each feature vector is relative to the others for this particular problem. This can provide us with a more in-depth understanding of the model behaviour, and the re lations between inputs and outputs. The vector of attention weights $a$ &#x3D; $[ a_{1}, \\ldots , a_{n_{f}}]$ $\\in$ $\\mathbb{R} ^{n_{f}}$ is used to produce the context vector $c\\in\\mathbb{R}^{d_v}$ by calculating a weighted average of the columns of the values matrix $V$ ,as shown in (4)<br>$$\\boldsymbol{c}<em>{d_v\\times1}&#x3D;\\sum</em>{l&#x3D;1}^{n_f}a_l:\\times:\\boldsymbol{v}_l:.$$</p>\n<p>As illustrated in Fig. 1, the context vector is then used in the output model to create the output $\\ddot{y}$ This output model translates the context vector into an output prediction. For example, it could be a simple softmax layer that takes as input the context vector $C$ , as shown in (5)</p>\n<p>$$\\hat{\\boldsymbol{y}}<em>{d</em>{y}\\times1}&#x3D;\\mathrm{softmax}(W_{c}:\\times:c_{d_{y}\\times1}:+:b_{c}:),$$</p>\n<p>where $d_{y}$ is the number of output choices or classes, and $W_{c}\\in\\mathbb{R}^{d_{y}\\times d_{v}}$ and $b_{c}\\in\\mathbb{R}^{d}$ are trainable weights</p>\n<h3 id=\"2-3-Attention-Applications\"><a href=\"#2-3-Attention-Applications\" class=\"headerlink\" title=\"2.3 Attention Applications\"></a>2.3 Attention Applications</h3><p>Attention is a rather general mechanism that can be used in a wide variety of problem domains. Consider the task of machine translation using an RNN model. Also, consider the problem of image classification using a basic CNN model. While an RNN produces a sequence of hidden state vectors, a CNN creates feature maps, where each region in the image is represented by a feature vector. The RNN hidden states are organized sequentially, while the CNN feature maps are organized spatially. Yet, attention can still be applied in both situations, since the attention mechanism does not inherently depend on the organization of thefeature vectors. This characteristic makes attention easy to implement in a wide variety of models in different domains.</p>\n<p>Another domain where attention can be applied is audio processing [24], [25]. Acoustic sequences can be represented by a sequence of feature vectors that relate to certain time periods of the audio sample. These vectors could simply be the raw input audio, or they can be extracted via,for example, an RNN or CNN. Video processing is another domain where attention can be applied intuitively [26], [27] Video data consists of sequences of images, so attention can be applied to the individual images, as well as the entire sequence. Recommender systems often incorporate a users interaction history to produce recommendations. Feature vectors can be extracted based on, for example, the id’s or other characteristics of the products the user interacted with, and attention can be applied to them [28]. Attention can generally also be applied to many problems that use a time series as input, be it medical [29], financial [30], or anything else, as long as feature vectors can be extracted The fact that attention does not rely on the organization</p>\n<p>of the feature vectors allows it to be applied to various problems that each use data with different structures, as illustrated by the previous domain examples.Yet,this can be taken even further by applying attention to data where there is irregular structure. For example, protein structures, city traffic flows, and communication networks cannot always be represented using neatly structured organizations, such as sequences, like time series, or grids, like images. In such cases, the different aspects of the data are often represented as nodes in a graph. These nodes can be represented by feature vectors, meaning that attention can be applied in domains that use graph-structured data as well [19],[31] In general, attention can be applied to any problem for</p>\n<p>which a set of feature vectors can be defined or extracted As such, the general attention model presented in Fig. 2 is applicable to a wide range of domains. The problem,how ever, is that there is a large variety of different applications and extensions of the general attention module. As such, in Section 3, a comprehensive overview is provided of a collection of different attention mechanisms.</p>\n<h2 id=\"3ATTENTION-TAXONOMY\"><a href=\"#3ATTENTION-TAXONOMY\" class=\"headerlink\" title=\"3ATTENTION TAXONOMY\"></a>3ATTENTION TAXONOMY</h2><p>There are many different types of attention mechanisms and extensions,and a model can use different combinations of these attention techniques. As such, we propose a taxonomy that can be used to classify different types of attention mechanisms. Fig. 3 provides a visual overview of the different categories and subcategories that the attention mechanisms can be organized in. The three major categories are based on whether an attention technique is designed to handle specific types of feature vectors (feature-related), specific types of model queries (query-related), or whether it is simply a general mechanism that is related to neither the feature model, nor the query model (general). Further explanations of these categories and their subcategories are provided in the following subsections. Each mechanism discussedin this section is either amodification to the existing inner mechanisms of the general attention module presented in Section 2, or an extension of it The presented taxonomy can also be used to analyze</p>\n<p>the architecture of attention models.Namely, the major categories and their subcategories can be interpreted as</p>\n<p><img src=\"/./images/fwzKYgHVze3V4K8ZYBR28uKos0SPY48WT.png\"></p>\n<p>Fig. 3. A taxonomy of attention mechanisms</p>\n<h3 id=\"TABLE-1-Notation\"><a href=\"#TABLE-1-Notation\" class=\"headerlink\" title=\"TABLE 1 Notation.\"></a>TABLE 1 Notation.</h3><p>attend to these various vectors.These features may have specific structures that require special attention mechanisms to handle them.These mechanisms can be categorized to deal with one of the following feature characteristics: the multiplicity of features, the levels of features, or the representations of features</p>\n<h3 id=\"3-1-1Multiplicity-of-Features\"><a href=\"#3-1-1Multiplicity-of-Features\" class=\"headerlink\" title=\"3.1.1Multiplicity of Features\"></a>3.1.1Multiplicity of Features</h3><p>Symbol\tDescription<br>$F$\tMatrix of size $d_f \\times n_f$ containing the feature vectors $f_1, \\ldots, f_{n_f} \\in \\mathbb{R}^{d_f}$ as columns. These feature vectors are extracted by the feature model.<br>$K$\tMatrix of size $d_k \\times n_j$ containing the key vectors $k_1, \\ldots, k_{n_j} \\in \\mathbb{R}^{d_k}$ as columns. These vectors are used to calculate the attention scores.<br>$V$\tMatrix of size $d_v \\times n_j$ containing the value vectors $v_1, \\ldots, v_{n_j} \\in \\mathbb{R}^{d_v}$ as columns. These vectors are used to calculate the context vector.<br>$W_K$\tWeights matrix of size $d_k \\times d_f$ used to create the $K$ matrix from the $F$ matrix.<br>$W_V$\tWeights matrix of size $d_v \\times d_f$ used to create the $V$ matrix from the $F$ matrix. This vector essentially represents a question, and is used to calculate the attention scores.<br>$q$\tQuery vector of size $d_q$. This vector is used to calculate the attention weights.<br>$c$\tContext vector of size $d_c$. This vector is the output of the attention model.<br>$e$\tScore vector of size $d_{e_i}$ containing the attention scores $e_1, \\ldots, e_{n_e} \\in \\mathbb{R}^1$. These are used to calculate the attention weights.<br>$a$\tAttention weights vector of size $d_{a_j}$ containing the attention weights $a_1, \\ldots, a_{n_a} \\in \\mathbb{R}^1$. These are the weights used in the calculation of the context vector.</p>\n<p>For most tasks, a model only processes a single input, such as an image, a sentence, or an acoustic sequence. We refer to such a mechanism as singular features attention. Other models are designed to use attention based on multiple inputs to allow one to introduce more information into the model that can be exploited in various ways.However, this does imply the presence of multiple feature matrices that require special attention mechanisms to be fully used. For example, [32] introduces a concept named co-attention to allow the proposed visual question answering (VQA) model to jointly attend to both an image and a question Co-attention mechanisms can generally be split up</p>\n<p>into two groups [33]: coarse-grained co-attention and fine-grained co-attention. The difference between the two groups is the way attention scores are calculated based on the two feature matrices. Coarse-grained attention mecha-. nisms use a compact representation of one feature matrix as a query when attending to the other feature vectors. Fine-grained co-attention, on the other hand, uses all feature vectors of one input as queries. As such, no information is lost, which is why these mechanisms are called fine-grained As an example of coarse-grained co-attention, [32] pro-</p>\n<p>orthogonal dimensions of an attention model. An attention model can consist of a combination of techniques taken from any or all categories. Some characteristics, such as the scoring and alignment functions, are generally required for any attention model. Other mechanisms, such as multihead attention or co-attention are not necessary in every situation. Lastly, in Table 1, an overview of used notation with corresponding descriptions is provided.</p>\n<p>poses an alternating co-attention mechanism that uses the context vector (which is a compact representation) from one attention module as the query for the other module, and vice versa. Alternating co-attention is presented in Fig. 4. Given a set of two input matrices $X^{(1)}$ and $X^{(2)}$ , features are extracted by a feature model to produce the feature extracts feature vectors so that the attention model canmatrices $F^{(1)}\\in\\mathbb{R}^{d_f^{(1)}\\times n_f^{(1)}}$ and $F^{(2)}\\in\\mathbb{R}^{d_{f}^{(2)}\\times n_{f}^{(2)}}$ ,where $d_{f}^{(1)}$</p>\n<h2 id=\"3-1-Feature-Related-Attention-Mechanisms\"><a href=\"#3-1-Feature-Related-Attention-Mechanisms\" class=\"headerlink\" title=\"3.1 Feature-Related Attention Mechanisms\"></a>3.1 Feature-Related Attention Mechanisms</h2><p>Based on a particular set of input data, a feature model</p>\n<p><img src=\"/./images/fTFNeAz730mbNSQS1tZxyhe0r9V8CHLx8.png\"></p>\n<p>Fig. 4. An illustration of alternating co-attention</p>\n<p>vectors extracted from the first and second inputs, while co-attention is used for VQA, so the two input matrices are the image data and the question data, for which the feature model for the image consists of a CNN model, and the feature modelfor the question consists of word embeddings a convolutional layer, a pooling layer, and an LSTM model. Firstly, attention is calculated for the first set of features $F^{(1)}$ without the use of a query (Attention Module in Fig. 4). In as a weighted average of the second set of value vectors [32], an adjusted additive attention score function is used for this attention mechanism.The general form of the regularattention module,which will produce the context vector additive score function can be seen in (6).<br>$$\\mathrm{score}(:\\boldsymbol{q}:,:\\boldsymbol{k}<em>{l}:)&#x3D;:\\boldsymbol{w}^{T}\\times\\mathrm{act}(\\boldsymbol{W}</em>{1}\\times:\\boldsymbol{q}:+:\\boldsymbol{W}<em>{2}\\times:\\boldsymbol{k}</em>{l}+\\boldsymbol{b}:),\\d_{q}\\times1\\quad d_{k}\\times1\\quad1\\times d_{w}\\quad d_{w}\\times d_{q}\\quad d_{q}\\times1\\quad d_{w}\\times d_{k}\\quad d_{k}\\times1\\quad d_{w}\\times1$$</p>\n<p>where act() is a non-linear activation function, and $w\\in$ $\\Pi^{\\mathcal{R}}d_{\\mathcal{n}\\mathcal{F}}$ ， $W_{1}$ $\\in$ $\\mathbb{R} ^{d_{w}\\times d_{q}}$ ， $W_{2}\\in$ $\\mathbb{R} ^{d_{w}\\times d_{k}}$, and $b\\in\\mathbb{R}^{d_{w}}$ are trainable weights matrices, for which $d_{w}$ is a predefined dimension of the weight matrices.A variant of this score function adapted to be calculated without a query for the application at hand can be seen in (7).</p>\n<p>$$e_{l}^{(0)}&#x3D;\\boldsymbol{w}^{(1)T}\\times\\operatorname{act}(\\boldsymbol{W}^{(1)}\\times\\boldsymbol{k}<em>{l}^{(1)}+\\boldsymbol{b}^{(1)}),\\1\\times1\\quad1\\times d</em>{w}\\quad d_{w}\\times d_{k}^{(1)}\\quad d_{k}^{(1)}\\times1\\quad d_{w}\\times1$$</p>\n<p>where $w^{(1)}\\in\\mathbb{R}^{d_{w}}$ ， $W^{(1)}\\in\\mathbb{R}^{d_{\\omega}\\times d_{k}^{(1)}}$ and $b^{(1)}\\in\\mathbb{R}^{d_{M}}$ areafter another. This may come with a computational distrainable weight matrices for Attention Module,. $k_l^{(1)}\\in$ advantage since it is not possible to parallize. Instead of $\\Pi\\text{砥}^{d_k^{(1)}}$ is the Ith column of the keys matrix $K^{(1)}$ that wasusing a sequential mechanism like alternating co-attention, obtained from $F^{(1)}$ via a linear transformation (see (1), for which $d_{w}$ is a prespecified dimension of the weight matrices and $d_{k}^{(1)}$ is a prespecifiedimension of the key vectors. Perhaps onemay wonder why the query is absent whenqueries,unweighted averages of the keyvectors areused as</p>\n<p>calculating attention in this manner. Essentially, the query in this attention modelis learned alongside the other trainable parameters. As such, the query can be interpreted as a general question: “Which feature vectors contain the most important information?”.This is also known as a selfattentive mechanism, since attention is calculated based only on the feature vectors themselves. Self-attention is explained in more detail in Subsection 3.3.1 The scores are combined with an alignment function</p>\n<p>(see (3)), such as the softmax function, to create attention weights used to calculate the context vector $\\boldsymbol{c}^{(0)}\\in\\mathbb{R}^{\\boldsymbol{d}<em>{\\mathbb{V}}^{(1)}}$ (see (4). This context vector is not used as the output of the attention model, but rather as a query for calculating the context vector $\\boldsymbol{c}^{(2)}\\in\\mathbb{R}^{d</em>{w}^{(2)}}$ based on the second feature vectors $c^{(1)}$ and $c^{(2)}$ matrix $F^{(2)}$ where $d_{v}^{\\langle2\\rangle}$ is thedimension of the valuevectors obtained from $F^{(2)}$ via a linear transformation (see (1).For compact representation of one input to use as a query when</p>\n<p><img src=\"/./images/ftdzr2qwpgGo72yaqTGymWrhgqyyzp9TO.png\"></p>\n<p>Fig. 5. An illustration of interactive co-attention</p>\n<p>and $d_{f}^{(2)}$ represent, respectively the dimension of the feature this module (Attention Modulez in Fig. 4), atention scores are calculated using another score function with $C_{0}$ as query $n_f^{(1)}$ Tand $n_{f}^{(2)}$ re esen et ely t ento e ei ut on e n ie ne nc oncie e n </p>\n<p>$$e_{l}^{(2)}&#x3D;\\mathrm{score}(\\begin{array}{c}c^{(0)}:,:k_{l}^{(2)}\\d_{v}^{(1)}\\times1:d_{k}^{(2)}\\times1\\end{array}).$$</p>\n<p>These attention scores are then used to calculate attentior weights using, for example, a softmax function as alignment function,after which the context vector $c^{(2)}$ can bederived Finally, the context vector $c^{(2)}$ is used as a query for the first $c^{(1)}$ for the first feature matrix $F^{(1)}$ .Attention scores are calculated according to (9). In [32], the same function and weight matrices as seen in (7) are used, but with an added query making it the same as the general additive score function (see (6)). The rest of the attention calculation is similar as before</p>\n<p>$$e_{l}^{(1)}&#x3D;\\mathrm{score}(\\begin{array}{c}c^{(2)}:,:k_{l}^{(1)}:).\\d_{v}^{(2)}\\times1:d_{k}^{(1)}\\times1\\end{array}$$</p>\n<p>The produced context vectors $c^{(1)}$ and $c^{(2)}$ are concatenated and used for prediction in the output model.Alternating co-attention inherently contains a form of sequentiality due to the fact that context vectors need to be calculated one [34] proposes the interactive co-attention mechanism that can calculate attention on both feature matrices in parallel, as depicted in Fig. 5. Instead of using the context vectors as queries.The calculation of the average keys are provided in (10), and the calculation of the attention scores are shown in (11). Any score function can be used in this case, but an additive score function is used in [34]</p>\n<p>$$\\bar{\\boldsymbol{k}}<em>{d_k^{(1)}\\times1}^{(1)}&#x3D;\\frac{1}{n_f^{(1)}}\\sum\\limits</em>{l&#x3D;1}^{n_f^{(1)}}\\boldsymbol{k}<em>l^{(1)},\\quad\\bar{\\boldsymbol{k}}</em>{d_k^{(2)}\\times1}^{(2)}&#x3D;\\frac{1}{n_f^{(2)}}\\sum\\limits_{l&#x3D;1}^{n_f^{(2)}}\\boldsymbol{k}_l^{(2)};$$</p>\n<p>$$e_{l}^{(1)}&#x3D;\\mathrm{score}(:\\bar{\\boldsymbol{k}}^{(2)}:,:\\boldsymbol{k}<em>{l}^{(1)}:),:e</em>{l}^{(2)}&#x3D;\\mathrm{score}(:\\bar{\\boldsymbol{k}}^{(1)}:,:\\boldsymbol{k}<em>{l}^{(2)}:).\\1\\times1\\quad d</em>{k}^{(2)}\\times1\\quad d_{k}^{(1)}\\times1\\quad1\\times1\\quad d_{k}^{(1)}\\times1\\quad d_{k}^{(2)}\\times1$$</p>\n<p>From the attention scores, attention weights are created via an alignment function, and are used to produce the context</p>\n<p>While coarse-grained co-attention mechanisms use a</p>\n<p>calculating attention for another input, fine-grained coattention considers every element of each input individually when calculating attention scores.In this case, the query becomes a matrix. An example of fine-grained co-attention is parallel co-attention [32]. Similarly to interactive coattention, parallel co-attention calculates attention on the two feat ue mariesa hesaemeti e $K^{(1)}\\in\\mathbb{R}^{d_k^{(1)}\\times n_f^{(1)}}$ .d $K^{(2)}\\in\\mathbb{R}<em>{k}^{d</em>{k}^{(2)}\\times n_{f}^{(2)}}$ that are obtained by linearly transforming the feature matrices $F^{(1)}$ and $F^{(2)}$ ,where $d_k^{(1)}$ and $d_k^{(2)}$ are $K^{(1)}$ C $K^{(2)}$ Sopdehna tiset dtoedere $A$ $\\in$ $\\mathbb{R} ^{n_f^{( \\dagger ) }\\times n_f^{( 2) }}$ difet die eastons sn afitysaeix atento sore asho na 6) translate one keys matrix to the space of the other keys. In [32], $A$ is calculated as shown in (12).<br>$$A_{n_{f}^{(1)}\\times n_{f}^{(2)}}&#x3D;\\operatorname{act}(\\begin{array}{ccc}K^{(1)^{T}}&amp;\\times&amp;W_{A}&amp;\\times&amp;K^{(2)}\\n_{f}^{(1)}\\times d_{k}^{(1)}&amp;&amp;d_{k}^{(1)}\\times d_{k}^{(2)}&amp;&amp;d_{k}^{(2)}\\times n_{f}^{(2)}\\end{array}),$$</p>\n<p>weights using an alignment function,so that two context</p>\n<p>where $W_{A}\\in\\mathbb{R}^{d_{k}^{(1)}\\times d_{k}^{(2)}}$ is a trainable weights matrix and $\\arctan()$ is an activation function for which the tanh() function this matrix,i.e., one can use (13) to calculate each individual element $A_{i,j}$ of the matrix $A$</p>\n<p>$$A_{i,j}&#x3D;:\\boldsymbol{w}<em>{A}^{T}:\\times\\mathrm{concat}(\\boldsymbol{k}</em>{i}^{(1)},\\boldsymbol{k}<em>{j}^{(2)},\\boldsymbol{k}</em>{i}^{(1)}\\circ\\boldsymbol{k}<em>{j}^{(2)}),_{1\\times1}\\quad1\\times3d</em>{k}\\quad d_{k}\\times1\\quad d_{k}\\times1\\quad d_{k}\\times1\\quad d_{k}\\times1$$</p>\n<p>where $w_{A}\\in\\mathbb{R}^{3d_{k}}$ denotes a trainable vector of weights, concat() denotes vector concatenation, and 0 denotes element-wise multiplication, also known as the Hadamard product. Note that the keys of each keys matrix in this case must have the same dimension $d_{k}$ for the element wise multiplication to work.The affinity matrix can be interpreted as a similarity matrix for the columns of the two keys matrices, and helps translate, for example, image keys to the same space as the keys of the words in a sentence, and vice versa.The vectors of attention scores $e^{(1)}$ and $e^{(2)}$ can be calculated using an altered version of the additive score function as presented in (14) and (15). The previous attention score example i this survey all used a score func- fllowing feature matrices:. $\\boldsymbol{F}^{t}$ &#x3D; $[ \\boldsymbol{f}<em>{1}^{t}, \\ldots , \\boldsymbol{f}</em>{n_{f}^{t}}^{t}]$ $\\in$ $\\mathbb{R} ^{d_{f}^{t}\\times n_{f}^{t}}$ tion to calculate each attention score for each value vector individually. However, (14) and (15) are used to calculate $F^{l}$ &#x3D; $[ f_{1}^{l}, \\ldots , f_{n_{f}^{l}}^{l}]$ $\\in$ $\\mathbb{R} ^{d_{f}^{l}\\times n_{f}^{l}}.$ and $F^{r}$ &#x3D; $[ f_{1}^{r}, \\ldots , f_{n_{f}^{r}}^{r}]$ $\\in$ the complete vector of all attention scores. Essentially, the $\\mathbb{R}^{d_f^r\\times n_f^r}$, for the target phrase words, left context words, attention scores are calculated in an aggregated form.</p>\n<p>$$\\begin{gathered}<br>e^{(1)} &#x3D;:\\boldsymbol{w}<em>{1}\\times\\mathrm{act}(\\boldsymbol{W}</em>{2}\\times\\boldsymbol{K}^{(2)}\\times:\\boldsymbol{A}^{T}:+\\boldsymbol{W}<em>{1}\\times\\boldsymbol{K}^{(1)}); \\<br>1\\times n</em>{f}^{(1)} 1\\times d_{w}\\quad d_{w}\\times d_{k}^{(2)}:d_{k}^{(2)}\\times n_{f}^{(2)}:n_{f}^{(2)}\\times n_{f}^{(1)}:d_{w}\\times d_{h}^{(1)}:d_{k}^{(1)}\\times n_{f}^{(1)} \\<br>e^{(2)} &#x3D;:\\boldsymbol{w}<em>{2}\\times\\mathrm{act}(\\boldsymbol{W}</em>{1}\\times\\boldsymbol{K}^{(1)}\\times:\\boldsymbol{A}:+\\boldsymbol{W}<em>{2}\\times\\boldsymbol{K}^{(2)}), \\<br>1\\times r</em>{e}^{(2)}f 1\\times d_{w}\\quad d_{w}\\times d_{k}^{(1)}:d_{k}^{(1)}\\times n_{f}^{(1)}:n_{f}^{(1)}\\times n_{f}^{(2)}:d_{w}\\times d_{k}^{(2)}:d_{k}^{(2)}\\times n_{f}^{(2)}<br>\\end{gathered}$$</p>\n<p>where $w_1\\in\\mathbb{R}^d_u$ ， $w_2\\in\\mathbb{R}^dw$ ， $W_{1}$ $\\in$ $\\mathbb{R} ^{d_{\\omega }\\times d_{h}^{( 1) }}$ and that tanh() is used in [32] for the activation function, and the feature matrices are used as the key matrices. In that case, the affinity matrix $A$ can be seen as a translator between feature spaces. As mentioned before, the affinity matrix is essentially a similarity matrix for the key vectors of the two</p>\n<p><img src=\"/./images/fcezbCFVVAgYitIXTykvdtgkU8QRrQGOK.png\"></p>\n<p>Fig. 6. An illustration of parallel co-attention</p>\n<p>the maximum similarity value in a row or column as the</p>\n<p>$$e_{i}^{(1)}&#x3D;\\max_{j&#x3D;1,\\ldots,n_{f}^{(2)}}A_{i,j},\\quad e_{j}^{(2)}&#x3D;\\max_{i&#x3D;1,\\ldots,n_{f}^{(1)}}A_{i,j}.$$</p>\n<p>Next,the attention scores are used to calculate attention vectors $c^{(1)}$ and $c^{(2)}$ can be derived as weighted averages of the value vectors that are obtained from linearly transform ing the features. For the alignment function, [32] proposes is used in[32].[35] proposes a different way of calculatingtouse a softmaxfunction,and thevalue vectors are simply set equal to the feature vectors. The resulting context vectors can be either concatenated or added together Finally, coarse-grained and fine-grained co-attention can</p>\n<p>be combined to create an even more complex co-attention mechanism. [33] proposes the multi-grained co-attention mechanism that calculates both coarse-grained and finegrained co-attention for two inputs.Each mechanism produces one context vector per input. The four resulting context vectors are concatenated and used in the output model for prediction A mechanism separate from co-attention that still uses</p>\n<p>multiple inputs is the rotatory attention mechanism [36] This technique is typically used in a text sentiment analysis setting where there are three inputs involved: the phrase for which the sentiment needs to be determined (target phrase) the text before the target phrase (left context), and the text after the target phrase (right context). The words in these three inputs are all encoded by the feature model, producing the</p>\n<p>$d_{f}^{r}$ d rghtenete ns nesctie hee $d_{f}^{t}$, $d_{f}^{l}$, do $d_{f}^{l}$, rand corresponding inputs, and $n_{f}^{t}$, $n_{f^{\\prime }}^{l}$, n $n_{f}^{l}$, and $n_{f}^{r}$ represent the number of feature vectors for the corresponding inputs. The feature model used in[36]consists of word embeddings and separate Bi-LSTM models for the target phrase, the left context, and the right context. This means that the feature vectors arein fact thehidden statevectors obtained from the $W_{2}\\in$ $\\mathbb{R} ^{d_{w}\\times d_{k}^{( 2) }}$ are trainable weight matrices, for which Bi-LSTM models. Using these features, the idea is to extract $d_{w}$ is a prespecified dimension of the weight matrices. Note a single vector $T^*$ from the inputs such that a softmax layer can be used for classification. As such, we are now faced with two challenges: how to represent the inputs as a single vector, and how to incorporate the information from the left and right context into that vector. [36] proposes to use the rotatory attention mechanism for this purpose.</p>\n<p>Firstly, a single target phrase representation is created most importantinformation in the actual target phrase itself by using a pooling layer that takes the average over the Essentially, the mechanism rotates attention between the columns of $F^{t}$ ,as shown in (17).</p>\n<p>$$r_{<em>{d</em>{f}^{t}\\times1}}^{t}&#x3D;\\frac{1}{n_{f}^{t}}\\sum_{i&#x3D;1}^{n_{f}^{t}}f_{i}^{t}:.$$</p>\n<p>$r^{l}$ is then used as a query to create a context vector out of various scans or documents, that can provide different types the left and right contexts, separately. For example, for the left context,the key vectors $k_{1}^{l},\\ldots,k_{n_{f}^{l}}^{l}\\in\\mathbb{R}^{d_{k}^{l}}$ and value vectors $\\boldsymbol{v}<em>{1}^{l},\\ldots,\\boldsymbol{v}</em>{n_{f}^{l}}^{l}\\in\\mathbb{R}^{d_{\\nu}^{l}}$ are extracted rom the let context mages and semantic l gsimuldaneously iel aly n $d_{k}^{l}$ tire $d_{v}^{l}$ cors $f_{1}^{l},\\ldots,f_{n_{f}^{l}}^{l}\\in\\mathbb{R}^{d_{f}^{l}}$ sima ee ee e e respectively. Note that [36] proposes to use the original feature vectors as keys and values, meaning that the linear transformation consists of a multiplication by an identity matrix. Next, the scores are calculated using (18).</p>\n<p>$$e_{i}^{l}&#x3D;\\text{score}(\\begin{array}{c}r^{t}\\d_{f}^{t}\\times1\\end{array},\\begin{array}{c}k_{i}^{l}\\d_{k}^{l}\\times1\\end{array}).$$</p>\n<p>For the score function, [36] proposes to use an activated general score function [34] with a tanh activation function. The attention scores can be combined with an alignment function and the corresponding value vectors to produce the context vector $r^{l}\\in\\mathbb{R}^{d_{v}^{l}}$ The alignment function used in [36] takes the form of a softmax function. An analogous procedure can be performed to obtain the representation of the right context, $r^{T}$ .These two context representations can then be used to create new representations of the target phrase, $\\Pi\\text{思}^d_{k_0}^r$ and value vectors $\\boldsymbol{v}<em>{1}^{t}, \\ldots , \\boldsymbol{v}</em>{n_{f}^{t}}^{t}$ $\\in$ $\\mathbb{R} ^{d_{v}^{t}}$ are extracted from the target phrase feature vectors $f_{1}^{t},\\ldots,f_{n_{f}^{t}}^{t}\\in\\mathbb{R}^{d_{f}^{t}}$ 3.1.2 Feature Levels similarly as before, using a linear transformation, where $d_{k}^{t}$ and $d_{v}^{t}$ are the dimensions of the key and value vectors respectively. Note, again, that the original feature vectors as keys and values in [36]. The attention scores for the leftaware target representation are then calculated using (19).<br>$$e_{i}^{l_{t}}&#x3D;\\mathrm{score}(\\begin{array}{c}r^{l}\\d_{v}^{l}\\times1\\end{array},\\begin{array}{c}k_{i}^{t}\\d_{k}^{t}\\times1\\end{array}).$$</p>\n<p>The attention scores can be combined with an alignment function and the corresponding value vectors to produce the context vector $r^{l_t}\\in\\mathbb{R}^{d_v^t}$ . For this attention calculation, [34] proposes to use the same score and alignment functions as before. The right-aware target representation $r^{r^{\\prime}t}$ can be calculated in a similar manner. Finally, to obtain the full representation vector 7 that is used to determine the classification, the vectors $r^{l}$, $r^{r}$, $r^{l_{t}}$, $r^{l_{s}}$ p and $r^{r_t}$ are concatenated together, as shown in (20).</p>\n<p>$$r_{(d_{v}^{l}+d_{v}^{r}+d_{v}^{t}+d_{v}^{t})\\times1}&#x3D;\\mathrm{concat}(\\begin{array}{c}r^{l},\\d_{v}^{l}\\times1\\end{array},\\begin{array}{c}r^{r},\\d_{v}^{t}\\times1\\end{array},\\begin{array}{c}r^{r_{t}}\\d_{v}^{t}\\times1\\end{array}).$$</p>\n<p>To summarize, rotatory attention uses the target phrase to compute new representations for the left and right context using attention, and then uses these left and right reprephrase. The first step is designed to capture the words in the left and right contexts that are most important to the target phrase. The second step is there to capture the</p>\n<p>target and the contexts to improve the representations. There are many applications where combining informa-</p>\n<p>tion from different inputs into a single model can be highly beneficial. For example,in the field of medical data,there are often many different types of data available, such as of information. In [37], a co-attention mechanism is used for automatic medical report generation to attend to both study is used in [38] to show that the co-attention part of the model specifically improves performance. A field where multi-feature attention has been extensively explored is the domain of recommender systems.For example,in [39], a coattention network is proposed that attends to both product reviews and the reviews a user has written. In[40], a model is proposed for video recommendation that attends to both user features and video features. Co-attention techniques have also been used in combination with graph networks for the purpose of, for example, reading comprehension across multiple documents [41] and fake news detection [42]. In comparison to co-attention, rotatory attention has typically been explored only in the field of sentiment analysis, which is most likely due to the specific structure of the data that is necessary to use this technique. An implementation of rotatory attention is proposed in [43] for sentiment analysis, where the mechanism is extended by repeating the attention again, using attention. Firstly,the key vectors $k_{1}^{t},\\ldots,k_{n_{f}^{t}}^{t}\\in$ wotationto mectively furthen dep byepe repgese aateons</p>\n<p>The previously discussed attention mechanisms process. data at a single level. We refer to these attention techniques as single-level attention mechanisms. However, some data. types can be analyzed and represented on multiple levels. For example, when analyzing documents, one can analyze the document at the sentence level, word level,or even the character level. When representations or embeddings of all these levels are available, one can exploit the extra levels of information. For example, one could choose to perform translation based on either just the characters, or just the words of the sentence. However, in [44], a technique named attention-via-attention is introduced that allows one to incorporate information from both the character, and the word levels. The idea is to predict the sentence translation character-by-character, while also incorporating information from a word-level attention module. To begin with, a feature model (consisting of, for ex</p>\n<p>ample,word embeddings and RNNs) is used to encode the input sentence into both a character-level feature matrix $\\dot{F^{( c) }}$ $\\in$ $\\mathbb{R} ^{d_{f}^{( \\mathrm{c} ) }\\times n_{f}^{( \\mathrm{c} ) }}$ and a word-level feature matrix $F^{( w) }\\in$ $\\mathbb{R} ^{d_{f}^{( w) }\\times n_{f}^{( w) }}$ where $d_{f}^{(c)}$ and $n_{f}^{(c)}$ represent, respec tively, the dimension of the embeddings of the characters $d_{\\int}^{(w)}$ and $n_{f}^{(w)}$ reee that each level in the data can be represented or embedded When attempting to predict a character in the translated</p>\n<p><img src=\"/./images/fSEFcGafUNBvMGze5Vma05K9gNTsnQ3cq.png\"></p>\n<p>Fig. 7. An illustration of attention-via-attention.</p>\n<p>sentence, a query $q^{(c)}\\in\\mathbb{R}^dq$ is created by the query model (like a character-level RNN), where $d_{q}$ is the dimension of the query vectors. As illustrated in Fig. 7, the query is used tocaleulate tentononthe corod $c^{(w)}\\in\\mathbb{R}^{d^{(w)}}$ etors $F^{(w)}$ $d_v^{(w)}$ ate ion module ta esentily suemarieste entencec</p>\n<p>Iee ne eien e e ee et e $d_{f}^{(S)}$ etreate $F^{(s)}\\in\\mathbb{R}^{d_f^{(S)}\\times n_s}$ fore $s&#x3D;1,\\ldots,n_{S}$, where which words contain the most important information for predicting the next character. If we know which words are most important,then it becomes easier to identify which characters in the input sentence are most important. Thus, the next step is to attend to the character-level features in $F^{(c)}$ , with an additional query input: the word-level context vector $c^{(w)}$ . The actual query input for the attention model will therefore be the concatenation of the query $q^{(c)}$ and the are essentially su-amaries of the words in the sentences. The word contex vector $c^{(w)}$ The ou tput of $c^{(c)}$ character-level matrix of context vectors $C&#x3D;[\\boldsymbol{c}^{(1)},\\ldots,\\boldsymbol{c}^{(n_S)}]\\in\\mathbb{R}^{d_v^{(S)}\\times n_S}$ context output of the attention model is the concatenation of the word-level, and character-level context vectors The attention-via-attention technique uses representa</p>\n<p>not always be available for each level of the data, or it may be desirable to let the model create the representations during the process by building them from lower level representations. A technique referred to as hierarchical attention [5] can be used in this situation. Hierarchical attention is another technique that allows one to apply attention on different levels of the data.Yet, the exact mechanisms work quite differently compared to attention-via-attention. The idea is to start at the lowest level, and then create representations, or summaries, of the next level using attention. This process is repeated till the highest level is reached. To make this a little clearer, suppose one attempts to create a model for document classification, similarly to the implementation from [5]. We analyze a document containing $7lS$ sentences, with the sth sentence containing $n_{s}$ words, for $s$ &#x3D; $l, \\ldots , n_{S}$ . One could use attention based on just the collection of words to classify the document. However a significant amount of important context is then left out of the analysis, since the model will consider all words as a single long sentence, and will therefore not consider the context within the separate sentences.Instead, one can use the hierarchical structure of a document (words form sentences, and sentences form the document) Fig. 8 illustrates the structure of hierarchical attention</p>\n<p>For each sentence in the document,a sentence representa tion $\\boldsymbol{c}^{(s)}\\in\\mathbb{R}^{d_{v}^{(S)}}$ is produced, for $s&#x3D;1,\\ldots,n_{S}$, where $d_v^{(S)}$ is the dimension of the value vectors used in the attention model for sentence representations (Attention Modules in</p>\n<p><img src=\"/./images/fLQHc0WG3s8qDYeSbBDR2XDFQUQhGQiKx.png\"></p>\n<p>Fig. 8. An illustration of hierarchical attention</p>\n<p>Fig. 8). The representation is a context vector from an word, and $Tl_{8}$ represents the amount of words in sentence S For extra clarification, the columns of $F^{(s)}$ are feature vectors that correspond to the words in sentence 5 .As shown in Fig. 8, each feature matrix $F^{(s)}$ is used as input for an atten tion model,which produces the context vector $c^{(s)}$ , for each $s&#x3D;1,\\ldots,n_{S}$ . No queries are used in this step, so it can be considered a self-attentive mechanism. The context vectors is constructed by grouping all the obtained context vectors together as columns. Finally, attention is calculated using $C$ as feature input, producing the representation of the entire tons oitetochis adoent in thonte vector $\\boldsymbol{c}^{(D)}\\in\\mathbb{R}^{d_{v}^{(D)}}$ where $d_{v}^{(D)}$ is the dimension of the value vectors in the attention model for document representation (Attention Modulep in Fig. 8) This context vector can be used to classify the document, since it is essentially a summary of all the sentences (and therefore also the words)in the document Multi-level models can be used in a variety of tasks.</p>\n<p>For example, in [28], hierarchical attention is used in a recommender system to model user preferences at the longterm level and the short-term level. Similarly, [45] proposes a hierarchical model for recommending social media images based on user preferences. Hierarchical attention has also been successfully applied in other domains.For example [46] proposes to use hierarchical attention in a video action recognition model to capture motion information at the the long-term level and the short-term level. Furthermore, [47] proposes a hierarchical attention model for cross-domain. sentiment classification. In [48], a hierarchical attention model for chatbot response generation is proposed. Lastly using image data,[49] proposes a hierarchical attention model for crowdcounting</p>\n<h3 id=\"3-1-3Feature-Representations\"><a href=\"#3-1-3Feature-Representations\" class=\"headerlink\" title=\"3.1.3Feature Representations\"></a>3.1.3Feature Representations</h3><p>In a basic attention model, a single embedding or representation model is used to produce feature representa tions for the model to attend to.This is referred to as single-representational attention. Yet, one may also opt to incorporate multiple representations into the model. In. [50], it is argued that allowing a model access to multiple</p>\n<p>TABLE 2 Overview of score function (score $(\\boldsymbol{q},\\boldsymbol{k}_l)]$ forms.</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Function</th>\n<th>Parameters</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Additive (Concatenate) [3]</td>\n<td>$w \\times act(W_1 q + W_2 k) + b$</td>\n<td>$w \\in \\mathbb{R}^{d \\times d}$, $W_1 \\in \\mathbb{R}^{d \\times d \\times k}$, $W_2 \\in \\mathbb{R}^{d \\times d \\times k}$, $b \\in \\mathbb{R}^d$</td>\n</tr>\n<tr>\n<td>Multiplicative (Dot-Product) [4]</td>\n<td>$q \\times k_i$</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Scaled Multiplicative [13]</td>\n<td>$\\frac{k_i}{d} \\times W \\times q$</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Generalized [4]</td>\n<td>$k_i &#x2F; (d \\times W \\times q)$</td>\n<td>$W \\in \\mathbb{R}^{d \\times d \\times k}$</td>\n</tr>\n<tr>\n<td>Biased General [54]</td>\n<td>$k_i&#x2F;(W \\times x \\times q) + b$</td>\n<td>$W \\in \\mathbb{R}^{d \\times d \\times k}$, $x \\in \\mathbb{R}^{d \\times d \\times k}$, $b \\in \\mathbb{R}^d$</td>\n</tr>\n<tr>\n<td>Activated General [34]</td>\n<td>$act(k_i&#x2F;(W \\times x \\times q))$</td>\n<td>$W \\in \\mathbb{R}^{d \\times d \\times k}$, $x \\in \\mathbb{R}^{d \\times d \\times k}$, $b \\in \\mathbb{R}^d$</td>\n</tr>\n<tr>\n<td>Similarity [55]</td>\n<td>$q \\times k_i$</td>\n<td>-</td>\n</tr>\n</tbody></table>\n<p>embeddings can allow one to create even higher quality representations. Similarly, [51] incorporates multiple representations of the same book (textual, syntactic, semantic visual etc.) into the feature model. Feature representations are an important part of the attention model, but attention can also be an important part of the feature model. The idea is to create a new representation by taking a weighted average of multiple representations, where the weights are determined via attention.This technique is referred to as multi-representational attention, and allows one to create so-called meta-embeddings. Suppose one wants to create a meta-embedding for a word 2 for which $E$ embeddings $\\boldsymbol{x}^{(e_1)},\\ldots,\\boldsymbol{x}^{(e_E)}$ are available. Each embedding $x^{(e_{i})}$ is of size $d_{e_{i}t}$ for $i&#x3D;1,\\ldots,E$ Since not all embeddings are of the same size,a transformation is performed to normalize the embedding dimensions. Using embedding-specific weight parameters, each embedding $x^{(e_i)}$ is transformed into the size-normalized embedding $x^{(t_{i})}\\in\\mathbb{R}^{d_{t}}$ ,where $d_{t}$ is the size of every transformed word embedding, as shown in (21).<br>$$x^{(t_{i})}&#x3D;W_{e_{i}}\\times x^{(e_{i})}+b_{e_{i}},\\d_{t}\\times1\\quad d_{t}\\times d_{e_{i}}\\quad d_{e_{i}}\\times1\\quad d_{t}\\times1$$</p>\n<p>where $W_{e_{i}}$ $\\in$ $\\mathbb{R} ^{d_{t}\\times d_{e_{i}}}$ and $b_{e_i}\\in\\mathbb{R}^{d_t}$ are trainable, embedding-specific weights matrices. The final embedding. $x^{(e)}\\in\\mathbb{R}^{d_t}$ is a weighted average of the previously calculated transformed representations, as shown in (22)</p>\n<p>$$\\boldsymbol{x}<em>{d_t\\times1}^{(e)}&#x3D;\\sum</em>{i&#x3D;1}^Ea_i\\times\\boldsymbol{x}_{d_t\\times1}^{(t_i)}.$$</p>\n<p>The final representation $x^{(e)}$ can be interpreted as the context vector from an attention model,meaning that the weights $a_1,\\ldots,a_E\\in\\mathbb{R}^1$ are attention weights. Attention can be calculated as normally, where the columns of the features matrix $F$ are the transformed representations $\\boldsymbol{x}^{(t_1)},\\ldots,\\boldsymbol{x}^{(t_E)}$ Thequery in this case can be ignored since it is constant in all cases. Essentially, the query is “Which representations are the most important?”” in every situation. As such, this is a self-attentive mechanism While an interesting idea, applications of multi-</p>\n<p>representational attention are limited. One example of the application of this technique is found in[52], where a multirepresentational attention mechanism has been applied to generate multi-lingual meta-embeddings. Another example is [53], where a multi-representational text classification embeddings from part-of-speech tagging, named entity rec ognizers, and character-level and word-level embeddings.</p>\n<h3 id=\"3-2-General-Attention-Mechanisms\"><a href=\"#3-2-General-Attention-Mechanisms\" class=\"headerlink\" title=\"3.2 General Attention Mechanisms\"></a>3.2 General Attention Mechanisms</h3><p>can be applied in any type of attention model. The structure of this component can be broken down into the following sub-aspects: the attention score function, the attention alignment, and attention dimensionality</p>\n<h2 id=\"3-2-1Attention-Scoring\"><a href=\"#3-2-1Attention-Scoring\" class=\"headerlink\" title=\"3.2.1Attention Scoring\"></a>3.2.1Attention Scoring</h2><p>The attention score function is a crucial component in how attention is calculated. Various approaches have been developed that each have their own advantages and disadvantages.An overview of these functions is provided in Table 2.</p>\n<p>Each row of Table 2 presents a possible form for the function $\\operatorname{score}(\\boldsymbol{q},\\boldsymbol{k}<em>l)$ , as seen in (23), where $q$ is the query vector, and $k</em>{l}$ is thelth column of $K$ Note that the score functions presented in this section can be more efficiently calculated in matrix form using $K$ instead of each column separately Nevertheless, the score functions are presented using $k_{I}$ to more clearly illustrate the relation between a key and query.</p>\n<p>$$e_l&#x3D;\\text{score}(\\begin{array}{c}q\\d_q\\times1\\end{array},\\begin{array}{c}k_l\\d_k\\times1\\end{array}).$$</p>\n<p>Due to their simplicity, the most popular choices for the score function are the concatenate score function[3] and the multiplicative score function [4]. The multiplicative score function has the advantage of being computationally inexpensive due to highly optimized vector operations. How. ever, the multiplicative function may produce non-optimal results when the dimension $d_{k}$ is too large [56]. When $d_{k}$ is large, the dot-product between $q$ and $k_{I}$ can grow large in magnitude.To illustrate this, in [13], an example is used where the elements of $q$ and $k_l$ are all normally distributed with a mean equal to zero, and a variance equal to one. Then, the dot-product of the vectors has a variance of $d_{k}$ A higher variance means a higher chance of numbers that are large in magnitude. When the softmax function of the alignment step is then applied using these large numbers, the gradient will become very small, meaning the model will have trouble converging [13]. To adjust for this, [13] proposes to scale the multiplicative function by the factor mode is eaepesed t at nerpeate dferent eesene $\\frac{1}{\\sqrt{d_{k_{k}}}}$ prod ucing the caled utiplictiescore nc en </p>\n<p>introducing a weights matrix $W$ .This form, referred to as the general score function, allows for an extra transformation of $k_l$ .The biased general score function [54]is a further extension of the general function that introduces This major category consists of attention mechanisms that a bias weight vector $b$ A final extension on this function named the activated general score function is introduced in [34], and includes the use of both a bias weight $b$, and an activation function act(). The previously presented score functions are all based on</p>\n<p>determining a type of similarity between the key vector and the query vector. As such, more typical similarity measures such as the Euclidean $(\\mathrm{L}_{2})$ distance and cosine similarity can also be implemented [55]. These scoring methods are summarized under the similarity score function which is represented by the similarity() function</p>\n<p>There typically is no common usage across domains regarding score functions. The choice of score function for a particular task is most often based on empirical experiments. However, there are exceptions when, for example efficiency is vital. In models where this is the case, the multiplicative or scaled multiplicative score functions are typi-However, a compromise can be made in certain situations cally the best choice.An example of this is the TransformerLocal alignment [4] is a method that implements a softmax model, which is generally computationally expensive.</p>\n<h3 id=\"3-2-2Attention-Alignment\"><a href=\"#3-2-2Attention-Alignment\" class=\"headerlink\" title=\"3.2.2Attention Alignment\"></a>3.2.2Attention Alignment</h3><p>The attention alignment is the step after the attention scoring. This alignment process directly determines which parts of the input data the model will attend to. The alignment function is denoted as align() and has various forms. The align() function takes as input the previously calculated attention score vector $e$ and calculates for each element $el$ of e the attention weight $u_{l}$ .These attention weights can then average of the value vectors $v_{1},\\cdots,v_{n_{f}}$</p>\n<p>$$\\boldsymbol{c}<em>{d_v\\times1}&#x3D;\\sum</em>{l&#x3D;1}^{n_f}a_l:\\times:\\boldsymbol{v}_l:.$$</p>\n<p>The most popular alignment method to calculate these weights is a simple softmax function, as depicted in (25)</p>\n<p>$$\\underset{1\\times1}{a_l}&#x3D;\\underset{1\\times1}{\\text{align}}(\\underset{1\\times1}{e_l}:;:\\boldsymbol{e}:)&#x3D;\\frac{\\exp(e_l)}{\\sum_{j&#x3D;1}^{n_f}\\exp(e_j)}.$$</p>\n<p>This alignment method is often referred to as soft alignment in computervision settings [8],or global alignment for sequence data [4]. Nevertheless, both these terms represent the same function and can be interpreted similarly. Soft&#x2F;global alignment can be interpreted as the model attending to all feature vectors.For example, the model attends to all regions in an image, or all words in a sentence. Even though the attention model generally does focus more on specific parts of the input, every part of the input will receive at least some amount of attention due to the nature of the softmax function. Furthermore, an advantage of the softmax function. is that it introduces a probabilistic interpretation to the input vectors. This allows one to easily analyze which parts of the input are important to the output predictions In contrast to soft&#x2F;global alignment, other methods aim</p>\n<p>to achieve a more focused form of alignment. For example hard alignment [8], also known as hard attention or nondeterministic attention, is an alignment type that forces the attention model to focus on exactly one feature vector. Firstly, this method implements the softmax function in the exact same way as global alignment. However, the outputs $a_{1},\\cdots,a_{n_{f}}$ are not used as weights for the context vector calculation. Instead, these values are used as probabilities to draw the choice of the one value vector from.A value $m\\in\\mathbb{R}^1$ is drawn from a multinomial distribution with $a_{1},\\cdots,a_{n_{f}}$ as parameters for the probabilities. Then, the context vector is simply defined as follows:</p>\n<p>$$\\begin{array}{c}c\\d_v\\times1\\end{array}&#x3D;\\begin{array}{c}v_m\\d_v\\times1\\end{array}.$$</p>\n<p>Hard alignment is typically more efficient at inference compared to soft alignment.On the other hand, the main disadvantage of hard attention is that, due to the stochastic</p>\n<p>alignment of attention, the training of the model cannot be done via the regular backpropagation method. Instead simulation and sampling, or reinforcement learning [57] are required to calculate the gradient at the hard attention layer. As such, soft&#x2F;global attention is generally preferred. distribution, similarly to soft&#x2F;global alignment. But, the. softmax distribution is calculated based only on a subset of the inputs. This method is generally used in combination with sequence data.One has to specify a variable $p\\in\\mathbb{R}^{1}$ that determines the position of the region.Feature vectors close to $p$ will be attended to by the model, and vectors too far from $P$ will be ignored. The size of the subset will be determined by the variable $D\\in\\mathbb{R}^{1}$ . Summarizing the attention model will apply a softmax function on the attention scores in the subset $[p-D,p+D]$ . In other words be used to create the context vector C by taking a weighteda window is placed on the input and soft&#x2F;global attention is calculated within that window：<br>$$a_{l}&#x3D;\\text{align}(\\begin{smallmatrix}e_{l}&amp;;&amp;e\\1\\times1&amp;n_{f}\\times1\\end{smallmatrix})&#x3D;\\frac{\\exp(e_{l})}{\\sum_{j&#x3D;p-D}^{p+D}\\exp(e_{j})}.$$</p>\n<p>The question that remains is how to determine the location parameter $P$ The first method is referred to as monotonic alignment. This straightforward method entails simply setting the location parameter equal to the location of the prediction in the output sequence.Another method of deter mining the position of the region is referred to as predictive alignment. As the name entails, the model attempts to actually predict the location of interest in the sequence</p>\n<p>$$p:&#x3D;:S:\\times\\mathrm{sigmoid}(\\boldsymbol{w}<em>{1\\times d</em>{p}}^{T}:\\times\\mathrm{tanh}(\\boldsymbol{W}_{p}:\\times:\\boldsymbol{q}:)),$$</p>\n<p>where $S\\in\\mathbb{R}^{1}$ is the length of the input sequence, and $w_{p}\\in\\mathbb{R}^{d_{P}}$ and $W_{p}$ $\\in$ $\\mathbb{R} ^{d_{p}\\times d_{q}}$ are both trainable weights parameters. The sigmoid function multiplied by $S$ makes sure that $p$ is in the range $[0,S]$ .Additionally, in [4], it is recommended to add an additional term to the alignment function to favor alignment around $P$</p>\n<p>$$a_{l}&#x3D;\\operatorname{align}(\\begin{smallmatrix}e_{l}&amp;;&amp;e\\1\\times1&amp;n_{f}\\times1\\end{smallmatrix})\\exp(-\\frac{(l-p)^{2})}{2\\sigma^{2}}),$$</p>\n<p>where $\\sigma\\in\\mathbb{R}^{1}$ is empirically set equal to $\\frac{D}{2}$ according to [4]. Another proposed method for compromising between soft and hard alignment is reinforced alignment[58]. Similarly to local alignment, a subset of the feature vectors is determined, for which soft alignment is calculated. However, instead of using a window to determine the subset, reinforced alignment uses a reinforcement learning agent. [57], similarly to hard alignment, to choose the subset of feature vectors. The attention calculation based on these chosen feature vectors is the same as regular soft alignment Soft alignment is often regarded as the standard align</p>\n<p>ment function for attention models in practically every domain.Yet, the other alignment methods have also seen inter esting uses in various domains. For example, hard attention is used in [59] for the task of visual question answering In [60], both soft and hard attention are used in a graph attention model for multi-agent game abstraction. Similarly. in [61], both global and local alignment are used for review rating predictions. Reinforced alignment has been employed</p>\n<p>in combination with a co-attention structure in [62] for the extended with the multi-dimensional attention mechanisn task of aspect sentiment classification. In [63], reinforced alignment is used for the task of person re-identification using surveillance images</p>\n<h3 id=\"3-2-3-Attention-Dimensionality\"><a href=\"#3-2-3-Attention-Dimensionality\" class=\"headerlink\" title=\"3.2.3 Attention Dimensionality\"></a>3.2.3 Attention Dimensionality</h3><p>All previous model specifications of attention use a scalar weight $a_l$ for each value vector $v_l$ . This technique is referred to as single-dimensional attention. However, instead of determining a single attention score and weight for the entire vector, [64] proposes to calculate weights for every single feature in those vectors separately. This technique is referred to as multi-dimensional attention,since the attention weights now become higher dimensional vectors. The idea is that the model no longer has to attend to entire vectors, but it can instead pick and choose specific elements from those vectors. More specifically, attention is calculated for each dimension.As such,the model must create a vecto of attention weights $a_l\\in\\mathbb{H}^d_v$ for each value vector $v_l\\in\\mathbb{R}^dv$ The context vector can then be calculated by summing the element-wise multiplications (o) of the value vectors $\\boldsymbol{v}<em>1,\\ldots,\\boldsymbol{v}</em>{n_f}\\in\\mathbb{R}^d$, and the corresponding attention weight vectors $a_{1},\\ldots,a_{n_{f}}\\in\\mathbb{R}^{d_{\\upsilon}}$ as follows:</p>\n<p>$$\\boldsymbol{c}<em>{d_v\\times1}&#x3D;\\sum</em>{l&#x3D;1}^{n_f}\\boldsymbol{a}_l\\circ\\boldsymbol{v}_l.$$</p>\n<p>However, since one needs to create attention weight vectors, this technique requires adjusted attention score and weight calculations. For example, the concatenate score function found in Table 2 can be adjusted by changing the $w\\in\\mathbb{R}^{d_{\\cos}}$ weights vector to the weight matrix $W_{d}\\in\\mathbb{R}^{d_{w}\\times d_{v}}$</p>\n<p>$$e_{l}:&#x3D;:W_{d}^{T}:\\times\\mathrm{act}(:W_{1}:\\times:q:+:W_{2}:\\times:k_{l}:+:b:).$$</p>\n<p>This new score function produces the attention score vectors $\\boldsymbol{e}<em>1,\\ldots,\\boldsymbol{e}</em>{n_f}\\in\\mathbb{R}^dv$ . These score vectors can be combined into a matrix of scores $e$ &#x3D; $[ \\boldsymbol{e}<em>{1}, \\ldots , \\boldsymbol{e}</em>{n_{f}}]$ $\\in$ $\\mathbb{R} ^{d_{v}\\times n_{f}}$ To produce multi-dimensional attention weights, the alignment function stays the same, but it is applied for each feature across the attention score columns. To illustrate, when implementing soft attention, the attention weight produced from the ith element of score vector $e_{l}$ is defined as follows:</p>\n<p>$$\\underset{1\\times1}{a_{l,i}}&#x3D;\\underset{1\\times1}{\\text{align}}(e_{l,i};\\quad\\boldsymbol{e}<em>{d</em>{v}\\times n_{f}})&#x3D;\\frac{\\exp(e_{l,i})}{\\sum_{j&#x3D;1}^{n_{f}}\\exp(e_{j,i})},$$</p>\n<p>where $el,i$ represents the ith element of score vector $e_{l,}$ and $ul,i$ is the ith element of the attention weights vector $a_{l}$ .Finally, these attention weight vectors can be used to compute the context vector as presented in (30). Multi-dimensional attention is a very general mecha</p>\n<p>nism that can be applied in practically every attention model, but actual applications of the technique have been relatively sparse. One application example is [65], where multi-dimensional attention is used in a model for named entity recognition based on text and visual context from multimedia posts. In [66], multi-dimensional attention is used in a model for answer selection in community question answering. In [67], the U-net model for medical image segmentation is extended with a multi-dimensional attention mechanism. Similarly, in [68], the Transformer model is</p>\n<p>for the task of dialogue response generation. In [69], multidimensional attention is used to extend graph attention networks for dialogue state tracking. Lastly, for the task of next-item recommendation, [70] proposes a model that incorporates multi-dimensional attention</p>\n<h3 id=\"3-3-Query-Related-Attention-Mechanisms\"><a href=\"#3-3-Query-Related-Attention-Mechanisms\" class=\"headerlink\" title=\"3.3 Query-Related Attention Mechanisms\"></a>3.3 Query-Related Attention Mechanisms</h3><p>Queries are an important part of any attention model, since they directly determine which information is extracted from the feature vectors. These queries are based on the desired output of the task model, and can be interpreted as literal questions. Some queries have specific characteristics that require specific types of mechanisms to process them. As such, this category encapsulates the attention mechanisms that deal with specific types of query characteristics. The mechanisms in this category dealwith one of the two following query characteristics: the type of queries or the multiplicity of queries</p>\n<h2 id=\"3-3-1Type-of-Queries\"><a href=\"#3-3-1Type-of-Queries\" class=\"headerlink\" title=\"3.3.1Type of Queries\"></a>3.3.1Type of Queries</h2><p>Different attention models employ attention for different purposes, meaning that distinct query types are necessary There are basic queries, which are queries that are typically straightforward to definebased on the data and model.For example, the hidden state for one prediction in an RNN is often used as the query for the next prediction. One could also use a vector of auxiliary variables as query. For example, when doing medical image classification, general patient characteristics can be incorporated into a query Some attention mechanisms, such as co-attention, rota-</p>\n<p>tory attention, and attention-over-attention, use specialized queries. For example, rotatory attention uses the context vector from another attention module as query, while interactive co-attention uses an averaged keys vector based on another input. Another case one can consider is when attention is calculated based purely on the feature vectors This concept has been mentioned before and is referred to as self-attention or intra-attention [71]. We say that the models use self-attentive queries. There are two ways of interpreting such queries. Firstly, one can say that the query is constant. For example, document classification requires only a single classification as the output of the model. As such, the query is always the same, namely: “What is the class of the document?”. The query can be ignored and attention can be calculated based only on the features themselves Score functions can be adjusted for this by making the query vector a vector of constants or removing it entirely:<br>$$\\mathrm{score}(\\boldsymbol{k}<em>{l})&#x3D;:\\boldsymbol{w}^{T}\\times\\mathrm{act}(\\boldsymbol{W}\\times:\\boldsymbol{k}</em>{l}+\\boldsymbol{b}:).\\d_{k}\\times1\\quad1\\times d_{w}\\quad d_{w}\\times d_{k}\\quad d_{k}\\times1\\quad d_{w}\\times1$$</p>\n<p>Additionally, one can also interpret self-attention as learning the query along the way, meaning that the query can be defined as a trainable vector of weights.For example,the dot-product score function may take the following form</p>\n<p>$$\\begin{array}{rcl}\\mathrm{score}(\\boldsymbol{k}_l)&amp;&#x3D;&amp;\\boldsymbol{q}^T\\times\\boldsymbol{k}_l\\d_k\\times1&amp;&amp;1\\times d_k&amp;d_k\\times1\\end{array},$$</p>\n<p>where $q\\in\\mathbb{R}^{d_k}$ is a trainable vector of weights. One could also interpret vector $b\\in\\mathbb{R}^{d_{w}}$ as the query in (33).Another</p>\n<p>use of self-attention istouncover the relations between the feature vectors $f_{1},\\ldots,f_{n_{f}}$ . These relations can then be used as additional information to incorporate into new representations of the feature vectors. With basic attention mechanisms, the keys matrix $K$, and the values matrix $V$ are extracted from the features matrix $F_{\\prime}$ while the query $q$ is produced separately. For this type of self-attention, the query vectors are extracted in a similar process as the keys and values, via a transformation matrix of trainable weights $W_{Q}\\in\\mathbb{R}^{d_{q}\\times d_{f}}$ . We define the matrix $Q&#x3D;\\left[\\boldsymbol{q}<em>{1},\\ldots,\\boldsymbol{q}</em>{n_{f}}\\right]\\in$ $\\Pi$玻 $d_q\\times nf$ which canbe obtained asfollows:</p>\n<p>$$Q_{d_{q}\\times n_{f}}&#x3D;W_{Q}\\times F_{d_{f}\\times n_{f}}.$$</p>\n<p>Each column of $Q$ can be used as the query for the attention model. When attention is calculated using a query $q.$ the resulting context vector C will summarize the information the query, or a column of $Q$ , is now also a feature vector representation, the context vector contains the information of allfeature vectors that are important to that specific feature vector. In other words, the context vectors capture the relations between the feature vectors. For example, selfattention allows one to extract the relations between words: which verbs refer to which nouns, which pronouns refer to which nouns, etc. For images, self-attention can be used to determine which image regions relate to each other While self-attention is placed in the query-related cat-</p>\n<p>egory, it is also very much related to the feature model. Namely, self-attention is a technique that is often used in the feature model to create improved representations of the feature vectors. For example, the Transformer model for language processing [13], and the Transformer model for image processing [15], both use multiple rounds of (multihead) self-attention to improve the representation of the feature vectors. The relations captured by the self-attention mechanism are incorporated into new representations. A. simple method of determining such a new representation is to simply set the feature vectors equal to the acquired self-attention context vectors [71], as presented in (36).</p>\n<p>$$f_{_{d_f\\times1}}^{(\\mathrm{new})}&#x3D;\\underset{d_f\\times1}{\\operatorname*{c}},$$</p>\n<p>where $f^{(\\mathrm{new})}$ is the updated feature vector. Another possibility is to add the context vectors to the previous feature vectors with an additional normalization layer [13]:</p>\n<p>$$f_{d_f\\times1}^{(\\mathrm{new})}&#x3D;\\mathrm{Normalize}(f_{d_f\\times1}^{(\\mathrm{old})}+\\sum_{d_f\\times1}),$$</p>\n<p>where $f^{(\\mathrm{old})}$ is the previous feature vector, and Normalize( is a normalization layer[72]. Using such techniques, self embeddings that enhance model accuracy [71]. Self-attention is arguably one of the more important</p>\n<p>types of attention, partly due to its vital role in the highly popular Transformer model. Self-attention is a very general mechanism and can be applied to practically any problem. As such, self-attention has been extensively explored in many different fields in both Transformer-based architec tures and other types of models. For example, in [73], selfattention is explored for image recognition tasks, and results</p>\n<p>indicate that the technique may have substantial advantages with regards to robustness and generalization. In [74], selfattention is used in a generative adversarial network (GAN) [75] to determine which regions of the input image to focus on when generating the regions of a new image. In [76], selfattention is used to design a state-of-the-art medical image segmentation model. Naturally, self-attention can also be used for video processing. In [77], a self-attention model is proposed for the purpose of video summarization that reaches state-of-the-art results. In other fields, like audio processing, self-attention has been explored as well. In [78], self-attention is used to create a speech recognition model Self-attention has also been explored in overlapping domains. For example, in [79], the self-attention Transformer architecture is used to create a model that can recognize phrases from audio and by lip-reading from a video. For the problem of next item recommendation, [80] proposes in the feature vectors that is important to the query. Sincea Transformer model that explicitly captures item-item re lations using self-attention. Self-attention also has applica-. tions in any natural language processing fields. For example, in [81], self-attention is used for sentiment analysis. Selfattention is also highly popular for graph models. For example, self-attention is explored in [82] for the purpose of representation learning in communication networks and rating networks. Additionally, the first attention model for graph networks was based on self-attention [83]</p>\n<h3 id=\"3-3-2Multiplicity-ot-Queries\"><a href=\"#3-3-2Multiplicity-ot-Queries\" class=\"headerlink\" title=\"3.3.2Multiplicity ot Queries\"></a>3.3.2Multiplicity ot Queries</h3><p>In previous examples, the attention model generally used a single query for a prediction.We say that such models use singular query attention. However, there are attention architectures that allow the model to compute attention using multiple queries. Note that this is different from, for example, an RNN that may involve multiple queries to produce a sequence of predictions. Namely, such a model still requires only a single query per prediction One example of a technique that incorporates multiple</p>\n<p>queries is multi-head attention [13], as presented in Fig. 9. Multi-head attention works by implementing multiple attention modules in parallel by utilizing multiple different versions of the same query. The idea is to linearly transform the query $q$ using different weight matrices. Each newly formed query essentially asks fora different type of relevant information, allowing the attention model to introduce more information into the context vector calculation. An attention model implements $d\\geq1$ heads with each attention head having its own query vector, keys matrix, and values matrix $q^{(j)}$ ， $K^{(j)}$ and $V^{(j)}$ ，for $j$ &#x3D; $1, \\ldots , d$ The query $q^{(j)}$ is obtained by linearly transforming the original query $\\Psi$, while the matrices $K^{(j)}$ and $V^{(j)}$ are obtained through linear transformations of $F$ .As such,each attention head attention has been used to create improved word or sentence has its own learnable weights matrices $W_{q}^{(j)}$ ” $W_{K}^{(j)}$ and $W_{V}^{(j)}$ for these transformations.The calculation of the query keys, and values for the $j$ th head are defined as follows:<br>$$\\boldsymbol{q}<em>{d</em>{q}\\times1}^{(j)}&#x3D;\\boldsymbol{W}<em>{q}^{(j)}\\times\\boldsymbol{q}</em>{d_{q}\\times1},\\quad\\boldsymbol{K}<em>{d</em>{k}\\times n_{f}}^{(j)}&#x3D;\\boldsymbol{W}<em>{K}^{(j)}\\times\\boldsymbol{F}</em>{d_{f}\\times n_{f}},\\\\boldsymbol{V}<em>{d</em>{v}\\times n_{f}}^{(j)}&#x3D;\\boldsymbol{W}<em>{V}^{(j)}\\times\\boldsymbol{F}</em>{d_{f}\\times n_{f}}.$$</p>\n<p>Thus,each head creates its own representations of the query $q$, and the input matrix $F$ Each head can therefore</p>\n<p><img src=\"/./images/fLxYvsz8GoGucwnBRCXegWEcG76geRpkm.png\"></p>\n<p>Fig. 9. An illustration of multi-head attention</p>\n<p>learn to focus on different parts of the inputs, allowing the model to attend to more information.For example when training a machine translation model, one attention head can learn to focus on which nouns (e.g., student, car, apple) do certain verbs (e.g., walking, driving, buying) refer to,while another attention head learns to focus on which $\\boldsymbol{e}^{( j) }$ &#x3D; $[ e_{1}^{( j) }, \\ldots , e_{n_{f}}^{( j) }]$ $\\in$ $\\mathbb{R} ^{n_{f}}$ ae $q^{(s)}$ Pr $q^{(0)}$ A $S$, Cat $q^{(s+1)}$ of ttention weights $\\boldsymbol{a}^{( j) }$ &#x3D; $[ a_{1}^{( j) }, \\ldots , a_{n_{f}}^{( j) }]$ $\\in$ $\\mathbb{R} ^{n_{f}}$ As canpossibly using the current context vector $c^{(s)}$ as another be expected, each attention model produces its own context vector $\\boldsymbol{c}^{(j)}\\in\\mathbb{R}^{dv}$ ,as follows:</p>\n<p>$$\\boldsymbol{c}<em>{d</em>{v}\\times1}^{(j)}&#x3D;\\sum_{l&#x3D;1}^{n_{f}}a_{l}^{(j)}\\times\\boldsymbol{v}_{l}^{(j)}.$$</p>\n<p>The goal is still to create a single context vector as output of the attention model.As such, the context vectors produced by the individual attention heads are concatenated into a single vector. Afterwards, a linear transformation is applied using the weight matrix $W_{O}\\in\\mathbb{R}^{d_{c}\\times d_{v}d}$ tomake sure the resulting context vector $c\\in\\mathbb{R}^{d_{c}}$ has the desired dimension. This calculation is presented in (40). The dimension $d_{c}$ can be pre-specified by, for example, setting it equal to $d_{v,}$ so that the context vector dimension is unchanged.</p>\n<p>$$c:&#x3D;:W_{O}:\\times\\mathrm{concat}(\\boldsymbol{c}^{(1)},…,\\boldsymbol{c}^{(d)}).$$</p>\n<p>Multi-head attention processes multiple attention modmented sequentially to iteratively adjust the context vec tors.Each of these attention modules are referred to as “repetitions” or “rounds” of attention. Such attention architectures are referred to as multi-hop attention models also known as multi-step attention models.An important note to consider is the fact that multi-hop attention tant mote to con sider is be ap thas m unt hap ustention she that the actunl o $c^{(s)}$ ind $q^{(s+1)}$ the atentoen noedelin throughout various works. While the mechanism always involves multiple rounds of attention, the multi-hop implementation proposed in [84] differs from the mechanism proposed in[85]or [86]. Another interesting example is [87], where a “multi-hop”attention model is proposed that would actually be considered alternating co-attention in this survey, as explained in Subsection 3.1.1. We present a general form of multi-hop attention that</p>\n<p>is largely a generalization of the techniquesintroduced into produce the next context vector $c^{(s+1)}$ .Onemust note [85] and [88]. Fig. 10 provides an example implementation of a multi-hop attention mechanism. The general idea isweights,meaning that the number of parameters do not to iteratively transform the query, and use the query to transform the context vector,such that the model can extract different information in each step. Remember that a query</p>\n<p><img src=\"/./images/fC2yeUZnAYXZanLazGfbKKQkNuntGh8I1.png\"></p>\n<p>Fig. 10.An example illustration of multi-hop attention. Solid arrows represent the base multi-hop model structure, while dotted arrows represent optional connections.</p>\n<p>is similar to a literal question. As such, one can interpret the transformed queries as asking the same question in a different manner or from a different perspective, similarly to the queries in multi-head attention. The query that was previously denoted by $\\Psi$ is now referred to as the initial input, and some transformation function transform()</p>\n<p>$$\\boldsymbol{q}<em>{d</em>{q}\\times1}^{(s+1)}&#x3D;\\mathrm{transform}(\\boldsymbol{q}^{(s)},:\\boldsymbol{c}^{(s)}).$$</p>\n<p>For the specific form of the transformation function transform(),[85]proposes to use a mechanism similar to self-attention. Essentially, the queries used by the question answer matching model proposed in [85] were originally based on a set of feature vectors extracted from a question [85] also defines the original query $q^{(0)}$ as the unweighted average of these feature vectors.At each hop $S$, attention can be calculated on these feature vectors using the previous query $q^{(s)}$ as the query in this process.The resulting context vector of this calculation is the next query vector. Using the context vector $c^{(s)}$ instead of $q^{(s)}$ as the query for this process is also a possibility, which is similar to the LCRRot-hop model proposed in [43] and the multi-step model proposed in [88]. Such a connection is represented by the dotted arrows in Fig. 10. The transformation mechanism ules in parallel, but attention modules can also be imple-uses either the $q^{(s)}$ or the context vector $c^{(s)}$ as query, but a combination via concatenation is also possible. Each query representation is used as input for the at</p>\n<p>tention module to compute attention on the columns of the feature matrix $F$ ,as seen previously. One main difference. however, is that the context vector $c^{(s)}$ is also used as input, score function is presented in (42). Note that the initial context vector $c^{(0)}$ is predefined.One way of doing thisis by setting it equal to the unweighted average of the value vectors $\\boldsymbol{v}<em>1,\\ldots,\\boldsymbol{v}</em>{n_f}\\in\\mathbb{R}^{d_v}$ extracted from $F$</p>\n<p>$$e_{l}^{(s)}&#x3D;\\text{score}(\\text{concat}(\\boldsymbol{q}<em>{d</em>{q}\\times1}^{(s+1)},\\boldsymbol{c}^{(s)}),\\boldsymbol{k}_{l}).$$</p>\n<p>An alignment function and the value vectors are then used that in [85], the weights used in each iteration are the same scale with the number of repetitions. Yet,using multiple hops with different weight matrices can also be viable, as shown by the Transformer model [13] and in [88]. It may be</p>\n<p>difficult to grasp why $c^{(s)}$ is part of the query input for the attention model. Essentially, this technique is closely related to self-attention in the sense that,in each iteration,a new context representation is created from the feature vectors and the context vector. The essence of this mechanism is that one wants to iteratively alter the query and the context vector,while attending to the feature vectors.In the process, the new representations of the context vector absorb more differentkinds of information.This is also the main difference between this type of attention and multi-head attention. Multi-head attention creates multiple context vectors from multiple queries and combines them to create a final context vector as output. Multi-hop attention iteratively. refines the context vector by incorporating information from the different queries. This does have the disadvantage of having to calculate attention sequentially Interestingly, due to the variations in which multi-hop</p>\n<p>attention has been proposed, some consider the Transformer model’s encoder and decoder to consist of several singlehop attention mechanisms [84] instead of being a multihop model. However, in the context of this survey, we consider the Transformer model to be an alternative form of the multi-hop mechanism, as the features matrix $F$ is not directly reused in each step. Instead, $F$ is only used as an input for the first hop, and is transformed via self-attention into a new representation. The self-attention mechanism uses each feature vector in $F$ as a query, resulting in a matrix of context vectors as output of each attention hop. The $p_{1},\\cdots,p_{d_{[0]}}$ as accurately as possible compared to the true intermediate context vectors are turned into matrices and represent iterative transformations of the matrix $F$ which are used in the consecutive steps. Thus, the Transformer model iteratively refines the features matrix $F$ by extracting and incorporating new information When dealing with a classification task, another idea is</p>\n<p>to use a different query for each class. This is the basic principle behind capsule-based attention [89], as inspired by the capsule networks [90]. Suppose we have the feature classes that the model can predict. Then, a capsule-based attention model defines a capsule for each of the $d_{y}$ classes that each take as input the feature vectors.Each capsule consists of, in order, an attention module, a probability module, and a reconstruction module, which are depicted in. Fig. 11. The attention modules all use self-attentive queries so each module learns its own query: “Which feature vectors are important to identify this class?”. In [89], a self-attentive multiplicative score function is used for this purpose</p>\n<p>$$e_{c,l}&#x3D;\\begin{array}{ccc}\\boldsymbol{q}_c^T&amp;\\times&amp;\\boldsymbol{k}_l\\1\\times1&amp;1\\times d_k&amp;d_k\\times1\\end{array},$$</p>\n<p>where $e_{c,l}\\in\\mathbb{R}^1$ is the attention score for vector $l$ in capsule $U_{f}$ and $q_{c}\\in\\mathbb{R}^{d_k}$ is a trainable query for capsule $C_{f}$ for $c&#x3D;1,\\ldots,d_{y}$ . Each attention module then uses an alignment function, and uses the produced attention weights to determine a context vector $c_{c}\\in\\mathbb{R}^{d_{v}}$ .Next, the context vector $C_{t}$ is fed through a probability layer consisting of a linear transformation with a sigmoid activation function:</p>\n<p>$$\\begin{aligned}p_{c}&amp;&#x3D;\\mathrm{sigmoid}(\\boldsymbol{w}<em>{c}^{T}\\times\\boldsymbol{c}</em>{c}:+:b_{c}:),\\&amp;<em>{1\\times1}\\times d</em>{v}\\quad d_{v}\\times1\\quad1\\times1\\end{aligned}$$</p>\n<p><img src=\"/./images/fFOVLaKiKKX8YVu44wZt6GwmcOzskqWOp.png\"></p>\n<p>Fig. 11. An illustration of capsule-based attention</p>\n<p>where $w_c\\in\\mathbb{R}^dv$ and $b_{c}\\in\\mathbb{R}^{1}$ are trainable capsule-specific weights parameters, and $p_{c}\\in\\mathbb{R}^{1}$ is the predicted probability that the correct class is class $t$ . The final layer is the reconstruction module that creates a class vector representation This representation $r_{c}\\in\\mathbb{R}^{dv}$ is determined by simply multiplying the context vector $C_{c}$ by the probability $P_{c}$</p>\n<p>$$\\begin{array}{rcl}r_c&amp;&#x3D;&amp;p_c\\times&amp;c_c\\d_v\\times1&amp;&amp;1\\times1&amp;&amp;d_v\\times1\\end{array}.$$</p>\n<p>The capsule representation is used when training the model. First of all, the model is trained to predict the probabilities values. Secondly, via a joint loss function, the model is also trained to accurately construct the capsule representations ${\\boldsymbol{r}}1,\\cdots,{\\boldsymbol{r}}d_{\\psi}$ . A features representation $f\\in\\mathbb{R}^{d_f}$ is defined which is simply the unweighted average of the original feature vectors. The idea is to train the model such that vector representations from capsules that are not the correct class differ significantly from $f$ while the representation from the correct capsule is very similar to $f$ A dot-product between the capsule representations and the features representation vectors $f_{1}, \\ldots , f_{n_{f}}$ $\\in$ $\\mathbb{R} ^{d_{f}}$ , and suppose there are are $d_{y}$ is used in [89] as a measure of the distance between the vectors. Note that $d_v$ must equal $d_{f}$ in this case, otherwise the vectors would have incompatible dimensions. Interestingly, since attention is calculated for each class individually, one can track which specific feature vectors are important for which specific class. In [89], this idea is used to discover which words correspond to which sentiment class. The number of tasks that can make use of multiple</p>\n<p>queries is substantial, due to how general the mechanisms are.As such, the techniques described in this section have been extensively explored in various domains.For example, multi-head attention has been used for speaker recog nition based on audio spectrograms [91]. In [92], multihead attention is used for recommendation of news articles. Additionally, multi-head attention can be beneficial for graph attention models as well [83]. As for multi-hop attention, quite a few papers have been mentioned before but there are still many other interesting examples.For example, in [93], a multi-hop attention model is proposed for medication recommendation. Furthermore, practically every Transformer model makes use of both multi-head and multi-hop attention. The Transformer model has been extensively explored in various domains. For example, in [94], a Transformer model is implemented for image cap-</p>\n<p>tioning. In [95], Transformers are explored for medical image segmentation. In [96], a Transformer model is used for emotion recognition in text messages. A last example of an application of Transformers is [17], which proposes a Transformer model for recommender systems. In comparison with multi-head and multi-hop attention, capsule-based attention is arguably the least popular of the mechanisms discussed for the multiplicity of queries. One example is [97], where an attention-based capsule network is proposed that also includes a multi-hop attention mechanism for the purpose of visual question answering. Another example is [98], where capsule-based attention is used for aspect-level sentiment analysis of restaurant reviews. The multiplicity of queries is a particularly interesting ang Parlprioyesong ar th nthe fieldof audipor 108</p>\n<p>category due to the Transformer model [13], which com provements and iterations of the model have been produced that typically aim to improve the predictive performance, the computational efficiency, or both.For example, the Transformer-XL [99] is an extension of the original Trans former that uses a recurrence mechanism to not be limited by a context window when processing the outputs.This allows the model to learn significantly longer dependencies while also being computationally more efficient during the evaluation phase. Another extension of the Transformer is known as the Reformer model [100]. This model is signifi cantly more efficient computationally, by means of localitysensitive hashing, and memory-wise, by means of reversible residual layers. Such computational improvements are vital, since one of the main disadvantages of the Transformer model is the sheer computational cost due to the complexity of the model scaling quadratically with the amount of input feature vectors. The Linformer model [101] manages to reduce the complexity of the model to scale linearly, while achieving similar performance as the Transformer model This is achieved by approximating the attention weights using a low-rank matrix. The Lite-Transformer model proposed in [102] achieves similar results by implementing two branches within the Transformer block that specialize in capturing global and local information. Another interesting Transformer architecture is the Synthesizer [103] This model replaces the pairwise self-attention mechanism with “synthetic attention weights. Interestingly, the per formance of this model is relatively close to the original Transformer, meaning that the necessity of the pairwise self-attention mechanism of the Transformer model may be questionable.For a more comprehensive overview of Transformer architectures, we refer to [104].</p>\n<h2 id=\"4EVALUATIOn-OF-ATTenTIOn-MOdeLs\"><a href=\"#4EVALUATIOn-OF-ATTenTIOn-MOdeLs\" class=\"headerlink\" title=\"4EVALUATIOn OF ATTenTIOn MOdeLs\"></a>4EVALUATIOn OF ATTenTIOn MOdeLs</h2><p>In this section, we present various types of evaluation for attention models.Firstly, one can evaluate the structure of attention models using the taxonomy presented in Section3 For such an analysis, we consider the attention mechanism categories (see Fig. 3) as orthogonal dimensions of a model. The structure of a model can be analyzed by determining which mechanism a model uses for each category. Table 3 provides an overview of attention models found in the liter-</p>\n<p>ature with a corresponding analysis based on the attention mechanisms the models implement Secondly, we discuss various techniques for evaluating</p>\n<p>the performance of attention models. The performance of attention models can be evaluated using extrinsic or intrinsic performance measures, which are discussed in Subsections. 4.1 and 4.2, respectively</p>\n<h3 id=\"4-1-Extrinsic-Evaluation\"><a href=\"#4-1-Extrinsic-Evaluation\" class=\"headerlink\" title=\"4.1 Extrinsic Evaluation\"></a>4.1 Extrinsic Evaluation</h3><p>In general, the performance of an attention model is measured using extrinsic performance measures. For example,. performance measures typically used in the field of natural language processing are the BLEU [107], METEOR [108] the Word Error Rate[110] and Phoneme Error Rate[111] are bins fom of mult-hop ad multi-hd attntion Due gerally mpoyed For genal assfiation tasks, eo rates, precision, and recall are generally used.For computer vision tasks, the PSNR[112], SSIM [113], or IoU [114] metrics are used. Using these performance measures, an attention model can either be compared to other state-of-the-art models, or an ablation study can be performed. If possible, the importance of the attention mechanism can be tested by replacing it with another mechanism and observing whether the overall performance of the model decreases [105], [115] An example of this is replacing the weighted average used to produce the context vector with a simple unweighted average and observing whether there is a decrease in overall model performance [35]. This ablation method can be used to evaluate whether the attention weights can actually distinguish important from irrelevant informationd</p>\n<h3 id=\"4-2Intrinsic-Evaluation\"><a href=\"#4-2Intrinsic-Evaluation\" class=\"headerlink\" title=\"4.2Intrinsic Evaluation\"></a>4.2Intrinsic Evaluation</h3><p>Attention models can also be evaluated using attentionspecific intrinsic performance measures.In [4], the attention weights are formally evaluatedvia the Alignment Error Rate (AER) to measure the accuracy of the attention weights with respect to annotated attention vectors. [116] incorporates this idea into an attention model by supervising the attention mechanism using gold attention vectors A joint loss function consisting of the regular task-specific loss and the attention weights loss function is constructed for this purpose. The gold attention vectors are based on annotated text data sets where keywords are hand-labelled. However, since attention is inspired by human attention, one could evaluate attention models by comparing them to the attention behaviour of humans.</p>\n<h3 id=\"4-2-1-Evaluation-via-Human-Attention\"><a href=\"#4-2-1-Evaluation-via-Human-Attention\" class=\"headerlink\" title=\"4.2.1 Evaluation via Human Attention\"></a>4.2.1 Evaluation via Human Attention</h3><p>In [117], the concept of attention correctness is proposed which is a quantitative intrinsic performance metric that evaluates the quality of the attention mechanism based on actual human attention behaviour. Firstly, the calculation of this metric requires data that includes the attention behaviour of a human.For example, a data set containing images with the corresponding regions that a human focuses on when performing a certain task, such as image captioning.The collection of regions focused on by the human is referred to as the ground truth region. Suppose an attention model attends to the $n_{f}$ feature vectors $f_1,\\ldots,\\boldsymbol{f}<em>{n_f}\\in\\mathbb{R}^{d_f}$ Feature vector $f_i$ corresponds to region $R</em>{i}$ of the given</p>\n<p>TABLE 3 Attention models analyzed based on the proposed taxonomy. A plus sign (+) between two mechanisms indicates that both techniques were combined in the same model, while a comma (,) indicates that both mechanisms were tested in the same paper, but not necessarily as a combination in the same model</p>\n<p>Buhleran et al. [3]<br>Multiplicity<br>Levels<br>Representations<br>Scoring<br>Generalization<br>Dimensionality<br>Type<br>Query-Related Multiplicity<br>Singular<br>Single-Level<br>Single-Representational<br>Additive<br>Global<br>Single-Dimensional<br>Basic<br>Singular</p>\n<p>Luong et al. [4]<br>Singular<br>Single-Level<br>Single-Representational<br>Multiplicative<br>Global, Local<br>Single-Dimensional<br>Basic<br>Singular</p>\n<p>Xu et al. [8]<br>Singular<br>Single-Level<br>Single-Representational<br>Additive<br>Soft, Hard<br>Single-Dimensional<br>Basic<br>Singular</p>\n<p>Tu et al. [32]<br>Parallel &#x2F; Co-attention<br>Hierarchical<br>Single-Representational<br>Additive<br>Global<br>Single-Dimensional<br>Specialized<br>Singular</p>\n<p>Yang et al. [5]<br>Singular<br>Hierarchical<br>Single-Representational<br>Additive<br>Global<br>Single-Dimensional<br>Self-Attentive<br>Singular</p>\n<p>Li et al. [14]<br>Singular<br>Hierarchical<br>Single-Representational<br>Additive<br>Global<br>Single-Dimensional<br>Self-Attentive<br>Singular</p>\n<p>Koivisto et al. [13]<br>Singular<br>Parallel &#x2F; Co-attention<br>Single-Representational<br>Multiplicative<br>Global<br>Single-Dimensional<br>Multi-Hop<br>Singular</p>\n<p>Waalstra and Frasincar [15]<br>Rotatory<br>Single-Level<br>Single-Representational<br>Activated<br>Global<br>Single-Dimensional<br>Specialized<br>Multi-Hop</p>\n<p>Kiela et al. [50]<br>Singular<br>Single-Level<br>Multi-Representational<br>Additive<br>Global<br>Multi-Dimensional<br>Singular</p>\n<p>Shen et al. [64]<br>Singular<br>Single-Level<br>Single-Representational<br>Additive<br>Global<br>Multi-Dimensional<br>Singular</p>\n<p>Zhang et al. [74]<br>Single-Level<br>Single-Representational<br>Multiplicative<br>Global<br>Single-Dimensional<br>Self-Attentive<br>Singular</p>\n<p>Li et al. [115]<br>Parallel &#x2F; Co-attention<br>Hierarchical<br>Single-Representational<br>Additive<br>Global<br>Single-Dimensional<br>Specialized + Multi-Hop</p>\n<p>Yu et al. [106]<br>Parallel &#x2F; Co-attention<br>Single-Level<br>Single-Representational<br>Multiplicative<br>Global<br>Single-Dimensional<br>Self-Attentive + Specialized</p>\n<p>Wang et al. [62]<br>Parallel &#x2F; Co-attention<br>Hierarchical<br>Single-Representational<br>Additive<br>Reinforced<br>Single-Dimensional<br>Singular</p>\n<p>Oktay et al. [67]<br>Singular<br>Single-Level<br>Single-Representational<br>Additive<br>Global<br>Multi-Dimensional<br>Self-Attentive + Specialized<br>Singular</p>\n<p>Winata et al. [52]<br>Singular<br>Single-Level<br>Multi-Representational<br>Additive<br>Global<br>Single-Dimensional<br>Self-Attentive<br>Multi-Hop</p>\n<p>Wang et al. [89]<br>Singular<br>Single-Level<br>Single-Representational<br>Multiplicative<br>Global<br>Single-Dimensional<br>Self-Attentive<br>Capsule-Based</p>\n<p>image, for $i$ &#x3D; $1, \\ldots , n_{f}$ .We define the set $G$ as the set of regions that belong to the ground truth region, such that $R_{i}\\in G$ if $R_{i}$ is part of the ground truth region. The attentionmodel is behaving correctly. This allows for some interpremodel calculates the attention weights $a_1,\\ldots,a_{n_f}\\in\\mathbb{R}^1$ via the usual attention process. The Attention Correctness $(AC)$ metric can then be calculated using (46).</p>\n<p>$$\\begin{matrix}AC&#x3D;\\sum_{i:R_i\\in G}a_i\\\\end{matrix}.$$</p>\n<p>Thus, this metric is equal to the sum of the attention weights for the ground truth regions.Since the attention weights sum up to 1 due to, for example, a softmax alignment function, the $AC$ value will be a value between O and 1. If the model attends to only the ground truth regions, then $AC$ is equal to 1, and if the attention model does not attend to any of the ground truth regions, $AC$ will be equal to 0. In [118], a rank correlation metric is used to compare</p>\n<p>important. Therefore, observing which parts of the inputs the model focuses on can be helpful in determining if the tation of the behaviour of models that are typically known to be black boxes.However, rather than checking if the model focuses onthemost important parts of thedatasome use the attention weights to determine which parts of the data are most important. This would imply that attention models provide a type of explanation, which is a subject of contention among researchers. Particularly, in [120], extensive experiments are conducted for various natural language processing tasks to investigate the relation between attention weights and important information to determine whether attention can actually provide meaningful explanations. In this paper titled “Attention is not Explanation” it is found that attention weights do not tend to correlate with important features. Additionally, the authors are able to replace the produced attention weights with completely different values while keeping the model output the same. These so-called “adversarial attention distributions show that an attention model may focus on completely different information and still come to the same conclusions, which makes interpretation difficult. Yet, in another paper titled “Attention is not not Explanation”[121], the claim that attention is not explanation is questioned by challenging the assumptions of the previous work.It is found that the adversarial attention distributions do not perform as reliably well as the learned attention weights, indicating that it was not proved that attention is not viable for explanation. In general, the conclusion regarding the interpretability</p>\n<p>the generated attention weights to the attention behaviour of humans.The conclusion of this work is that attention maps generated by standard attention models generally do not correspond to human attention. Attention models often focus on much larger regions or multiple small non-adjacent regions.As such,a technique to improve attention models is to allow the model to learn from human attention patterns via a joint loss of the regular loss function and an attention weight loss function based on the human gaze behaviour, similarly to how annotated attention vectors are used in [116] to supervise the attention mechanism. [117] proposes to use human attention data to supervise the attention mechanism in such a manner. Similarly, a state-of-the-art video captioning model is proposed in [119] that learns fromof attention models is that researchers must be extremely human gaze data to improve the attention mechanism</p>\n<h3 id=\"4-2-2Manual-Evaluation\"><a href=\"#4-2-2Manual-Evaluation\" class=\"headerlink\" title=\"4.2.2Manual Evaluation\"></a>4.2.2Manual Evaluation</h3><p>careful when drawing conclusions based on attention patterns.For example,problems with an attention model can be diagnosed via the attention weights if the model is found to focus on the incorrect parts of the data, if such information the manual inspection of attention weights.As previouslyis available. Yet, conversely, attention weights may only be used to obtain plausible explanations for why certain parts of the data are focused on,rather than concluding that those</p>\n<p>A method that is often used to evaluate attention models is mentioned, the attention weights are a direct indication of which parts of the data the attention model finds most</p>\n<p>parts are significant to the problem [121]. However, one should still be cautious as the viability of such approaches can depend on the model architecture [122]</p>\n<h2 id=\"5CONCLUSION\"><a href=\"#5CONCLUSION\" class=\"headerlink\" title=\"5CONCLUSION\"></a>5CONCLUSION</h2><p>In this survey, we have provided an overview of recent research on attention models in deep learning. Attention mechanisms have been a prominent development for deep learning models as they have shown to improve model performance significantly, producing state-of-the-art results for various tasks in several fields of research. We have presented a comprehensive taxonomy that can be used to categorize and explain the diverse number of attention mechanisms proposed in the literature. The organization of the taxonomy wasmotivated based on the structure ofa taskmodel that consists of a feature model, an attention model, a query model, and an output model. Furthermore, the attention mechanisms have been discussed using a framework based on queries, keys, and values. Last, we have shown how one can use extrinsic and intrinsic measures to evaluate the performance of attention models, and how one can use the taxonomy to analyze the structure of attention models The attention mechanism is typically relatively simple</p>\n<p>to understand and implement and can led to signficant25. 2015, 412-1 He, Amola, and E Hovy improvements in performance.As such, it is no surprise that this is a highly active field of research with new attention mechanisms and models being developed constantly. Not only are new mechanisms consistently being developed but there is also still ample opportunity for the exploration of existing mechanisms for new tasks. For example, multidimensional attention[64] is a technique that shows promis dienqaM almost any attention model. However, it has not seen much application in current works. Similarly, multi-head atten-. tion [13] is a technique that can be efficiently parallelized and implemented in practically any attention model.Yet it is mostly seen only in Transformer-based architectures. Lastly, similarly to how [43] combines rotatory attention with multi-hop attention, combining multi-dimensional at-. tention, multi-head attention, capsule-based attention, or any of the other mechanisms presented in this survey may produce new state-of-the-art results for the various fields of research mentioned in this survey. This survey has mainly focused on attention mech</p>\n<p>anisms for supervised models,since these comprise the largest proportion of the attention models in the literature. In comparison to the total amount of research that has been done on attention models,research on attention models for semi-supervised learning [123], [124] or unsupervised learning [125], [126] has received limited attention and has only become active recently. Attention may play a more significant role for such tasks in the future as obtaining research on attention models can advance even further. For example, we mentioned the fact that attention weights can be trained directly based on hand-annotated data[116] or actual human attention behaviour[117], [119]. As new data sets are released,future research may focus on developing attention models that can incorporate those types of data.</p>\n<p>While attention is intuitively easy to understand,there still is a substantial lack of theoretical support for attention As such, we expect more theoretical studies to additionally contribute to the understanding of the attention mechanisms in complex deep learning systems. Nevertheless, the practical advantages of attention models are clear. Since attention models provide significant performance improve-. ments in a variety of fields, and as there are ample opportunities for more advancements, we foresee that these models will still receive significant attention in the time to come.</p>\n<h3 id=\"REFERENCES\"><a href=\"#REFERENCES\" class=\"headerlink\" title=\"REFERENCES\"></a>REFERENCES</h3><p>[1]H. Larochelle and G. E. Hinton, “Learning to combine foveal glimpses with a third-order Boltzmann machine,” in 24th Annuai Conference in Neural Information Processing Systems (NIPS 2010) Curran Associates, Inc., 2010, pp. 12431251. [2]V. Mnih, N. Heess, A. Graves, and k. kavukcuoglu, “Recurrenf models of visual attention,”in 27th Armual Conference on Neurai Information Processing Systems (NIPs 2014).Curran Associates Inc., 2014, pp. 22042212 [3]D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” in 3rd Internationai Conference on Learning Representation (ICLR 2015), 2015 [4]T. Luong, H. Pham, and C. D. Manning, “Effective approaches to attention-based neural machine translation,” in 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP Hierarchical attention networks for document classification, in 2016 Conference of the NorthAmerican Chapter of the Association fon Computational Linguistics: Human Language Technologies (NAACL [6]YW2016 ACL, 206, 148012890,”Attention-based LSTM for aspect-level sentiment classification, in 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP 2016). and L. Zhang, “Bottom-up and top-down attention for image captioning and visual question answering,” in 2018 IEEE&#x2F;CVF Conference on Computer Vision and Pattern Recognition (CVPR 2018), 2018, pp. 60776086 [8]K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov R. Zemel, and Y. Bengio, “Show, attend and tell: Neural image caption generation with visual attention,” in 32nd Internationa Conference on Machine Learning (ICML 2015),vol. 37.PMLR 2015, pp. 20482057 [9]Y. Ma, H. Peng, and E. Cambria, “Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive LSTM, in 32nd AAAI Conference orn Artificial Intelligence (AAAI 2018).AAAI Press, 2018, Pp. 58765883 [10] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio “Attention-based models for speech recognition,” in 28th Annuai Conference on Neural Information Processing Systems (NIPS 2015) [1 D.Brandasociates lnc 2015D pe575,PB Brakl and Y Bengio End-to-end attention-based large vocabulary speech recogni tion, in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2016). IEEE Signal Processing Society, 2016, pp. 49454949 [12] S. Kim, T. Hori, and S. Watanabe, “Joint CTC-attention based end-to-end speech recognition using multi-task learning, in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2017).IEEE Signal Processing Society, 2017, lag m a d sk ma Jo Gomez, L. u. Kaiser, and I Polosukhin, “Attention is all you need,” in 31st Annual Conference on Neural Information Processing Systems (NIPS 2017).Curran Associates, Inc., 2017, Pp. 5998- 6008 [14] K. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio, $“On$ the properties of neural machine translation: Encoder-decoder approaches,” in Sth Workshop on Syntax, Semarntics and Structure in Statistical Translation (SSST 2014).ACL, 2014, pp. 103111</p>\n<p>[15] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, [36] S. Zheng and R. Xia, “Leff-center-right separated neural network and D. Tran, “Image Transformer,” in 35th Internafional Conference on Machine Learning (ICML 2018), vol. 80. PMLR, 2018, pp. 40554064. [16] L. Zhou, Y. Zhou, J. J. Corso, R. Socher, and C. Xiong, “End-toend dense video captioning with masked transformer,” in 2018 IEEE&#x2F;CVF Conference on Computer Vision and Patfern Recognifion [38]J. Gao, X. Wang, Y. Wang, Z. Yang, J. Gao, J. Wang, W. Tang [17] EVP 01,1ECompufer ocity,2018, p, 8739 874 iang BERT4Rec: Sequential recommendation with bidirectional en coder representations from transformer,” in 28th ACM Interna-[39]Y. Tay, A. T. Luu, and S. C. Hui, “Multi-pointer co-attention nettional Conference on Information and Knowledge Management (CIKM 2019).ACM, 2019, p. 14411450 [18] Wangnand D,MiT.Tas “Ssrvey on theatention based N23, [40 S LMu,2018,bpe,239-31 and X Hu, User-video co-ttention 2016 [19] J. B. Lee, R. A. Rossi, S. Kim, N. K. Ahmed, and E. Koh, “Attention models in graphs: A survey,” ACM Transitions on Knowledge Discovery fror Data, vol. 13, pp. 62:162:25, 2019. [20] S. Chaudhari, V. Mithal, G. Polatkan, and R.Ramanath, “An attentive survey of attention models, ACM Transactions on Intelligent Systems and Technology, vol. 12, no. 5, pp. 132, 2021. [1]D.ue”An introductory survey onatention mechanisms in NLe 42] mnd tol incanec ah w. 270-27enton network (IntelliSys 2019), ser. AISC, vol. 1038. Springer, 2020, pp. 432448 [22]A. Galassi, M. Lippi, and P. Torroni,”Attention, please! a critical review of neuralatention modes in natural language pocess 43] O. Wallat and E raincar, A hybrid approach for aspet [23].Danilek in Rockt schl l Weab, and . iedel “ rustratinel national Conference on Learning Representations (ICLR 2017), 2017 [24] Xionan onga Hongs Wng ed D. Pumbeuat 4] and han ttentionvia-atentin neural machine model for weakly supervised audio tagging,” in Proceedings of fhe 18th Annual Conference of the International Speech Communication Association (Interspeech 2017).ISCA, 2017, pp. 30833087 [25]C. Yu, K. S. Barsim, Q. Kong, and B. Yang, *Multi-level attention model for weakly supervised audio classification, in Proceedings fohe Detetionase Clas, cotio pf.Acoustic ces and Eents 018[46] . Wang S. Wang J. Tang N. OHare, Y. Chang, and B Li, [26] S. Sharma, R. Kiros, and R. Salakhutdinov, “Action recognition using visual atention” in Procedings ohth (4h zorn, one.47 Li, Y Wei, Zhang and Q Yang. “Hierarchical attention [27] L. Gao, Z. Guo, H. Zhang, X. Xu, and H. T. Shen, *Video captioning with attention-based LSTM and semantic consistency,” IEEE Trarsacfions on Meltimedia, vol. 19, no. 9, pp. 20452055, 2017 [28]H. Ying, F. Zhuang, F. Zhang, Y. Liu, G. Xu, X. Xie, H. Xiong, and J. Wu, “Sequential recommender system based on hierarchical attention networks, in 27th International Joint Conference on Artificial Intelligence (IJCAI 2018).IJCAI, 2018, pp. 39263932 [29] H. Song, D. Rajan, J. Thiagarajan, and A. Spanias, “Attend and diagnose: Clinical time series analysis using attention models,” in 32nd AAAI Conference on Artificial Intelligence (AAAI 2018). AAAI[50]D. Kiela, C. Wang, and K. Cho, Dynamic meta-embeddingg Press, 2018, pp. 40914098 [30] D.T. Tran, A. Iosifidis, J. Kanniainen, and M. Gabbouj, “Temporal attention-augmented bilinear network for financial time-series data analysis,” IEEE Transactions on Neural Netuorks and Learning [51] S. Maharjan, M. Montes, F. A. Gonzalez, and T. Solorio, “A Systems, vol. 30, no. 5, pp. 14071418, 2019 [31] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio, “Graph attention networks,” in 6th Internationai Conference on Learning Representations (ICLR 2018), 2018. [32]J. Lu, J. Yang, D. Batra, and D. Parikh, “Hierarchical question image co-attention for visual question answering,” in 30th Anrual Conference on Neural Information Processing Systems (NIPs 2016). Curran Associates, Inc., 2016, pp. 289297 [33] F. Fan, Y. Feng, and D. Zhao, “Multi-grained attention network for aspect-level sentiment classification, in 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP 2018).[54] A. Sordoni, P. Bachman, A. Trischler, and Y. Bengio,It [34 D.CLM,2018. P, 3-34g, and H. Wang, Interativ atenton networks for aspect-level sentiment classification,” in 26th In- [55] A. Graves, G. Wayne, and I. Danihelka, “Neural Turing maternational Joint Conference on Artificial Intelligence (IJCAI 2017). IJCAI, 2017, pp. 40684074 [35] M. Seo, A. Kembhavi, A. Farhadi, and H. Hajishirzi, “Bidirectional attention flow for machine comprehension,” in 4th Irterna tional Conference on Learning Represertations (ICLR 2016), 2016</p>\n<p>for aspect-based sentiment analysis with rotatory attention, arXiv:1802.00892, 2018 [37] B. Jing, P. Xie, and E. Xing, “On the automatic generation of med ical imaging reports,” in 56th Anrual Meeting of the Association for Computational Linguistics (ACL 2018).ACL, 2018, pp. 25772586 and X. Xie, “CAMP: Co-attention memory networks for diagnosis prediction in healthcare,” in 2019 IEEE International Conference o Data Minting (ICDM 2019).IEEE, 2019, pp. 10361041. works for recommendation, in 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD 2018) network for personalized micro-video recommendation, in 2019 World Wide Web Conference (WWW 2019).ACM, 2019, pp.30203026 [41] M. Tu, G. Wang, J. Huang, Y. Tang, X. He, and B. Zhou, “Multihop reading comprehension across multiple documents by reasoning over heterogeneous graphs in 57thAnmual Meeting of the Association for Computational Linguistics (ACL 2019).Association for Computational Linguistics, 2019, pp. 27042713 for explainable fake news detection on social media,” in 58fh Annual Meeting of fhe Association for Computational Linguistics (ACL 2020).ACL, 2020, Pp. 505514. based sentiment aalysusing a leicalied domain ontology Corference (ESWC 2019), ser. LNCS, vol. 11503.Springer, 2019) Pp. 363378 translation, in 32nd AAAI Conference on Artificial Intelligence (AAAI 2018).AAAI Press, 2018, pp. 563570. [45]L. Wu, L. Chen, R. Hong, Y. Fu, X. Xie, and M. Wang, “A hierarchical attention model for social contextual image recom mendation, IEEE Transactions on Knowledge and Data Engineering, 2019 Hierarchical attention network for action recognition in videos,” arXiv:1607.06416, 2016. transfer network for cross-domain sentiment classification, in 32nd AAAI Conference on Artificial Intelligence (AAAI 2018). AAAI Press, 2018, p. 58525859 [48] C. Xing, Y. Wu, W. Wu, Y. Huang, and M. Zhou, *Hierarchical recurrent attention network for response generation,” in 32nd AAAI Conference on Artificial Intelligence (AAAI 2018). AAAI Press, 2018, pp. 56105617 [49] V. A. Sindagi and V. M. Patel, “HA-CCN: Hierarchical attention based crowd counting network,” IEEE Transactions on Image Processing, vol. 29, pp. 323335, 2019 for improved sentence representations,” in 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP 2018) ACL, 2018, pp. 14661477. genre-aware attention model to improve the likability prediction of books,”in 2018 Conference onEmpirical Methods in Naturai Language Processing (EMNLP 2018).ACL, 2018, pp. 33813391. [52] G. I. Winata, Z. Lin, and P. Fung, “Learming multilingual metaembeddings for code-switching named entity recognition,” in 4fh Workshop on Representation Learning for NLP (RepL4NLP 2019) ACL, 2019, pp. 181186 [53] R. Jin, L. Lu, J. Lee, and A. Usman, “Multi-representational con volutional neural networks for text classification, Computationa Intelligence, vol.35, no. 3, pp. 599609, 2019 erative alternating neural attention for machine reading, arXiv:1606.02245, 2016. chines, arXiv:1410.5401, 2014 [56] D. Britz, A. Goldie, M.-T. Luong, and Q. Le, “Massive exploration of neural machine translation architectures,” in 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017). ACL, 2017, pp. 14421451.</p>\n<p>[57]R. J. Williams, Simple statistical gradient-following algorithms for connectionist reinforcement learning,” Machine Learning vol. 8, no. 3, pp. 229256, 1992. [58] T.Shen, T. Zhou, G. Long, J. Jiang, S. Wang, and C. Zhang, *Reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling,” in 27th International Joint Cornference on Artificial Intelligence (IJCAI 2018).IJCAI, 2018, pp. 43454352 [59] M. Malinowski, C. Doersch, A. Santoro, and P. Battaglia, “Learning visual question answering by bootstrapping hard attention,” in 2018 European Conference on Computer Vision (ECCV 2018), 2018. [60] Y. Liu, W. Wang, Y. Hu, J. Hao, X. Chen, and Y. Gao, “Multiagent game abstraction via graph attention neural network,”” in 34th AAAI Conference on Artificial Intelligence (AAAI 2020), vol. 34, no. 05.AAAI Press, 2020, pp. 72117218. [61] S.Seo,J.Huang, H. Yang, and Y. Liu, “Interpretable convolutional neural networks with dual local and global attention for review rating prediction,in 11th ACM Conference on Recommender Systems (RecSys 2017).ACM, 2017, pp. 297305 [62] J. Wang, C. Sun, S. Li, X. Liu, L. Si, M. Zhang, and G. Zhou, “Aspect sentiment classification towards question-answering with reinforced bidirectional attention network, in 57th Ammual Meeting of fhe Association for Computational Linguistics (ACL 2019). ACL, 2019, pp. 35483557. [63] M. Jiang, C. Li, J. Kong, Z. Teng, and D. Zhuang, “Crosslevel reinforced attention network for person re-identification, Journal of Visual Communication and Image Representation, vol. 69 p. 102775, 2020. [64] T. Shen, T. Zhou, G. Long, J. Jiang, S. Pan, and C. Zhang, “DiSAN: Directional self-attention network for RNN&#x2F;CNN-free language understanding,” in 32nd AAAI Conference on Artificial Intelligence. (AAAI 2018).AAAI Press, 2018, Pp. 54465455 [65] O. Arshad, I Gallo, S. Nawaz, and A. Calefati, “Aiding intra-text representations with visual context for multimodal named entity recognition,” in 2019 International Conference on Document Analysis. and Recognition (ICDAR 2019).IEEE, 2019, pp. 337342 [66] W. Wu, X. Sun, and H. Wang, “Question condensing networks for answer selection in community question answering,” in Proceed. ings of the 56th Annual Meeting of the Association for Coputational Linguistics (ACL 2018).ACL, 2018, pp. 17461755 [67] . Okta.a, Schlem se MDonaFol,go MH aemeMa, Beiric, B. Glocker, and D. Rueckert, “Attention U-Net: Learming where to look for the pancreas in 1st Medical Imaging with Deep Learning Conference (MIDL 2018), 2018 [68] R. Tan, J. Sun, B. Su, and G. Liu, “Extending the transformer with context and multi-dimensional mechanism for dialogue response generation, in 8th International Conference on Natural Language Processing and Chinese Computing (NLPCC 2019), ser. LNCS, J. Tang, M.-Y. Kan, D. Zhao, S. Li, and H. Zan, Eds., vol. 11839. Springer, 2019, pp. 189199 [69] L. Chen, B. Lv, C. Wang, S. Zhu, B. Tan, and K. Yu, “”Schemaguided multi-domain dialogue state tracking with graph attention neural networks,in 34th AAAI Conference on Artificial Intelligence (AAAI 2020), vol. 34, no. 05.AAAI Press, 2020, pp. 75217528 [70] H. Wang, G. Liu, A. Liu, Z. Li, and K. Zheng, “Dmran: A hierarchical fine-grained attention-based network for recommenda-. tion, in 28th International Joint Conference on Artificial Intelligence (IJCAI 2019). [71] Z. Lin, M. Feng, C. N. d. Santos, M. Yu, B. Xiang, B. Zhou, and Y. Bengio, “A structured self-attentive sentence embedding in 5th International Conference on Learning Representations (ICLR 2017), 2017. [72]J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization, arXiv:1607.06450, 2016. [73] H. Zhao, J. Jia, and V. Koltun,“Exploring self-attention for image recognition,” in 2020 IEEE&#x2F;CVF Conference on Computer Vision and Patfern Recognition (CVPR 2020), 2020, pp. 10 07610 085. [74] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, “Selfattention generative adversarial networks, in 36th International Conference on Machine Learning (ICML 2019), ser. Proceedings of Machine Learning Research, K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97. PMLR, 2019, pp. 73547363 [75] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. WardeFarley, S. Ozair, A. Courville, and Y. Bengio, “Generative ad versarial nets,in 27th Anmual Conference on Neural Information</p>\n<p>Processing Systems (NIPs 2014).Curran Associates, Inc., 2014 Pp. 2672-2680 [76] A. Sinha and J. Dolz, “Multi-scale self-guided attention for medical image segmentation,” IEEE Journal of Biomedical and Health Informafics, vol. 25, no. 1, pp. 121-130, 2021 [77] J. Fajtl, H. S. Sokeh, V. Argyriou, D. Monekosso, and P. Re magnino, Summarizing videos with attention, in 2018 Asian Corferernce on Computer Vision (ACCV 2018), ser. LNCS, vol. 11367 Springer, 2018, pp. 3954. [78] J. Salazar, K. Kirchhoff, and Z. Huang, “”Self-attention networks for connectionist temporal classification in speech recognition, in 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2019).IEEE, 2019, pp. 71157119 [79]T. Afouras, J. S. Chung, A.Senior, O. Vinyals, and A.Zisserman, Deep audio-visual speech recognition, IEEE Transactions or Pattern Analysis and Machine Intelligence, pp. 11, 2018 [80]S. Zhang, Y. Tay, L. Yao, and A. Sun, *Next item recommendation with self-attention, arXiv preprint arXiv:1808.06414, 2018 [81]G. Letarte, F. Paradis, P. Giguere, and F. Laviolette, “Impor tance of self-attention for sentiment analysis,” in 2018 Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP (BlackboxNLP 2018).ACL, 2018, pp. 267275 [82] A. Sankar, Y. Wu, L. Gou, W. Zhang, and H. Yang, “Dysat Deep neural representation learning on dynamic graphs via selfattention networks, in 13th International Conference on Web Search and Data Minting (WSDM 2020), 2020, pp. 519527 [83] P. Velickovic, G.Cucurull, A. Casanova, A.Romero, P. Lio and Y. Bengio, Graph attention networks, in 5th International Conference on Learning Representations (ICLR 2017), 2017 [84] S. Iida, R Kimura, H. Cui, P-H. Hung, T. Utsuro, and M. Nagata, “Attention over heads: A multi-hop attention for neural machine translation, in 57th Anmual Meeting of the Association for Com putational Linguistics: Student Research Workshop (ACL-SRW 2019). ACL, 2019, pp. 217222 [85] N. K. Tran and C. Niedereee, “Multihop attention networks for question answer matching, in 41st ACM SIGIR International Conference on Research &amp; Development in Information Retrieval (SIGIR 2018).ACM, 2018, Pp. 325334 [86]Y. Gong and S. R. Bowman, “Ruminating reader: Reasoning with gated multi-hop attention, in 5th International Conference on Learning Representation (ICLR 2017), 2017 [87]S. Yoon, S. Byun, S. Dey, and K. Jung, “Speech emotion recognition using multi-hop attention mechanism, in 2019 IEEE Interna fional Conference on Acoustics, Speech and Signal Processing (ICASSP 2019).IEEE, 2019, pp. 28222826. [88]Z. Yang, X. He, J. Gao, L. Deng, and A. Smola, “Stacked attention networks for image question answering,” in 2016 IEEE&#x2F;CVF Con ference on Compufer Vision and Pattern Recognition (CVPR 2016) 2016, pp.2129 [89]Y. Wang, A. Sun, J. Han, Y. Liu, and X. Zhu, “Sentiment analysis by capsules,” in 2018 World Wide Web Conference (WWW 2018). ACM, 2018, p. 11651174 [90]S. Sabour, Ni. Frosst, and G. E. Hinton, “Dynamic routing between capsules,in 31st Anmual Conference on Neural Information Prosng Systems NIPs 2017). Curran Associates Inc, 2017, p. [91] M. India, P. Safari, and J. Hernando, “Self multi-head attention for speaker recognition,” in Proceedings of fhe 20th Anmual Conference of the International Speech Communication Association (Interspeecl 2019). ISCA, 2019, pp. 28222826 [92] C. Wu, F. Wu, S. Ge, T. Qi, Y. Huang, and X. Xie, “Neural news recommendation with multi-head self-attention, in 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP 2019).ACL, 2019, pp. 63896394. [93]Y. Wang, W. Chen, D. Pi, and L. Yue,”Adversarially regularized medication recommendation model with multi-hop memory net-. work,Knowledge and Information Systems, vol.63, no. 1, pp. 125142, 2021 [94] M. Cornia, M. Stefanini, L. Baraldi, and R. Cucchiara, “Meshedmemory transformer for image captioning,” in 2020 IEEE&#x2F;CVF Conference on Computer Vision and Pattern Recognition (CVPR 2020), 2020, pp. 10 57810 587 [95] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille and Y. Zhou, “TransUnet: Transformers make strong encoders for medical image segmentation, arXiv preprint arXiv:2102.043062021</p>\n<p>[96]P. Zhong, D. Wang, and C. Miao, “Knowledge-enriched transformer for emotion detection in textual conversations,”in 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. (EMNLP-IJCNLP 2019). ACL, 2019, Pp. 165176 [97]Y. Zhou, R. Ji, J. Su, X. Sun, and W. Chen, “Dynamic capsule 2019, pp. 93249331 [98] Y. Wang, A. Sun, M. Huang, and X. Zhu, “Aspect-level sentiment analysis using AS-capsules,” in The World Wide Web Conference, [9 21Da,2 233-20.Yang . Carboell . Le, and R slakhtdinovg Transformer-XL: Attentive language models beyond a fixed length context,”in 57th Annual Meeting of the Association for Computational Linguistics (ACL 2019).ACL, 2019, pp. 29782988. [100] N. Kitaev, L. Kaiser, and A. Levskaya, Reformer: The efficient Transformer, in 8th International Conference on Learning Represen. attention with linear complexity,” rXiv:2006.04768, 2020. [102] Z. Wu, Z. Liu, J. Lin, Y. Lin, and S. Han, “Lite transformer with long-short range attention, in 8th International Conference on Learning Representations (ICLR 2020), 2020 [103] Y. Tay, D. Bahri, D. Metzler, D.-C. Juan, Z. Zhao, and C. Zheng, Synthesizer: Rethinking self-attention for transformer models, in Proceedings of the38th International Cornference on Machine Learn ing (ICML 2021), vol. 139.PMLR, 2021, pp. 10 18310192 [104] Y. Tay, M. Dehghani, D. Bahri, and D. Metzler, “Efficient transformers: A survey, arXiv:2009.06732, 2020. [105] X. Li, J. Song, L. Gao, X. Liu, W. Huang, X. He, and C. Gan, Beyond RNNs: Positional self-attention with co-attention for video question answering,” in 33rd AAAI Conference on Artificial Intelligence (AAAI 2019), vol. 33.AAAI Press, 2019, Pp. 86588665 [106] A. W. Yu, D. Dohan, M.-T. Luong, R. Zhao, K. Chen, M. Norouzi and Q. V. Le, “QANet: Combining local convolution with global self-attention for reading comprehension,” in 6th International Conference on Learning Representations (ICLR 2018), 2018 [107] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, *BLEU: a method for automatic evaluation of machine translation,” in 40th Annual Meeting of fhe Association for Compufational Linguistics (ACL 2002). ACL, 2002, pp. 311318. [108] S. Banerjee and A. Lavie, “METEOR: An automatic metric for MT evaluation with improved correlation with human judgments, in 2005 Workshop oni Intrinsic and Extrinsic Eoaluation Measures for Machine Translation and&#x2F;or Summarization. ACL, 2005, pp. 6572 [109] R. Sennrich, “Perplexity minimization for translation model domain adaptation in statistical machine translation,” in 13th Conference of fhe European Chuapter of the Association for Computational Linguistics (EACL 2012).ACL, 2012, pp. 539549 [110] M. Popovic and H. Ney, “Word error rates: Decomposition over POS classes and applications for error analysis,” in 2nd Workshop on Statistical Machtine Translation (WMT 2007).ACL, 2007, Pp 48-55 [111] P. Schwarz, P. Matejka, and J. Cernocky, “Towards lower error rates in phoneme recognition, in 7th International Conference. on Text, Speech and Diaiogue (TSD 2004), ser. LNCS, vol. 3206 Springer, 2004, pp. 465472. [112] D. S. Turaga, Y. Chen, and J. Caviedes, “No reference PSNR estimation for compressed pictures,” Signal Processing: Image Commticatfion, vol. 19, no. 2, pp. 173184, 2004 [113] P. Ndajah, H. Kikuchi, M. Yukawa, H. Watanabe, and S. Muramatsu, “SSIM image quality metric for denoised images,” in 3rd WSEAS International Conference on Visualization, Imaging and Simulation (VIS 2010).WSEAS, 2010, pp. 5358 [114] M. A. Rahman and Y. Wang, “Optimizing intersection-over-union in deep neural networks for image segmentation,” in 12th International Symposium on Visual Computing (ISVC 2016), ser. LNCS, vol. 10072.Springer, 2016, pp. 234244. [115] X. Chen, L. Yao, and Y. Zhang, “Residual attention U-net for auto mated multi-class segmentation of COVID-19 chest CT images, arXiv:2004.05645, 2020 [116] S. Liu, Y. Chen, K. Liu, and J. Zhao, “Exploiting argument information to improve event detection via supervised attention of the Journal of Web Engineering. Dr. Frasincar is a member of the mechanisms,” in 55th Arnual Meeting of the Association for Coputational Linguistics (ACL 2017).ACL, 2017, pp. 17891798</p>\n<p>[117] C. Liu, J. Mao, F. Sha, and A. Yuille, “Attention correctness in neural image captioning,” in 31st AAAl Conference on Artificial Intelligence (AAAI 2017). AAAI Press, 2017, Pp. 41764182 [118] A. Das, H. Agrawal, L. Zitnick, D. Parikh, and D. Batra, “Human atention novisual question anowering: Do hu mans and dee neural attention models for video captioning by human gaze data,”in 2017 IEEE Conference on Computer Vision and Paftern Recognition (CVPR 2017).IEEE Computer Society, 2017 [120] S. Jain and B. C. Wallace, “Attention is not explanation,” in 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2019).ACL, 2019, pp. 35433556 [121] S. Wiegreffe and Y. Pinter, “Attention is not not explanation,” in 2019 Conference on Empirical Methods in Natural Language Process ing and fhe9th International Joint Conference on Nafural Language 1 B. V. Srinivasan, and B. Ravindran, Towards transparent and explainable attention models,” in 58th Anmual Meeting of the Association for Computational Linguistics (ACL 2020).ACL, 2020 pp. 4206-4216. [123] K. K. Thekumparampil, C. Wang, S. Oh, and L-J. Li, “Attention based graph neural network for semi-supervised learning, arXiv:1803.03735, 2018 [124]D. Nie, Y. Gao, L. Wang, and D. Shen, “ASDNet: Attention based semi-supervised deep networks for medical image segmenta tion, in 21st International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI 2018), ser. LNCS, vol 11073.Springer, 2018,pp.370-378 [125]Y. Alami Mejati, C. Richardt, J. Tompkin, D. Cosker, and K. I. Kim, Unsupervised attention-guided image-to-image transla tion, in 32nd Anmual Conference on Neural Information Processing Systems (NIPS 2018).Curran Associates, Inc., 2018, Pp. 36933703 [126] R. He, W. S. Lee, H. T. Ng, and D. Dahlmeier, “An unsupervised neural attention model for aspect extraction,” in 55thi Annua Meeting of the Association for Computational Linguistics (ACL 2017) ACL, 2017, pp. 388397.</p>\n<p>Gianni Brauwers was born in Spijkenisse,the Netherlands.in 1998. He received the B.S. de gree in econometrics and operations research from Erasmus University Rotterdam, Rotterdam the Netherlands,in 2019,and is currently pursu ing the M.S. degree in econometrics and man agement science at Erasmus University Rotter dam. He is a Research Assistant at Erasmus Uni-</p>\n<p><img src=\"/./images/fwKubs9NrrsE4E6IOIuyNmz1254MnctEC.png\"></p>\n<p>versity Rotterdam, focusing his research on neural attention models and sentiment analysis</p>\n<p>Flavius Frasincar was born in Bucharest.Ro mania, in 1971. He received the M.S. degree in computer science,in 1996,and the M.Phil degree in computer science, in 1997, from Politehnica University of Bucharest, Bucharest, Romania, and the P.D.Eng. degree in computer sci ence, in 2000, and the Ph.D. degree in computer science, in 2005, from Eindhoven University of Technology, Eindhoven, the Netherlands Since 2005,he has been an Assistant Profes</p>\n<p><img src=\"/./images/fRt7WcnS0GB13Asmx3FTox8eFYu3ccbMN.png\"></p>\n<p>sor in computer science at Erasmus University Rotterdam, Rotterdam, the Netherlands. He has published in numerous</p>\n<p>conferences and journals in the areas of databases,Web information systems, personalization, machine learning, and the Semantic Web. He is a member of the editorial boards of Decision Support Systems. International Journal of Web Engineering and Technology, and Com putational Linguistics in the Netherlands Journal, and co-editor-in-chief Association for Computing Machinery</p>\n","length":21010,"excerpt":"","more":"<h1 id=\"A-General-Survey-on-Attention-Mechanisms-in-Deep-Learning\"><a href=\"#A-General-Survey-on-Attention-Mechanisms-in-Deep-Learning\" class=\"headerlink\" title=\"A General Survey on Attention Mechanisms in Deep Learning\"></a>A General Survey on Attention Mechanisms in Deep Learning</h1><p>Gianni Brauwers and Flavius Frasincar</p>\n<p>Abstract—Attention is an important mechanism that can be employed for a variety of deep learning models across many different domains and tasks. This survey provides an overview of the most important attention mechanisms proposed in the literature. The various attention mechanisms are explained by means of a framework consisting of a general attention model, uniform notation, and a comprehensive taxonomy of attention mechanisms. Furthermore, the various measures for evaluating attention models are reviewed and methods to characterize the structure of attention models based on the proposed framework are discussed. Last, future work in the field of attention models is considered</p>\n<p>Index Terms-Attention models, deep learning, introductory and survey, neural nets, supervised learning</p>\n<h3 id=\"1INTRODUCTION\"><a href=\"#1INTRODUCTION\" class=\"headerlink\" title=\"1INTRODUCTION\"></a>1INTRODUCTION</h3><p>Hdemao model was created for machine translation, but was quickly adopted tobe used for other tasks, such as image processing [15], video processing [16], and recommender systems [17]. The purpose of this survey is to explain the general</p>\n<p>the computational complexity of image processing while improving performance by introducing a model that would only focus on specific regions of images instead of the entire picture. Although, the true starting point of the attention mechanisms we know today is often attributed to originate in the field of natural language processing [3]. Bahdanau et al.[3] implement attention in a machine translation model to address certain issues with the structure of recurrent neural networks. After Bahdanau et al. [3] emphasized the advantages of attention, the attention techniques were refined [4] and quickly became popular for a variety of tasks, such as text classification [5], [6], image captioning [7], [8], sentiment analysis [6], [9], and speech recognition [10], [11], [12] Attention has become a popular technique in deep learn-</p>\n<p>form of attention,and provide a comprehensive overview of attention techniques in deep learning. Other surveys have already been published on the subject of attention models For example, in [18], a survey is presented on attention in computer vision, [19] provides an overview of attention in graph models, and [20], [21],[22] are all surveys on attention in natural language processing. This paper partly builds on the information presented in the previously mentioned surveys. Yet, we provide our own significant contributions The main difference between this survey and the previously mentioned ones is that the other surveys generally focus on attention models within a certain domain. This survey however, provides a cross-domain overview of attention techniques. We discuss the attention techniques in a general way, allowing them to be understood and applied in a variety of domains. Furthermore, we found the taxonomies presented in previous surveys to be lacking the depth and structure needed to properly distinguish the various attention mechanisms. Additionally, certain significant attention techniques have not yet been properly discussed in previous surveys, while other presented attention mechanisms seem to be lacking either technical details or intuitive explanations. Therefore, in this paper, we present important attention techniques by means of a single framework using a uniform notation,a combination of both technical and in tuitive explanations for each presented attention technique, and a comprehensive taxonomy of attention mechanisms The structure of this paper is as follows. Section 2 in-</p>\n<p>ing for several reasons. Firstly, models that incorporate attention mechanisms attain state-of-the-art results for all of the previously mentioned tasks, and many others. Furthermore, most attention mechanisms can be trained jointly with a base model, such as a recurrent neural network or a convolutional neural network using regular backpropa gation [3]. Additionally, attention introduces a certain type of interpretation into neural network models [8] that are generally known to be highly complicated to interpret. Moreover, the popularity of attention mechanisms was additionally boosted after the introduction of the Transformer model [i3] that further proved how effective attention can be.Attention was originally introduced as an extension to recurrent neural networks [14]. However, the Transformer model proposed in [13] poses a major development in attention research as it demonstrates that the attention mechanism is sufficient to build a state-of-the-art model. This means that disadvantages, such as the fact that recurrent neural networks are particularly difficult to parallelize, can</p>\n<p>troduces a general attention model that provides the reader with a basic understanding of the properties of attention and how it can be applied. One of the main contributions of this paper is the taxonomy of attention techniques presented in Section 3. In this section, attention mechanisms are explained and categorized according to the presented</p>\n<p><img src=\"/./images/fXAE7qMmeZtPgm2VeKxkN3y8GAQXG8CbW.png\"></p>\n<p>Fig. 1. An illustration of the general structure of the task model.</p>\n<p>taxonomy. Section 4 provides an overview of performance measures and methods for evaluating attention models. Furthermore,the taxonomy is used to evaluate the structure of various attention models. Lastly, in Section 5, we give oun conclusions and suggestions for further research</p>\n<h2 id=\"2-General-Attention-Model\"><a href=\"#2-General-Attention-Model\" class=\"headerlink\" title=\"2 General Attention Model\"></a>2 General Attention Model</h2><p>This section presents a general form of attention with corresponding notation. The notation introduced here is based on the notation that was introduced in [23] and popularized in [13]. The framework presented in this section is used throughout the rest of this paper. To implement a general attention model, it is necessary</p>\n<p>to first describe the general characteristics of a model that can employ attention. First of all, we will refer to the complete model as the task model, of which the structure is presented in Fig. 1. This model simply takes an input, carries out the specified task, and produces the desired output.For example, the task model can be a language model that takes as input a piece of text, and produces as output a summary of the contents,a classification of the sentiment, or the text translated word for word to another language. Alternatively the task model can take an image, and produce a captionthe general attention module is the query $q\\in\\mathbb{R}^{d_{q}}$, andthe or segmentation for that image. The task model consists of four submodels: the feature model, the query model, the attention model, and the output model. In Subsection 2.1, the feature model and query model are discussed, which are used to prepare the input for the attention calculation. In Subsection 2.2, the attention model and output model are discussed, which are concerned with producing the output</p>\n<h2 id=\"2-1Attention-Input\"><a href=\"#2-1Attention-Input\" class=\"headerlink\" title=\"2.1Attention Input\"></a>2.1Attention Input</h2><p>Suppose the task model takes as input the matrix $X\\in$ $\\Pi_{\\mathcal{R}}d_{\\mathcal{F}}\\times\\boldsymbol{n}<em>{\\mathcal{X}}$ where $d</em>{x}$ represents the size of the input vectors and $n_{x}$ represents the amount of input vectors. The columns in this matrix can represent the words in a sentence the pixels in an image, the characteristics of an acoustic sequence, or any other collection of inputs.The feature model is then employed to extract the $n_{f}$ feature vectors $f_1,\\ldots,f_{n_f}\\in\\mathbb{R}^d_f$ from $X$ where $d_{f}$ represents the size of the feature vectors.The feature model can be a recurrent neural network (RNN), a convolutional neural network (CNN), a simple embedding layer, a linear transformation of the original data, or no transformation at all. Essentially, the feature model consists of all the steps that transform the original input $X$ into the feature vectors $f_{1},\\ldots,f_{n_{f}}$ that the attention modelwill attend to.</p>\n<p><img src=\"/./images/fxeeSs49eiCx6zygk9gTruATCtkbbN2Om.png\"></p>\n<p>Fig. 2. The inner mechanisms of the general attention module</p>\n<p>To determine which vectors to attend to,the attention model requires the query $q\\in\\mathbb{H}^d_q$ ,where $d_q$ indicates the size of the query vector. This query is extracted by the query model, and is generally designed based on the type of output that is desired of the model. A query tells the attention model which feature vectors to attend to.It can be interpreted literally as a query, or a question. For example, for the task of image captioning, suppose that one uses a decoder RNN model to produce the output caption based on feature vectors obtained from the image by a CNN.At each prediction step, the hidden state of the RNN model can be used as a query to attend to the CNN feature vectors. In each step, the query is a question in the sense that it asks for the necessary information from the feature vectors based on the current prediction context</p>\n<h3 id=\"2-2-Attention-Output\"><a href=\"#2-2-Attention-Output\" class=\"headerlink\" title=\"2.2 Attention Output\"></a>2.2 Attention Output</h3><p>The feature vectors and query are used as input for the attention model. This model consists of a single, or a collection of general attention modules.An overview of a general attention module is presented in Fig. 2. The input of matrix of feature vectors $\\boldsymbol{F}&#x3D;[\\boldsymbol{f}<em>{1},\\ldots,\\boldsymbol{f}</em>{n_{f}}]\\in\\mathbb{R}^{d_{f}\\times n_{f}}.$ Two separate matrices are extracted from the matrix $F$ : the keys matrix $K&#x3D;[k_{1},\\ldots,k_{n_{f}}]\\in\\mathbb{R}^{d_{k}\\times n_{f}}$, and the values matrix $V$ &#x3D; $[ \\boldsymbol{v}<em>1, \\ldots , \\boldsymbol{v}</em>{n_f}]$ $\\in$ $\\mathbb{R} ^{d_v\\times n_f}$, where $d_{k}$ and $d_{v}$ indicate, respectively, the dimensions of the key vectors (columns of $K$ ) and value vectors (columns of $V$ ). The general way of obtaining these matrices is through a linear transformation of $F$ using the weight matrices. $W_{K}\\in\\mathbb{R}^{d_{k}\\times d_{f}}$ and $W_{V}\\in\\mathbb{R}^{d_{v}\\times d_{f}}$ ,for $K$ and $V$ , respectively. The calculations of $K$ and $V$ are presented in (1). Both weight matrices can be learned during training or predefined by the researcher For example, one can choose to define both $W_{K}$ and $W_{V}$ as equal to the identity matrix to retain the original feature vectors. Other ways of defining the keys and the values are also possible, such as using completely separate inputs for the keys and values. The only constraint to be obeyed is that the number of columns in $K$ and $V$ remains the same<br>$$K_{d_{k}\\times n_{f}}&#x3D;W_{K}\\times F_{d_{f}\\times n_{f}},\\quad V_{d_{v}\\times n_{f}}&#x3D;W_{V}\\times F_{d_{f}\\times n_{f}}.$$</p>\n<p>The goal of the attention module is to produce a weighted average of the value vectors in $V$ .The weights used to produce this output are obtained via an attention scoring and alignment step.The query $q$ and the keys matrix $K$ are used to calculate thevector of attention scores</p>\n<p>$\\boldsymbol{e}&#x3D;\\left[e_{1},\\ldots,e_{n_{f}}\\right]\\in\\mathbb{R}^{n_{f}}.$ This is done via the score function score(), as illustrated in (2)</p>\n<p>$$e_{l}:&#x3D;:\\mathrm{score}(:q:,:k_{l}:).$$</p>\n<p>As discussed before, the query symbolizes a request for information. The attention score $e_{1}$ represents how important the information contained in the key vector $k_{l}$ is according to the query. If the dimensions of the query and key vectors are the same, an example of a score function would be to take the dot-product of the vectors. The different types of score functions are further discussed in Section 3.2.1. Next, the attention scores are processed further through</p>\n<p>an alignment layer. The attention scores can generally have a wide range outside of [0,1] . However, since the goal is to produce a weighted average, the scores are redistributed via an alignment function align() as defined in (3).</p>\n<p>$$\\begin{matrix}a_l&amp;&#x3D;\\text{align}(\\begin{array}{c}e_l:;&amp;\\boldsymbol{e}\\1\\times1&amp;n_f\\times1\\end{array}),\\end{matrix}$$</p>\n<p>where $a_l\\in\\mathbb{R}^1$ is the attention weight corresponding to the lth value vector.One example of an alignment function wouldbe touse a softmax function,but the various other alignment types are discussed in Section 3.2.2. The attention weights provide a rather intuitive interpretation for the attention module. Each weight is a direct indication of how important each feature vector is relative to the others for this particular problem. This can provide us with a more in-depth understanding of the model behaviour, and the re lations between inputs and outputs. The vector of attention weights $a$ &#x3D; $[ a_{1}, \\ldots , a_{n_{f}}]$ $\\in$ $\\mathbb{R} ^{n_{f}}$ is used to produce the context vector $c\\in\\mathbb{R}^{d_v}$ by calculating a weighted average of the columns of the values matrix $V$ ,as shown in (4)<br>$$\\boldsymbol{c}<em>{d_v\\times1}&#x3D;\\sum</em>{l&#x3D;1}^{n_f}a_l:\\times:\\boldsymbol{v}_l:.$$</p>\n<p>As illustrated in Fig. 1, the context vector is then used in the output model to create the output $\\ddot{y}$ This output model translates the context vector into an output prediction. For example, it could be a simple softmax layer that takes as input the context vector $C$ , as shown in (5)</p>\n<p>$$\\hat{\\boldsymbol{y}}<em>{d</em>{y}\\times1}&#x3D;\\mathrm{softmax}(W_{c}:\\times:c_{d_{y}\\times1}:+:b_{c}:),$$</p>\n<p>where $d_{y}$ is the number of output choices or classes, and $W_{c}\\in\\mathbb{R}^{d_{y}\\times d_{v}}$ and $b_{c}\\in\\mathbb{R}^{d}$ are trainable weights</p>\n<h3 id=\"2-3-Attention-Applications\"><a href=\"#2-3-Attention-Applications\" class=\"headerlink\" title=\"2.3 Attention Applications\"></a>2.3 Attention Applications</h3><p>Attention is a rather general mechanism that can be used in a wide variety of problem domains. Consider the task of machine translation using an RNN model. Also, consider the problem of image classification using a basic CNN model. While an RNN produces a sequence of hidden state vectors, a CNN creates feature maps, where each region in the image is represented by a feature vector. The RNN hidden states are organized sequentially, while the CNN feature maps are organized spatially. Yet, attention can still be applied in both situations, since the attention mechanism does not inherently depend on the organization of thefeature vectors. This characteristic makes attention easy to implement in a wide variety of models in different domains.</p>\n<p>Another domain where attention can be applied is audio processing [24], [25]. Acoustic sequences can be represented by a sequence of feature vectors that relate to certain time periods of the audio sample. These vectors could simply be the raw input audio, or they can be extracted via,for example, an RNN or CNN. Video processing is another domain where attention can be applied intuitively [26], [27] Video data consists of sequences of images, so attention can be applied to the individual images, as well as the entire sequence. Recommender systems often incorporate a users interaction history to produce recommendations. Feature vectors can be extracted based on, for example, the id’s or other characteristics of the products the user interacted with, and attention can be applied to them [28]. Attention can generally also be applied to many problems that use a time series as input, be it medical [29], financial [30], or anything else, as long as feature vectors can be extracted The fact that attention does not rely on the organization</p>\n<p>of the feature vectors allows it to be applied to various problems that each use data with different structures, as illustrated by the previous domain examples.Yet,this can be taken even further by applying attention to data where there is irregular structure. For example, protein structures, city traffic flows, and communication networks cannot always be represented using neatly structured organizations, such as sequences, like time series, or grids, like images. In such cases, the different aspects of the data are often represented as nodes in a graph. These nodes can be represented by feature vectors, meaning that attention can be applied in domains that use graph-structured data as well [19],[31] In general, attention can be applied to any problem for</p>\n<p>which a set of feature vectors can be defined or extracted As such, the general attention model presented in Fig. 2 is applicable to a wide range of domains. The problem,how ever, is that there is a large variety of different applications and extensions of the general attention module. As such, in Section 3, a comprehensive overview is provided of a collection of different attention mechanisms.</p>\n<h2 id=\"3ATTENTION-TAXONOMY\"><a href=\"#3ATTENTION-TAXONOMY\" class=\"headerlink\" title=\"3ATTENTION TAXONOMY\"></a>3ATTENTION TAXONOMY</h2><p>There are many different types of attention mechanisms and extensions,and a model can use different combinations of these attention techniques. As such, we propose a taxonomy that can be used to classify different types of attention mechanisms. Fig. 3 provides a visual overview of the different categories and subcategories that the attention mechanisms can be organized in. The three major categories are based on whether an attention technique is designed to handle specific types of feature vectors (feature-related), specific types of model queries (query-related), or whether it is simply a general mechanism that is related to neither the feature model, nor the query model (general). Further explanations of these categories and their subcategories are provided in the following subsections. Each mechanism discussedin this section is either amodification to the existing inner mechanisms of the general attention module presented in Section 2, or an extension of it The presented taxonomy can also be used to analyze</p>\n<p>the architecture of attention models.Namely, the major categories and their subcategories can be interpreted as</p>\n<p><img src=\"/./images/fwzKYgHVze3V4K8ZYBR28uKos0SPY48WT.png\"></p>\n<p>Fig. 3. A taxonomy of attention mechanisms</p>\n<h3 id=\"TABLE-1-Notation\"><a href=\"#TABLE-1-Notation\" class=\"headerlink\" title=\"TABLE 1 Notation.\"></a>TABLE 1 Notation.</h3><p>attend to these various vectors.These features may have specific structures that require special attention mechanisms to handle them.These mechanisms can be categorized to deal with one of the following feature characteristics: the multiplicity of features, the levels of features, or the representations of features</p>\n<h3 id=\"3-1-1Multiplicity-of-Features\"><a href=\"#3-1-1Multiplicity-of-Features\" class=\"headerlink\" title=\"3.1.1Multiplicity of Features\"></a>3.1.1Multiplicity of Features</h3><p>Symbol\tDescription<br>$F$\tMatrix of size $d_f \\times n_f$ containing the feature vectors $f_1, \\ldots, f_{n_f} \\in \\mathbb{R}^{d_f}$ as columns. These feature vectors are extracted by the feature model.<br>$K$\tMatrix of size $d_k \\times n_j$ containing the key vectors $k_1, \\ldots, k_{n_j} \\in \\mathbb{R}^{d_k}$ as columns. These vectors are used to calculate the attention scores.<br>$V$\tMatrix of size $d_v \\times n_j$ containing the value vectors $v_1, \\ldots, v_{n_j} \\in \\mathbb{R}^{d_v}$ as columns. These vectors are used to calculate the context vector.<br>$W_K$\tWeights matrix of size $d_k \\times d_f$ used to create the $K$ matrix from the $F$ matrix.<br>$W_V$\tWeights matrix of size $d_v \\times d_f$ used to create the $V$ matrix from the $F$ matrix. This vector essentially represents a question, and is used to calculate the attention scores.<br>$q$\tQuery vector of size $d_q$. This vector is used to calculate the attention weights.<br>$c$\tContext vector of size $d_c$. This vector is the output of the attention model.<br>$e$\tScore vector of size $d_{e_i}$ containing the attention scores $e_1, \\ldots, e_{n_e} \\in \\mathbb{R}^1$. These are used to calculate the attention weights.<br>$a$\tAttention weights vector of size $d_{a_j}$ containing the attention weights $a_1, \\ldots, a_{n_a} \\in \\mathbb{R}^1$. These are the weights used in the calculation of the context vector.</p>\n<p>For most tasks, a model only processes a single input, such as an image, a sentence, or an acoustic sequence. We refer to such a mechanism as singular features attention. Other models are designed to use attention based on multiple inputs to allow one to introduce more information into the model that can be exploited in various ways.However, this does imply the presence of multiple feature matrices that require special attention mechanisms to be fully used. For example, [32] introduces a concept named co-attention to allow the proposed visual question answering (VQA) model to jointly attend to both an image and a question Co-attention mechanisms can generally be split up</p>\n<p>into two groups [33]: coarse-grained co-attention and fine-grained co-attention. The difference between the two groups is the way attention scores are calculated based on the two feature matrices. Coarse-grained attention mecha-. nisms use a compact representation of one feature matrix as a query when attending to the other feature vectors. Fine-grained co-attention, on the other hand, uses all feature vectors of one input as queries. As such, no information is lost, which is why these mechanisms are called fine-grained As an example of coarse-grained co-attention, [32] pro-</p>\n<p>orthogonal dimensions of an attention model. An attention model can consist of a combination of techniques taken from any or all categories. Some characteristics, such as the scoring and alignment functions, are generally required for any attention model. Other mechanisms, such as multihead attention or co-attention are not necessary in every situation. Lastly, in Table 1, an overview of used notation with corresponding descriptions is provided.</p>\n<p>poses an alternating co-attention mechanism that uses the context vector (which is a compact representation) from one attention module as the query for the other module, and vice versa. Alternating co-attention is presented in Fig. 4. Given a set of two input matrices $X^{(1)}$ and $X^{(2)}$ , features are extracted by a feature model to produce the feature extracts feature vectors so that the attention model canmatrices $F^{(1)}\\in\\mathbb{R}^{d_f^{(1)}\\times n_f^{(1)}}$ and $F^{(2)}\\in\\mathbb{R}^{d_{f}^{(2)}\\times n_{f}^{(2)}}$ ,where $d_{f}^{(1)}$</p>\n<h2 id=\"3-1-Feature-Related-Attention-Mechanisms\"><a href=\"#3-1-Feature-Related-Attention-Mechanisms\" class=\"headerlink\" title=\"3.1 Feature-Related Attention Mechanisms\"></a>3.1 Feature-Related Attention Mechanisms</h2><p>Based on a particular set of input data, a feature model</p>\n<p><img src=\"/./images/fTFNeAz730mbNSQS1tZxyhe0r9V8CHLx8.png\"></p>\n<p>Fig. 4. An illustration of alternating co-attention</p>\n<p>vectors extracted from the first and second inputs, while co-attention is used for VQA, so the two input matrices are the image data and the question data, for which the feature model for the image consists of a CNN model, and the feature modelfor the question consists of word embeddings a convolutional layer, a pooling layer, and an LSTM model. Firstly, attention is calculated for the first set of features $F^{(1)}$ without the use of a query (Attention Module in Fig. 4). In as a weighted average of the second set of value vectors [32], an adjusted additive attention score function is used for this attention mechanism.The general form of the regularattention module,which will produce the context vector additive score function can be seen in (6).<br>$$\\mathrm{score}(:\\boldsymbol{q}:,:\\boldsymbol{k}<em>{l}:)&#x3D;:\\boldsymbol{w}^{T}\\times\\mathrm{act}(\\boldsymbol{W}</em>{1}\\times:\\boldsymbol{q}:+:\\boldsymbol{W}<em>{2}\\times:\\boldsymbol{k}</em>{l}+\\boldsymbol{b}:),\\d_{q}\\times1\\quad d_{k}\\times1\\quad1\\times d_{w}\\quad d_{w}\\times d_{q}\\quad d_{q}\\times1\\quad d_{w}\\times d_{k}\\quad d_{k}\\times1\\quad d_{w}\\times1$$</p>\n<p>where act() is a non-linear activation function, and $w\\in$ $\\Pi^{\\mathcal{R}}d_{\\mathcal{n}\\mathcal{F}}$ ， $W_{1}$ $\\in$ $\\mathbb{R} ^{d_{w}\\times d_{q}}$ ， $W_{2}\\in$ $\\mathbb{R} ^{d_{w}\\times d_{k}}$, and $b\\in\\mathbb{R}^{d_{w}}$ are trainable weights matrices, for which $d_{w}$ is a predefined dimension of the weight matrices.A variant of this score function adapted to be calculated without a query for the application at hand can be seen in (7).</p>\n<p>$$e_{l}^{(0)}&#x3D;\\boldsymbol{w}^{(1)T}\\times\\operatorname{act}(\\boldsymbol{W}^{(1)}\\times\\boldsymbol{k}<em>{l}^{(1)}+\\boldsymbol{b}^{(1)}),\\1\\times1\\quad1\\times d</em>{w}\\quad d_{w}\\times d_{k}^{(1)}\\quad d_{k}^{(1)}\\times1\\quad d_{w}\\times1$$</p>\n<p>where $w^{(1)}\\in\\mathbb{R}^{d_{w}}$ ， $W^{(1)}\\in\\mathbb{R}^{d_{\\omega}\\times d_{k}^{(1)}}$ and $b^{(1)}\\in\\mathbb{R}^{d_{M}}$ areafter another. This may come with a computational distrainable weight matrices for Attention Module,. $k_l^{(1)}\\in$ advantage since it is not possible to parallize. Instead of $\\Pi\\text{砥}^{d_k^{(1)}}$ is the Ith column of the keys matrix $K^{(1)}$ that wasusing a sequential mechanism like alternating co-attention, obtained from $F^{(1)}$ via a linear transformation (see (1), for which $d_{w}$ is a prespecified dimension of the weight matrices and $d_{k}^{(1)}$ is a prespecifiedimension of the key vectors. Perhaps onemay wonder why the query is absent whenqueries,unweighted averages of the keyvectors areused as</p>\n<p>calculating attention in this manner. Essentially, the query in this attention modelis learned alongside the other trainable parameters. As such, the query can be interpreted as a general question: “Which feature vectors contain the most important information?”.This is also known as a selfattentive mechanism, since attention is calculated based only on the feature vectors themselves. Self-attention is explained in more detail in Subsection 3.3.1 The scores are combined with an alignment function</p>\n<p>(see (3)), such as the softmax function, to create attention weights used to calculate the context vector $\\boldsymbol{c}^{(0)}\\in\\mathbb{R}^{\\boldsymbol{d}<em>{\\mathbb{V}}^{(1)}}$ (see (4). This context vector is not used as the output of the attention model, but rather as a query for calculating the context vector $\\boldsymbol{c}^{(2)}\\in\\mathbb{R}^{d</em>{w}^{(2)}}$ based on the second feature vectors $c^{(1)}$ and $c^{(2)}$ matrix $F^{(2)}$ where $d_{v}^{\\langle2\\rangle}$ is thedimension of the valuevectors obtained from $F^{(2)}$ via a linear transformation (see (1).For compact representation of one input to use as a query when</p>\n<p><img src=\"/./images/ftdzr2qwpgGo72yaqTGymWrhgqyyzp9TO.png\"></p>\n<p>Fig. 5. An illustration of interactive co-attention</p>\n<p>and $d_{f}^{(2)}$ represent, respectively the dimension of the feature this module (Attention Modulez in Fig. 4), atention scores are calculated using another score function with $C_{0}$ as query $n_f^{(1)}$ Tand $n_{f}^{(2)}$ re esen et ely t ento e ei ut on e n ie ne nc oncie e n </p>\n<p>$$e_{l}^{(2)}&#x3D;\\mathrm{score}(\\begin{array}{c}c^{(0)}:,:k_{l}^{(2)}\\d_{v}^{(1)}\\times1:d_{k}^{(2)}\\times1\\end{array}).$$</p>\n<p>These attention scores are then used to calculate attentior weights using, for example, a softmax function as alignment function,after which the context vector $c^{(2)}$ can bederived Finally, the context vector $c^{(2)}$ is used as a query for the first $c^{(1)}$ for the first feature matrix $F^{(1)}$ .Attention scores are calculated according to (9). In [32], the same function and weight matrices as seen in (7) are used, but with an added query making it the same as the general additive score function (see (6)). The rest of the attention calculation is similar as before</p>\n<p>$$e_{l}^{(1)}&#x3D;\\mathrm{score}(\\begin{array}{c}c^{(2)}:,:k_{l}^{(1)}:).\\d_{v}^{(2)}\\times1:d_{k}^{(1)}\\times1\\end{array}$$</p>\n<p>The produced context vectors $c^{(1)}$ and $c^{(2)}$ are concatenated and used for prediction in the output model.Alternating co-attention inherently contains a form of sequentiality due to the fact that context vectors need to be calculated one [34] proposes the interactive co-attention mechanism that can calculate attention on both feature matrices in parallel, as depicted in Fig. 5. Instead of using the context vectors as queries.The calculation of the average keys are provided in (10), and the calculation of the attention scores are shown in (11). Any score function can be used in this case, but an additive score function is used in [34]</p>\n<p>$$\\bar{\\boldsymbol{k}}<em>{d_k^{(1)}\\times1}^{(1)}&#x3D;\\frac{1}{n_f^{(1)}}\\sum\\limits</em>{l&#x3D;1}^{n_f^{(1)}}\\boldsymbol{k}<em>l^{(1)},\\quad\\bar{\\boldsymbol{k}}</em>{d_k^{(2)}\\times1}^{(2)}&#x3D;\\frac{1}{n_f^{(2)}}\\sum\\limits_{l&#x3D;1}^{n_f^{(2)}}\\boldsymbol{k}_l^{(2)};$$</p>\n<p>$$e_{l}^{(1)}&#x3D;\\mathrm{score}(:\\bar{\\boldsymbol{k}}^{(2)}:,:\\boldsymbol{k}<em>{l}^{(1)}:),:e</em>{l}^{(2)}&#x3D;\\mathrm{score}(:\\bar{\\boldsymbol{k}}^{(1)}:,:\\boldsymbol{k}<em>{l}^{(2)}:).\\1\\times1\\quad d</em>{k}^{(2)}\\times1\\quad d_{k}^{(1)}\\times1\\quad1\\times1\\quad d_{k}^{(1)}\\times1\\quad d_{k}^{(2)}\\times1$$</p>\n<p>From the attention scores, attention weights are created via an alignment function, and are used to produce the context</p>\n<p>While coarse-grained co-attention mechanisms use a</p>\n<p>calculating attention for another input, fine-grained coattention considers every element of each input individually when calculating attention scores.In this case, the query becomes a matrix. An example of fine-grained co-attention is parallel co-attention [32]. Similarly to interactive coattention, parallel co-attention calculates attention on the two feat ue mariesa hesaemeti e $K^{(1)}\\in\\mathbb{R}^{d_k^{(1)}\\times n_f^{(1)}}$ .d $K^{(2)}\\in\\mathbb{R}<em>{k}^{d</em>{k}^{(2)}\\times n_{f}^{(2)}}$ that are obtained by linearly transforming the feature matrices $F^{(1)}$ and $F^{(2)}$ ,where $d_k^{(1)}$ and $d_k^{(2)}$ are $K^{(1)}$ C $K^{(2)}$ Sopdehna tiset dtoedere $A$ $\\in$ $\\mathbb{R} ^{n_f^{( \\dagger ) }\\times n_f^{( 2) }}$ difet die eastons sn afitysaeix atento sore asho na 6) translate one keys matrix to the space of the other keys. In [32], $A$ is calculated as shown in (12).<br>$$A_{n_{f}^{(1)}\\times n_{f}^{(2)}}&#x3D;\\operatorname{act}(\\begin{array}{ccc}K^{(1)^{T}}&amp;\\times&amp;W_{A}&amp;\\times&amp;K^{(2)}\\n_{f}^{(1)}\\times d_{k}^{(1)}&amp;&amp;d_{k}^{(1)}\\times d_{k}^{(2)}&amp;&amp;d_{k}^{(2)}\\times n_{f}^{(2)}\\end{array}),$$</p>\n<p>weights using an alignment function,so that two context</p>\n<p>where $W_{A}\\in\\mathbb{R}^{d_{k}^{(1)}\\times d_{k}^{(2)}}$ is a trainable weights matrix and $\\arctan()$ is an activation function for which the tanh() function this matrix,i.e., one can use (13) to calculate each individual element $A_{i,j}$ of the matrix $A$</p>\n<p>$$A_{i,j}&#x3D;:\\boldsymbol{w}<em>{A}^{T}:\\times\\mathrm{concat}(\\boldsymbol{k}</em>{i}^{(1)},\\boldsymbol{k}<em>{j}^{(2)},\\boldsymbol{k}</em>{i}^{(1)}\\circ\\boldsymbol{k}<em>{j}^{(2)}),_{1\\times1}\\quad1\\times3d</em>{k}\\quad d_{k}\\times1\\quad d_{k}\\times1\\quad d_{k}\\times1\\quad d_{k}\\times1$$</p>\n<p>where $w_{A}\\in\\mathbb{R}^{3d_{k}}$ denotes a trainable vector of weights, concat() denotes vector concatenation, and 0 denotes element-wise multiplication, also known as the Hadamard product. Note that the keys of each keys matrix in this case must have the same dimension $d_{k}$ for the element wise multiplication to work.The affinity matrix can be interpreted as a similarity matrix for the columns of the two keys matrices, and helps translate, for example, image keys to the same space as the keys of the words in a sentence, and vice versa.The vectors of attention scores $e^{(1)}$ and $e^{(2)}$ can be calculated using an altered version of the additive score function as presented in (14) and (15). The previous attention score example i this survey all used a score func- fllowing feature matrices:. $\\boldsymbol{F}^{t}$ &#x3D; $[ \\boldsymbol{f}<em>{1}^{t}, \\ldots , \\boldsymbol{f}</em>{n_{f}^{t}}^{t}]$ $\\in$ $\\mathbb{R} ^{d_{f}^{t}\\times n_{f}^{t}}$ tion to calculate each attention score for each value vector individually. However, (14) and (15) are used to calculate $F^{l}$ &#x3D; $[ f_{1}^{l}, \\ldots , f_{n_{f}^{l}}^{l}]$ $\\in$ $\\mathbb{R} ^{d_{f}^{l}\\times n_{f}^{l}}.$ and $F^{r}$ &#x3D; $[ f_{1}^{r}, \\ldots , f_{n_{f}^{r}}^{r}]$ $\\in$ the complete vector of all attention scores. Essentially, the $\\mathbb{R}^{d_f^r\\times n_f^r}$, for the target phrase words, left context words, attention scores are calculated in an aggregated form.</p>\n<p>$$\\begin{gathered}<br>e^{(1)} &#x3D;:\\boldsymbol{w}<em>{1}\\times\\mathrm{act}(\\boldsymbol{W}</em>{2}\\times\\boldsymbol{K}^{(2)}\\times:\\boldsymbol{A}^{T}:+\\boldsymbol{W}<em>{1}\\times\\boldsymbol{K}^{(1)}); \\<br>1\\times n</em>{f}^{(1)} 1\\times d_{w}\\quad d_{w}\\times d_{k}^{(2)}:d_{k}^{(2)}\\times n_{f}^{(2)}:n_{f}^{(2)}\\times n_{f}^{(1)}:d_{w}\\times d_{h}^{(1)}:d_{k}^{(1)}\\times n_{f}^{(1)} \\<br>e^{(2)} &#x3D;:\\boldsymbol{w}<em>{2}\\times\\mathrm{act}(\\boldsymbol{W}</em>{1}\\times\\boldsymbol{K}^{(1)}\\times:\\boldsymbol{A}:+\\boldsymbol{W}<em>{2}\\times\\boldsymbol{K}^{(2)}), \\<br>1\\times r</em>{e}^{(2)}f 1\\times d_{w}\\quad d_{w}\\times d_{k}^{(1)}:d_{k}^{(1)}\\times n_{f}^{(1)}:n_{f}^{(1)}\\times n_{f}^{(2)}:d_{w}\\times d_{k}^{(2)}:d_{k}^{(2)}\\times n_{f}^{(2)}<br>\\end{gathered}$$</p>\n<p>where $w_1\\in\\mathbb{R}^d_u$ ， $w_2\\in\\mathbb{R}^dw$ ， $W_{1}$ $\\in$ $\\mathbb{R} ^{d_{\\omega }\\times d_{h}^{( 1) }}$ and that tanh() is used in [32] for the activation function, and the feature matrices are used as the key matrices. In that case, the affinity matrix $A$ can be seen as a translator between feature spaces. As mentioned before, the affinity matrix is essentially a similarity matrix for the key vectors of the two</p>\n<p><img src=\"/./images/fcezbCFVVAgYitIXTykvdtgkU8QRrQGOK.png\"></p>\n<p>Fig. 6. An illustration of parallel co-attention</p>\n<p>the maximum similarity value in a row or column as the</p>\n<p>$$e_{i}^{(1)}&#x3D;\\max_{j&#x3D;1,\\ldots,n_{f}^{(2)}}A_{i,j},\\quad e_{j}^{(2)}&#x3D;\\max_{i&#x3D;1,\\ldots,n_{f}^{(1)}}A_{i,j}.$$</p>\n<p>Next,the attention scores are used to calculate attention vectors $c^{(1)}$ and $c^{(2)}$ can be derived as weighted averages of the value vectors that are obtained from linearly transform ing the features. For the alignment function, [32] proposes is used in[32].[35] proposes a different way of calculatingtouse a softmaxfunction,and thevalue vectors are simply set equal to the feature vectors. The resulting context vectors can be either concatenated or added together Finally, coarse-grained and fine-grained co-attention can</p>\n<p>be combined to create an even more complex co-attention mechanism. [33] proposes the multi-grained co-attention mechanism that calculates both coarse-grained and finegrained co-attention for two inputs.Each mechanism produces one context vector per input. The four resulting context vectors are concatenated and used in the output model for prediction A mechanism separate from co-attention that still uses</p>\n<p>multiple inputs is the rotatory attention mechanism [36] This technique is typically used in a text sentiment analysis setting where there are three inputs involved: the phrase for which the sentiment needs to be determined (target phrase) the text before the target phrase (left context), and the text after the target phrase (right context). The words in these three inputs are all encoded by the feature model, producing the</p>\n<p>$d_{f}^{r}$ d rghtenete ns nesctie hee $d_{f}^{t}$, $d_{f}^{l}$, do $d_{f}^{l}$, rand corresponding inputs, and $n_{f}^{t}$, $n_{f^{\\prime }}^{l}$, n $n_{f}^{l}$, and $n_{f}^{r}$ represent the number of feature vectors for the corresponding inputs. The feature model used in[36]consists of word embeddings and separate Bi-LSTM models for the target phrase, the left context, and the right context. This means that the feature vectors arein fact thehidden statevectors obtained from the $W_{2}\\in$ $\\mathbb{R} ^{d_{w}\\times d_{k}^{( 2) }}$ are trainable weight matrices, for which Bi-LSTM models. Using these features, the idea is to extract $d_{w}$ is a prespecified dimension of the weight matrices. Note a single vector $T^*$ from the inputs such that a softmax layer can be used for classification. As such, we are now faced with two challenges: how to represent the inputs as a single vector, and how to incorporate the information from the left and right context into that vector. [36] proposes to use the rotatory attention mechanism for this purpose.</p>\n<p>Firstly, a single target phrase representation is created most importantinformation in the actual target phrase itself by using a pooling layer that takes the average over the Essentially, the mechanism rotates attention between the columns of $F^{t}$ ,as shown in (17).</p>\n<p>$$r_{<em>{d</em>{f}^{t}\\times1}}^{t}&#x3D;\\frac{1}{n_{f}^{t}}\\sum_{i&#x3D;1}^{n_{f}^{t}}f_{i}^{t}:.$$</p>\n<p>$r^{l}$ is then used as a query to create a context vector out of various scans or documents, that can provide different types the left and right contexts, separately. For example, for the left context,the key vectors $k_{1}^{l},\\ldots,k_{n_{f}^{l}}^{l}\\in\\mathbb{R}^{d_{k}^{l}}$ and value vectors $\\boldsymbol{v}<em>{1}^{l},\\ldots,\\boldsymbol{v}</em>{n_{f}^{l}}^{l}\\in\\mathbb{R}^{d_{\\nu}^{l}}$ are extracted rom the let context mages and semantic l gsimuldaneously iel aly n $d_{k}^{l}$ tire $d_{v}^{l}$ cors $f_{1}^{l},\\ldots,f_{n_{f}^{l}}^{l}\\in\\mathbb{R}^{d_{f}^{l}}$ sima ee ee e e respectively. Note that [36] proposes to use the original feature vectors as keys and values, meaning that the linear transformation consists of a multiplication by an identity matrix. Next, the scores are calculated using (18).</p>\n<p>$$e_{i}^{l}&#x3D;\\text{score}(\\begin{array}{c}r^{t}\\d_{f}^{t}\\times1\\end{array},\\begin{array}{c}k_{i}^{l}\\d_{k}^{l}\\times1\\end{array}).$$</p>\n<p>For the score function, [36] proposes to use an activated general score function [34] with a tanh activation function. The attention scores can be combined with an alignment function and the corresponding value vectors to produce the context vector $r^{l}\\in\\mathbb{R}^{d_{v}^{l}}$ The alignment function used in [36] takes the form of a softmax function. An analogous procedure can be performed to obtain the representation of the right context, $r^{T}$ .These two context representations can then be used to create new representations of the target phrase, $\\Pi\\text{思}^d_{k_0}^r$ and value vectors $\\boldsymbol{v}<em>{1}^{t}, \\ldots , \\boldsymbol{v}</em>{n_{f}^{t}}^{t}$ $\\in$ $\\mathbb{R} ^{d_{v}^{t}}$ are extracted from the target phrase feature vectors $f_{1}^{t},\\ldots,f_{n_{f}^{t}}^{t}\\in\\mathbb{R}^{d_{f}^{t}}$ 3.1.2 Feature Levels similarly as before, using a linear transformation, where $d_{k}^{t}$ and $d_{v}^{t}$ are the dimensions of the key and value vectors respectively. Note, again, that the original feature vectors as keys and values in [36]. The attention scores for the leftaware target representation are then calculated using (19).<br>$$e_{i}^{l_{t}}&#x3D;\\mathrm{score}(\\begin{array}{c}r^{l}\\d_{v}^{l}\\times1\\end{array},\\begin{array}{c}k_{i}^{t}\\d_{k}^{t}\\times1\\end{array}).$$</p>\n<p>The attention scores can be combined with an alignment function and the corresponding value vectors to produce the context vector $r^{l_t}\\in\\mathbb{R}^{d_v^t}$ . For this attention calculation, [34] proposes to use the same score and alignment functions as before. The right-aware target representation $r^{r^{\\prime}t}$ can be calculated in a similar manner. Finally, to obtain the full representation vector 7 that is used to determine the classification, the vectors $r^{l}$, $r^{r}$, $r^{l_{t}}$, $r^{l_{s}}$ p and $r^{r_t}$ are concatenated together, as shown in (20).</p>\n<p>$$r_{(d_{v}^{l}+d_{v}^{r}+d_{v}^{t}+d_{v}^{t})\\times1}&#x3D;\\mathrm{concat}(\\begin{array}{c}r^{l},\\d_{v}^{l}\\times1\\end{array},\\begin{array}{c}r^{r},\\d_{v}^{t}\\times1\\end{array},\\begin{array}{c}r^{r_{t}}\\d_{v}^{t}\\times1\\end{array}).$$</p>\n<p>To summarize, rotatory attention uses the target phrase to compute new representations for the left and right context using attention, and then uses these left and right reprephrase. The first step is designed to capture the words in the left and right contexts that are most important to the target phrase. The second step is there to capture the</p>\n<p>target and the contexts to improve the representations. There are many applications where combining informa-</p>\n<p>tion from different inputs into a single model can be highly beneficial. For example,in the field of medical data,there are often many different types of data available, such as of information. In [37], a co-attention mechanism is used for automatic medical report generation to attend to both study is used in [38] to show that the co-attention part of the model specifically improves performance. A field where multi-feature attention has been extensively explored is the domain of recommender systems.For example,in [39], a coattention network is proposed that attends to both product reviews and the reviews a user has written. In[40], a model is proposed for video recommendation that attends to both user features and video features. Co-attention techniques have also been used in combination with graph networks for the purpose of, for example, reading comprehension across multiple documents [41] and fake news detection [42]. In comparison to co-attention, rotatory attention has typically been explored only in the field of sentiment analysis, which is most likely due to the specific structure of the data that is necessary to use this technique. An implementation of rotatory attention is proposed in [43] for sentiment analysis, where the mechanism is extended by repeating the attention again, using attention. Firstly,the key vectors $k_{1}^{t},\\ldots,k_{n_{f}^{t}}^{t}\\in$ wotationto mectively furthen dep byepe repgese aateons</p>\n<p>The previously discussed attention mechanisms process. data at a single level. We refer to these attention techniques as single-level attention mechanisms. However, some data. types can be analyzed and represented on multiple levels. For example, when analyzing documents, one can analyze the document at the sentence level, word level,or even the character level. When representations or embeddings of all these levels are available, one can exploit the extra levels of information. For example, one could choose to perform translation based on either just the characters, or just the words of the sentence. However, in [44], a technique named attention-via-attention is introduced that allows one to incorporate information from both the character, and the word levels. The idea is to predict the sentence translation character-by-character, while also incorporating information from a word-level attention module. To begin with, a feature model (consisting of, for ex</p>\n<p>ample,word embeddings and RNNs) is used to encode the input sentence into both a character-level feature matrix $\\dot{F^{( c) }}$ $\\in$ $\\mathbb{R} ^{d_{f}^{( \\mathrm{c} ) }\\times n_{f}^{( \\mathrm{c} ) }}$ and a word-level feature matrix $F^{( w) }\\in$ $\\mathbb{R} ^{d_{f}^{( w) }\\times n_{f}^{( w) }}$ where $d_{f}^{(c)}$ and $n_{f}^{(c)}$ represent, respec tively, the dimension of the embeddings of the characters $d_{\\int}^{(w)}$ and $n_{f}^{(w)}$ reee that each level in the data can be represented or embedded When attempting to predict a character in the translated</p>\n<p><img src=\"/./images/fSEFcGafUNBvMGze5Vma05K9gNTsnQ3cq.png\"></p>\n<p>Fig. 7. An illustration of attention-via-attention.</p>\n<p>sentence, a query $q^{(c)}\\in\\mathbb{R}^dq$ is created by the query model (like a character-level RNN), where $d_{q}$ is the dimension of the query vectors. As illustrated in Fig. 7, the query is used tocaleulate tentononthe corod $c^{(w)}\\in\\mathbb{R}^{d^{(w)}}$ etors $F^{(w)}$ $d_v^{(w)}$ ate ion module ta esentily suemarieste entencec</p>\n<p>Iee ne eien e e ee et e $d_{f}^{(S)}$ etreate $F^{(s)}\\in\\mathbb{R}^{d_f^{(S)}\\times n_s}$ fore $s&#x3D;1,\\ldots,n_{S}$, where which words contain the most important information for predicting the next character. If we know which words are most important,then it becomes easier to identify which characters in the input sentence are most important. Thus, the next step is to attend to the character-level features in $F^{(c)}$ , with an additional query input: the word-level context vector $c^{(w)}$ . The actual query input for the attention model will therefore be the concatenation of the query $q^{(c)}$ and the are essentially su-amaries of the words in the sentences. The word contex vector $c^{(w)}$ The ou tput of $c^{(c)}$ character-level matrix of context vectors $C&#x3D;[\\boldsymbol{c}^{(1)},\\ldots,\\boldsymbol{c}^{(n_S)}]\\in\\mathbb{R}^{d_v^{(S)}\\times n_S}$ context output of the attention model is the concatenation of the word-level, and character-level context vectors The attention-via-attention technique uses representa</p>\n<p>not always be available for each level of the data, or it may be desirable to let the model create the representations during the process by building them from lower level representations. A technique referred to as hierarchical attention [5] can be used in this situation. Hierarchical attention is another technique that allows one to apply attention on different levels of the data.Yet, the exact mechanisms work quite differently compared to attention-via-attention. The idea is to start at the lowest level, and then create representations, or summaries, of the next level using attention. This process is repeated till the highest level is reached. To make this a little clearer, suppose one attempts to create a model for document classification, similarly to the implementation from [5]. We analyze a document containing $7lS$ sentences, with the sth sentence containing $n_{s}$ words, for $s$ &#x3D; $l, \\ldots , n_{S}$ . One could use attention based on just the collection of words to classify the document. However a significant amount of important context is then left out of the analysis, since the model will consider all words as a single long sentence, and will therefore not consider the context within the separate sentences.Instead, one can use the hierarchical structure of a document (words form sentences, and sentences form the document) Fig. 8 illustrates the structure of hierarchical attention</p>\n<p>For each sentence in the document,a sentence representa tion $\\boldsymbol{c}^{(s)}\\in\\mathbb{R}^{d_{v}^{(S)}}$ is produced, for $s&#x3D;1,\\ldots,n_{S}$, where $d_v^{(S)}$ is the dimension of the value vectors used in the attention model for sentence representations (Attention Modules in</p>\n<p><img src=\"/./images/fLQHc0WG3s8qDYeSbBDR2XDFQUQhGQiKx.png\"></p>\n<p>Fig. 8. An illustration of hierarchical attention</p>\n<p>Fig. 8). The representation is a context vector from an word, and $Tl_{8}$ represents the amount of words in sentence S For extra clarification, the columns of $F^{(s)}$ are feature vectors that correspond to the words in sentence 5 .As shown in Fig. 8, each feature matrix $F^{(s)}$ is used as input for an atten tion model,which produces the context vector $c^{(s)}$ , for each $s&#x3D;1,\\ldots,n_{S}$ . No queries are used in this step, so it can be considered a self-attentive mechanism. The context vectors is constructed by grouping all the obtained context vectors together as columns. Finally, attention is calculated using $C$ as feature input, producing the representation of the entire tons oitetochis adoent in thonte vector $\\boldsymbol{c}^{(D)}\\in\\mathbb{R}^{d_{v}^{(D)}}$ where $d_{v}^{(D)}$ is the dimension of the value vectors in the attention model for document representation (Attention Modulep in Fig. 8) This context vector can be used to classify the document, since it is essentially a summary of all the sentences (and therefore also the words)in the document Multi-level models can be used in a variety of tasks.</p>\n<p>For example, in [28], hierarchical attention is used in a recommender system to model user preferences at the longterm level and the short-term level. Similarly, [45] proposes a hierarchical model for recommending social media images based on user preferences. Hierarchical attention has also been successfully applied in other domains.For example [46] proposes to use hierarchical attention in a video action recognition model to capture motion information at the the long-term level and the short-term level. Furthermore, [47] proposes a hierarchical attention model for cross-domain. sentiment classification. In [48], a hierarchical attention model for chatbot response generation is proposed. Lastly using image data,[49] proposes a hierarchical attention model for crowdcounting</p>\n<h3 id=\"3-1-3Feature-Representations\"><a href=\"#3-1-3Feature-Representations\" class=\"headerlink\" title=\"3.1.3Feature Representations\"></a>3.1.3Feature Representations</h3><p>In a basic attention model, a single embedding or representation model is used to produce feature representa tions for the model to attend to.This is referred to as single-representational attention. Yet, one may also opt to incorporate multiple representations into the model. In. [50], it is argued that allowing a model access to multiple</p>\n<p>TABLE 2 Overview of score function (score $(\\boldsymbol{q},\\boldsymbol{k}_l)]$ forms.</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Function</th>\n<th>Parameters</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Additive (Concatenate) [3]</td>\n<td>$w \\times act(W_1 q + W_2 k) + b$</td>\n<td>$w \\in \\mathbb{R}^{d \\times d}$, $W_1 \\in \\mathbb{R}^{d \\times d \\times k}$, $W_2 \\in \\mathbb{R}^{d \\times d \\times k}$, $b \\in \\mathbb{R}^d$</td>\n</tr>\n<tr>\n<td>Multiplicative (Dot-Product) [4]</td>\n<td>$q \\times k_i$</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Scaled Multiplicative [13]</td>\n<td>$\\frac{k_i}{d} \\times W \\times q$</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Generalized [4]</td>\n<td>$k_i &#x2F; (d \\times W \\times q)$</td>\n<td>$W \\in \\mathbb{R}^{d \\times d \\times k}$</td>\n</tr>\n<tr>\n<td>Biased General [54]</td>\n<td>$k_i&#x2F;(W \\times x \\times q) + b$</td>\n<td>$W \\in \\mathbb{R}^{d \\times d \\times k}$, $x \\in \\mathbb{R}^{d \\times d \\times k}$, $b \\in \\mathbb{R}^d$</td>\n</tr>\n<tr>\n<td>Activated General [34]</td>\n<td>$act(k_i&#x2F;(W \\times x \\times q))$</td>\n<td>$W \\in \\mathbb{R}^{d \\times d \\times k}$, $x \\in \\mathbb{R}^{d \\times d \\times k}$, $b \\in \\mathbb{R}^d$</td>\n</tr>\n<tr>\n<td>Similarity [55]</td>\n<td>$q \\times k_i$</td>\n<td>-</td>\n</tr>\n</tbody></table>\n<p>embeddings can allow one to create even higher quality representations. Similarly, [51] incorporates multiple representations of the same book (textual, syntactic, semantic visual etc.) into the feature model. Feature representations are an important part of the attention model, but attention can also be an important part of the feature model. The idea is to create a new representation by taking a weighted average of multiple representations, where the weights are determined via attention.This technique is referred to as multi-representational attention, and allows one to create so-called meta-embeddings. Suppose one wants to create a meta-embedding for a word 2 for which $E$ embeddings $\\boldsymbol{x}^{(e_1)},\\ldots,\\boldsymbol{x}^{(e_E)}$ are available. Each embedding $x^{(e_{i})}$ is of size $d_{e_{i}t}$ for $i&#x3D;1,\\ldots,E$ Since not all embeddings are of the same size,a transformation is performed to normalize the embedding dimensions. Using embedding-specific weight parameters, each embedding $x^{(e_i)}$ is transformed into the size-normalized embedding $x^{(t_{i})}\\in\\mathbb{R}^{d_{t}}$ ,where $d_{t}$ is the size of every transformed word embedding, as shown in (21).<br>$$x^{(t_{i})}&#x3D;W_{e_{i}}\\times x^{(e_{i})}+b_{e_{i}},\\d_{t}\\times1\\quad d_{t}\\times d_{e_{i}}\\quad d_{e_{i}}\\times1\\quad d_{t}\\times1$$</p>\n<p>where $W_{e_{i}}$ $\\in$ $\\mathbb{R} ^{d_{t}\\times d_{e_{i}}}$ and $b_{e_i}\\in\\mathbb{R}^{d_t}$ are trainable, embedding-specific weights matrices. The final embedding. $x^{(e)}\\in\\mathbb{R}^{d_t}$ is a weighted average of the previously calculated transformed representations, as shown in (22)</p>\n<p>$$\\boldsymbol{x}<em>{d_t\\times1}^{(e)}&#x3D;\\sum</em>{i&#x3D;1}^Ea_i\\times\\boldsymbol{x}_{d_t\\times1}^{(t_i)}.$$</p>\n<p>The final representation $x^{(e)}$ can be interpreted as the context vector from an attention model,meaning that the weights $a_1,\\ldots,a_E\\in\\mathbb{R}^1$ are attention weights. Attention can be calculated as normally, where the columns of the features matrix $F$ are the transformed representations $\\boldsymbol{x}^{(t_1)},\\ldots,\\boldsymbol{x}^{(t_E)}$ Thequery in this case can be ignored since it is constant in all cases. Essentially, the query is “Which representations are the most important?”” in every situation. As such, this is a self-attentive mechanism While an interesting idea, applications of multi-</p>\n<p>representational attention are limited. One example of the application of this technique is found in[52], where a multirepresentational attention mechanism has been applied to generate multi-lingual meta-embeddings. Another example is [53], where a multi-representational text classification embeddings from part-of-speech tagging, named entity rec ognizers, and character-level and word-level embeddings.</p>\n<h3 id=\"3-2-General-Attention-Mechanisms\"><a href=\"#3-2-General-Attention-Mechanisms\" class=\"headerlink\" title=\"3.2 General Attention Mechanisms\"></a>3.2 General Attention Mechanisms</h3><p>can be applied in any type of attention model. The structure of this component can be broken down into the following sub-aspects: the attention score function, the attention alignment, and attention dimensionality</p>\n<h2 id=\"3-2-1Attention-Scoring\"><a href=\"#3-2-1Attention-Scoring\" class=\"headerlink\" title=\"3.2.1Attention Scoring\"></a>3.2.1Attention Scoring</h2><p>The attention score function is a crucial component in how attention is calculated. Various approaches have been developed that each have their own advantages and disadvantages.An overview of these functions is provided in Table 2.</p>\n<p>Each row of Table 2 presents a possible form for the function $\\operatorname{score}(\\boldsymbol{q},\\boldsymbol{k}<em>l)$ , as seen in (23), where $q$ is the query vector, and $k</em>{l}$ is thelth column of $K$ Note that the score functions presented in this section can be more efficiently calculated in matrix form using $K$ instead of each column separately Nevertheless, the score functions are presented using $k_{I}$ to more clearly illustrate the relation between a key and query.</p>\n<p>$$e_l&#x3D;\\text{score}(\\begin{array}{c}q\\d_q\\times1\\end{array},\\begin{array}{c}k_l\\d_k\\times1\\end{array}).$$</p>\n<p>Due to their simplicity, the most popular choices for the score function are the concatenate score function[3] and the multiplicative score function [4]. The multiplicative score function has the advantage of being computationally inexpensive due to highly optimized vector operations. How. ever, the multiplicative function may produce non-optimal results when the dimension $d_{k}$ is too large [56]. When $d_{k}$ is large, the dot-product between $q$ and $k_{I}$ can grow large in magnitude.To illustrate this, in [13], an example is used where the elements of $q$ and $k_l$ are all normally distributed with a mean equal to zero, and a variance equal to one. Then, the dot-product of the vectors has a variance of $d_{k}$ A higher variance means a higher chance of numbers that are large in magnitude. When the softmax function of the alignment step is then applied using these large numbers, the gradient will become very small, meaning the model will have trouble converging [13]. To adjust for this, [13] proposes to scale the multiplicative function by the factor mode is eaepesed t at nerpeate dferent eesene $\\frac{1}{\\sqrt{d_{k_{k}}}}$ prod ucing the caled utiplictiescore nc en </p>\n<p>introducing a weights matrix $W$ .This form, referred to as the general score function, allows for an extra transformation of $k_l$ .The biased general score function [54]is a further extension of the general function that introduces This major category consists of attention mechanisms that a bias weight vector $b$ A final extension on this function named the activated general score function is introduced in [34], and includes the use of both a bias weight $b$, and an activation function act(). The previously presented score functions are all based on</p>\n<p>determining a type of similarity between the key vector and the query vector. As such, more typical similarity measures such as the Euclidean $(\\mathrm{L}_{2})$ distance and cosine similarity can also be implemented [55]. These scoring methods are summarized under the similarity score function which is represented by the similarity() function</p>\n<p>There typically is no common usage across domains regarding score functions. The choice of score function for a particular task is most often based on empirical experiments. However, there are exceptions when, for example efficiency is vital. In models where this is the case, the multiplicative or scaled multiplicative score functions are typi-However, a compromise can be made in certain situations cally the best choice.An example of this is the TransformerLocal alignment [4] is a method that implements a softmax model, which is generally computationally expensive.</p>\n<h3 id=\"3-2-2Attention-Alignment\"><a href=\"#3-2-2Attention-Alignment\" class=\"headerlink\" title=\"3.2.2Attention Alignment\"></a>3.2.2Attention Alignment</h3><p>The attention alignment is the step after the attention scoring. This alignment process directly determines which parts of the input data the model will attend to. The alignment function is denoted as align() and has various forms. The align() function takes as input the previously calculated attention score vector $e$ and calculates for each element $el$ of e the attention weight $u_{l}$ .These attention weights can then average of the value vectors $v_{1},\\cdots,v_{n_{f}}$</p>\n<p>$$\\boldsymbol{c}<em>{d_v\\times1}&#x3D;\\sum</em>{l&#x3D;1}^{n_f}a_l:\\times:\\boldsymbol{v}_l:.$$</p>\n<p>The most popular alignment method to calculate these weights is a simple softmax function, as depicted in (25)</p>\n<p>$$\\underset{1\\times1}{a_l}&#x3D;\\underset{1\\times1}{\\text{align}}(\\underset{1\\times1}{e_l}:;:\\boldsymbol{e}:)&#x3D;\\frac{\\exp(e_l)}{\\sum_{j&#x3D;1}^{n_f}\\exp(e_j)}.$$</p>\n<p>This alignment method is often referred to as soft alignment in computervision settings [8],or global alignment for sequence data [4]. Nevertheless, both these terms represent the same function and can be interpreted similarly. Soft&#x2F;global alignment can be interpreted as the model attending to all feature vectors.For example, the model attends to all regions in an image, or all words in a sentence. Even though the attention model generally does focus more on specific parts of the input, every part of the input will receive at least some amount of attention due to the nature of the softmax function. Furthermore, an advantage of the softmax function. is that it introduces a probabilistic interpretation to the input vectors. This allows one to easily analyze which parts of the input are important to the output predictions In contrast to soft&#x2F;global alignment, other methods aim</p>\n<p>to achieve a more focused form of alignment. For example hard alignment [8], also known as hard attention or nondeterministic attention, is an alignment type that forces the attention model to focus on exactly one feature vector. Firstly, this method implements the softmax function in the exact same way as global alignment. However, the outputs $a_{1},\\cdots,a_{n_{f}}$ are not used as weights for the context vector calculation. Instead, these values are used as probabilities to draw the choice of the one value vector from.A value $m\\in\\mathbb{R}^1$ is drawn from a multinomial distribution with $a_{1},\\cdots,a_{n_{f}}$ as parameters for the probabilities. Then, the context vector is simply defined as follows:</p>\n<p>$$\\begin{array}{c}c\\d_v\\times1\\end{array}&#x3D;\\begin{array}{c}v_m\\d_v\\times1\\end{array}.$$</p>\n<p>Hard alignment is typically more efficient at inference compared to soft alignment.On the other hand, the main disadvantage of hard attention is that, due to the stochastic</p>\n<p>alignment of attention, the training of the model cannot be done via the regular backpropagation method. Instead simulation and sampling, or reinforcement learning [57] are required to calculate the gradient at the hard attention layer. As such, soft&#x2F;global attention is generally preferred. distribution, similarly to soft&#x2F;global alignment. But, the. softmax distribution is calculated based only on a subset of the inputs. This method is generally used in combination with sequence data.One has to specify a variable $p\\in\\mathbb{R}^{1}$ that determines the position of the region.Feature vectors close to $p$ will be attended to by the model, and vectors too far from $P$ will be ignored. The size of the subset will be determined by the variable $D\\in\\mathbb{R}^{1}$ . Summarizing the attention model will apply a softmax function on the attention scores in the subset $[p-D,p+D]$ . In other words be used to create the context vector C by taking a weighteda window is placed on the input and soft&#x2F;global attention is calculated within that window：<br>$$a_{l}&#x3D;\\text{align}(\\begin{smallmatrix}e_{l}&amp;;&amp;e\\1\\times1&amp;n_{f}\\times1\\end{smallmatrix})&#x3D;\\frac{\\exp(e_{l})}{\\sum_{j&#x3D;p-D}^{p+D}\\exp(e_{j})}.$$</p>\n<p>The question that remains is how to determine the location parameter $P$ The first method is referred to as monotonic alignment. This straightforward method entails simply setting the location parameter equal to the location of the prediction in the output sequence.Another method of deter mining the position of the region is referred to as predictive alignment. As the name entails, the model attempts to actually predict the location of interest in the sequence</p>\n<p>$$p:&#x3D;:S:\\times\\mathrm{sigmoid}(\\boldsymbol{w}<em>{1\\times d</em>{p}}^{T}:\\times\\mathrm{tanh}(\\boldsymbol{W}_{p}:\\times:\\boldsymbol{q}:)),$$</p>\n<p>where $S\\in\\mathbb{R}^{1}$ is the length of the input sequence, and $w_{p}\\in\\mathbb{R}^{d_{P}}$ and $W_{p}$ $\\in$ $\\mathbb{R} ^{d_{p}\\times d_{q}}$ are both trainable weights parameters. The sigmoid function multiplied by $S$ makes sure that $p$ is in the range $[0,S]$ .Additionally, in [4], it is recommended to add an additional term to the alignment function to favor alignment around $P$</p>\n<p>$$a_{l}&#x3D;\\operatorname{align}(\\begin{smallmatrix}e_{l}&amp;;&amp;e\\1\\times1&amp;n_{f}\\times1\\end{smallmatrix})\\exp(-\\frac{(l-p)^{2})}{2\\sigma^{2}}),$$</p>\n<p>where $\\sigma\\in\\mathbb{R}^{1}$ is empirically set equal to $\\frac{D}{2}$ according to [4]. Another proposed method for compromising between soft and hard alignment is reinforced alignment[58]. Similarly to local alignment, a subset of the feature vectors is determined, for which soft alignment is calculated. However, instead of using a window to determine the subset, reinforced alignment uses a reinforcement learning agent. [57], similarly to hard alignment, to choose the subset of feature vectors. The attention calculation based on these chosen feature vectors is the same as regular soft alignment Soft alignment is often regarded as the standard align</p>\n<p>ment function for attention models in practically every domain.Yet, the other alignment methods have also seen inter esting uses in various domains. For example, hard attention is used in [59] for the task of visual question answering In [60], both soft and hard attention are used in a graph attention model for multi-agent game abstraction. Similarly. in [61], both global and local alignment are used for review rating predictions. Reinforced alignment has been employed</p>\n<p>in combination with a co-attention structure in [62] for the extended with the multi-dimensional attention mechanisn task of aspect sentiment classification. In [63], reinforced alignment is used for the task of person re-identification using surveillance images</p>\n<h3 id=\"3-2-3-Attention-Dimensionality\"><a href=\"#3-2-3-Attention-Dimensionality\" class=\"headerlink\" title=\"3.2.3 Attention Dimensionality\"></a>3.2.3 Attention Dimensionality</h3><p>All previous model specifications of attention use a scalar weight $a_l$ for each value vector $v_l$ . This technique is referred to as single-dimensional attention. However, instead of determining a single attention score and weight for the entire vector, [64] proposes to calculate weights for every single feature in those vectors separately. This technique is referred to as multi-dimensional attention,since the attention weights now become higher dimensional vectors. The idea is that the model no longer has to attend to entire vectors, but it can instead pick and choose specific elements from those vectors. More specifically, attention is calculated for each dimension.As such,the model must create a vecto of attention weights $a_l\\in\\mathbb{H}^d_v$ for each value vector $v_l\\in\\mathbb{R}^dv$ The context vector can then be calculated by summing the element-wise multiplications (o) of the value vectors $\\boldsymbol{v}<em>1,\\ldots,\\boldsymbol{v}</em>{n_f}\\in\\mathbb{R}^d$, and the corresponding attention weight vectors $a_{1},\\ldots,a_{n_{f}}\\in\\mathbb{R}^{d_{\\upsilon}}$ as follows:</p>\n<p>$$\\boldsymbol{c}<em>{d_v\\times1}&#x3D;\\sum</em>{l&#x3D;1}^{n_f}\\boldsymbol{a}_l\\circ\\boldsymbol{v}_l.$$</p>\n<p>However, since one needs to create attention weight vectors, this technique requires adjusted attention score and weight calculations. For example, the concatenate score function found in Table 2 can be adjusted by changing the $w\\in\\mathbb{R}^{d_{\\cos}}$ weights vector to the weight matrix $W_{d}\\in\\mathbb{R}^{d_{w}\\times d_{v}}$</p>\n<p>$$e_{l}:&#x3D;:W_{d}^{T}:\\times\\mathrm{act}(:W_{1}:\\times:q:+:W_{2}:\\times:k_{l}:+:b:).$$</p>\n<p>This new score function produces the attention score vectors $\\boldsymbol{e}<em>1,\\ldots,\\boldsymbol{e}</em>{n_f}\\in\\mathbb{R}^dv$ . These score vectors can be combined into a matrix of scores $e$ &#x3D; $[ \\boldsymbol{e}<em>{1}, \\ldots , \\boldsymbol{e}</em>{n_{f}}]$ $\\in$ $\\mathbb{R} ^{d_{v}\\times n_{f}}$ To produce multi-dimensional attention weights, the alignment function stays the same, but it is applied for each feature across the attention score columns. To illustrate, when implementing soft attention, the attention weight produced from the ith element of score vector $e_{l}$ is defined as follows:</p>\n<p>$$\\underset{1\\times1}{a_{l,i}}&#x3D;\\underset{1\\times1}{\\text{align}}(e_{l,i};\\quad\\boldsymbol{e}<em>{d</em>{v}\\times n_{f}})&#x3D;\\frac{\\exp(e_{l,i})}{\\sum_{j&#x3D;1}^{n_{f}}\\exp(e_{j,i})},$$</p>\n<p>where $el,i$ represents the ith element of score vector $e_{l,}$ and $ul,i$ is the ith element of the attention weights vector $a_{l}$ .Finally, these attention weight vectors can be used to compute the context vector as presented in (30). Multi-dimensional attention is a very general mecha</p>\n<p>nism that can be applied in practically every attention model, but actual applications of the technique have been relatively sparse. One application example is [65], where multi-dimensional attention is used in a model for named entity recognition based on text and visual context from multimedia posts. In [66], multi-dimensional attention is used in a model for answer selection in community question answering. In [67], the U-net model for medical image segmentation is extended with a multi-dimensional attention mechanism. Similarly, in [68], the Transformer model is</p>\n<p>for the task of dialogue response generation. In [69], multidimensional attention is used to extend graph attention networks for dialogue state tracking. Lastly, for the task of next-item recommendation, [70] proposes a model that incorporates multi-dimensional attention</p>\n<h3 id=\"3-3-Query-Related-Attention-Mechanisms\"><a href=\"#3-3-Query-Related-Attention-Mechanisms\" class=\"headerlink\" title=\"3.3 Query-Related Attention Mechanisms\"></a>3.3 Query-Related Attention Mechanisms</h3><p>Queries are an important part of any attention model, since they directly determine which information is extracted from the feature vectors. These queries are based on the desired output of the task model, and can be interpreted as literal questions. Some queries have specific characteristics that require specific types of mechanisms to process them. As such, this category encapsulates the attention mechanisms that deal with specific types of query characteristics. The mechanisms in this category dealwith one of the two following query characteristics: the type of queries or the multiplicity of queries</p>\n<h2 id=\"3-3-1Type-of-Queries\"><a href=\"#3-3-1Type-of-Queries\" class=\"headerlink\" title=\"3.3.1Type of Queries\"></a>3.3.1Type of Queries</h2><p>Different attention models employ attention for different purposes, meaning that distinct query types are necessary There are basic queries, which are queries that are typically straightforward to definebased on the data and model.For example, the hidden state for one prediction in an RNN is often used as the query for the next prediction. One could also use a vector of auxiliary variables as query. For example, when doing medical image classification, general patient characteristics can be incorporated into a query Some attention mechanisms, such as co-attention, rota-</p>\n<p>tory attention, and attention-over-attention, use specialized queries. For example, rotatory attention uses the context vector from another attention module as query, while interactive co-attention uses an averaged keys vector based on another input. Another case one can consider is when attention is calculated based purely on the feature vectors This concept has been mentioned before and is referred to as self-attention or intra-attention [71]. We say that the models use self-attentive queries. There are two ways of interpreting such queries. Firstly, one can say that the query is constant. For example, document classification requires only a single classification as the output of the model. As such, the query is always the same, namely: “What is the class of the document?”. The query can be ignored and attention can be calculated based only on the features themselves Score functions can be adjusted for this by making the query vector a vector of constants or removing it entirely:<br>$$\\mathrm{score}(\\boldsymbol{k}<em>{l})&#x3D;:\\boldsymbol{w}^{T}\\times\\mathrm{act}(\\boldsymbol{W}\\times:\\boldsymbol{k}</em>{l}+\\boldsymbol{b}:).\\d_{k}\\times1\\quad1\\times d_{w}\\quad d_{w}\\times d_{k}\\quad d_{k}\\times1\\quad d_{w}\\times1$$</p>\n<p>Additionally, one can also interpret self-attention as learning the query along the way, meaning that the query can be defined as a trainable vector of weights.For example,the dot-product score function may take the following form</p>\n<p>$$\\begin{array}{rcl}\\mathrm{score}(\\boldsymbol{k}_l)&amp;&#x3D;&amp;\\boldsymbol{q}^T\\times\\boldsymbol{k}_l\\d_k\\times1&amp;&amp;1\\times d_k&amp;d_k\\times1\\end{array},$$</p>\n<p>where $q\\in\\mathbb{R}^{d_k}$ is a trainable vector of weights. One could also interpret vector $b\\in\\mathbb{R}^{d_{w}}$ as the query in (33).Another</p>\n<p>use of self-attention istouncover the relations between the feature vectors $f_{1},\\ldots,f_{n_{f}}$ . These relations can then be used as additional information to incorporate into new representations of the feature vectors. With basic attention mechanisms, the keys matrix $K$, and the values matrix $V$ are extracted from the features matrix $F_{\\prime}$ while the query $q$ is produced separately. For this type of self-attention, the query vectors are extracted in a similar process as the keys and values, via a transformation matrix of trainable weights $W_{Q}\\in\\mathbb{R}^{d_{q}\\times d_{f}}$ . We define the matrix $Q&#x3D;\\left[\\boldsymbol{q}<em>{1},\\ldots,\\boldsymbol{q}</em>{n_{f}}\\right]\\in$ $\\Pi$玻 $d_q\\times nf$ which canbe obtained asfollows:</p>\n<p>$$Q_{d_{q}\\times n_{f}}&#x3D;W_{Q}\\times F_{d_{f}\\times n_{f}}.$$</p>\n<p>Each column of $Q$ can be used as the query for the attention model. When attention is calculated using a query $q.$ the resulting context vector C will summarize the information the query, or a column of $Q$ , is now also a feature vector representation, the context vector contains the information of allfeature vectors that are important to that specific feature vector. In other words, the context vectors capture the relations between the feature vectors. For example, selfattention allows one to extract the relations between words: which verbs refer to which nouns, which pronouns refer to which nouns, etc. For images, self-attention can be used to determine which image regions relate to each other While self-attention is placed in the query-related cat-</p>\n<p>egory, it is also very much related to the feature model. Namely, self-attention is a technique that is often used in the feature model to create improved representations of the feature vectors. For example, the Transformer model for language processing [13], and the Transformer model for image processing [15], both use multiple rounds of (multihead) self-attention to improve the representation of the feature vectors. The relations captured by the self-attention mechanism are incorporated into new representations. A. simple method of determining such a new representation is to simply set the feature vectors equal to the acquired self-attention context vectors [71], as presented in (36).</p>\n<p>$$f_{_{d_f\\times1}}^{(\\mathrm{new})}&#x3D;\\underset{d_f\\times1}{\\operatorname*{c}},$$</p>\n<p>where $f^{(\\mathrm{new})}$ is the updated feature vector. Another possibility is to add the context vectors to the previous feature vectors with an additional normalization layer [13]:</p>\n<p>$$f_{d_f\\times1}^{(\\mathrm{new})}&#x3D;\\mathrm{Normalize}(f_{d_f\\times1}^{(\\mathrm{old})}+\\sum_{d_f\\times1}),$$</p>\n<p>where $f^{(\\mathrm{old})}$ is the previous feature vector, and Normalize( is a normalization layer[72]. Using such techniques, self embeddings that enhance model accuracy [71]. Self-attention is arguably one of the more important</p>\n<p>types of attention, partly due to its vital role in the highly popular Transformer model. Self-attention is a very general mechanism and can be applied to practically any problem. As such, self-attention has been extensively explored in many different fields in both Transformer-based architec tures and other types of models. For example, in [73], selfattention is explored for image recognition tasks, and results</p>\n<p>indicate that the technique may have substantial advantages with regards to robustness and generalization. In [74], selfattention is used in a generative adversarial network (GAN) [75] to determine which regions of the input image to focus on when generating the regions of a new image. In [76], selfattention is used to design a state-of-the-art medical image segmentation model. Naturally, self-attention can also be used for video processing. In [77], a self-attention model is proposed for the purpose of video summarization that reaches state-of-the-art results. In other fields, like audio processing, self-attention has been explored as well. In [78], self-attention is used to create a speech recognition model Self-attention has also been explored in overlapping domains. For example, in [79], the self-attention Transformer architecture is used to create a model that can recognize phrases from audio and by lip-reading from a video. For the problem of next item recommendation, [80] proposes in the feature vectors that is important to the query. Sincea Transformer model that explicitly captures item-item re lations using self-attention. Self-attention also has applica-. tions in any natural language processing fields. For example, in [81], self-attention is used for sentiment analysis. Selfattention is also highly popular for graph models. For example, self-attention is explored in [82] for the purpose of representation learning in communication networks and rating networks. Additionally, the first attention model for graph networks was based on self-attention [83]</p>\n<h3 id=\"3-3-2Multiplicity-ot-Queries\"><a href=\"#3-3-2Multiplicity-ot-Queries\" class=\"headerlink\" title=\"3.3.2Multiplicity ot Queries\"></a>3.3.2Multiplicity ot Queries</h3><p>In previous examples, the attention model generally used a single query for a prediction.We say that such models use singular query attention. However, there are attention architectures that allow the model to compute attention using multiple queries. Note that this is different from, for example, an RNN that may involve multiple queries to produce a sequence of predictions. Namely, such a model still requires only a single query per prediction One example of a technique that incorporates multiple</p>\n<p>queries is multi-head attention [13], as presented in Fig. 9. Multi-head attention works by implementing multiple attention modules in parallel by utilizing multiple different versions of the same query. The idea is to linearly transform the query $q$ using different weight matrices. Each newly formed query essentially asks fora different type of relevant information, allowing the attention model to introduce more information into the context vector calculation. An attention model implements $d\\geq1$ heads with each attention head having its own query vector, keys matrix, and values matrix $q^{(j)}$ ， $K^{(j)}$ and $V^{(j)}$ ，for $j$ &#x3D; $1, \\ldots , d$ The query $q^{(j)}$ is obtained by linearly transforming the original query $\\Psi$, while the matrices $K^{(j)}$ and $V^{(j)}$ are obtained through linear transformations of $F$ .As such,each attention head attention has been used to create improved word or sentence has its own learnable weights matrices $W_{q}^{(j)}$ ” $W_{K}^{(j)}$ and $W_{V}^{(j)}$ for these transformations.The calculation of the query keys, and values for the $j$ th head are defined as follows:<br>$$\\boldsymbol{q}<em>{d</em>{q}\\times1}^{(j)}&#x3D;\\boldsymbol{W}<em>{q}^{(j)}\\times\\boldsymbol{q}</em>{d_{q}\\times1},\\quad\\boldsymbol{K}<em>{d</em>{k}\\times n_{f}}^{(j)}&#x3D;\\boldsymbol{W}<em>{K}^{(j)}\\times\\boldsymbol{F}</em>{d_{f}\\times n_{f}},\\\\boldsymbol{V}<em>{d</em>{v}\\times n_{f}}^{(j)}&#x3D;\\boldsymbol{W}<em>{V}^{(j)}\\times\\boldsymbol{F}</em>{d_{f}\\times n_{f}}.$$</p>\n<p>Thus,each head creates its own representations of the query $q$, and the input matrix $F$ Each head can therefore</p>\n<p><img src=\"/./images/fLxYvsz8GoGucwnBRCXegWEcG76geRpkm.png\"></p>\n<p>Fig. 9. An illustration of multi-head attention</p>\n<p>learn to focus on different parts of the inputs, allowing the model to attend to more information.For example when training a machine translation model, one attention head can learn to focus on which nouns (e.g., student, car, apple) do certain verbs (e.g., walking, driving, buying) refer to,while another attention head learns to focus on which $\\boldsymbol{e}^{( j) }$ &#x3D; $[ e_{1}^{( j) }, \\ldots , e_{n_{f}}^{( j) }]$ $\\in$ $\\mathbb{R} ^{n_{f}}$ ae $q^{(s)}$ Pr $q^{(0)}$ A $S$, Cat $q^{(s+1)}$ of ttention weights $\\boldsymbol{a}^{( j) }$ &#x3D; $[ a_{1}^{( j) }, \\ldots , a_{n_{f}}^{( j) }]$ $\\in$ $\\mathbb{R} ^{n_{f}}$ As canpossibly using the current context vector $c^{(s)}$ as another be expected, each attention model produces its own context vector $\\boldsymbol{c}^{(j)}\\in\\mathbb{R}^{dv}$ ,as follows:</p>\n<p>$$\\boldsymbol{c}<em>{d</em>{v}\\times1}^{(j)}&#x3D;\\sum_{l&#x3D;1}^{n_{f}}a_{l}^{(j)}\\times\\boldsymbol{v}_{l}^{(j)}.$$</p>\n<p>The goal is still to create a single context vector as output of the attention model.As such, the context vectors produced by the individual attention heads are concatenated into a single vector. Afterwards, a linear transformation is applied using the weight matrix $W_{O}\\in\\mathbb{R}^{d_{c}\\times d_{v}d}$ tomake sure the resulting context vector $c\\in\\mathbb{R}^{d_{c}}$ has the desired dimension. This calculation is presented in (40). The dimension $d_{c}$ can be pre-specified by, for example, setting it equal to $d_{v,}$ so that the context vector dimension is unchanged.</p>\n<p>$$c:&#x3D;:W_{O}:\\times\\mathrm{concat}(\\boldsymbol{c}^{(1)},…,\\boldsymbol{c}^{(d)}).$$</p>\n<p>Multi-head attention processes multiple attention modmented sequentially to iteratively adjust the context vec tors.Each of these attention modules are referred to as “repetitions” or “rounds” of attention. Such attention architectures are referred to as multi-hop attention models also known as multi-step attention models.An important note to consider is the fact that multi-hop attention tant mote to con sider is be ap thas m unt hap ustention she that the actunl o $c^{(s)}$ ind $q^{(s+1)}$ the atentoen noedelin throughout various works. While the mechanism always involves multiple rounds of attention, the multi-hop implementation proposed in [84] differs from the mechanism proposed in[85]or [86]. Another interesting example is [87], where a “multi-hop”attention model is proposed that would actually be considered alternating co-attention in this survey, as explained in Subsection 3.1.1. We present a general form of multi-hop attention that</p>\n<p>is largely a generalization of the techniquesintroduced into produce the next context vector $c^{(s+1)}$ .Onemust note [85] and [88]. Fig. 10 provides an example implementation of a multi-hop attention mechanism. The general idea isweights,meaning that the number of parameters do not to iteratively transform the query, and use the query to transform the context vector,such that the model can extract different information in each step. Remember that a query</p>\n<p><img src=\"/./images/fC2yeUZnAYXZanLazGfbKKQkNuntGh8I1.png\"></p>\n<p>Fig. 10.An example illustration of multi-hop attention. Solid arrows represent the base multi-hop model structure, while dotted arrows represent optional connections.</p>\n<p>is similar to a literal question. As such, one can interpret the transformed queries as asking the same question in a different manner or from a different perspective, similarly to the queries in multi-head attention. The query that was previously denoted by $\\Psi$ is now referred to as the initial input, and some transformation function transform()</p>\n<p>$$\\boldsymbol{q}<em>{d</em>{q}\\times1}^{(s+1)}&#x3D;\\mathrm{transform}(\\boldsymbol{q}^{(s)},:\\boldsymbol{c}^{(s)}).$$</p>\n<p>For the specific form of the transformation function transform(),[85]proposes to use a mechanism similar to self-attention. Essentially, the queries used by the question answer matching model proposed in [85] were originally based on a set of feature vectors extracted from a question [85] also defines the original query $q^{(0)}$ as the unweighted average of these feature vectors.At each hop $S$, attention can be calculated on these feature vectors using the previous query $q^{(s)}$ as the query in this process.The resulting context vector of this calculation is the next query vector. Using the context vector $c^{(s)}$ instead of $q^{(s)}$ as the query for this process is also a possibility, which is similar to the LCRRot-hop model proposed in [43] and the multi-step model proposed in [88]. Such a connection is represented by the dotted arrows in Fig. 10. The transformation mechanism ules in parallel, but attention modules can also be imple-uses either the $q^{(s)}$ or the context vector $c^{(s)}$ as query, but a combination via concatenation is also possible. Each query representation is used as input for the at</p>\n<p>tention module to compute attention on the columns of the feature matrix $F$ ,as seen previously. One main difference. however, is that the context vector $c^{(s)}$ is also used as input, score function is presented in (42). Note that the initial context vector $c^{(0)}$ is predefined.One way of doing thisis by setting it equal to the unweighted average of the value vectors $\\boldsymbol{v}<em>1,\\ldots,\\boldsymbol{v}</em>{n_f}\\in\\mathbb{R}^{d_v}$ extracted from $F$</p>\n<p>$$e_{l}^{(s)}&#x3D;\\text{score}(\\text{concat}(\\boldsymbol{q}<em>{d</em>{q}\\times1}^{(s+1)},\\boldsymbol{c}^{(s)}),\\boldsymbol{k}_{l}).$$</p>\n<p>An alignment function and the value vectors are then used that in [85], the weights used in each iteration are the same scale with the number of repetitions. Yet,using multiple hops with different weight matrices can also be viable, as shown by the Transformer model [13] and in [88]. It may be</p>\n<p>difficult to grasp why $c^{(s)}$ is part of the query input for the attention model. Essentially, this technique is closely related to self-attention in the sense that,in each iteration,a new context representation is created from the feature vectors and the context vector. The essence of this mechanism is that one wants to iteratively alter the query and the context vector,while attending to the feature vectors.In the process, the new representations of the context vector absorb more differentkinds of information.This is also the main difference between this type of attention and multi-head attention. Multi-head attention creates multiple context vectors from multiple queries and combines them to create a final context vector as output. Multi-hop attention iteratively. refines the context vector by incorporating information from the different queries. This does have the disadvantage of having to calculate attention sequentially Interestingly, due to the variations in which multi-hop</p>\n<p>attention has been proposed, some consider the Transformer model’s encoder and decoder to consist of several singlehop attention mechanisms [84] instead of being a multihop model. However, in the context of this survey, we consider the Transformer model to be an alternative form of the multi-hop mechanism, as the features matrix $F$ is not directly reused in each step. Instead, $F$ is only used as an input for the first hop, and is transformed via self-attention into a new representation. The self-attention mechanism uses each feature vector in $F$ as a query, resulting in a matrix of context vectors as output of each attention hop. The $p_{1},\\cdots,p_{d_{[0]}}$ as accurately as possible compared to the true intermediate context vectors are turned into matrices and represent iterative transformations of the matrix $F$ which are used in the consecutive steps. Thus, the Transformer model iteratively refines the features matrix $F$ by extracting and incorporating new information When dealing with a classification task, another idea is</p>\n<p>to use a different query for each class. This is the basic principle behind capsule-based attention [89], as inspired by the capsule networks [90]. Suppose we have the feature classes that the model can predict. Then, a capsule-based attention model defines a capsule for each of the $d_{y}$ classes that each take as input the feature vectors.Each capsule consists of, in order, an attention module, a probability module, and a reconstruction module, which are depicted in. Fig. 11. The attention modules all use self-attentive queries so each module learns its own query: “Which feature vectors are important to identify this class?”. In [89], a self-attentive multiplicative score function is used for this purpose</p>\n<p>$$e_{c,l}&#x3D;\\begin{array}{ccc}\\boldsymbol{q}_c^T&amp;\\times&amp;\\boldsymbol{k}_l\\1\\times1&amp;1\\times d_k&amp;d_k\\times1\\end{array},$$</p>\n<p>where $e_{c,l}\\in\\mathbb{R}^1$ is the attention score for vector $l$ in capsule $U_{f}$ and $q_{c}\\in\\mathbb{R}^{d_k}$ is a trainable query for capsule $C_{f}$ for $c&#x3D;1,\\ldots,d_{y}$ . Each attention module then uses an alignment function, and uses the produced attention weights to determine a context vector $c_{c}\\in\\mathbb{R}^{d_{v}}$ .Next, the context vector $C_{t}$ is fed through a probability layer consisting of a linear transformation with a sigmoid activation function:</p>\n<p>$$\\begin{aligned}p_{c}&amp;&#x3D;\\mathrm{sigmoid}(\\boldsymbol{w}<em>{c}^{T}\\times\\boldsymbol{c}</em>{c}:+:b_{c}:),\\&amp;<em>{1\\times1}\\times d</em>{v}\\quad d_{v}\\times1\\quad1\\times1\\end{aligned}$$</p>\n<p><img src=\"/./images/fFOVLaKiKKX8YVu44wZt6GwmcOzskqWOp.png\"></p>\n<p>Fig. 11. An illustration of capsule-based attention</p>\n<p>where $w_c\\in\\mathbb{R}^dv$ and $b_{c}\\in\\mathbb{R}^{1}$ are trainable capsule-specific weights parameters, and $p_{c}\\in\\mathbb{R}^{1}$ is the predicted probability that the correct class is class $t$ . The final layer is the reconstruction module that creates a class vector representation This representation $r_{c}\\in\\mathbb{R}^{dv}$ is determined by simply multiplying the context vector $C_{c}$ by the probability $P_{c}$</p>\n<p>$$\\begin{array}{rcl}r_c&amp;&#x3D;&amp;p_c\\times&amp;c_c\\d_v\\times1&amp;&amp;1\\times1&amp;&amp;d_v\\times1\\end{array}.$$</p>\n<p>The capsule representation is used when training the model. First of all, the model is trained to predict the probabilities values. Secondly, via a joint loss function, the model is also trained to accurately construct the capsule representations ${\\boldsymbol{r}}1,\\cdots,{\\boldsymbol{r}}d_{\\psi}$ . A features representation $f\\in\\mathbb{R}^{d_f}$ is defined which is simply the unweighted average of the original feature vectors. The idea is to train the model such that vector representations from capsules that are not the correct class differ significantly from $f$ while the representation from the correct capsule is very similar to $f$ A dot-product between the capsule representations and the features representation vectors $f_{1}, \\ldots , f_{n_{f}}$ $\\in$ $\\mathbb{R} ^{d_{f}}$ , and suppose there are are $d_{y}$ is used in [89] as a measure of the distance between the vectors. Note that $d_v$ must equal $d_{f}$ in this case, otherwise the vectors would have incompatible dimensions. Interestingly, since attention is calculated for each class individually, one can track which specific feature vectors are important for which specific class. In [89], this idea is used to discover which words correspond to which sentiment class. The number of tasks that can make use of multiple</p>\n<p>queries is substantial, due to how general the mechanisms are.As such, the techniques described in this section have been extensively explored in various domains.For example, multi-head attention has been used for speaker recog nition based on audio spectrograms [91]. In [92], multihead attention is used for recommendation of news articles. Additionally, multi-head attention can be beneficial for graph attention models as well [83]. As for multi-hop attention, quite a few papers have been mentioned before but there are still many other interesting examples.For example, in [93], a multi-hop attention model is proposed for medication recommendation. Furthermore, practically every Transformer model makes use of both multi-head and multi-hop attention. The Transformer model has been extensively explored in various domains. For example, in [94], a Transformer model is implemented for image cap-</p>\n<p>tioning. In [95], Transformers are explored for medical image segmentation. In [96], a Transformer model is used for emotion recognition in text messages. A last example of an application of Transformers is [17], which proposes a Transformer model for recommender systems. In comparison with multi-head and multi-hop attention, capsule-based attention is arguably the least popular of the mechanisms discussed for the multiplicity of queries. One example is [97], where an attention-based capsule network is proposed that also includes a multi-hop attention mechanism for the purpose of visual question answering. Another example is [98], where capsule-based attention is used for aspect-level sentiment analysis of restaurant reviews. The multiplicity of queries is a particularly interesting ang Parlprioyesong ar th nthe fieldof audipor 108</p>\n<p>category due to the Transformer model [13], which com provements and iterations of the model have been produced that typically aim to improve the predictive performance, the computational efficiency, or both.For example, the Transformer-XL [99] is an extension of the original Trans former that uses a recurrence mechanism to not be limited by a context window when processing the outputs.This allows the model to learn significantly longer dependencies while also being computationally more efficient during the evaluation phase. Another extension of the Transformer is known as the Reformer model [100]. This model is signifi cantly more efficient computationally, by means of localitysensitive hashing, and memory-wise, by means of reversible residual layers. Such computational improvements are vital, since one of the main disadvantages of the Transformer model is the sheer computational cost due to the complexity of the model scaling quadratically with the amount of input feature vectors. The Linformer model [101] manages to reduce the complexity of the model to scale linearly, while achieving similar performance as the Transformer model This is achieved by approximating the attention weights using a low-rank matrix. The Lite-Transformer model proposed in [102] achieves similar results by implementing two branches within the Transformer block that specialize in capturing global and local information. Another interesting Transformer architecture is the Synthesizer [103] This model replaces the pairwise self-attention mechanism with “synthetic attention weights. Interestingly, the per formance of this model is relatively close to the original Transformer, meaning that the necessity of the pairwise self-attention mechanism of the Transformer model may be questionable.For a more comprehensive overview of Transformer architectures, we refer to [104].</p>\n<h2 id=\"4EVALUATIOn-OF-ATTenTIOn-MOdeLs\"><a href=\"#4EVALUATIOn-OF-ATTenTIOn-MOdeLs\" class=\"headerlink\" title=\"4EVALUATIOn OF ATTenTIOn MOdeLs\"></a>4EVALUATIOn OF ATTenTIOn MOdeLs</h2><p>In this section, we present various types of evaluation for attention models.Firstly, one can evaluate the structure of attention models using the taxonomy presented in Section3 For such an analysis, we consider the attention mechanism categories (see Fig. 3) as orthogonal dimensions of a model. The structure of a model can be analyzed by determining which mechanism a model uses for each category. Table 3 provides an overview of attention models found in the liter-</p>\n<p>ature with a corresponding analysis based on the attention mechanisms the models implement Secondly, we discuss various techniques for evaluating</p>\n<p>the performance of attention models. The performance of attention models can be evaluated using extrinsic or intrinsic performance measures, which are discussed in Subsections. 4.1 and 4.2, respectively</p>\n<h3 id=\"4-1-Extrinsic-Evaluation\"><a href=\"#4-1-Extrinsic-Evaluation\" class=\"headerlink\" title=\"4.1 Extrinsic Evaluation\"></a>4.1 Extrinsic Evaluation</h3><p>In general, the performance of an attention model is measured using extrinsic performance measures. For example,. performance measures typically used in the field of natural language processing are the BLEU [107], METEOR [108] the Word Error Rate[110] and Phoneme Error Rate[111] are bins fom of mult-hop ad multi-hd attntion Due gerally mpoyed For genal assfiation tasks, eo rates, precision, and recall are generally used.For computer vision tasks, the PSNR[112], SSIM [113], or IoU [114] metrics are used. Using these performance measures, an attention model can either be compared to other state-of-the-art models, or an ablation study can be performed. If possible, the importance of the attention mechanism can be tested by replacing it with another mechanism and observing whether the overall performance of the model decreases [105], [115] An example of this is replacing the weighted average used to produce the context vector with a simple unweighted average and observing whether there is a decrease in overall model performance [35]. This ablation method can be used to evaluate whether the attention weights can actually distinguish important from irrelevant informationd</p>\n<h3 id=\"4-2Intrinsic-Evaluation\"><a href=\"#4-2Intrinsic-Evaluation\" class=\"headerlink\" title=\"4.2Intrinsic Evaluation\"></a>4.2Intrinsic Evaluation</h3><p>Attention models can also be evaluated using attentionspecific intrinsic performance measures.In [4], the attention weights are formally evaluatedvia the Alignment Error Rate (AER) to measure the accuracy of the attention weights with respect to annotated attention vectors. [116] incorporates this idea into an attention model by supervising the attention mechanism using gold attention vectors A joint loss function consisting of the regular task-specific loss and the attention weights loss function is constructed for this purpose. The gold attention vectors are based on annotated text data sets where keywords are hand-labelled. However, since attention is inspired by human attention, one could evaluate attention models by comparing them to the attention behaviour of humans.</p>\n<h3 id=\"4-2-1-Evaluation-via-Human-Attention\"><a href=\"#4-2-1-Evaluation-via-Human-Attention\" class=\"headerlink\" title=\"4.2.1 Evaluation via Human Attention\"></a>4.2.1 Evaluation via Human Attention</h3><p>In [117], the concept of attention correctness is proposed which is a quantitative intrinsic performance metric that evaluates the quality of the attention mechanism based on actual human attention behaviour. Firstly, the calculation of this metric requires data that includes the attention behaviour of a human.For example, a data set containing images with the corresponding regions that a human focuses on when performing a certain task, such as image captioning.The collection of regions focused on by the human is referred to as the ground truth region. Suppose an attention model attends to the $n_{f}$ feature vectors $f_1,\\ldots,\\boldsymbol{f}<em>{n_f}\\in\\mathbb{R}^{d_f}$ Feature vector $f_i$ corresponds to region $R</em>{i}$ of the given</p>\n<p>TABLE 3 Attention models analyzed based on the proposed taxonomy. A plus sign (+) between two mechanisms indicates that both techniques were combined in the same model, while a comma (,) indicates that both mechanisms were tested in the same paper, but not necessarily as a combination in the same model</p>\n<p>Buhleran et al. [3]<br>Multiplicity<br>Levels<br>Representations<br>Scoring<br>Generalization<br>Dimensionality<br>Type<br>Query-Related Multiplicity<br>Singular<br>Single-Level<br>Single-Representational<br>Additive<br>Global<br>Single-Dimensional<br>Basic<br>Singular</p>\n<p>Luong et al. [4]<br>Singular<br>Single-Level<br>Single-Representational<br>Multiplicative<br>Global, Local<br>Single-Dimensional<br>Basic<br>Singular</p>\n<p>Xu et al. [8]<br>Singular<br>Single-Level<br>Single-Representational<br>Additive<br>Soft, Hard<br>Single-Dimensional<br>Basic<br>Singular</p>\n<p>Tu et al. [32]<br>Parallel &#x2F; Co-attention<br>Hierarchical<br>Single-Representational<br>Additive<br>Global<br>Single-Dimensional<br>Specialized<br>Singular</p>\n<p>Yang et al. [5]<br>Singular<br>Hierarchical<br>Single-Representational<br>Additive<br>Global<br>Single-Dimensional<br>Self-Attentive<br>Singular</p>\n<p>Li et al. [14]<br>Singular<br>Hierarchical<br>Single-Representational<br>Additive<br>Global<br>Single-Dimensional<br>Self-Attentive<br>Singular</p>\n<p>Koivisto et al. [13]<br>Singular<br>Parallel &#x2F; Co-attention<br>Single-Representational<br>Multiplicative<br>Global<br>Single-Dimensional<br>Multi-Hop<br>Singular</p>\n<p>Waalstra and Frasincar [15]<br>Rotatory<br>Single-Level<br>Single-Representational<br>Activated<br>Global<br>Single-Dimensional<br>Specialized<br>Multi-Hop</p>\n<p>Kiela et al. [50]<br>Singular<br>Single-Level<br>Multi-Representational<br>Additive<br>Global<br>Multi-Dimensional<br>Singular</p>\n<p>Shen et al. [64]<br>Singular<br>Single-Level<br>Single-Representational<br>Additive<br>Global<br>Multi-Dimensional<br>Singular</p>\n<p>Zhang et al. [74]<br>Single-Level<br>Single-Representational<br>Multiplicative<br>Global<br>Single-Dimensional<br>Self-Attentive<br>Singular</p>\n<p>Li et al. [115]<br>Parallel &#x2F; Co-attention<br>Hierarchical<br>Single-Representational<br>Additive<br>Global<br>Single-Dimensional<br>Specialized + Multi-Hop</p>\n<p>Yu et al. [106]<br>Parallel &#x2F; Co-attention<br>Single-Level<br>Single-Representational<br>Multiplicative<br>Global<br>Single-Dimensional<br>Self-Attentive + Specialized</p>\n<p>Wang et al. [62]<br>Parallel &#x2F; Co-attention<br>Hierarchical<br>Single-Representational<br>Additive<br>Reinforced<br>Single-Dimensional<br>Singular</p>\n<p>Oktay et al. [67]<br>Singular<br>Single-Level<br>Single-Representational<br>Additive<br>Global<br>Multi-Dimensional<br>Self-Attentive + Specialized<br>Singular</p>\n<p>Winata et al. [52]<br>Singular<br>Single-Level<br>Multi-Representational<br>Additive<br>Global<br>Single-Dimensional<br>Self-Attentive<br>Multi-Hop</p>\n<p>Wang et al. [89]<br>Singular<br>Single-Level<br>Single-Representational<br>Multiplicative<br>Global<br>Single-Dimensional<br>Self-Attentive<br>Capsule-Based</p>\n<p>image, for $i$ &#x3D; $1, \\ldots , n_{f}$ .We define the set $G$ as the set of regions that belong to the ground truth region, such that $R_{i}\\in G$ if $R_{i}$ is part of the ground truth region. The attentionmodel is behaving correctly. This allows for some interpremodel calculates the attention weights $a_1,\\ldots,a_{n_f}\\in\\mathbb{R}^1$ via the usual attention process. The Attention Correctness $(AC)$ metric can then be calculated using (46).</p>\n<p>$$\\begin{matrix}AC&#x3D;\\sum_{i:R_i\\in G}a_i\\\\end{matrix}.$$</p>\n<p>Thus, this metric is equal to the sum of the attention weights for the ground truth regions.Since the attention weights sum up to 1 due to, for example, a softmax alignment function, the $AC$ value will be a value between O and 1. If the model attends to only the ground truth regions, then $AC$ is equal to 1, and if the attention model does not attend to any of the ground truth regions, $AC$ will be equal to 0. In [118], a rank correlation metric is used to compare</p>\n<p>important. Therefore, observing which parts of the inputs the model focuses on can be helpful in determining if the tation of the behaviour of models that are typically known to be black boxes.However, rather than checking if the model focuses onthemost important parts of thedatasome use the attention weights to determine which parts of the data are most important. This would imply that attention models provide a type of explanation, which is a subject of contention among researchers. Particularly, in [120], extensive experiments are conducted for various natural language processing tasks to investigate the relation between attention weights and important information to determine whether attention can actually provide meaningful explanations. In this paper titled “Attention is not Explanation” it is found that attention weights do not tend to correlate with important features. Additionally, the authors are able to replace the produced attention weights with completely different values while keeping the model output the same. These so-called “adversarial attention distributions show that an attention model may focus on completely different information and still come to the same conclusions, which makes interpretation difficult. Yet, in another paper titled “Attention is not not Explanation”[121], the claim that attention is not explanation is questioned by challenging the assumptions of the previous work.It is found that the adversarial attention distributions do not perform as reliably well as the learned attention weights, indicating that it was not proved that attention is not viable for explanation. In general, the conclusion regarding the interpretability</p>\n<p>the generated attention weights to the attention behaviour of humans.The conclusion of this work is that attention maps generated by standard attention models generally do not correspond to human attention. Attention models often focus on much larger regions or multiple small non-adjacent regions.As such,a technique to improve attention models is to allow the model to learn from human attention patterns via a joint loss of the regular loss function and an attention weight loss function based on the human gaze behaviour, similarly to how annotated attention vectors are used in [116] to supervise the attention mechanism. [117] proposes to use human attention data to supervise the attention mechanism in such a manner. Similarly, a state-of-the-art video captioning model is proposed in [119] that learns fromof attention models is that researchers must be extremely human gaze data to improve the attention mechanism</p>\n<h3 id=\"4-2-2Manual-Evaluation\"><a href=\"#4-2-2Manual-Evaluation\" class=\"headerlink\" title=\"4.2.2Manual Evaluation\"></a>4.2.2Manual Evaluation</h3><p>careful when drawing conclusions based on attention patterns.For example,problems with an attention model can be diagnosed via the attention weights if the model is found to focus on the incorrect parts of the data, if such information the manual inspection of attention weights.As previouslyis available. Yet, conversely, attention weights may only be used to obtain plausible explanations for why certain parts of the data are focused on,rather than concluding that those</p>\n<p>A method that is often used to evaluate attention models is mentioned, the attention weights are a direct indication of which parts of the data the attention model finds most</p>\n<p>parts are significant to the problem [121]. However, one should still be cautious as the viability of such approaches can depend on the model architecture [122]</p>\n<h2 id=\"5CONCLUSION\"><a href=\"#5CONCLUSION\" class=\"headerlink\" title=\"5CONCLUSION\"></a>5CONCLUSION</h2><p>In this survey, we have provided an overview of recent research on attention models in deep learning. Attention mechanisms have been a prominent development for deep learning models as they have shown to improve model performance significantly, producing state-of-the-art results for various tasks in several fields of research. We have presented a comprehensive taxonomy that can be used to categorize and explain the diverse number of attention mechanisms proposed in the literature. The organization of the taxonomy wasmotivated based on the structure ofa taskmodel that consists of a feature model, an attention model, a query model, and an output model. Furthermore, the attention mechanisms have been discussed using a framework based on queries, keys, and values. Last, we have shown how one can use extrinsic and intrinsic measures to evaluate the performance of attention models, and how one can use the taxonomy to analyze the structure of attention models The attention mechanism is typically relatively simple</p>\n<p>to understand and implement and can led to signficant25. 2015, 412-1 He, Amola, and E Hovy improvements in performance.As such, it is no surprise that this is a highly active field of research with new attention mechanisms and models being developed constantly. Not only are new mechanisms consistently being developed but there is also still ample opportunity for the exploration of existing mechanisms for new tasks. For example, multidimensional attention[64] is a technique that shows promis dienqaM almost any attention model. However, it has not seen much application in current works. Similarly, multi-head atten-. tion [13] is a technique that can be efficiently parallelized and implemented in practically any attention model.Yet it is mostly seen only in Transformer-based architectures. Lastly, similarly to how [43] combines rotatory attention with multi-hop attention, combining multi-dimensional at-. tention, multi-head attention, capsule-based attention, or any of the other mechanisms presented in this survey may produce new state-of-the-art results for the various fields of research mentioned in this survey. This survey has mainly focused on attention mech</p>\n<p>anisms for supervised models,since these comprise the largest proportion of the attention models in the literature. In comparison to the total amount of research that has been done on attention models,research on attention models for semi-supervised learning [123], [124] or unsupervised learning [125], [126] has received limited attention and has only become active recently. Attention may play a more significant role for such tasks in the future as obtaining research on attention models can advance even further. For example, we mentioned the fact that attention weights can be trained directly based on hand-annotated data[116] or actual human attention behaviour[117], [119]. As new data sets are released,future research may focus on developing attention models that can incorporate those types of data.</p>\n<p>While attention is intuitively easy to understand,there still is a substantial lack of theoretical support for attention As such, we expect more theoretical studies to additionally contribute to the understanding of the attention mechanisms in complex deep learning systems. Nevertheless, the practical advantages of attention models are clear. Since attention models provide significant performance improve-. ments in a variety of fields, and as there are ample opportunities for more advancements, we foresee that these models will still receive significant attention in the time to come.</p>\n<h3 id=\"REFERENCES\"><a href=\"#REFERENCES\" class=\"headerlink\" title=\"REFERENCES\"></a>REFERENCES</h3><p>[1]H. Larochelle and G. E. Hinton, “Learning to combine foveal glimpses with a third-order Boltzmann machine,” in 24th Annuai Conference in Neural Information Processing Systems (NIPS 2010) Curran Associates, Inc., 2010, pp. 12431251. [2]V. Mnih, N. Heess, A. Graves, and k. kavukcuoglu, “Recurrenf models of visual attention,”in 27th Armual Conference on Neurai Information Processing Systems (NIPs 2014).Curran Associates Inc., 2014, pp. 22042212 [3]D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” in 3rd Internationai Conference on Learning Representation (ICLR 2015), 2015 [4]T. Luong, H. Pham, and C. D. Manning, “Effective approaches to attention-based neural machine translation,” in 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP Hierarchical attention networks for document classification, in 2016 Conference of the NorthAmerican Chapter of the Association fon Computational Linguistics: Human Language Technologies (NAACL [6]YW2016 ACL, 206, 148012890,”Attention-based LSTM for aspect-level sentiment classification, in 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP 2016). and L. Zhang, “Bottom-up and top-down attention for image captioning and visual question answering,” in 2018 IEEE&#x2F;CVF Conference on Computer Vision and Pattern Recognition (CVPR 2018), 2018, pp. 60776086 [8]K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov R. Zemel, and Y. Bengio, “Show, attend and tell: Neural image caption generation with visual attention,” in 32nd Internationa Conference on Machine Learning (ICML 2015),vol. 37.PMLR 2015, pp. 20482057 [9]Y. Ma, H. Peng, and E. Cambria, “Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive LSTM, in 32nd AAAI Conference orn Artificial Intelligence (AAAI 2018).AAAI Press, 2018, Pp. 58765883 [10] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio “Attention-based models for speech recognition,” in 28th Annuai Conference on Neural Information Processing Systems (NIPS 2015) [1 D.Brandasociates lnc 2015D pe575,PB Brakl and Y Bengio End-to-end attention-based large vocabulary speech recogni tion, in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2016). IEEE Signal Processing Society, 2016, pp. 49454949 [12] S. Kim, T. Hori, and S. Watanabe, “Joint CTC-attention based end-to-end speech recognition using multi-task learning, in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2017).IEEE Signal Processing Society, 2017, lag m a d sk ma Jo Gomez, L. u. Kaiser, and I Polosukhin, “Attention is all you need,” in 31st Annual Conference on Neural Information Processing Systems (NIPS 2017).Curran Associates, Inc., 2017, Pp. 5998- 6008 [14] K. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio, $“On$ the properties of neural machine translation: Encoder-decoder approaches,” in Sth Workshop on Syntax, Semarntics and Structure in Statistical Translation (SSST 2014).ACL, 2014, pp. 103111</p>\n<p>[15] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, [36] S. Zheng and R. Xia, “Leff-center-right separated neural network and D. Tran, “Image Transformer,” in 35th Internafional Conference on Machine Learning (ICML 2018), vol. 80. PMLR, 2018, pp. 40554064. [16] L. Zhou, Y. Zhou, J. J. Corso, R. Socher, and C. Xiong, “End-toend dense video captioning with masked transformer,” in 2018 IEEE&#x2F;CVF Conference on Computer Vision and Patfern Recognifion [38]J. Gao, X. Wang, Y. Wang, Z. Yang, J. Gao, J. Wang, W. Tang [17] EVP 01,1ECompufer ocity,2018, p, 8739 874 iang BERT4Rec: Sequential recommendation with bidirectional en coder representations from transformer,” in 28th ACM Interna-[39]Y. Tay, A. T. Luu, and S. C. Hui, “Multi-pointer co-attention nettional Conference on Information and Knowledge Management (CIKM 2019).ACM, 2019, p. 14411450 [18] Wangnand D,MiT.Tas “Ssrvey on theatention based N23, [40 S LMu,2018,bpe,239-31 and X Hu, User-video co-ttention 2016 [19] J. B. Lee, R. A. Rossi, S. Kim, N. K. Ahmed, and E. Koh, “Attention models in graphs: A survey,” ACM Transitions on Knowledge Discovery fror Data, vol. 13, pp. 62:162:25, 2019. [20] S. Chaudhari, V. Mithal, G. Polatkan, and R.Ramanath, “An attentive survey of attention models, ACM Transactions on Intelligent Systems and Technology, vol. 12, no. 5, pp. 132, 2021. [1]D.ue”An introductory survey onatention mechanisms in NLe 42] mnd tol incanec ah w. 270-27enton network (IntelliSys 2019), ser. AISC, vol. 1038. Springer, 2020, pp. 432448 [22]A. Galassi, M. Lippi, and P. Torroni,”Attention, please! a critical review of neuralatention modes in natural language pocess 43] O. Wallat and E raincar, A hybrid approach for aspet [23].Danilek in Rockt schl l Weab, and . iedel “ rustratinel national Conference on Learning Representations (ICLR 2017), 2017 [24] Xionan onga Hongs Wng ed D. Pumbeuat 4] and han ttentionvia-atentin neural machine model for weakly supervised audio tagging,” in Proceedings of fhe 18th Annual Conference of the International Speech Communication Association (Interspeech 2017).ISCA, 2017, pp. 30833087 [25]C. Yu, K. S. Barsim, Q. Kong, and B. Yang, *Multi-level attention model for weakly supervised audio classification, in Proceedings fohe Detetionase Clas, cotio pf.Acoustic ces and Eents 018[46] . Wang S. Wang J. Tang N. OHare, Y. Chang, and B Li, [26] S. Sharma, R. Kiros, and R. Salakhutdinov, “Action recognition using visual atention” in Procedings ohth (4h zorn, one.47 Li, Y Wei, Zhang and Q Yang. “Hierarchical attention [27] L. Gao, Z. Guo, H. Zhang, X. Xu, and H. T. Shen, *Video captioning with attention-based LSTM and semantic consistency,” IEEE Trarsacfions on Meltimedia, vol. 19, no. 9, pp. 20452055, 2017 [28]H. Ying, F. Zhuang, F. Zhang, Y. Liu, G. Xu, X. Xie, H. Xiong, and J. Wu, “Sequential recommender system based on hierarchical attention networks, in 27th International Joint Conference on Artificial Intelligence (IJCAI 2018).IJCAI, 2018, pp. 39263932 [29] H. Song, D. Rajan, J. Thiagarajan, and A. Spanias, “Attend and diagnose: Clinical time series analysis using attention models,” in 32nd AAAI Conference on Artificial Intelligence (AAAI 2018). AAAI[50]D. Kiela, C. Wang, and K. Cho, Dynamic meta-embeddingg Press, 2018, pp. 40914098 [30] D.T. Tran, A. Iosifidis, J. Kanniainen, and M. Gabbouj, “Temporal attention-augmented bilinear network for financial time-series data analysis,” IEEE Transactions on Neural Netuorks and Learning [51] S. Maharjan, M. Montes, F. A. Gonzalez, and T. Solorio, “A Systems, vol. 30, no. 5, pp. 14071418, 2019 [31] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio, “Graph attention networks,” in 6th Internationai Conference on Learning Representations (ICLR 2018), 2018. [32]J. Lu, J. Yang, D. Batra, and D. Parikh, “Hierarchical question image co-attention for visual question answering,” in 30th Anrual Conference on Neural Information Processing Systems (NIPs 2016). Curran Associates, Inc., 2016, pp. 289297 [33] F. Fan, Y. Feng, and D. Zhao, “Multi-grained attention network for aspect-level sentiment classification, in 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP 2018).[54] A. Sordoni, P. Bachman, A. Trischler, and Y. Bengio,It [34 D.CLM,2018. P, 3-34g, and H. Wang, Interativ atenton networks for aspect-level sentiment classification,” in 26th In- [55] A. Graves, G. Wayne, and I. Danihelka, “Neural Turing maternational Joint Conference on Artificial Intelligence (IJCAI 2017). IJCAI, 2017, pp. 40684074 [35] M. Seo, A. Kembhavi, A. Farhadi, and H. Hajishirzi, “Bidirectional attention flow for machine comprehension,” in 4th Irterna tional Conference on Learning Represertations (ICLR 2016), 2016</p>\n<p>for aspect-based sentiment analysis with rotatory attention, arXiv:1802.00892, 2018 [37] B. Jing, P. Xie, and E. Xing, “On the automatic generation of med ical imaging reports,” in 56th Anrual Meeting of the Association for Computational Linguistics (ACL 2018).ACL, 2018, pp. 25772586 and X. Xie, “CAMP: Co-attention memory networks for diagnosis prediction in healthcare,” in 2019 IEEE International Conference o Data Minting (ICDM 2019).IEEE, 2019, pp. 10361041. works for recommendation, in 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD 2018) network for personalized micro-video recommendation, in 2019 World Wide Web Conference (WWW 2019).ACM, 2019, pp.30203026 [41] M. Tu, G. Wang, J. Huang, Y. Tang, X. He, and B. Zhou, “Multihop reading comprehension across multiple documents by reasoning over heterogeneous graphs in 57thAnmual Meeting of the Association for Computational Linguistics (ACL 2019).Association for Computational Linguistics, 2019, pp. 27042713 for explainable fake news detection on social media,” in 58fh Annual Meeting of fhe Association for Computational Linguistics (ACL 2020).ACL, 2020, Pp. 505514. based sentiment aalysusing a leicalied domain ontology Corference (ESWC 2019), ser. LNCS, vol. 11503.Springer, 2019) Pp. 363378 translation, in 32nd AAAI Conference on Artificial Intelligence (AAAI 2018).AAAI Press, 2018, pp. 563570. [45]L. Wu, L. Chen, R. Hong, Y. Fu, X. Xie, and M. Wang, “A hierarchical attention model for social contextual image recom mendation, IEEE Transactions on Knowledge and Data Engineering, 2019 Hierarchical attention network for action recognition in videos,” arXiv:1607.06416, 2016. transfer network for cross-domain sentiment classification, in 32nd AAAI Conference on Artificial Intelligence (AAAI 2018). AAAI Press, 2018, p. 58525859 [48] C. Xing, Y. Wu, W. Wu, Y. Huang, and M. Zhou, *Hierarchical recurrent attention network for response generation,” in 32nd AAAI Conference on Artificial Intelligence (AAAI 2018). AAAI Press, 2018, pp. 56105617 [49] V. A. Sindagi and V. M. Patel, “HA-CCN: Hierarchical attention based crowd counting network,” IEEE Transactions on Image Processing, vol. 29, pp. 323335, 2019 for improved sentence representations,” in 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP 2018) ACL, 2018, pp. 14661477. genre-aware attention model to improve the likability prediction of books,”in 2018 Conference onEmpirical Methods in Naturai Language Processing (EMNLP 2018).ACL, 2018, pp. 33813391. [52] G. I. Winata, Z. Lin, and P. Fung, “Learming multilingual metaembeddings for code-switching named entity recognition,” in 4fh Workshop on Representation Learning for NLP (RepL4NLP 2019) ACL, 2019, pp. 181186 [53] R. Jin, L. Lu, J. Lee, and A. Usman, “Multi-representational con volutional neural networks for text classification, Computationa Intelligence, vol.35, no. 3, pp. 599609, 2019 erative alternating neural attention for machine reading, arXiv:1606.02245, 2016. chines, arXiv:1410.5401, 2014 [56] D. Britz, A. Goldie, M.-T. Luong, and Q. Le, “Massive exploration of neural machine translation architectures,” in 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017). ACL, 2017, pp. 14421451.</p>\n<p>[57]R. J. Williams, Simple statistical gradient-following algorithms for connectionist reinforcement learning,” Machine Learning vol. 8, no. 3, pp. 229256, 1992. [58] T.Shen, T. Zhou, G. Long, J. Jiang, S. Wang, and C. Zhang, *Reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling,” in 27th International Joint Cornference on Artificial Intelligence (IJCAI 2018).IJCAI, 2018, pp. 43454352 [59] M. Malinowski, C. Doersch, A. Santoro, and P. Battaglia, “Learning visual question answering by bootstrapping hard attention,” in 2018 European Conference on Computer Vision (ECCV 2018), 2018. [60] Y. Liu, W. Wang, Y. Hu, J. Hao, X. Chen, and Y. Gao, “Multiagent game abstraction via graph attention neural network,”” in 34th AAAI Conference on Artificial Intelligence (AAAI 2020), vol. 34, no. 05.AAAI Press, 2020, pp. 72117218. [61] S.Seo,J.Huang, H. Yang, and Y. Liu, “Interpretable convolutional neural networks with dual local and global attention for review rating prediction,in 11th ACM Conference on Recommender Systems (RecSys 2017).ACM, 2017, pp. 297305 [62] J. Wang, C. Sun, S. Li, X. Liu, L. Si, M. Zhang, and G. Zhou, “Aspect sentiment classification towards question-answering with reinforced bidirectional attention network, in 57th Ammual Meeting of fhe Association for Computational Linguistics (ACL 2019). ACL, 2019, pp. 35483557. [63] M. Jiang, C. Li, J. Kong, Z. Teng, and D. Zhuang, “Crosslevel reinforced attention network for person re-identification, Journal of Visual Communication and Image Representation, vol. 69 p. 102775, 2020. [64] T. Shen, T. Zhou, G. Long, J. Jiang, S. Pan, and C. Zhang, “DiSAN: Directional self-attention network for RNN&#x2F;CNN-free language understanding,” in 32nd AAAI Conference on Artificial Intelligence. (AAAI 2018).AAAI Press, 2018, Pp. 54465455 [65] O. Arshad, I Gallo, S. Nawaz, and A. Calefati, “Aiding intra-text representations with visual context for multimodal named entity recognition,” in 2019 International Conference on Document Analysis. and Recognition (ICDAR 2019).IEEE, 2019, pp. 337342 [66] W. Wu, X. Sun, and H. Wang, “Question condensing networks for answer selection in community question answering,” in Proceed. ings of the 56th Annual Meeting of the Association for Coputational Linguistics (ACL 2018).ACL, 2018, pp. 17461755 [67] . Okta.a, Schlem se MDonaFol,go MH aemeMa, Beiric, B. Glocker, and D. Rueckert, “Attention U-Net: Learming where to look for the pancreas in 1st Medical Imaging with Deep Learning Conference (MIDL 2018), 2018 [68] R. Tan, J. Sun, B. Su, and G. Liu, “Extending the transformer with context and multi-dimensional mechanism for dialogue response generation, in 8th International Conference on Natural Language Processing and Chinese Computing (NLPCC 2019), ser. LNCS, J. Tang, M.-Y. Kan, D. Zhao, S. Li, and H. Zan, Eds., vol. 11839. Springer, 2019, pp. 189199 [69] L. Chen, B. Lv, C. Wang, S. Zhu, B. Tan, and K. Yu, “”Schemaguided multi-domain dialogue state tracking with graph attention neural networks,in 34th AAAI Conference on Artificial Intelligence (AAAI 2020), vol. 34, no. 05.AAAI Press, 2020, pp. 75217528 [70] H. Wang, G. Liu, A. Liu, Z. Li, and K. Zheng, “Dmran: A hierarchical fine-grained attention-based network for recommenda-. tion, in 28th International Joint Conference on Artificial Intelligence (IJCAI 2019). [71] Z. Lin, M. Feng, C. N. d. Santos, M. Yu, B. Xiang, B. Zhou, and Y. Bengio, “A structured self-attentive sentence embedding in 5th International Conference on Learning Representations (ICLR 2017), 2017. [72]J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization, arXiv:1607.06450, 2016. [73] H. Zhao, J. Jia, and V. Koltun,“Exploring self-attention for image recognition,” in 2020 IEEE&#x2F;CVF Conference on Computer Vision and Patfern Recognition (CVPR 2020), 2020, pp. 10 07610 085. [74] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, “Selfattention generative adversarial networks, in 36th International Conference on Machine Learning (ICML 2019), ser. Proceedings of Machine Learning Research, K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97. PMLR, 2019, pp. 73547363 [75] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. WardeFarley, S. Ozair, A. Courville, and Y. Bengio, “Generative ad versarial nets,in 27th Anmual Conference on Neural Information</p>\n<p>Processing Systems (NIPs 2014).Curran Associates, Inc., 2014 Pp. 2672-2680 [76] A. Sinha and J. Dolz, “Multi-scale self-guided attention for medical image segmentation,” IEEE Journal of Biomedical and Health Informafics, vol. 25, no. 1, pp. 121-130, 2021 [77] J. Fajtl, H. S. Sokeh, V. Argyriou, D. Monekosso, and P. Re magnino, Summarizing videos with attention, in 2018 Asian Corferernce on Computer Vision (ACCV 2018), ser. LNCS, vol. 11367 Springer, 2018, pp. 3954. [78] J. Salazar, K. Kirchhoff, and Z. Huang, “”Self-attention networks for connectionist temporal classification in speech recognition, in 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2019).IEEE, 2019, pp. 71157119 [79]T. Afouras, J. S. Chung, A.Senior, O. Vinyals, and A.Zisserman, Deep audio-visual speech recognition, IEEE Transactions or Pattern Analysis and Machine Intelligence, pp. 11, 2018 [80]S. Zhang, Y. Tay, L. Yao, and A. Sun, *Next item recommendation with self-attention, arXiv preprint arXiv:1808.06414, 2018 [81]G. Letarte, F. Paradis, P. Giguere, and F. Laviolette, “Impor tance of self-attention for sentiment analysis,” in 2018 Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP (BlackboxNLP 2018).ACL, 2018, pp. 267275 [82] A. Sankar, Y. Wu, L. Gou, W. Zhang, and H. Yang, “Dysat Deep neural representation learning on dynamic graphs via selfattention networks, in 13th International Conference on Web Search and Data Minting (WSDM 2020), 2020, pp. 519527 [83] P. Velickovic, G.Cucurull, A. Casanova, A.Romero, P. Lio and Y. Bengio, Graph attention networks, in 5th International Conference on Learning Representations (ICLR 2017), 2017 [84] S. Iida, R Kimura, H. Cui, P-H. Hung, T. Utsuro, and M. Nagata, “Attention over heads: A multi-hop attention for neural machine translation, in 57th Anmual Meeting of the Association for Com putational Linguistics: Student Research Workshop (ACL-SRW 2019). ACL, 2019, pp. 217222 [85] N. K. Tran and C. Niedereee, “Multihop attention networks for question answer matching, in 41st ACM SIGIR International Conference on Research &amp; Development in Information Retrieval (SIGIR 2018).ACM, 2018, Pp. 325334 [86]Y. Gong and S. R. Bowman, “Ruminating reader: Reasoning with gated multi-hop attention, in 5th International Conference on Learning Representation (ICLR 2017), 2017 [87]S. Yoon, S. Byun, S. Dey, and K. Jung, “Speech emotion recognition using multi-hop attention mechanism, in 2019 IEEE Interna fional Conference on Acoustics, Speech and Signal Processing (ICASSP 2019).IEEE, 2019, pp. 28222826. [88]Z. Yang, X. He, J. Gao, L. Deng, and A. Smola, “Stacked attention networks for image question answering,” in 2016 IEEE&#x2F;CVF Con ference on Compufer Vision and Pattern Recognition (CVPR 2016) 2016, pp.2129 [89]Y. Wang, A. Sun, J. Han, Y. Liu, and X. Zhu, “Sentiment analysis by capsules,” in 2018 World Wide Web Conference (WWW 2018). ACM, 2018, p. 11651174 [90]S. Sabour, Ni. Frosst, and G. E. Hinton, “Dynamic routing between capsules,in 31st Anmual Conference on Neural Information Prosng Systems NIPs 2017). Curran Associates Inc, 2017, p. [91] M. India, P. Safari, and J. Hernando, “Self multi-head attention for speaker recognition,” in Proceedings of fhe 20th Anmual Conference of the International Speech Communication Association (Interspeecl 2019). ISCA, 2019, pp. 28222826 [92] C. Wu, F. Wu, S. Ge, T. Qi, Y. Huang, and X. Xie, “Neural news recommendation with multi-head self-attention, in 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP 2019).ACL, 2019, pp. 63896394. [93]Y. Wang, W. Chen, D. Pi, and L. Yue,”Adversarially regularized medication recommendation model with multi-hop memory net-. work,Knowledge and Information Systems, vol.63, no. 1, pp. 125142, 2021 [94] M. Cornia, M. Stefanini, L. Baraldi, and R. Cucchiara, “Meshedmemory transformer for image captioning,” in 2020 IEEE&#x2F;CVF Conference on Computer Vision and Pattern Recognition (CVPR 2020), 2020, pp. 10 57810 587 [95] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille and Y. Zhou, “TransUnet: Transformers make strong encoders for medical image segmentation, arXiv preprint arXiv:2102.043062021</p>\n<p>[96]P. Zhong, D. Wang, and C. Miao, “Knowledge-enriched transformer for emotion detection in textual conversations,”in 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. (EMNLP-IJCNLP 2019). ACL, 2019, Pp. 165176 [97]Y. Zhou, R. Ji, J. Su, X. Sun, and W. Chen, “Dynamic capsule 2019, pp. 93249331 [98] Y. Wang, A. Sun, M. Huang, and X. Zhu, “Aspect-level sentiment analysis using AS-capsules,” in The World Wide Web Conference, [9 21Da,2 233-20.Yang . Carboell . Le, and R slakhtdinovg Transformer-XL: Attentive language models beyond a fixed length context,”in 57th Annual Meeting of the Association for Computational Linguistics (ACL 2019).ACL, 2019, pp. 29782988. [100] N. Kitaev, L. Kaiser, and A. Levskaya, Reformer: The efficient Transformer, in 8th International Conference on Learning Represen. attention with linear complexity,” rXiv:2006.04768, 2020. [102] Z. Wu, Z. Liu, J. Lin, Y. Lin, and S. Han, “Lite transformer with long-short range attention, in 8th International Conference on Learning Representations (ICLR 2020), 2020 [103] Y. Tay, D. Bahri, D. Metzler, D.-C. Juan, Z. Zhao, and C. Zheng, Synthesizer: Rethinking self-attention for transformer models, in Proceedings of the38th International Cornference on Machine Learn ing (ICML 2021), vol. 139.PMLR, 2021, pp. 10 18310192 [104] Y. Tay, M. Dehghani, D. Bahri, and D. Metzler, “Efficient transformers: A survey, arXiv:2009.06732, 2020. [105] X. Li, J. Song, L. Gao, X. Liu, W. Huang, X. He, and C. Gan, Beyond RNNs: Positional self-attention with co-attention for video question answering,” in 33rd AAAI Conference on Artificial Intelligence (AAAI 2019), vol. 33.AAAI Press, 2019, Pp. 86588665 [106] A. W. Yu, D. Dohan, M.-T. Luong, R. Zhao, K. Chen, M. Norouzi and Q. V. Le, “QANet: Combining local convolution with global self-attention for reading comprehension,” in 6th International Conference on Learning Representations (ICLR 2018), 2018 [107] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, *BLEU: a method for automatic evaluation of machine translation,” in 40th Annual Meeting of fhe Association for Compufational Linguistics (ACL 2002). ACL, 2002, pp. 311318. [108] S. Banerjee and A. Lavie, “METEOR: An automatic metric for MT evaluation with improved correlation with human judgments, in 2005 Workshop oni Intrinsic and Extrinsic Eoaluation Measures for Machine Translation and&#x2F;or Summarization. ACL, 2005, pp. 6572 [109] R. Sennrich, “Perplexity minimization for translation model domain adaptation in statistical machine translation,” in 13th Conference of fhe European Chuapter of the Association for Computational Linguistics (EACL 2012).ACL, 2012, pp. 539549 [110] M. Popovic and H. Ney, “Word error rates: Decomposition over POS classes and applications for error analysis,” in 2nd Workshop on Statistical Machtine Translation (WMT 2007).ACL, 2007, Pp 48-55 [111] P. Schwarz, P. Matejka, and J. Cernocky, “Towards lower error rates in phoneme recognition, in 7th International Conference. on Text, Speech and Diaiogue (TSD 2004), ser. LNCS, vol. 3206 Springer, 2004, pp. 465472. [112] D. S. Turaga, Y. Chen, and J. Caviedes, “No reference PSNR estimation for compressed pictures,” Signal Processing: Image Commticatfion, vol. 19, no. 2, pp. 173184, 2004 [113] P. Ndajah, H. Kikuchi, M. Yukawa, H. Watanabe, and S. Muramatsu, “SSIM image quality metric for denoised images,” in 3rd WSEAS International Conference on Visualization, Imaging and Simulation (VIS 2010).WSEAS, 2010, pp. 5358 [114] M. A. Rahman and Y. Wang, “Optimizing intersection-over-union in deep neural networks for image segmentation,” in 12th International Symposium on Visual Computing (ISVC 2016), ser. LNCS, vol. 10072.Springer, 2016, pp. 234244. [115] X. Chen, L. Yao, and Y. Zhang, “Residual attention U-net for auto mated multi-class segmentation of COVID-19 chest CT images, arXiv:2004.05645, 2020 [116] S. Liu, Y. Chen, K. Liu, and J. Zhao, “Exploiting argument information to improve event detection via supervised attention of the Journal of Web Engineering. Dr. Frasincar is a member of the mechanisms,” in 55th Arnual Meeting of the Association for Coputational Linguistics (ACL 2017).ACL, 2017, pp. 17891798</p>\n<p>[117] C. Liu, J. Mao, F. Sha, and A. Yuille, “Attention correctness in neural image captioning,” in 31st AAAl Conference on Artificial Intelligence (AAAI 2017). AAAI Press, 2017, Pp. 41764182 [118] A. Das, H. Agrawal, L. Zitnick, D. Parikh, and D. Batra, “Human atention novisual question anowering: Do hu mans and dee neural attention models for video captioning by human gaze data,”in 2017 IEEE Conference on Computer Vision and Paftern Recognition (CVPR 2017).IEEE Computer Society, 2017 [120] S. Jain and B. C. Wallace, “Attention is not explanation,” in 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2019).ACL, 2019, pp. 35433556 [121] S. Wiegreffe and Y. Pinter, “Attention is not not explanation,” in 2019 Conference on Empirical Methods in Natural Language Process ing and fhe9th International Joint Conference on Nafural Language 1 B. V. Srinivasan, and B. Ravindran, Towards transparent and explainable attention models,” in 58th Anmual Meeting of the Association for Computational Linguistics (ACL 2020).ACL, 2020 pp. 4206-4216. [123] K. K. Thekumparampil, C. Wang, S. Oh, and L-J. Li, “Attention based graph neural network for semi-supervised learning, arXiv:1803.03735, 2018 [124]D. Nie, Y. Gao, L. Wang, and D. Shen, “ASDNet: Attention based semi-supervised deep networks for medical image segmenta tion, in 21st International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI 2018), ser. LNCS, vol 11073.Springer, 2018,pp.370-378 [125]Y. Alami Mejati, C. Richardt, J. Tompkin, D. Cosker, and K. I. Kim, Unsupervised attention-guided image-to-image transla tion, in 32nd Anmual Conference on Neural Information Processing Systems (NIPS 2018).Curran Associates, Inc., 2018, Pp. 36933703 [126] R. He, W. S. Lee, H. T. Ng, and D. Dahlmeier, “An unsupervised neural attention model for aspect extraction,” in 55thi Annua Meeting of the Association for Computational Linguistics (ACL 2017) ACL, 2017, pp. 388397.</p>\n<p>Gianni Brauwers was born in Spijkenisse,the Netherlands.in 1998. He received the B.S. de gree in econometrics and operations research from Erasmus University Rotterdam, Rotterdam the Netherlands,in 2019,and is currently pursu ing the M.S. degree in econometrics and man agement science at Erasmus University Rotter dam. He is a Research Assistant at Erasmus Uni-</p>\n<p><img src=\"/./images/fwKubs9NrrsE4E6IOIuyNmz1254MnctEC.png\"></p>\n<p>versity Rotterdam, focusing his research on neural attention models and sentiment analysis</p>\n<p>Flavius Frasincar was born in Bucharest.Ro mania, in 1971. He received the M.S. degree in computer science,in 1996,and the M.Phil degree in computer science, in 1997, from Politehnica University of Bucharest, Bucharest, Romania, and the P.D.Eng. degree in computer sci ence, in 2000, and the Ph.D. degree in computer science, in 2005, from Eindhoven University of Technology, Eindhoven, the Netherlands Since 2005,he has been an Assistant Profes</p>\n<p><img src=\"/./images/fRt7WcnS0GB13Asmx3FTox8eFYu3ccbMN.png\"></p>\n<p>sor in computer science at Erasmus University Rotterdam, Rotterdam, the Netherlands. He has published in numerous</p>\n<p>conferences and journals in the areas of databases,Web information systems, personalization, machine learning, and the Semantic Web. He is a member of the editorial boards of Decision Support Systems. International Journal of Web Engineering and Technology, and Com putational Linguistics in the Netherlands Journal, and co-editor-in-chief Association for Computing Machinery</p>\n"}],"Post":[{"title":"Attention Overview","date":"2025-07-10T11:56:23.000Z","_content":"\n# Is Attention All My Need ?\n\n> 注意力机制在图神经网络中扮演着越来越重要的角色。~~但鼠鼠现在连正常的Attention有哪些都不清楚捏~~本文鼠鼠将从一般的Attention出发，给出Attention的总体结构，然后按分类介绍现有的主要的Attention\n\n本文主要来自于一篇论文，基本可以看作[那篇论文](/paper/Brauwers和Frasincar%20-%202023%20-%20A%20General%20Survey%20on%20Attention%20Mechanisms%20in%20Deep%20Learning.pdf)的阅读笔记\n\n<!-- more -->\n\n## 🎯 引言\n\n在深度学习领域，注意力机制已经成为一个革命性的创新，特别是在处理序列数据和图像数据方面取得了巨大成功。而在图神经网络中，注意力机制的引入不仅提高了模型的表现力，还增强了模型的可解释性。\n\n在图结构数据中应用注意力机制主要有以下优势：\n1. 自适应性：能够根据任务动态调整不同邻居节点的重要性\n2. 可解释性：通过注意力权重可以直观理解模型的决策过程\n3. 长程依赖：有效缓解了传统GNN中的过平滑问题\n4. 异质性处理：更好地处理异质图中的不同类型节点和边\n\n## 📚 总览Attention\n\n本章节主要参考了论文[📄 Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning](/paper/Brauwers和Frasincar%20-%202023%20-%20A%20General%20Survey%20on%20Attention%20Mechanisms%20in%20Deep%20Learning.pdf)有兴趣的话可以看看原文捏\n\n<embed src=\"/paper/Brauwers和Frasincar%20-%202023%20-%20A%20General%20Survey%20on%20Attention%20Mechanisms%20in%20Deep%20Learning.pdf\" width=\"45%\" height=\"400\" type=\"application/pdf\">\n\n### Attention的一般结构\n\n<img src=\"/img/Attention/TotalModel.png\" alt=\"TotalModel\" width=\"60%\" height=\"auto\">\n\n上图是从总体上看Attention在整个任务模型框架中的位置\n\n框架包含四个核心组件：\n1. **特征模型**：负责输入数据的特征提取\n2. **查询模型**：生成注意力查询向量\n3. **注意力模型**：计算注意力权重\n4. **输出模型**：生成最终预测结果\n\n接下来，我们会从 *输入* 的角度来看**特征模型**和**查询模型**，从 *输出* 的角度来看**注意力模型**和**输出模型**\n\n#### 输入处理机制\n\n1. **特征模型**，即将任务的输入进行embedding\n   \n    对于输入矩阵$ X \\in \\mathbb{R}^{d_x \\times n_x} $，特征模型提取特征向量：$\\boldsymbol{F} = [f_1, \\ldots, f_{n_f}] \\in \\mathbb{R}^{d_f \\times n_f}$\n\n2. **查询模型**，查询模型产生查询向量$ \\boldsymbol{q} \\in \\mathbb{R}^{d_q} $，用以告诉注意力模型哪一个特征是重要的\n\n一般情况下，这两个模型可以用CNN或RNN\n\n#### 输出计算机制\n\n<img src=\"/img/Attention/GeneralAttentionModule.png\" alt=\"GeneralAttentionModule\" width=\"50%\" height=\"auto\">\n\n上图是Attention模型总体结构的说明，下面对这张图进行详细的说明\n\n1. 特征矩阵$\\boldsymbol{F} = [\\boldsymbol{f}\\_1, \\ldots, \\boldsymbol{f}\\_{n\\_f}] \\in \\mathbb{R}^{d\\_f \\times n\\_f}$，通过*某些方法*将其分为Keys矩阵$\\boldsymbol{K} = [\\boldsymbol{k}\\_1, \\ldots, \\boldsymbol{k}\\_{n\\_f}] \\in \\mathbb{R}^{d\\_k \\times n\\_f}$和Values矩阵$\\boldsymbol{V} = [\\boldsymbol{v}_1, \\ldots, \\boldsymbol{v}\\_{n\\_f}] \\in \\mathbb{R}^{d\\_v \\times n\\_f}$，这里的*某些方法*，一般情况下，按以下的方式通过**线性变换**得到：\n\n$$\n\\underset{d\\_{k} \\times n\\_{f}}{\\boldsymbol{K}}=\\underset{d\\_{k} \\times d\\_{f}}{\\boldsymbol{W}\\_{K}} \\times \\underset{d\\_{f} \\times n\\_{f}}{\\boldsymbol{F}}, \\quad \\underset{d\\_{v} \\times n\\_{f}}{\\boldsymbol{V}}=\\underset{d\\_{v} \\times d\\_{f}}{\\boldsymbol{W}\\_{V}} \\times \\underset{d\\_{f} \\times n\\_{f}}{\\boldsymbol{F}} .\n$$\n\n2. `Attention Scores`模块根据 $\\boldsymbol{q}$ 计算每一个key向量对应的分数$\\boldsymbol{e} = [e_1, \\ldots, e_{n_f}] \\in \\mathbb{R}^{n_f}$：\n\n    $$\n    \\underset{1\\times 1}{e\\_l} = \\text{score}(\\underset{d\\_q \\times 1}{\\boldsymbol{q}}, \\underset{d\\_k \\times 1}{\\boldsymbol{k}\\_l}) \n    $$\n\n    如前所述，查询象征着对信息的请求。注意力分数$e_l$表示根据查询，关键向量$\\boldsymbol{k}_l$中包含的信息的重要性。如果查询和关键向量的维度相同，则得分函数的一个例子是取向量的点积。\n\n3. 由于经过这么一堆操作之后，分数有很大的可能已经飞起来了捏，这个时候就需要`Attention Alignment`模块对其进行**归一化**之类的操作了捏\n\n    $$\n    \\underset{1\\times 1}{a\\_l} = \\text{align}(\\underset{d\\_q \\times 1}{\\boldsymbol{e\\_l}}, \\underset{n\\_f \\times 1}{\\boldsymbol{e}}) \n    $$\n\n注意力权重$\\boldsymbol{a} = [a_1, \\ldots, a_{n_f}] \\in \\mathbb{R}^{n_f}$为注意力模块提供了一个相当直观的解释。每个权重直接表明了每个特征向量相对于其他特征向量对于这个问题的重要性。 \n\n4. 在`Weight Average`模块完成**上下文生成**：\n\n    $$\n    \\underset{d\\_v \\times 1}{\\boldsymbol{c}} = \\sum\\_{l = 1}^{n\\_f} \\underset{1 \\times 1}{a\\_l}\\times \\underset{d\\_v \\times 1}{\\boldsymbol{v}\\_l}\n    $$\n\n5. 输出处理就想怎么搞就怎么搞了捏，例如 用于分类\n\n    $$\n    \\underset{d\\_y \\times 1}{\\hat{\\boldsymbol{y}}} = \\text{softmax}( \\underset{d\\_y \\times d\\_v}{\\boldsymbol{W}\\_c}\\times \\underset{d\\_v \\times 1}{\\boldsymbol{c}} + \\underset{d\\_y \\times 1}{\\boldsymbol{b}\\_c})\n    $$\n\n### Attention分类\n\n<img src=\"/img/Attention/Taxonomy.png\" style=\"max-width: 100%; height: auto;\">\n\n论文按照上图的方式给Attention进行了分类\n\n由于篇幅限制，这里决定重开几个博文来分别介绍这些Attention，链接如下：\n\n{% post_link 'Feature-Related-Attention' %}\n\n\n\n\n\n# 📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\n<a href=\"/paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf\" target=\"_blank\">📄 Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a>","source":"_posts/Attention.md","raw":"---\ntitle: Attention Overview\ndate: 2025-07-10 19:56:23\ncategories:\n  - CDR\n  - model\n  - attention\ntags:\n  - CDR\n  - model\n  - Basic\n  - 还没写完捏\n  - deep learning\n  - PyTorch\n---\n\n# Is Attention All My Need ?\n\n> 注意力机制在图神经网络中扮演着越来越重要的角色。~~但鼠鼠现在连正常的Attention有哪些都不清楚捏~~本文鼠鼠将从一般的Attention出发，给出Attention的总体结构，然后按分类介绍现有的主要的Attention\n\n本文主要来自于一篇论文，基本可以看作[那篇论文](/paper/Brauwers和Frasincar%20-%202023%20-%20A%20General%20Survey%20on%20Attention%20Mechanisms%20in%20Deep%20Learning.pdf)的阅读笔记\n\n<!-- more -->\n\n## 🎯 引言\n\n在深度学习领域，注意力机制已经成为一个革命性的创新，特别是在处理序列数据和图像数据方面取得了巨大成功。而在图神经网络中，注意力机制的引入不仅提高了模型的表现力，还增强了模型的可解释性。\n\n在图结构数据中应用注意力机制主要有以下优势：\n1. 自适应性：能够根据任务动态调整不同邻居节点的重要性\n2. 可解释性：通过注意力权重可以直观理解模型的决策过程\n3. 长程依赖：有效缓解了传统GNN中的过平滑问题\n4. 异质性处理：更好地处理异质图中的不同类型节点和边\n\n## 📚 总览Attention\n\n本章节主要参考了论文[📄 Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning](/paper/Brauwers和Frasincar%20-%202023%20-%20A%20General%20Survey%20on%20Attention%20Mechanisms%20in%20Deep%20Learning.pdf)有兴趣的话可以看看原文捏\n\n<embed src=\"/paper/Brauwers和Frasincar%20-%202023%20-%20A%20General%20Survey%20on%20Attention%20Mechanisms%20in%20Deep%20Learning.pdf\" width=\"45%\" height=\"400\" type=\"application/pdf\">\n\n### Attention的一般结构\n\n<img src=\"/img/Attention/TotalModel.png\" alt=\"TotalModel\" width=\"60%\" height=\"auto\">\n\n上图是从总体上看Attention在整个任务模型框架中的位置\n\n框架包含四个核心组件：\n1. **特征模型**：负责输入数据的特征提取\n2. **查询模型**：生成注意力查询向量\n3. **注意力模型**：计算注意力权重\n4. **输出模型**：生成最终预测结果\n\n接下来，我们会从 *输入* 的角度来看**特征模型**和**查询模型**，从 *输出* 的角度来看**注意力模型**和**输出模型**\n\n#### 输入处理机制\n\n1. **特征模型**，即将任务的输入进行embedding\n   \n    对于输入矩阵$ X \\in \\mathbb{R}^{d_x \\times n_x} $，特征模型提取特征向量：$\\boldsymbol{F} = [f_1, \\ldots, f_{n_f}] \\in \\mathbb{R}^{d_f \\times n_f}$\n\n2. **查询模型**，查询模型产生查询向量$ \\boldsymbol{q} \\in \\mathbb{R}^{d_q} $，用以告诉注意力模型哪一个特征是重要的\n\n一般情况下，这两个模型可以用CNN或RNN\n\n#### 输出计算机制\n\n<img src=\"/img/Attention/GeneralAttentionModule.png\" alt=\"GeneralAttentionModule\" width=\"50%\" height=\"auto\">\n\n上图是Attention模型总体结构的说明，下面对这张图进行详细的说明\n\n1. 特征矩阵$\\boldsymbol{F} = [\\boldsymbol{f}\\_1, \\ldots, \\boldsymbol{f}\\_{n\\_f}] \\in \\mathbb{R}^{d\\_f \\times n\\_f}$，通过*某些方法*将其分为Keys矩阵$\\boldsymbol{K} = [\\boldsymbol{k}\\_1, \\ldots, \\boldsymbol{k}\\_{n\\_f}] \\in \\mathbb{R}^{d\\_k \\times n\\_f}$和Values矩阵$\\boldsymbol{V} = [\\boldsymbol{v}_1, \\ldots, \\boldsymbol{v}\\_{n\\_f}] \\in \\mathbb{R}^{d\\_v \\times n\\_f}$，这里的*某些方法*，一般情况下，按以下的方式通过**线性变换**得到：\n\n$$\n\\underset{d\\_{k} \\times n\\_{f}}{\\boldsymbol{K}}=\\underset{d\\_{k} \\times d\\_{f}}{\\boldsymbol{W}\\_{K}} \\times \\underset{d\\_{f} \\times n\\_{f}}{\\boldsymbol{F}}, \\quad \\underset{d\\_{v} \\times n\\_{f}}{\\boldsymbol{V}}=\\underset{d\\_{v} \\times d\\_{f}}{\\boldsymbol{W}\\_{V}} \\times \\underset{d\\_{f} \\times n\\_{f}}{\\boldsymbol{F}} .\n$$\n\n2. `Attention Scores`模块根据 $\\boldsymbol{q}$ 计算每一个key向量对应的分数$\\boldsymbol{e} = [e_1, \\ldots, e_{n_f}] \\in \\mathbb{R}^{n_f}$：\n\n    $$\n    \\underset{1\\times 1}{e\\_l} = \\text{score}(\\underset{d\\_q \\times 1}{\\boldsymbol{q}}, \\underset{d\\_k \\times 1}{\\boldsymbol{k}\\_l}) \n    $$\n\n    如前所述，查询象征着对信息的请求。注意力分数$e_l$表示根据查询，关键向量$\\boldsymbol{k}_l$中包含的信息的重要性。如果查询和关键向量的维度相同，则得分函数的一个例子是取向量的点积。\n\n3. 由于经过这么一堆操作之后，分数有很大的可能已经飞起来了捏，这个时候就需要`Attention Alignment`模块对其进行**归一化**之类的操作了捏\n\n    $$\n    \\underset{1\\times 1}{a\\_l} = \\text{align}(\\underset{d\\_q \\times 1}{\\boldsymbol{e\\_l}}, \\underset{n\\_f \\times 1}{\\boldsymbol{e}}) \n    $$\n\n注意力权重$\\boldsymbol{a} = [a_1, \\ldots, a_{n_f}] \\in \\mathbb{R}^{n_f}$为注意力模块提供了一个相当直观的解释。每个权重直接表明了每个特征向量相对于其他特征向量对于这个问题的重要性。 \n\n4. 在`Weight Average`模块完成**上下文生成**：\n\n    $$\n    \\underset{d\\_v \\times 1}{\\boldsymbol{c}} = \\sum\\_{l = 1}^{n\\_f} \\underset{1 \\times 1}{a\\_l}\\times \\underset{d\\_v \\times 1}{\\boldsymbol{v}\\_l}\n    $$\n\n5. 输出处理就想怎么搞就怎么搞了捏，例如 用于分类\n\n    $$\n    \\underset{d\\_y \\times 1}{\\hat{\\boldsymbol{y}}} = \\text{softmax}( \\underset{d\\_y \\times d\\_v}{\\boldsymbol{W}\\_c}\\times \\underset{d\\_v \\times 1}{\\boldsymbol{c}} + \\underset{d\\_y \\times 1}{\\boldsymbol{b}\\_c})\n    $$\n\n### Attention分类\n\n<img src=\"/img/Attention/Taxonomy.png\" style=\"max-width: 100%; height: auto;\">\n\n论文按照上图的方式给Attention进行了分类\n\n由于篇幅限制，这里决定重开几个博文来分别介绍这些Attention，链接如下：\n\n{% post_link 'Feature-Related-Attention' %}\n\n\n\n\n\n# 📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\n<a href=\"/paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf\" target=\"_blank\">📄 Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a>","slug":"Attention","published":1,"updated":"2025-07-14T14:38:29.037Z","comments":1,"layout":"post","photos":[],"_id":"cmd39u3jb0001qk41d8z59j9n","content":"<h1 id=\"Is-Attention-All-My-Need\"><a href=\"#Is-Attention-All-My-Need\" class=\"headerlink\" title=\"Is Attention All My Need ?\"></a>Is Attention All My Need ?</h1><blockquote>\n<p>注意力机制在图神经网络中扮演着越来越重要的角色。<del>但鼠鼠现在连正常的Attention有哪些都不清楚捏</del>本文鼠鼠将从一般的Attention出发，给出Attention的总体结构，然后按分类介绍现有的主要的Attention</p>\n</blockquote>\n<p>本文主要来自于一篇论文，基本可以看作<a href=\"/paper/Brauwers%E5%92%8CFrasincar%20-%202023%20-%20A%20General%20Survey%20on%20Attention%20Mechanisms%20in%20Deep%20Learning.pdf\">那篇论文</a>的阅读笔记</p>\n<span id=\"more\"></span>\n\n<h2 id=\"🎯-引言\"><a href=\"#🎯-引言\" class=\"headerlink\" title=\"🎯 引言\"></a>🎯 引言</h2><p>在深度学习领域，注意力机制已经成为一个革命性的创新，特别是在处理序列数据和图像数据方面取得了巨大成功。而在图神经网络中，注意力机制的引入不仅提高了模型的表现力，还增强了模型的可解释性。</p>\n<p>在图结构数据中应用注意力机制主要有以下优势：</p>\n<ol>\n<li>自适应性：能够根据任务动态调整不同邻居节点的重要性</li>\n<li>可解释性：通过注意力权重可以直观理解模型的决策过程</li>\n<li>长程依赖：有效缓解了传统GNN中的过平滑问题</li>\n<li>异质性处理：更好地处理异质图中的不同类型节点和边</li>\n</ol>\n<h2 id=\"📚-总览Attention\"><a href=\"#📚-总览Attention\" class=\"headerlink\" title=\"📚 总览Attention\"></a>📚 总览Attention</h2><p>本章节主要参考了论文<a href=\"/paper/Brauwers%E5%92%8CFrasincar%20-%202023%20-%20A%20General%20Survey%20on%20Attention%20Mechanisms%20in%20Deep%20Learning.pdf\">📄 Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a>有兴趣的话可以看看原文捏</p>\n<embed src=\"/paper/Brauwers和Frasincar%20-%202023%20-%20A%20General%20Survey%20on%20Attention%20Mechanisms%20in%20Deep%20Learning.pdf\" width=\"45%\" height=\"400\" type=\"application/pdf\">\n\n<h3 id=\"Attention的一般结构\"><a href=\"#Attention的一般结构\" class=\"headerlink\" title=\"Attention的一般结构\"></a>Attention的一般结构</h3><img src=\"/img/Attention/TotalModel.png\" alt=\"TotalModel\" width=\"60%\" height=\"auto\">\n\n<p>上图是从总体上看Attention在整个任务模型框架中的位置</p>\n<p>框架包含四个核心组件：</p>\n<ol>\n<li><strong>特征模型</strong>：负责输入数据的特征提取</li>\n<li><strong>查询模型</strong>：生成注意力查询向量</li>\n<li><strong>注意力模型</strong>：计算注意力权重</li>\n<li><strong>输出模型</strong>：生成最终预测结果</li>\n</ol>\n<p>接下来，我们会从 <em>输入</em> 的角度来看<strong>特征模型</strong>和<strong>查询模型</strong>，从 <em>输出</em> 的角度来看<strong>注意力模型</strong>和<strong>输出模型</strong></p>\n<h4 id=\"输入处理机制\"><a href=\"#输入处理机制\" class=\"headerlink\" title=\"输入处理机制\"></a>输入处理机制</h4><ol>\n<li><p><strong>特征模型</strong>，即将任务的输入进行embedding</p>\n<p> 对于输入矩阵$ X \\in \\mathbb{R}^{d_x \\times n_x} $，特征模型提取特征向量：$\\boldsymbol{F} &#x3D; [f_1, \\ldots, f_{n_f}] \\in \\mathbb{R}^{d_f \\times n_f}$</p>\n</li>\n<li><p><strong>查询模型</strong>，查询模型产生查询向量$ \\boldsymbol{q} \\in \\mathbb{R}^{d_q} $，用以告诉注意力模型哪一个特征是重要的</p>\n</li>\n</ol>\n<p>一般情况下，这两个模型可以用CNN或RNN</p>\n<h4 id=\"输出计算机制\"><a href=\"#输出计算机制\" class=\"headerlink\" title=\"输出计算机制\"></a>输出计算机制</h4><img src=\"/img/Attention/GeneralAttentionModule.png\" alt=\"GeneralAttentionModule\" width=\"50%\" height=\"auto\">\n\n<p>上图是Attention模型总体结构的说明，下面对这张图进行详细的说明</p>\n<ol>\n<li>特征矩阵$\\boldsymbol{F} &#x3D; [\\boldsymbol{f}_1, \\ldots, \\boldsymbol{f}_{n_f}] \\in \\mathbb{R}^{d_f \\times n_f}$，通过<em>某些方法</em>将其分为Keys矩阵$\\boldsymbol{K} &#x3D; [\\boldsymbol{k}_1, \\ldots, \\boldsymbol{k}_{n_f}] \\in \\mathbb{R}^{d_k \\times n_f}$和Values矩阵$\\boldsymbol{V} &#x3D; [\\boldsymbol{v}_1, \\ldots, \\boldsymbol{v}_{n_f}] \\in \\mathbb{R}^{d_v \\times n_f}$，这里的<em>某些方法</em>，一般情况下，按以下的方式通过<strong>线性变换</strong>得到：</li>\n</ol>\n<p>$$<br>\\underset{d_{k} \\times n_{f}}{\\boldsymbol{K}}&#x3D;\\underset{d_{k} \\times d_{f}}{\\boldsymbol{W}_{K}} \\times \\underset{d_{f} \\times n_{f}}{\\boldsymbol{F}}, \\quad \\underset{d_{v} \\times n_{f}}{\\boldsymbol{V}}&#x3D;\\underset{d_{v} \\times d_{f}}{\\boldsymbol{W}_{V}} \\times \\underset{d_{f} \\times n_{f}}{\\boldsymbol{F}} .<br>$$</p>\n<ol start=\"2\">\n<li><p><code>Attention Scores</code>模块根据 $\\boldsymbol{q}$ 计算每一个key向量对应的分数$\\boldsymbol{e} &#x3D; [e_1, \\ldots, e_{n_f}] \\in \\mathbb{R}^{n_f}$：</p>\n<p> $$<br> \\underset{1\\times 1}{e_l} &#x3D; \\text{score}(\\underset{d_q \\times 1}{\\boldsymbol{q}}, \\underset{d_k \\times 1}{\\boldsymbol{k}_l})<br> $$</p>\n<p> 如前所述，查询象征着对信息的请求。注意力分数$e_l$表示根据查询，关键向量$\\boldsymbol{k}_l$中包含的信息的重要性。如果查询和关键向量的维度相同，则得分函数的一个例子是取向量的点积。</p>\n</li>\n<li><p>由于经过这么一堆操作之后，分数有很大的可能已经飞起来了捏，这个时候就需要<code>Attention Alignment</code>模块对其进行<strong>归一化</strong>之类的操作了捏</p>\n<p> $$<br> \\underset{1\\times 1}{a_l} &#x3D; \\text{align}(\\underset{d_q \\times 1}{\\boldsymbol{e_l}}, \\underset{n_f \\times 1}{\\boldsymbol{e}})<br> $$</p>\n</li>\n</ol>\n<p>注意力权重$\\boldsymbol{a} &#x3D; [a_1, \\ldots, a_{n_f}] \\in \\mathbb{R}^{n_f}$为注意力模块提供了一个相当直观的解释。每个权重直接表明了每个特征向量相对于其他特征向量对于这个问题的重要性。 </p>\n<ol start=\"4\">\n<li><p>在<code>Weight Average</code>模块完成<strong>上下文生成</strong>：</p>\n<p> $$<br> \\underset{d_v \\times 1}{\\boldsymbol{c}} &#x3D; \\sum_{l &#x3D; 1}^{n_f} \\underset{1 \\times 1}{a_l}\\times \\underset{d_v \\times 1}{\\boldsymbol{v}_l}<br> $$</p>\n</li>\n<li><p>输出处理就想怎么搞就怎么搞了捏，例如 用于分类</p>\n<p> $$<br> \\underset{d_y \\times 1}{\\hat{\\boldsymbol{y}}} &#x3D; \\text{softmax}( \\underset{d_y \\times d_v}{\\boldsymbol{W}_c}\\times \\underset{d_v \\times 1}{\\boldsymbol{c}} + \\underset{d_y \\times 1}{\\boldsymbol{b}_c})<br> $$</p>\n</li>\n</ol>\n<h3 id=\"Attention分类\"><a href=\"#Attention分类\" class=\"headerlink\" title=\"Attention分类\"></a>Attention分类</h3><img src=\"/img/Attention/Taxonomy.png\" style=\"max-width: 100%; height: auto;\">\n\n<p>论文按照上图的方式给Attention进行了分类</p>\n<p>由于篇幅限制，这里决定重开几个博文来分别介绍这些Attention，链接如下：</p>\n<a href=\"/2025/07/14/Feature-Related-Attention/\" title=\"Feature-Related Attention\">Feature-Related Attention</a>\n\n\n\n\n\n<h1 id=\"📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"><a href=\"#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\" class=\"headerlink\" title=\"📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1><p><a href=\"/paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf\" target=\"_blank\">📄 Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a></p>\n","length":1293,"excerpt":"<h1 id=\"Is-Attention-All-My-Need\"><a href=\"#Is-Attention-All-My-Need\" class=\"headerlink\" title=\"Is Attention All My Need ?\"></a>Is Attention All My Need ?</h1><blockquote>\n<p>注意力机制在图神经网络中扮演着越来越重要的角色。<del>但鼠鼠现在连正常的Attention有哪些都不清楚捏</del>本文鼠鼠将从一般的Attention出发，给出Attention的总体结构，然后按分类介绍现有的主要的Attention</p>\n</blockquote>\n<p>本文主要来自于一篇论文，基本可以看作<a href=\"/paper/Brauwers%E5%92%8CFrasincar%20-%202023%20-%20A%20General%20Survey%20on%20Attention%20Mechanisms%20in%20Deep%20Learning.pdf\">那篇论文</a>的阅读笔记</p>","more":"<h2 id=\"🎯-引言\"><a href=\"#🎯-引言\" class=\"headerlink\" title=\"🎯 引言\"></a>🎯 引言</h2><p>在深度学习领域，注意力机制已经成为一个革命性的创新，特别是在处理序列数据和图像数据方面取得了巨大成功。而在图神经网络中，注意力机制的引入不仅提高了模型的表现力，还增强了模型的可解释性。</p>\n<p>在图结构数据中应用注意力机制主要有以下优势：</p>\n<ol>\n<li>自适应性：能够根据任务动态调整不同邻居节点的重要性</li>\n<li>可解释性：通过注意力权重可以直观理解模型的决策过程</li>\n<li>长程依赖：有效缓解了传统GNN中的过平滑问题</li>\n<li>异质性处理：更好地处理异质图中的不同类型节点和边</li>\n</ol>\n<h2 id=\"📚-总览Attention\"><a href=\"#📚-总览Attention\" class=\"headerlink\" title=\"📚 总览Attention\"></a>📚 总览Attention</h2><p>本章节主要参考了论文<a href=\"/paper/Brauwers%E5%92%8CFrasincar%20-%202023%20-%20A%20General%20Survey%20on%20Attention%20Mechanisms%20in%20Deep%20Learning.pdf\">📄 Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a>有兴趣的话可以看看原文捏</p>\n<embed src=\"/paper/Brauwers和Frasincar%20-%202023%20-%20A%20General%20Survey%20on%20Attention%20Mechanisms%20in%20Deep%20Learning.pdf\" width=\"45%\" height=\"400\" type=\"application/pdf\">\n\n<h3 id=\"Attention的一般结构\"><a href=\"#Attention的一般结构\" class=\"headerlink\" title=\"Attention的一般结构\"></a>Attention的一般结构</h3><img src=\"/img/Attention/TotalModel.png\" alt=\"TotalModel\" width=\"60%\" height=\"auto\">\n\n<p>上图是从总体上看Attention在整个任务模型框架中的位置</p>\n<p>框架包含四个核心组件：</p>\n<ol>\n<li><strong>特征模型</strong>：负责输入数据的特征提取</li>\n<li><strong>查询模型</strong>：生成注意力查询向量</li>\n<li><strong>注意力模型</strong>：计算注意力权重</li>\n<li><strong>输出模型</strong>：生成最终预测结果</li>\n</ol>\n<p>接下来，我们会从 <em>输入</em> 的角度来看<strong>特征模型</strong>和<strong>查询模型</strong>，从 <em>输出</em> 的角度来看<strong>注意力模型</strong>和<strong>输出模型</strong></p>\n<h4 id=\"输入处理机制\"><a href=\"#输入处理机制\" class=\"headerlink\" title=\"输入处理机制\"></a>输入处理机制</h4><ol>\n<li><p><strong>特征模型</strong>，即将任务的输入进行embedding</p>\n<p> 对于输入矩阵$ X \\in \\mathbb{R}^{d_x \\times n_x} $，特征模型提取特征向量：$\\boldsymbol{F} &#x3D; [f_1, \\ldots, f_{n_f}] \\in \\mathbb{R}^{d_f \\times n_f}$</p>\n</li>\n<li><p><strong>查询模型</strong>，查询模型产生查询向量$ \\boldsymbol{q} \\in \\mathbb{R}^{d_q} $，用以告诉注意力模型哪一个特征是重要的</p>\n</li>\n</ol>\n<p>一般情况下，这两个模型可以用CNN或RNN</p>\n<h4 id=\"输出计算机制\"><a href=\"#输出计算机制\" class=\"headerlink\" title=\"输出计算机制\"></a>输出计算机制</h4><img src=\"/img/Attention/GeneralAttentionModule.png\" alt=\"GeneralAttentionModule\" width=\"50%\" height=\"auto\">\n\n<p>上图是Attention模型总体结构的说明，下面对这张图进行详细的说明</p>\n<ol>\n<li>特征矩阵$\\boldsymbol{F} &#x3D; [\\boldsymbol{f}_1, \\ldots, \\boldsymbol{f}_{n_f}] \\in \\mathbb{R}^{d_f \\times n_f}$，通过<em>某些方法</em>将其分为Keys矩阵$\\boldsymbol{K} &#x3D; [\\boldsymbol{k}_1, \\ldots, \\boldsymbol{k}_{n_f}] \\in \\mathbb{R}^{d_k \\times n_f}$和Values矩阵$\\boldsymbol{V} &#x3D; [\\boldsymbol{v}_1, \\ldots, \\boldsymbol{v}_{n_f}] \\in \\mathbb{R}^{d_v \\times n_f}$，这里的<em>某些方法</em>，一般情况下，按以下的方式通过<strong>线性变换</strong>得到：</li>\n</ol>\n<p>$$<br>\\underset{d_{k} \\times n_{f}}{\\boldsymbol{K}}&#x3D;\\underset{d_{k} \\times d_{f}}{\\boldsymbol{W}_{K}} \\times \\underset{d_{f} \\times n_{f}}{\\boldsymbol{F}}, \\quad \\underset{d_{v} \\times n_{f}}{\\boldsymbol{V}}&#x3D;\\underset{d_{v} \\times d_{f}}{\\boldsymbol{W}_{V}} \\times \\underset{d_{f} \\times n_{f}}{\\boldsymbol{F}} .<br>$$</p>\n<ol start=\"2\">\n<li><p><code>Attention Scores</code>模块根据 $\\boldsymbol{q}$ 计算每一个key向量对应的分数$\\boldsymbol{e} &#x3D; [e_1, \\ldots, e_{n_f}] \\in \\mathbb{R}^{n_f}$：</p>\n<p> $$<br> \\underset{1\\times 1}{e_l} &#x3D; \\text{score}(\\underset{d_q \\times 1}{\\boldsymbol{q}}, \\underset{d_k \\times 1}{\\boldsymbol{k}_l})<br> $$</p>\n<p> 如前所述，查询象征着对信息的请求。注意力分数$e_l$表示根据查询，关键向量$\\boldsymbol{k}_l$中包含的信息的重要性。如果查询和关键向量的维度相同，则得分函数的一个例子是取向量的点积。</p>\n</li>\n<li><p>由于经过这么一堆操作之后，分数有很大的可能已经飞起来了捏，这个时候就需要<code>Attention Alignment</code>模块对其进行<strong>归一化</strong>之类的操作了捏</p>\n<p> $$<br> \\underset{1\\times 1}{a_l} &#x3D; \\text{align}(\\underset{d_q \\times 1}{\\boldsymbol{e_l}}, \\underset{n_f \\times 1}{\\boldsymbol{e}})<br> $$</p>\n</li>\n</ol>\n<p>注意力权重$\\boldsymbol{a} &#x3D; [a_1, \\ldots, a_{n_f}] \\in \\mathbb{R}^{n_f}$为注意力模块提供了一个相当直观的解释。每个权重直接表明了每个特征向量相对于其他特征向量对于这个问题的重要性。 </p>\n<ol start=\"4\">\n<li><p>在<code>Weight Average</code>模块完成<strong>上下文生成</strong>：</p>\n<p> $$<br> \\underset{d_v \\times 1}{\\boldsymbol{c}} &#x3D; \\sum_{l &#x3D; 1}^{n_f} \\underset{1 \\times 1}{a_l}\\times \\underset{d_v \\times 1}{\\boldsymbol{v}_l}<br> $$</p>\n</li>\n<li><p>输出处理就想怎么搞就怎么搞了捏，例如 用于分类</p>\n<p> $$<br> \\underset{d_y \\times 1}{\\hat{\\boldsymbol{y}}} &#x3D; \\text{softmax}( \\underset{d_y \\times d_v}{\\boldsymbol{W}_c}\\times \\underset{d_v \\times 1}{\\boldsymbol{c}} + \\underset{d_y \\times 1}{\\boldsymbol{b}_c})<br> $$</p>\n</li>\n</ol>\n<h3 id=\"Attention分类\"><a href=\"#Attention分类\" class=\"headerlink\" title=\"Attention分类\"></a>Attention分类</h3><img src=\"/img/Attention/Taxonomy.png\" style=\"max-width: 100%; height: auto;\">\n\n<p>论文按照上图的方式给Attention进行了分类</p>\n<p>由于篇幅限制，这里决定重开几个博文来分别介绍这些Attention，链接如下：</p>\n<a href=\"/2025/07/14/Feature-Related-Attention/\" title=\"Feature-Related Attention\">Feature-Related Attention</a>\n\n\n\n\n\n<h1 id=\"📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"><a href=\"#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\" class=\"headerlink\" title=\"📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1><p><a href=\"/paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf\" target=\"_blank\">📄 Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a></p>"},{"title":"CDR Input Data Analysis","date":"2025-07-09T13:24:58.000Z","_content":"\n# CDR 数据源分析\n\n本文主要是介绍一下 **深度学习** 在 _药物反应预测_ 中运用到的数据源。~~但由于本人比较捞~~ 本文主要从 **深度学习** 角度来看待这些数据源，对其在医学方面的意义~~（主要是鼠鼠也不会捏）~~不会有太多的描述\n\n<!-- more -->\n\n## CDR = Cancer Drug Response\n\n我们的数据源有三种：\n\n- _Cancer Representations_（癌症特征的表示）\n- _Representations of Drug Compounds_（药物特征的表示）\n- _Representations of Treatment Response_（治疗响应的表示）\n\n接下来会按顺序进行说明\n\n---\n\n### Cancer Representations\n\n癌症的特征是多组学的 ~~这不是理所应当吗~~\n\n#### 多组学类型\n\n通常基于以下四类组学数据：\n\n- 基因组（Genomic）\n\n  - 突变（Mutation）：体细胞突变（如单核苷酸变异 SNVs）可能驱动癌症进展，并影响药物靶点。\n  - 拷贝数变异（CNV）：基因拷贝数的增加或缺失可能影响药物敏感性（如 HER2 扩增与曲妥珠单抗疗效相关）。\n\n- 转录组（Transcriptomic）\n\n  - 基因表达（Gene Expression）：通过微阵列或 RNA 测序（RNA-Seq）量化基因的 mRNA 水平。例如，高表达的耐药基因可能预示治疗失败。\n\n- 表观组（Epigenomic）\n\n  - DNA 甲基化（Methylation）：启动子区域的甲基化可能沉默抑癌基因，影响药物反应。\n\n- 蛋白质组（Proteomic）\n  - 蛋白质表达（RPPA 等）：直接测量蛋白质丰度（如激酶活性），更接近功能表型。\n\n对于同一种组学数据，他们被表示成一组 **维数相同的向量**\n\n#### 预处理与整合\n\n1. 数据预处理\n\n- 包括标准化（normalization）、批次效应校正（batch effect correction）和质量控制（QC）。例如，RNA-Seq 数据需通过 RPKM 或 TPM 标准化。\n\n2. 多组学整合方法 ：\n   - 早期整合（Early Integration）：直接拼接不同组学特征为单一向量，但可能因维度灾难（curse of dimensionality）导致过拟合。\n   - 晚期整合（Late Integration）：通过独立子网络处理每组学数据（如 CNN 处理突变，GNN 处理表达数据），再融合特征。例如，MOLI 模型通过三重损失函数整合多组学数据，显著提升跨癌症模型的泛化能力。\n\n#### 基因特征具有优势及新兴趋势\n\n> 2014 年 NCI-DREAM 挑战赛表明， 基因表达数据在预测乳腺癌细胞系药物敏感性时最具预测力（优于突变或 CNV）。因此，约 90%的 DRP 模型使用基因表达（单独或联合其他组学）\n> <img src=\"/img/CDR-data-analysis/gene.png\" alt=\"gene\" width=\"50%\">\n\n##### 新兴趋势\n\n1. **结构生物学整合**：如利用蛋白质-蛋白质相互作用（PPI）网络（STRING 数据库）或通路信息（GSEA）构建生物网络，增强模型可解释性。\n2. **图神经网络（GNN）**：将基因视为节点、相互作用为边，学习拓扑特征（如 GraOmicDRP 模型）。\n\n---\n\n### Representations of Drug Compounds\n\n对药物的表示主要分为三种，一般只选取其中的一种 ~~虽然也有选用几种的 **创新** 方式~~。值得一提的是，在选定药物的表示方式后，之后的特征工程的方式目前来看非常的统一。接下来一一说明每一种表示方式。\n\n#### SMILES（简化分子输入行条目系统）\n\n1. _定义_：SMILES 是一种**线性字符串**表示法，通过符号编码分子结构（如`CCO`表示乙醇）。\n2. _优势_：\n   - 易于存储和处理，广泛用于化学信息学工具（如 RDKit）。\n   - 可直接用于序列模型（如 RNN、Transformer）或通过预处理转换为其他表示（如图结构）。\n\n#### 分子指纹（Fingerprints, FPs）和描述符（Descriptors）\n\n1. 分子指纹\n\n   - _定义_：**二进制向量**，表示分子中是否存在特定子结构（如药效团或官能团）。\n   - _常用类型_：\n     - **Morgan 指纹（ECFP）**：基于原子邻域的圆形拓扑指纹，长度通常为 512 或 1024 位。\n     - **RDKit 指纹**：开源工具生成的二进制指纹。\n   - _优势_：固定长度，适合传统机器学习模型（如随机森林）。\n\n2. 分子描述符\n\n   - _定义_：**数值向量**，编码物理化学性质（如分子量、疏水性、极性表面积等）。\n   - _工具_：PaDEL、Mordred、Dragon 等软件可自动计算数百至数千个描述符。\n\n#### 图结构表示（Graph-based Representations）\n\n1. _定义_ ：将分子表示为**图**，其中原子为**节点**，化学键为**边**，节点和边可附加属性（如原子类型、键类型）。\n2. _优势_ ：\n   - 更自然地表征分子拓扑结构，适合图神经网络（GNN）。\n   - 可捕捉局部和全局分子特征（如官能团相互作用）。\n\n---\n\n### Representations of Treatment Response\n\n从构造模型的角度出发，这是 DRP 的核心数据源\n\n- 它决定了模型最后完成的**任务类型**：训练连续值的**回归任务**和训练离散值的**分类任务**\n- 他的数据质量很大程度上决定了模型的结果的优劣，即对该数据源对模型的好坏影响很大\n\n此外，很少有从数据分析的角度出发分析这个数据源的文献，于是在这里给出简要的说明\n\n#### 连续值表示（Continuous Measures）\n\n1. **IC50**\n\n   - 半数抑制浓度，即抑制 50%细胞活力所需的药物浓度。\n   - _优势_：直观反映药物效力，广泛用于回归模型（如预测 IC50 的数值）。\n   - _局限性_：仅反映单一浓度点的效果，可能忽略剂量-反应曲线的整体形状。\n\n2. **AUC/AAC**\n   - 剂量-反应曲线下面积（Area Under the Curve）或曲线上面积（Activity Area）。\n   - _优势_：全局度量，综合所有浓度点的效果，对噪声更鲁棒。\n   - _应用_：如 DeepCDR 等模型使用 AUC 作为回归目标，实证表明其泛化性优于 IC50。\n\n#### 分类表示（Categorical Measures）\n\n1. **二分类（敏感/耐药）**\n\n   - 通过阈值（如瀑布算法、LOBICO）将连续反应（如 IC50）转化为离散标签。\n   - _优势_：更贴近临床决策需求（如选择敏感药物）。\n   - _示例_：Sharifi-Noghabi et al. (2021) 使用二分类训练深度神经网络，预测患者肿瘤的敏感性。\n\n2. **多分类**\n   - 如低/中/高反应性，适用于更细粒度的临床分级。\n\n#### 排序表示（Ranking）\n\n1. _目标_\n\n   - 为个性化治疗推荐药物排序（如 Top-k 最有效药物）。\n\n2. _方法_\n\n   - Prasse et al. (2022)：将 IC50 转化为相关性分数，设计可微排序损失函数。\n   - PPORank：利用强化学习动态优化排序，适应新增数据。\n\n3. _优势_\n   - 直接支持临床优先级排序，优于传统回归或分类。\n\n#### 数据分析\n\n由于本人大概率会做个分类模型，所以会将主要分析的是**分类表示**的数据在**图神经网络**中比较重视的几个指标，这里分析 _CCLE_ 和 _GDSC_ 两个数据集在选用主流阈值选取方法之后的表示。\n\n直接先看结果捏（这里画了两个小图）\n\n- CCLE\n\n<img src=\"/img/CDR-data-analysis/comprehensive_bipartite_analysis_ccle.png\" alt=\"CCLE\" style=\"max-width: 100%; height: auto;\">\n\n- GDSC\n\n<img src=\"/img/CDR-data-analysis/comprehensive_bipartite_analysis_gdsc.png\" alt=\"GDSC\" style=\"max-width: 100%; height: auto;\">\n\n<p>\n  👉 <a href=\"/code/data_analysis/visualize_graph_analysis.py\" target=\"_blank\">查看用于生成上述图表的本地 Python 脚本：visualize_graph_analysis.py</a>\n</p>\n\n##### 🔍 关键数据对比\n\n| 特征         | CCLE   | GDSC    | 倍数差异    |\n| ------------ | ------ | ------- | ----------- |\n| **数据规模** |\n| 总节点数     | 341    | 783     | 2.3×        |\n| 第一类节点   | 317    | 561     | 1.8×        |\n| 第二类节点   | 24     | 222     | 9.3×        |\n| 总边数       | 7,307  | 100,572 | 13.8×       |\n| **图结构**   |\n| 密度         | 0.9604 | 0.8075  | 0.84×       |\n| 稀疏性       | 0.0396 | 0.1925  | 4.9×        |\n| 平均度       | 42.86  | 256.89  | 6.0×        |\n| 图直径       | 3      | 4       | 1.3×        |\n| **边分布**   |\n| 正边数量     | 1,375  | 11,591  | 8.4×        |\n| 负边数量     | 5,932  | 88,981  | 15.0×       |\n| 正边比例     | 18.8%  | 11.5%   | 0.61×       |\n| 正负边比例   | 1:4.3  | 1:7.7   | 1.8× 不平衡 |\n\n##### 📊 GNN 训练挑战分析\n\n###### 过平滑风险评估\n\n- **CCLE**: ⚠️ 高风险 (平均度 42.86)\n- **GDSC**: 🚨 极高风险 (平均度 256.89)\n\n###### 样本不平衡程度\n\n- **CCLE**: 正负边比例 1:4.3 (中等不平衡)\n- **GDSC**: 正负边比例 1:7.7 (严重不平衡)\n\n###### 邻居相似度分析\n\n```python\n# 邻居重叠度对比\nCCLE_similarity = {\n    \"第一类节点\": 0.9374,  # 高度相似\n    \"第二类节点\": 0.9274   # 高度相似\n}\n\nGDSC_similarity = {\n    \"第一类节点\": 0.7659,  # 中等相似\n    \"第二类节点\": 0.7143   # 中等相似\n}\n```\n\n**结论**: CCLE 结构更均匀但多样性不足，GDSC 结构更复杂但多样性更好\n\n##### 🎯 GNN 架构建议对比\n\n###### 推荐架构优先级\n\n- CCLE 推荐架构\n\n  1. **Bipartite GNN** + Signed GCN\n  2. **简单异构图 GNN** (HetGNN)\n  3. **标准 GCN** + 强正则化\n\n- GDSC 推荐架构\n\n  1. **采样型 GNN** (GraphSAINT, FastGCN) + SGCN\n  2. **大规模异构图 GNN** (HGT, RGCN)\n  3. **图 Transformer** (处理复杂结构)\n\n###### 具体参数建议\n\n| 参数           | CCLE         | GDSC         | 原因                |\n| -------------- | ------------ | ------------ | ------------------- |\n| **网络深度**   | 2-3 层       | 严格 2 层    | GDSC 过平滑风险更高 |\n| **隐藏维度**   | 64-128       | 128-256      | GDSC 需要更大容量   |\n| **Dropout 率** | 0.3-0.5      | 0.5-0.7      | GDSC 需要更强正则化 |\n| **学习率**     | 0.001-0.01   | 0.0001-0.001 | GDSC 需要更保守训练 |\n| **批次大小**   | 32-64 个子图 | 16-32 个子图 | GDSC 内存限制       |\n| **采样策略**   | 可选         | 必须         | GDSC 无法全图训练   |\n\n# 📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\n\n<a href=\"/paper/Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends.pdf\" target=\"_blank\">📄 Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends</a>","source":"_posts/CDR-data-analysis.md","raw":"---\ntitle: CDR Input Data Analysis\ndate: 2025-07-09 21:24:58\ntags:\n  - CDR\n  - Data Analysis\n  - 可能有点用\n  - graph theory\ncategories:\n  - CDR\n  - Data Analysis\n---\n\n# CDR 数据源分析\n\n本文主要是介绍一下 **深度学习** 在 _药物反应预测_ 中运用到的数据源。~~但由于本人比较捞~~ 本文主要从 **深度学习** 角度来看待这些数据源，对其在医学方面的意义~~（主要是鼠鼠也不会捏）~~不会有太多的描述\n\n<!-- more -->\n\n## CDR = Cancer Drug Response\n\n我们的数据源有三种：\n\n- _Cancer Representations_（癌症特征的表示）\n- _Representations of Drug Compounds_（药物特征的表示）\n- _Representations of Treatment Response_（治疗响应的表示）\n\n接下来会按顺序进行说明\n\n---\n\n### Cancer Representations\n\n癌症的特征是多组学的 ~~这不是理所应当吗~~\n\n#### 多组学类型\n\n通常基于以下四类组学数据：\n\n- 基因组（Genomic）\n\n  - 突变（Mutation）：体细胞突变（如单核苷酸变异 SNVs）可能驱动癌症进展，并影响药物靶点。\n  - 拷贝数变异（CNV）：基因拷贝数的增加或缺失可能影响药物敏感性（如 HER2 扩增与曲妥珠单抗疗效相关）。\n\n- 转录组（Transcriptomic）\n\n  - 基因表达（Gene Expression）：通过微阵列或 RNA 测序（RNA-Seq）量化基因的 mRNA 水平。例如，高表达的耐药基因可能预示治疗失败。\n\n- 表观组（Epigenomic）\n\n  - DNA 甲基化（Methylation）：启动子区域的甲基化可能沉默抑癌基因，影响药物反应。\n\n- 蛋白质组（Proteomic）\n  - 蛋白质表达（RPPA 等）：直接测量蛋白质丰度（如激酶活性），更接近功能表型。\n\n对于同一种组学数据，他们被表示成一组 **维数相同的向量**\n\n#### 预处理与整合\n\n1. 数据预处理\n\n- 包括标准化（normalization）、批次效应校正（batch effect correction）和质量控制（QC）。例如，RNA-Seq 数据需通过 RPKM 或 TPM 标准化。\n\n2. 多组学整合方法 ：\n   - 早期整合（Early Integration）：直接拼接不同组学特征为单一向量，但可能因维度灾难（curse of dimensionality）导致过拟合。\n   - 晚期整合（Late Integration）：通过独立子网络处理每组学数据（如 CNN 处理突变，GNN 处理表达数据），再融合特征。例如，MOLI 模型通过三重损失函数整合多组学数据，显著提升跨癌症模型的泛化能力。\n\n#### 基因特征具有优势及新兴趋势\n\n> 2014 年 NCI-DREAM 挑战赛表明， 基因表达数据在预测乳腺癌细胞系药物敏感性时最具预测力（优于突变或 CNV）。因此，约 90%的 DRP 模型使用基因表达（单独或联合其他组学）\n> <img src=\"/img/CDR-data-analysis/gene.png\" alt=\"gene\" width=\"50%\">\n\n##### 新兴趋势\n\n1. **结构生物学整合**：如利用蛋白质-蛋白质相互作用（PPI）网络（STRING 数据库）或通路信息（GSEA）构建生物网络，增强模型可解释性。\n2. **图神经网络（GNN）**：将基因视为节点、相互作用为边，学习拓扑特征（如 GraOmicDRP 模型）。\n\n---\n\n### Representations of Drug Compounds\n\n对药物的表示主要分为三种，一般只选取其中的一种 ~~虽然也有选用几种的 **创新** 方式~~。值得一提的是，在选定药物的表示方式后，之后的特征工程的方式目前来看非常的统一。接下来一一说明每一种表示方式。\n\n#### SMILES（简化分子输入行条目系统）\n\n1. _定义_：SMILES 是一种**线性字符串**表示法，通过符号编码分子结构（如`CCO`表示乙醇）。\n2. _优势_：\n   - 易于存储和处理，广泛用于化学信息学工具（如 RDKit）。\n   - 可直接用于序列模型（如 RNN、Transformer）或通过预处理转换为其他表示（如图结构）。\n\n#### 分子指纹（Fingerprints, FPs）和描述符（Descriptors）\n\n1. 分子指纹\n\n   - _定义_：**二进制向量**，表示分子中是否存在特定子结构（如药效团或官能团）。\n   - _常用类型_：\n     - **Morgan 指纹（ECFP）**：基于原子邻域的圆形拓扑指纹，长度通常为 512 或 1024 位。\n     - **RDKit 指纹**：开源工具生成的二进制指纹。\n   - _优势_：固定长度，适合传统机器学习模型（如随机森林）。\n\n2. 分子描述符\n\n   - _定义_：**数值向量**，编码物理化学性质（如分子量、疏水性、极性表面积等）。\n   - _工具_：PaDEL、Mordred、Dragon 等软件可自动计算数百至数千个描述符。\n\n#### 图结构表示（Graph-based Representations）\n\n1. _定义_ ：将分子表示为**图**，其中原子为**节点**，化学键为**边**，节点和边可附加属性（如原子类型、键类型）。\n2. _优势_ ：\n   - 更自然地表征分子拓扑结构，适合图神经网络（GNN）。\n   - 可捕捉局部和全局分子特征（如官能团相互作用）。\n\n---\n\n### Representations of Treatment Response\n\n从构造模型的角度出发，这是 DRP 的核心数据源\n\n- 它决定了模型最后完成的**任务类型**：训练连续值的**回归任务**和训练离散值的**分类任务**\n- 他的数据质量很大程度上决定了模型的结果的优劣，即对该数据源对模型的好坏影响很大\n\n此外，很少有从数据分析的角度出发分析这个数据源的文献，于是在这里给出简要的说明\n\n#### 连续值表示（Continuous Measures）\n\n1. **IC50**\n\n   - 半数抑制浓度，即抑制 50%细胞活力所需的药物浓度。\n   - _优势_：直观反映药物效力，广泛用于回归模型（如预测 IC50 的数值）。\n   - _局限性_：仅反映单一浓度点的效果，可能忽略剂量-反应曲线的整体形状。\n\n2. **AUC/AAC**\n   - 剂量-反应曲线下面积（Area Under the Curve）或曲线上面积（Activity Area）。\n   - _优势_：全局度量，综合所有浓度点的效果，对噪声更鲁棒。\n   - _应用_：如 DeepCDR 等模型使用 AUC 作为回归目标，实证表明其泛化性优于 IC50。\n\n#### 分类表示（Categorical Measures）\n\n1. **二分类（敏感/耐药）**\n\n   - 通过阈值（如瀑布算法、LOBICO）将连续反应（如 IC50）转化为离散标签。\n   - _优势_：更贴近临床决策需求（如选择敏感药物）。\n   - _示例_：Sharifi-Noghabi et al. (2021) 使用二分类训练深度神经网络，预测患者肿瘤的敏感性。\n\n2. **多分类**\n   - 如低/中/高反应性，适用于更细粒度的临床分级。\n\n#### 排序表示（Ranking）\n\n1. _目标_\n\n   - 为个性化治疗推荐药物排序（如 Top-k 最有效药物）。\n\n2. _方法_\n\n   - Prasse et al. (2022)：将 IC50 转化为相关性分数，设计可微排序损失函数。\n   - PPORank：利用强化学习动态优化排序，适应新增数据。\n\n3. _优势_\n   - 直接支持临床优先级排序，优于传统回归或分类。\n\n#### 数据分析\n\n由于本人大概率会做个分类模型，所以会将主要分析的是**分类表示**的数据在**图神经网络**中比较重视的几个指标，这里分析 _CCLE_ 和 _GDSC_ 两个数据集在选用主流阈值选取方法之后的表示。\n\n直接先看结果捏（这里画了两个小图）\n\n- CCLE\n\n<img src=\"/img/CDR-data-analysis/comprehensive_bipartite_analysis_ccle.png\" alt=\"CCLE\" style=\"max-width: 100%; height: auto;\">\n\n- GDSC\n\n<img src=\"/img/CDR-data-analysis/comprehensive_bipartite_analysis_gdsc.png\" alt=\"GDSC\" style=\"max-width: 100%; height: auto;\">\n\n<p>\n  👉 <a href=\"/code/data_analysis/visualize_graph_analysis.py\" target=\"_blank\">查看用于生成上述图表的本地 Python 脚本：visualize_graph_analysis.py</a>\n</p>\n\n##### 🔍 关键数据对比\n\n| 特征         | CCLE   | GDSC    | 倍数差异    |\n| ------------ | ------ | ------- | ----------- |\n| **数据规模** |\n| 总节点数     | 341    | 783     | 2.3×        |\n| 第一类节点   | 317    | 561     | 1.8×        |\n| 第二类节点   | 24     | 222     | 9.3×        |\n| 总边数       | 7,307  | 100,572 | 13.8×       |\n| **图结构**   |\n| 密度         | 0.9604 | 0.8075  | 0.84×       |\n| 稀疏性       | 0.0396 | 0.1925  | 4.9×        |\n| 平均度       | 42.86  | 256.89  | 6.0×        |\n| 图直径       | 3      | 4       | 1.3×        |\n| **边分布**   |\n| 正边数量     | 1,375  | 11,591  | 8.4×        |\n| 负边数量     | 5,932  | 88,981  | 15.0×       |\n| 正边比例     | 18.8%  | 11.5%   | 0.61×       |\n| 正负边比例   | 1:4.3  | 1:7.7   | 1.8× 不平衡 |\n\n##### 📊 GNN 训练挑战分析\n\n###### 过平滑风险评估\n\n- **CCLE**: ⚠️ 高风险 (平均度 42.86)\n- **GDSC**: 🚨 极高风险 (平均度 256.89)\n\n###### 样本不平衡程度\n\n- **CCLE**: 正负边比例 1:4.3 (中等不平衡)\n- **GDSC**: 正负边比例 1:7.7 (严重不平衡)\n\n###### 邻居相似度分析\n\n```python\n# 邻居重叠度对比\nCCLE_similarity = {\n    \"第一类节点\": 0.9374,  # 高度相似\n    \"第二类节点\": 0.9274   # 高度相似\n}\n\nGDSC_similarity = {\n    \"第一类节点\": 0.7659,  # 中等相似\n    \"第二类节点\": 0.7143   # 中等相似\n}\n```\n\n**结论**: CCLE 结构更均匀但多样性不足，GDSC 结构更复杂但多样性更好\n\n##### 🎯 GNN 架构建议对比\n\n###### 推荐架构优先级\n\n- CCLE 推荐架构\n\n  1. **Bipartite GNN** + Signed GCN\n  2. **简单异构图 GNN** (HetGNN)\n  3. **标准 GCN** + 强正则化\n\n- GDSC 推荐架构\n\n  1. **采样型 GNN** (GraphSAINT, FastGCN) + SGCN\n  2. **大规模异构图 GNN** (HGT, RGCN)\n  3. **图 Transformer** (处理复杂结构)\n\n###### 具体参数建议\n\n| 参数           | CCLE         | GDSC         | 原因                |\n| -------------- | ------------ | ------------ | ------------------- |\n| **网络深度**   | 2-3 层       | 严格 2 层    | GDSC 过平滑风险更高 |\n| **隐藏维度**   | 64-128       | 128-256      | GDSC 需要更大容量   |\n| **Dropout 率** | 0.3-0.5      | 0.5-0.7      | GDSC 需要更强正则化 |\n| **学习率**     | 0.001-0.01   | 0.0001-0.001 | GDSC 需要更保守训练 |\n| **批次大小**   | 32-64 个子图 | 16-32 个子图 | GDSC 内存限制       |\n| **采样策略**   | 可选         | 必须         | GDSC 无法全图训练   |\n\n# 📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\n\n<a href=\"/paper/Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends.pdf\" target=\"_blank\">📄 Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends</a>","slug":"CDR-data-analysis","published":1,"updated":"2025-07-11T03:07:36.876Z","comments":1,"layout":"post","photos":[],"_id":"cmd39u3jd0003qk41hr428qgq","content":"<h1 id=\"CDR-数据源分析\"><a href=\"#CDR-数据源分析\" class=\"headerlink\" title=\"CDR 数据源分析\"></a>CDR 数据源分析</h1><p>本文主要是介绍一下 <strong>深度学习</strong> 在 <em>药物反应预测</em> 中运用到的数据源。<del>但由于本人比较捞</del> 本文主要从 <strong>深度学习</strong> 角度来看待这些数据源，对其在医学方面的意义<del>（主要是鼠鼠也不会捏）</del>不会有太多的描述</p>\n<span id=\"more\"></span>\n\n<h2 id=\"CDR-Cancer-Drug-Response\"><a href=\"#CDR-Cancer-Drug-Response\" class=\"headerlink\" title=\"CDR &#x3D; Cancer Drug Response\"></a>CDR &#x3D; Cancer Drug Response</h2><p>我们的数据源有三种：</p>\n<ul>\n<li><em>Cancer Representations</em>（癌症特征的表示）</li>\n<li><em>Representations of Drug Compounds</em>（药物特征的表示）</li>\n<li><em>Representations of Treatment Response</em>（治疗响应的表示）</li>\n</ul>\n<p>接下来会按顺序进行说明</p>\n<hr>\n<h3 id=\"Cancer-Representations\"><a href=\"#Cancer-Representations\" class=\"headerlink\" title=\"Cancer Representations\"></a>Cancer Representations</h3><p>癌症的特征是多组学的 <del>这不是理所应当吗</del></p>\n<h4 id=\"多组学类型\"><a href=\"#多组学类型\" class=\"headerlink\" title=\"多组学类型\"></a>多组学类型</h4><p>通常基于以下四类组学数据：</p>\n<ul>\n<li><p>基因组（Genomic）</p>\n<ul>\n<li>突变（Mutation）：体细胞突变（如单核苷酸变异 SNVs）可能驱动癌症进展，并影响药物靶点。</li>\n<li>拷贝数变异（CNV）：基因拷贝数的增加或缺失可能影响药物敏感性（如 HER2 扩增与曲妥珠单抗疗效相关）。</li>\n</ul>\n</li>\n<li><p>转录组（Transcriptomic）</p>\n<ul>\n<li>基因表达（Gene Expression）：通过微阵列或 RNA 测序（RNA-Seq）量化基因的 mRNA 水平。例如，高表达的耐药基因可能预示治疗失败。</li>\n</ul>\n</li>\n<li><p>表观组（Epigenomic）</p>\n<ul>\n<li>DNA 甲基化（Methylation）：启动子区域的甲基化可能沉默抑癌基因，影响药物反应。</li>\n</ul>\n</li>\n<li><p>蛋白质组（Proteomic）</p>\n<ul>\n<li>蛋白质表达（RPPA 等）：直接测量蛋白质丰度（如激酶活性），更接近功能表型。</li>\n</ul>\n</li>\n</ul>\n<p>对于同一种组学数据，他们被表示成一组 <strong>维数相同的向量</strong></p>\n<h4 id=\"预处理与整合\"><a href=\"#预处理与整合\" class=\"headerlink\" title=\"预处理与整合\"></a>预处理与整合</h4><ol>\n<li>数据预处理</li>\n</ol>\n<ul>\n<li>包括标准化（normalization）、批次效应校正（batch effect correction）和质量控制（QC）。例如，RNA-Seq 数据需通过 RPKM 或 TPM 标准化。</li>\n</ul>\n<ol start=\"2\">\n<li>多组学整合方法 ：<ul>\n<li>早期整合（Early Integration）：直接拼接不同组学特征为单一向量，但可能因维度灾难（curse of dimensionality）导致过拟合。</li>\n<li>晚期整合（Late Integration）：通过独立子网络处理每组学数据（如 CNN 处理突变，GNN 处理表达数据），再融合特征。例如，MOLI 模型通过三重损失函数整合多组学数据，显著提升跨癌症模型的泛化能力。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"基因特征具有优势及新兴趋势\"><a href=\"#基因特征具有优势及新兴趋势\" class=\"headerlink\" title=\"基因特征具有优势及新兴趋势\"></a>基因特征具有优势及新兴趋势</h4><blockquote>\n<p>2014 年 NCI-DREAM 挑战赛表明， 基因表达数据在预测乳腺癌细胞系药物敏感性时最具预测力（优于突变或 CNV）。因此，约 90%的 DRP 模型使用基因表达（单独或联合其他组学）<br><img src=\"/img/CDR-data-analysis/gene.png\" alt=\"gene\" width=\"50%\"></p>\n</blockquote>\n<h5 id=\"新兴趋势\"><a href=\"#新兴趋势\" class=\"headerlink\" title=\"新兴趋势\"></a>新兴趋势</h5><ol>\n<li><strong>结构生物学整合</strong>：如利用蛋白质-蛋白质相互作用（PPI）网络（STRING 数据库）或通路信息（GSEA）构建生物网络，增强模型可解释性。</li>\n<li><strong>图神经网络（GNN）</strong>：将基因视为节点、相互作用为边，学习拓扑特征（如 GraOmicDRP 模型）。</li>\n</ol>\n<hr>\n<h3 id=\"Representations-of-Drug-Compounds\"><a href=\"#Representations-of-Drug-Compounds\" class=\"headerlink\" title=\"Representations of Drug Compounds\"></a>Representations of Drug Compounds</h3><p>对药物的表示主要分为三种，一般只选取其中的一种 <del>虽然也有选用几种的 <strong>创新</strong> 方式</del>。值得一提的是，在选定药物的表示方式后，之后的特征工程的方式目前来看非常的统一。接下来一一说明每一种表示方式。</p>\n<h4 id=\"SMILES（简化分子输入行条目系统）\"><a href=\"#SMILES（简化分子输入行条目系统）\" class=\"headerlink\" title=\"SMILES（简化分子输入行条目系统）\"></a>SMILES（简化分子输入行条目系统）</h4><ol>\n<li><em>定义</em>：SMILES 是一种<strong>线性字符串</strong>表示法，通过符号编码分子结构（如<code>CCO</code>表示乙醇）。</li>\n<li><em>优势</em>：<ul>\n<li>易于存储和处理，广泛用于化学信息学工具（如 RDKit）。</li>\n<li>可直接用于序列模型（如 RNN、Transformer）或通过预处理转换为其他表示（如图结构）。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"分子指纹（Fingerprints-FPs）和描述符（Descriptors）\"><a href=\"#分子指纹（Fingerprints-FPs）和描述符（Descriptors）\" class=\"headerlink\" title=\"分子指纹（Fingerprints, FPs）和描述符（Descriptors）\"></a>分子指纹（Fingerprints, FPs）和描述符（Descriptors）</h4><ol>\n<li><p>分子指纹</p>\n<ul>\n<li><em>定义</em>：<strong>二进制向量</strong>，表示分子中是否存在特定子结构（如药效团或官能团）。</li>\n<li><em>常用类型</em>：<ul>\n<li><strong>Morgan 指纹（ECFP）</strong>：基于原子邻域的圆形拓扑指纹，长度通常为 512 或 1024 位。</li>\n<li><strong>RDKit 指纹</strong>：开源工具生成的二进制指纹。</li>\n</ul>\n</li>\n<li><em>优势</em>：固定长度，适合传统机器学习模型（如随机森林）。</li>\n</ul>\n</li>\n<li><p>分子描述符</p>\n<ul>\n<li><em>定义</em>：<strong>数值向量</strong>，编码物理化学性质（如分子量、疏水性、极性表面积等）。</li>\n<li><em>工具</em>：PaDEL、Mordred、Dragon 等软件可自动计算数百至数千个描述符。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"图结构表示（Graph-based-Representations）\"><a href=\"#图结构表示（Graph-based-Representations）\" class=\"headerlink\" title=\"图结构表示（Graph-based Representations）\"></a>图结构表示（Graph-based Representations）</h4><ol>\n<li><em>定义</em> ：将分子表示为<strong>图</strong>，其中原子为<strong>节点</strong>，化学键为<strong>边</strong>，节点和边可附加属性（如原子类型、键类型）。</li>\n<li><em>优势</em> ：<ul>\n<li>更自然地表征分子拓扑结构，适合图神经网络（GNN）。</li>\n<li>可捕捉局部和全局分子特征（如官能团相互作用）。</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h3 id=\"Representations-of-Treatment-Response\"><a href=\"#Representations-of-Treatment-Response\" class=\"headerlink\" title=\"Representations of Treatment Response\"></a>Representations of Treatment Response</h3><p>从构造模型的角度出发，这是 DRP 的核心数据源</p>\n<ul>\n<li>它决定了模型最后完成的<strong>任务类型</strong>：训练连续值的<strong>回归任务</strong>和训练离散值的<strong>分类任务</strong></li>\n<li>他的数据质量很大程度上决定了模型的结果的优劣，即对该数据源对模型的好坏影响很大</li>\n</ul>\n<p>此外，很少有从数据分析的角度出发分析这个数据源的文献，于是在这里给出简要的说明</p>\n<h4 id=\"连续值表示（Continuous-Measures）\"><a href=\"#连续值表示（Continuous-Measures）\" class=\"headerlink\" title=\"连续值表示（Continuous Measures）\"></a>连续值表示（Continuous Measures）</h4><ol>\n<li><p><strong>IC50</strong></p>\n<ul>\n<li>半数抑制浓度，即抑制 50%细胞活力所需的药物浓度。</li>\n<li><em>优势</em>：直观反映药物效力，广泛用于回归模型（如预测 IC50 的数值）。</li>\n<li><em>局限性</em>：仅反映单一浓度点的效果，可能忽略剂量-反应曲线的整体形状。</li>\n</ul>\n</li>\n<li><p><strong>AUC&#x2F;AAC</strong></p>\n<ul>\n<li>剂量-反应曲线下面积（Area Under the Curve）或曲线上面积（Activity Area）。</li>\n<li><em>优势</em>：全局度量，综合所有浓度点的效果，对噪声更鲁棒。</li>\n<li><em>应用</em>：如 DeepCDR 等模型使用 AUC 作为回归目标，实证表明其泛化性优于 IC50。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"分类表示（Categorical-Measures）\"><a href=\"#分类表示（Categorical-Measures）\" class=\"headerlink\" title=\"分类表示（Categorical Measures）\"></a>分类表示（Categorical Measures）</h4><ol>\n<li><p><strong>二分类（敏感&#x2F;耐药）</strong></p>\n<ul>\n<li>通过阈值（如瀑布算法、LOBICO）将连续反应（如 IC50）转化为离散标签。</li>\n<li><em>优势</em>：更贴近临床决策需求（如选择敏感药物）。</li>\n<li><em>示例</em>：Sharifi-Noghabi et al. (2021) 使用二分类训练深度神经网络，预测患者肿瘤的敏感性。</li>\n</ul>\n</li>\n<li><p><strong>多分类</strong></p>\n<ul>\n<li>如低&#x2F;中&#x2F;高反应性，适用于更细粒度的临床分级。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"排序表示（Ranking）\"><a href=\"#排序表示（Ranking）\" class=\"headerlink\" title=\"排序表示（Ranking）\"></a>排序表示（Ranking）</h4><ol>\n<li><p><em>目标</em></p>\n<ul>\n<li>为个性化治疗推荐药物排序（如 Top-k 最有效药物）。</li>\n</ul>\n</li>\n<li><p><em>方法</em></p>\n<ul>\n<li>Prasse et al. (2022)：将 IC50 转化为相关性分数，设计可微排序损失函数。</li>\n<li>PPORank：利用强化学习动态优化排序，适应新增数据。</li>\n</ul>\n</li>\n<li><p><em>优势</em></p>\n<ul>\n<li>直接支持临床优先级排序，优于传统回归或分类。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"数据分析\"><a href=\"#数据分析\" class=\"headerlink\" title=\"数据分析\"></a>数据分析</h4><p>由于本人大概率会做个分类模型，所以会将主要分析的是<strong>分类表示</strong>的数据在<strong>图神经网络</strong>中比较重视的几个指标，这里分析 <em>CCLE</em> 和 <em>GDSC</em> 两个数据集在选用主流阈值选取方法之后的表示。</p>\n<p>直接先看结果捏（这里画了两个小图）</p>\n<ul>\n<li>CCLE</li>\n</ul>\n<img src=\"/img/CDR-data-analysis/comprehensive_bipartite_analysis_ccle.png\" alt=\"CCLE\" style=\"max-width: 100%; height: auto;\">\n\n<ul>\n<li>GDSC</li>\n</ul>\n<img src=\"/img/CDR-data-analysis/comprehensive_bipartite_analysis_gdsc.png\" alt=\"GDSC\" style=\"max-width: 100%; height: auto;\">\n\n<p>\n  👉 <a href=\"/code/data_analysis/visualize_graph_analysis.py\" target=\"_blank\">查看用于生成上述图表的本地 Python 脚本：visualize_graph_analysis.py</a>\n</p>\n\n<h5 id=\"🔍-关键数据对比\"><a href=\"#🔍-关键数据对比\" class=\"headerlink\" title=\"🔍 关键数据对比\"></a>🔍 关键数据对比</h5><table>\n<thead>\n<tr>\n<th>特征</th>\n<th>CCLE</th>\n<th>GDSC</th>\n<th>倍数差异</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>数据规模</strong></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>总节点数</td>\n<td>341</td>\n<td>783</td>\n<td>2.3×</td>\n</tr>\n<tr>\n<td>第一类节点</td>\n<td>317</td>\n<td>561</td>\n<td>1.8×</td>\n</tr>\n<tr>\n<td>第二类节点</td>\n<td>24</td>\n<td>222</td>\n<td>9.3×</td>\n</tr>\n<tr>\n<td>总边数</td>\n<td>7,307</td>\n<td>100,572</td>\n<td>13.8×</td>\n</tr>\n<tr>\n<td><strong>图结构</strong></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>密度</td>\n<td>0.9604</td>\n<td>0.8075</td>\n<td>0.84×</td>\n</tr>\n<tr>\n<td>稀疏性</td>\n<td>0.0396</td>\n<td>0.1925</td>\n<td>4.9×</td>\n</tr>\n<tr>\n<td>平均度</td>\n<td>42.86</td>\n<td>256.89</td>\n<td>6.0×</td>\n</tr>\n<tr>\n<td>图直径</td>\n<td>3</td>\n<td>4</td>\n<td>1.3×</td>\n</tr>\n<tr>\n<td><strong>边分布</strong></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>正边数量</td>\n<td>1,375</td>\n<td>11,591</td>\n<td>8.4×</td>\n</tr>\n<tr>\n<td>负边数量</td>\n<td>5,932</td>\n<td>88,981</td>\n<td>15.0×</td>\n</tr>\n<tr>\n<td>正边比例</td>\n<td>18.8%</td>\n<td>11.5%</td>\n<td>0.61×</td>\n</tr>\n<tr>\n<td>正负边比例</td>\n<td>1:4.3</td>\n<td>1:7.7</td>\n<td>1.8× 不平衡</td>\n</tr>\n</tbody></table>\n<h5 id=\"📊-GNN-训练挑战分析\"><a href=\"#📊-GNN-训练挑战分析\" class=\"headerlink\" title=\"📊 GNN 训练挑战分析\"></a>📊 GNN 训练挑战分析</h5><h6 id=\"过平滑风险评估\"><a href=\"#过平滑风险评估\" class=\"headerlink\" title=\"过平滑风险评估\"></a>过平滑风险评估</h6><ul>\n<li><strong>CCLE</strong>: ⚠️ 高风险 (平均度 42.86)</li>\n<li><strong>GDSC</strong>: 🚨 极高风险 (平均度 256.89)</li>\n</ul>\n<h6 id=\"样本不平衡程度\"><a href=\"#样本不平衡程度\" class=\"headerlink\" title=\"样本不平衡程度\"></a>样本不平衡程度</h6><ul>\n<li><strong>CCLE</strong>: 正负边比例 1:4.3 (中等不平衡)</li>\n<li><strong>GDSC</strong>: 正负边比例 1:7.7 (严重不平衡)</li>\n</ul>\n<h6 id=\"邻居相似度分析\"><a href=\"#邻居相似度分析\" class=\"headerlink\" title=\"邻居相似度分析\"></a>邻居相似度分析</h6><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 邻居重叠度对比</span></span><br><span class=\"line\">CCLE_similarity = &#123;</span><br><span class=\"line\">    <span class=\"string\">&quot;第一类节点&quot;</span>: <span class=\"number\">0.9374</span>,  <span class=\"comment\"># 高度相似</span></span><br><span class=\"line\">    <span class=\"string\">&quot;第二类节点&quot;</span>: <span class=\"number\">0.9274</span>   <span class=\"comment\"># 高度相似</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">GDSC_similarity = &#123;</span><br><span class=\"line\">    <span class=\"string\">&quot;第一类节点&quot;</span>: <span class=\"number\">0.7659</span>,  <span class=\"comment\"># 中等相似</span></span><br><span class=\"line\">    <span class=\"string\">&quot;第二类节点&quot;</span>: <span class=\"number\">0.7143</span>   <span class=\"comment\"># 中等相似</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p><strong>结论</strong>: CCLE 结构更均匀但多样性不足，GDSC 结构更复杂但多样性更好</p>\n<h5 id=\"🎯-GNN-架构建议对比\"><a href=\"#🎯-GNN-架构建议对比\" class=\"headerlink\" title=\"🎯 GNN 架构建议对比\"></a>🎯 GNN 架构建议对比</h5><h6 id=\"推荐架构优先级\"><a href=\"#推荐架构优先级\" class=\"headerlink\" title=\"推荐架构优先级\"></a>推荐架构优先级</h6><ul>\n<li><p>CCLE 推荐架构</p>\n<ol>\n<li><strong>Bipartite GNN</strong> + Signed GCN</li>\n<li><strong>简单异构图 GNN</strong> (HetGNN)</li>\n<li><strong>标准 GCN</strong> + 强正则化</li>\n</ol>\n</li>\n<li><p>GDSC 推荐架构</p>\n<ol>\n<li><strong>采样型 GNN</strong> (GraphSAINT, FastGCN) + SGCN</li>\n<li><strong>大规模异构图 GNN</strong> (HGT, RGCN)</li>\n<li><strong>图 Transformer</strong> (处理复杂结构)</li>\n</ol>\n</li>\n</ul>\n<h6 id=\"具体参数建议\"><a href=\"#具体参数建议\" class=\"headerlink\" title=\"具体参数建议\"></a>具体参数建议</h6><table>\n<thead>\n<tr>\n<th>参数</th>\n<th>CCLE</th>\n<th>GDSC</th>\n<th>原因</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>网络深度</strong></td>\n<td>2-3 层</td>\n<td>严格 2 层</td>\n<td>GDSC 过平滑风险更高</td>\n</tr>\n<tr>\n<td><strong>隐藏维度</strong></td>\n<td>64-128</td>\n<td>128-256</td>\n<td>GDSC 需要更大容量</td>\n</tr>\n<tr>\n<td><strong>Dropout 率</strong></td>\n<td>0.3-0.5</td>\n<td>0.5-0.7</td>\n<td>GDSC 需要更强正则化</td>\n</tr>\n<tr>\n<td><strong>学习率</strong></td>\n<td>0.001-0.01</td>\n<td>0.0001-0.001</td>\n<td>GDSC 需要更保守训练</td>\n</tr>\n<tr>\n<td><strong>批次大小</strong></td>\n<td>32-64 个子图</td>\n<td>16-32 个子图</td>\n<td>GDSC 内存限制</td>\n</tr>\n<tr>\n<td><strong>采样策略</strong></td>\n<td>可选</td>\n<td>必须</td>\n<td>GDSC 无法全图训练</td>\n</tr>\n</tbody></table>\n<h1 id=\"📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"><a href=\"#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\" class=\"headerlink\" title=\"📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1><p><a href=\"/paper/Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends.pdf\" target=\"_blank\">📄 Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends</a></p>\n","length":2290,"excerpt":"<h1 id=\"CDR-数据源分析\"><a href=\"#CDR-数据源分析\" class=\"headerlink\" title=\"CDR 数据源分析\"></a>CDR 数据源分析</h1><p>本文主要是介绍一下 <strong>深度学习</strong> 在 <em>药物反应预测</em> 中运用到的数据源。<del>但由于本人比较捞</del> 本文主要从 <strong>深度学习</strong> 角度来看待这些数据源，对其在医学方面的意义<del>（主要是鼠鼠也不会捏）</del>不会有太多的描述</p>","more":"<h2 id=\"CDR-Cancer-Drug-Response\"><a href=\"#CDR-Cancer-Drug-Response\" class=\"headerlink\" title=\"CDR &#x3D; Cancer Drug Response\"></a>CDR &#x3D; Cancer Drug Response</h2><p>我们的数据源有三种：</p>\n<ul>\n<li><em>Cancer Representations</em>（癌症特征的表示）</li>\n<li><em>Representations of Drug Compounds</em>（药物特征的表示）</li>\n<li><em>Representations of Treatment Response</em>（治疗响应的表示）</li>\n</ul>\n<p>接下来会按顺序进行说明</p>\n<hr>\n<h3 id=\"Cancer-Representations\"><a href=\"#Cancer-Representations\" class=\"headerlink\" title=\"Cancer Representations\"></a>Cancer Representations</h3><p>癌症的特征是多组学的 <del>这不是理所应当吗</del></p>\n<h4 id=\"多组学类型\"><a href=\"#多组学类型\" class=\"headerlink\" title=\"多组学类型\"></a>多组学类型</h4><p>通常基于以下四类组学数据：</p>\n<ul>\n<li><p>基因组（Genomic）</p>\n<ul>\n<li>突变（Mutation）：体细胞突变（如单核苷酸变异 SNVs）可能驱动癌症进展，并影响药物靶点。</li>\n<li>拷贝数变异（CNV）：基因拷贝数的增加或缺失可能影响药物敏感性（如 HER2 扩增与曲妥珠单抗疗效相关）。</li>\n</ul>\n</li>\n<li><p>转录组（Transcriptomic）</p>\n<ul>\n<li>基因表达（Gene Expression）：通过微阵列或 RNA 测序（RNA-Seq）量化基因的 mRNA 水平。例如，高表达的耐药基因可能预示治疗失败。</li>\n</ul>\n</li>\n<li><p>表观组（Epigenomic）</p>\n<ul>\n<li>DNA 甲基化（Methylation）：启动子区域的甲基化可能沉默抑癌基因，影响药物反应。</li>\n</ul>\n</li>\n<li><p>蛋白质组（Proteomic）</p>\n<ul>\n<li>蛋白质表达（RPPA 等）：直接测量蛋白质丰度（如激酶活性），更接近功能表型。</li>\n</ul>\n</li>\n</ul>\n<p>对于同一种组学数据，他们被表示成一组 <strong>维数相同的向量</strong></p>\n<h4 id=\"预处理与整合\"><a href=\"#预处理与整合\" class=\"headerlink\" title=\"预处理与整合\"></a>预处理与整合</h4><ol>\n<li>数据预处理</li>\n</ol>\n<ul>\n<li>包括标准化（normalization）、批次效应校正（batch effect correction）和质量控制（QC）。例如，RNA-Seq 数据需通过 RPKM 或 TPM 标准化。</li>\n</ul>\n<ol start=\"2\">\n<li>多组学整合方法 ：<ul>\n<li>早期整合（Early Integration）：直接拼接不同组学特征为单一向量，但可能因维度灾难（curse of dimensionality）导致过拟合。</li>\n<li>晚期整合（Late Integration）：通过独立子网络处理每组学数据（如 CNN 处理突变，GNN 处理表达数据），再融合特征。例如，MOLI 模型通过三重损失函数整合多组学数据，显著提升跨癌症模型的泛化能力。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"基因特征具有优势及新兴趋势\"><a href=\"#基因特征具有优势及新兴趋势\" class=\"headerlink\" title=\"基因特征具有优势及新兴趋势\"></a>基因特征具有优势及新兴趋势</h4><blockquote>\n<p>2014 年 NCI-DREAM 挑战赛表明， 基因表达数据在预测乳腺癌细胞系药物敏感性时最具预测力（优于突变或 CNV）。因此，约 90%的 DRP 模型使用基因表达（单独或联合其他组学）<br><img src=\"/img/CDR-data-analysis/gene.png\" alt=\"gene\" width=\"50%\"></p>\n</blockquote>\n<h5 id=\"新兴趋势\"><a href=\"#新兴趋势\" class=\"headerlink\" title=\"新兴趋势\"></a>新兴趋势</h5><ol>\n<li><strong>结构生物学整合</strong>：如利用蛋白质-蛋白质相互作用（PPI）网络（STRING 数据库）或通路信息（GSEA）构建生物网络，增强模型可解释性。</li>\n<li><strong>图神经网络（GNN）</strong>：将基因视为节点、相互作用为边，学习拓扑特征（如 GraOmicDRP 模型）。</li>\n</ol>\n<hr>\n<h3 id=\"Representations-of-Drug-Compounds\"><a href=\"#Representations-of-Drug-Compounds\" class=\"headerlink\" title=\"Representations of Drug Compounds\"></a>Representations of Drug Compounds</h3><p>对药物的表示主要分为三种，一般只选取其中的一种 <del>虽然也有选用几种的 <strong>创新</strong> 方式</del>。值得一提的是，在选定药物的表示方式后，之后的特征工程的方式目前来看非常的统一。接下来一一说明每一种表示方式。</p>\n<h4 id=\"SMILES（简化分子输入行条目系统）\"><a href=\"#SMILES（简化分子输入行条目系统）\" class=\"headerlink\" title=\"SMILES（简化分子输入行条目系统）\"></a>SMILES（简化分子输入行条目系统）</h4><ol>\n<li><em>定义</em>：SMILES 是一种<strong>线性字符串</strong>表示法，通过符号编码分子结构（如<code>CCO</code>表示乙醇）。</li>\n<li><em>优势</em>：<ul>\n<li>易于存储和处理，广泛用于化学信息学工具（如 RDKit）。</li>\n<li>可直接用于序列模型（如 RNN、Transformer）或通过预处理转换为其他表示（如图结构）。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"分子指纹（Fingerprints-FPs）和描述符（Descriptors）\"><a href=\"#分子指纹（Fingerprints-FPs）和描述符（Descriptors）\" class=\"headerlink\" title=\"分子指纹（Fingerprints, FPs）和描述符（Descriptors）\"></a>分子指纹（Fingerprints, FPs）和描述符（Descriptors）</h4><ol>\n<li><p>分子指纹</p>\n<ul>\n<li><em>定义</em>：<strong>二进制向量</strong>，表示分子中是否存在特定子结构（如药效团或官能团）。</li>\n<li><em>常用类型</em>：<ul>\n<li><strong>Morgan 指纹（ECFP）</strong>：基于原子邻域的圆形拓扑指纹，长度通常为 512 或 1024 位。</li>\n<li><strong>RDKit 指纹</strong>：开源工具生成的二进制指纹。</li>\n</ul>\n</li>\n<li><em>优势</em>：固定长度，适合传统机器学习模型（如随机森林）。</li>\n</ul>\n</li>\n<li><p>分子描述符</p>\n<ul>\n<li><em>定义</em>：<strong>数值向量</strong>，编码物理化学性质（如分子量、疏水性、极性表面积等）。</li>\n<li><em>工具</em>：PaDEL、Mordred、Dragon 等软件可自动计算数百至数千个描述符。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"图结构表示（Graph-based-Representations）\"><a href=\"#图结构表示（Graph-based-Representations）\" class=\"headerlink\" title=\"图结构表示（Graph-based Representations）\"></a>图结构表示（Graph-based Representations）</h4><ol>\n<li><em>定义</em> ：将分子表示为<strong>图</strong>，其中原子为<strong>节点</strong>，化学键为<strong>边</strong>，节点和边可附加属性（如原子类型、键类型）。</li>\n<li><em>优势</em> ：<ul>\n<li>更自然地表征分子拓扑结构，适合图神经网络（GNN）。</li>\n<li>可捕捉局部和全局分子特征（如官能团相互作用）。</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h3 id=\"Representations-of-Treatment-Response\"><a href=\"#Representations-of-Treatment-Response\" class=\"headerlink\" title=\"Representations of Treatment Response\"></a>Representations of Treatment Response</h3><p>从构造模型的角度出发，这是 DRP 的核心数据源</p>\n<ul>\n<li>它决定了模型最后完成的<strong>任务类型</strong>：训练连续值的<strong>回归任务</strong>和训练离散值的<strong>分类任务</strong></li>\n<li>他的数据质量很大程度上决定了模型的结果的优劣，即对该数据源对模型的好坏影响很大</li>\n</ul>\n<p>此外，很少有从数据分析的角度出发分析这个数据源的文献，于是在这里给出简要的说明</p>\n<h4 id=\"连续值表示（Continuous-Measures）\"><a href=\"#连续值表示（Continuous-Measures）\" class=\"headerlink\" title=\"连续值表示（Continuous Measures）\"></a>连续值表示（Continuous Measures）</h4><ol>\n<li><p><strong>IC50</strong></p>\n<ul>\n<li>半数抑制浓度，即抑制 50%细胞活力所需的药物浓度。</li>\n<li><em>优势</em>：直观反映药物效力，广泛用于回归模型（如预测 IC50 的数值）。</li>\n<li><em>局限性</em>：仅反映单一浓度点的效果，可能忽略剂量-反应曲线的整体形状。</li>\n</ul>\n</li>\n<li><p><strong>AUC&#x2F;AAC</strong></p>\n<ul>\n<li>剂量-反应曲线下面积（Area Under the Curve）或曲线上面积（Activity Area）。</li>\n<li><em>优势</em>：全局度量，综合所有浓度点的效果，对噪声更鲁棒。</li>\n<li><em>应用</em>：如 DeepCDR 等模型使用 AUC 作为回归目标，实证表明其泛化性优于 IC50。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"分类表示（Categorical-Measures）\"><a href=\"#分类表示（Categorical-Measures）\" class=\"headerlink\" title=\"分类表示（Categorical Measures）\"></a>分类表示（Categorical Measures）</h4><ol>\n<li><p><strong>二分类（敏感&#x2F;耐药）</strong></p>\n<ul>\n<li>通过阈值（如瀑布算法、LOBICO）将连续反应（如 IC50）转化为离散标签。</li>\n<li><em>优势</em>：更贴近临床决策需求（如选择敏感药物）。</li>\n<li><em>示例</em>：Sharifi-Noghabi et al. (2021) 使用二分类训练深度神经网络，预测患者肿瘤的敏感性。</li>\n</ul>\n</li>\n<li><p><strong>多分类</strong></p>\n<ul>\n<li>如低&#x2F;中&#x2F;高反应性，适用于更细粒度的临床分级。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"排序表示（Ranking）\"><a href=\"#排序表示（Ranking）\" class=\"headerlink\" title=\"排序表示（Ranking）\"></a>排序表示（Ranking）</h4><ol>\n<li><p><em>目标</em></p>\n<ul>\n<li>为个性化治疗推荐药物排序（如 Top-k 最有效药物）。</li>\n</ul>\n</li>\n<li><p><em>方法</em></p>\n<ul>\n<li>Prasse et al. (2022)：将 IC50 转化为相关性分数，设计可微排序损失函数。</li>\n<li>PPORank：利用强化学习动态优化排序，适应新增数据。</li>\n</ul>\n</li>\n<li><p><em>优势</em></p>\n<ul>\n<li>直接支持临床优先级排序，优于传统回归或分类。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"数据分析\"><a href=\"#数据分析\" class=\"headerlink\" title=\"数据分析\"></a>数据分析</h4><p>由于本人大概率会做个分类模型，所以会将主要分析的是<strong>分类表示</strong>的数据在<strong>图神经网络</strong>中比较重视的几个指标，这里分析 <em>CCLE</em> 和 <em>GDSC</em> 两个数据集在选用主流阈值选取方法之后的表示。</p>\n<p>直接先看结果捏（这里画了两个小图）</p>\n<ul>\n<li>CCLE</li>\n</ul>\n<img src=\"/img/CDR-data-analysis/comprehensive_bipartite_analysis_ccle.png\" alt=\"CCLE\" style=\"max-width: 100%; height: auto;\">\n\n<ul>\n<li>GDSC</li>\n</ul>\n<img src=\"/img/CDR-data-analysis/comprehensive_bipartite_analysis_gdsc.png\" alt=\"GDSC\" style=\"max-width: 100%; height: auto;\">\n\n<p>\n  👉 <a href=\"/code/data_analysis/visualize_graph_analysis.py\" target=\"_blank\">查看用于生成上述图表的本地 Python 脚本：visualize_graph_analysis.py</a>\n</p>\n\n<h5 id=\"🔍-关键数据对比\"><a href=\"#🔍-关键数据对比\" class=\"headerlink\" title=\"🔍 关键数据对比\"></a>🔍 关键数据对比</h5><table>\n<thead>\n<tr>\n<th>特征</th>\n<th>CCLE</th>\n<th>GDSC</th>\n<th>倍数差异</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>数据规模</strong></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>总节点数</td>\n<td>341</td>\n<td>783</td>\n<td>2.3×</td>\n</tr>\n<tr>\n<td>第一类节点</td>\n<td>317</td>\n<td>561</td>\n<td>1.8×</td>\n</tr>\n<tr>\n<td>第二类节点</td>\n<td>24</td>\n<td>222</td>\n<td>9.3×</td>\n</tr>\n<tr>\n<td>总边数</td>\n<td>7,307</td>\n<td>100,572</td>\n<td>13.8×</td>\n</tr>\n<tr>\n<td><strong>图结构</strong></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>密度</td>\n<td>0.9604</td>\n<td>0.8075</td>\n<td>0.84×</td>\n</tr>\n<tr>\n<td>稀疏性</td>\n<td>0.0396</td>\n<td>0.1925</td>\n<td>4.9×</td>\n</tr>\n<tr>\n<td>平均度</td>\n<td>42.86</td>\n<td>256.89</td>\n<td>6.0×</td>\n</tr>\n<tr>\n<td>图直径</td>\n<td>3</td>\n<td>4</td>\n<td>1.3×</td>\n</tr>\n<tr>\n<td><strong>边分布</strong></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>正边数量</td>\n<td>1,375</td>\n<td>11,591</td>\n<td>8.4×</td>\n</tr>\n<tr>\n<td>负边数量</td>\n<td>5,932</td>\n<td>88,981</td>\n<td>15.0×</td>\n</tr>\n<tr>\n<td>正边比例</td>\n<td>18.8%</td>\n<td>11.5%</td>\n<td>0.61×</td>\n</tr>\n<tr>\n<td>正负边比例</td>\n<td>1:4.3</td>\n<td>1:7.7</td>\n<td>1.8× 不平衡</td>\n</tr>\n</tbody></table>\n<h5 id=\"📊-GNN-训练挑战分析\"><a href=\"#📊-GNN-训练挑战分析\" class=\"headerlink\" title=\"📊 GNN 训练挑战分析\"></a>📊 GNN 训练挑战分析</h5><h6 id=\"过平滑风险评估\"><a href=\"#过平滑风险评估\" class=\"headerlink\" title=\"过平滑风险评估\"></a>过平滑风险评估</h6><ul>\n<li><strong>CCLE</strong>: ⚠️ 高风险 (平均度 42.86)</li>\n<li><strong>GDSC</strong>: 🚨 极高风险 (平均度 256.89)</li>\n</ul>\n<h6 id=\"样本不平衡程度\"><a href=\"#样本不平衡程度\" class=\"headerlink\" title=\"样本不平衡程度\"></a>样本不平衡程度</h6><ul>\n<li><strong>CCLE</strong>: 正负边比例 1:4.3 (中等不平衡)</li>\n<li><strong>GDSC</strong>: 正负边比例 1:7.7 (严重不平衡)</li>\n</ul>\n<h6 id=\"邻居相似度分析\"><a href=\"#邻居相似度分析\" class=\"headerlink\" title=\"邻居相似度分析\"></a>邻居相似度分析</h6><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 邻居重叠度对比</span></span><br><span class=\"line\">CCLE_similarity = &#123;</span><br><span class=\"line\">    <span class=\"string\">&quot;第一类节点&quot;</span>: <span class=\"number\">0.9374</span>,  <span class=\"comment\"># 高度相似</span></span><br><span class=\"line\">    <span class=\"string\">&quot;第二类节点&quot;</span>: <span class=\"number\">0.9274</span>   <span class=\"comment\"># 高度相似</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">GDSC_similarity = &#123;</span><br><span class=\"line\">    <span class=\"string\">&quot;第一类节点&quot;</span>: <span class=\"number\">0.7659</span>,  <span class=\"comment\"># 中等相似</span></span><br><span class=\"line\">    <span class=\"string\">&quot;第二类节点&quot;</span>: <span class=\"number\">0.7143</span>   <span class=\"comment\"># 中等相似</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p><strong>结论</strong>: CCLE 结构更均匀但多样性不足，GDSC 结构更复杂但多样性更好</p>\n<h5 id=\"🎯-GNN-架构建议对比\"><a href=\"#🎯-GNN-架构建议对比\" class=\"headerlink\" title=\"🎯 GNN 架构建议对比\"></a>🎯 GNN 架构建议对比</h5><h6 id=\"推荐架构优先级\"><a href=\"#推荐架构优先级\" class=\"headerlink\" title=\"推荐架构优先级\"></a>推荐架构优先级</h6><ul>\n<li><p>CCLE 推荐架构</p>\n<ol>\n<li><strong>Bipartite GNN</strong> + Signed GCN</li>\n<li><strong>简单异构图 GNN</strong> (HetGNN)</li>\n<li><strong>标准 GCN</strong> + 强正则化</li>\n</ol>\n</li>\n<li><p>GDSC 推荐架构</p>\n<ol>\n<li><strong>采样型 GNN</strong> (GraphSAINT, FastGCN) + SGCN</li>\n<li><strong>大规模异构图 GNN</strong> (HGT, RGCN)</li>\n<li><strong>图 Transformer</strong> (处理复杂结构)</li>\n</ol>\n</li>\n</ul>\n<h6 id=\"具体参数建议\"><a href=\"#具体参数建议\" class=\"headerlink\" title=\"具体参数建议\"></a>具体参数建议</h6><table>\n<thead>\n<tr>\n<th>参数</th>\n<th>CCLE</th>\n<th>GDSC</th>\n<th>原因</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>网络深度</strong></td>\n<td>2-3 层</td>\n<td>严格 2 层</td>\n<td>GDSC 过平滑风险更高</td>\n</tr>\n<tr>\n<td><strong>隐藏维度</strong></td>\n<td>64-128</td>\n<td>128-256</td>\n<td>GDSC 需要更大容量</td>\n</tr>\n<tr>\n<td><strong>Dropout 率</strong></td>\n<td>0.3-0.5</td>\n<td>0.5-0.7</td>\n<td>GDSC 需要更强正则化</td>\n</tr>\n<tr>\n<td><strong>学习率</strong></td>\n<td>0.001-0.01</td>\n<td>0.0001-0.001</td>\n<td>GDSC 需要更保守训练</td>\n</tr>\n<tr>\n<td><strong>批次大小</strong></td>\n<td>32-64 个子图</td>\n<td>16-32 个子图</td>\n<td>GDSC 内存限制</td>\n</tr>\n<tr>\n<td><strong>采样策略</strong></td>\n<td>可选</td>\n<td>必须</td>\n<td>GDSC 无法全图训练</td>\n</tr>\n</tbody></table>\n<h1 id=\"📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"><a href=\"#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\" class=\"headerlink\" title=\"📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1><p><a href=\"/paper/Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends.pdf\" target=\"_blank\">📄 Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends</a></p>"},{"title":"Feature-Related Attention","date":"2025-07-14T14:30:57.000Z","_content":"\n# 在输入特征上做文章的Attention\n\n本文将接着详细说明一种基于输入特征分类Attention的方式，并介绍在这种分类方式下关注到的不同的Attention的架构。\n\n具体来说，本文主要探讨了基于输入特征特性的注意力机制变体。本节根据输入特征的不同特性，将特征相关的注意力机制分为三类：特征多重性(Multiplicity of Features)、特征层级(Levels of Features)和特征表示(Feature Representations)。\n\n在阅读这篇博客前请先阅读 {% post_link Attention %}\n\n<!-- more -->\n\n## 特征多重性(Multiplicity of Features)\n\n这部分讨论了如何处理多个输入源的特征，主要分为单一特征注意力和多特征注意力机制。\n\n### 单一特征注意力(Singular Features Attention)\n\n大多数任务模型只处理单一输入(如图像、句子或声音序列)，使用单一特征注意力机制。这种机制直接对单个输入的特征向量进行注意力计算。\n\n### 多特征注意力机制\n\n当模型需要同时处理多个输入源时，需要特殊的多特征注意力机制：\n\n**协同注意力(Co-attention)**\n   - 分为 **粗科粒度(Coarse-grained)** 和 **细颗粒度(Fine-grained)** 两种\n   - **粗颗粒度协同**注意力使用一个输入的*紧凑表示*作为查询来关注另一个输入\n   - **细颗粒度协同**注意力使用一个输入的所有特征向量作为查询\n\n#### 粗颗粒度协同\n\n论文给出的粗颗粒度协同的实例是**alternating co-attention**\n\n##### alternating co-attention\n\n<img src=\"/img/Attention/AlternatingCo-Attention.png\" alt=\"alternating co-attention\" width=\"60%\" height=\"auto\">\n\n如上图所示，这是alternating co-attention的架构图，该机制交替使用两个输入的特征矩阵，先计算第一个输入的注意力，将其上下文向量作为查询计算第二个输入的注意力，然后再用第二个输入的上下文向量重新计算第一个输入的注意力。\n\n这里现给出他的score函数\n\n对于有序列输入的Attention：\n\n$$\n\\mathrm{score}(\\underset{d\\_{q}\\times1}{\\boldsymbol{q}},\\underset{d\\_{k}\\times1}{\\boldsymbol{k}\\_{l}})=\\underset{1\\times d\\_{w}}{\\boldsymbol{w}^{T}}\\times\\mathrm{act}(\\underset{d\\_{w}\\times d\\_{q}}{\\boldsymbol{W}\\_{1}}\\times\\underset{d\\_{q}\\times1}{\\boldsymbol{q}}+\\underset{d\\_{w}\\times d\\_{k}}{\\boldsymbol{W}\\_{2}}\\times\\underset{d\\_{k}\\times1}{\\boldsymbol{k}\\_{l}}+\\underset{d\\_{w}\\times1}{\\boldsymbol{b}})\n$$\n\n对于无序列输入的Attention ~~（这是一种自注意力机制，后面会提到）~~ ：\n\n$$\n\\underset{1\\times1}{e\\_{l}^{(0)}}=\\underset{1\\times d\\_{w}}{\\boldsymbol{w}^{(1)T}}\\times\\operatorname{act}(\\underset{d\\_{w}\\times d\\_{k}^{(1)}}{\\boldsymbol{W}^{(1)}}\\times\\underset{d\\_{k}^{(1)}\\times1}{\\boldsymbol{k}\\_{l}^{(1)}}+\\underset{d\\_{w}\\times1}{\\boldsymbol{b}^{(1)}})\n$$\n\n对于第二层Attention：\n\n$$\n\\underset{1\\times1}{e\\_{l}^{(2)}}=\\mathrm{score}(\\underset{d\\_{v}^{(1)}\\times 1}{\\boldsymbol{c}^{(0)}},\\underset{d\\_{k}^{(2)}\\times1}{\\boldsymbol{k}\\_{l}^{(2)}})\n$$\n\n对于第三层Attention：\n\n$$\n\\underset{1\\times1}{e\\_{l}^{(1)}}=\\mathrm{score}(\\underset{d\\_{v}^{(2)}\\times 1}{\\boldsymbol{c}^{(2)}},\\underset{d\\_{k}^{(1)}\\times1}{\\boldsymbol{k}\\_{l}^{(1)}})\n$$\n\n生成的上下文向量$\\boldsymbol{c}^{(1)}$和$\\boldsymbol{c}^{(2)}$被连接起来，并在输出模型中用于预测。交替协同注意力由于需要一个接一个地计算上下文向量，因此本质上包含了*一种顺序性*。这可能会带来计算上的劣势，因为*无法并行*化。 \n\n##### interactive co-attention\n\n   - 并行计算两个输入的注意力\n   - 使用未加权平均的关键向量作为查询\n   - 计算效率更高，可以并行处理\n\n\n\n##### 并行协同注意力(Parallel Co-attention)\n   - 同时计算两个输入的注意力\n   - 使用亲和矩阵(Affinity Matrix)转换关键向量空间\n   - 通过聚合形式计算注意力分数\n\n\n\n##### 旋转注意力(Rotatory Attention)\n   - 主要用于情感分析任务\n   - 处理三个输入：目标短语、左上下文和右上下文\n   - 通过注意力机制迭代改进表示\n\n## 特征层级(Levels of Features)\n\n这部分讨论了如何处理具有层级结构的特征，主要分为单层级注意力和多层级注意力机制。\n\n### 单层级注意力(Single-Level Attention)\n\n传统注意力机制通常在单一层级上处理特征，如只关注单词级别或句子级别。\n\n### 多层级注意力机制\n\n1. **注意力叠加(Attention-via-Attention)**\n   - 同时处理字符级和词级特征\n   - 先计算词级注意力，用其上下文向量辅助计算字符级注意力\n   - 最终拼接两个层级的上下文向量\n\n\n\n2. **层级注意力(Hierarchical Attention)**\n   - 从最低层级开始，逐步构建高层级表示\n   - 常用于文档分类：词→句→文档\n   - 每个层级通过注意力机制生成摘要表示\n\n\n\n## 特征表示(Feature Representations)\n\n这部分讨论了特征表示方式的注意力机制变体，主要分为单一表示注意力和多表示注意力。\n\n### 单一表示注意力(Single-Representational Attention)\n\n传统方法使用单一嵌入或表示模型生成特征表示。\n\n### 多表示注意力(Multi-Representational Attention)\n\n1. **元嵌入(Meta-embeddings)**\n   - 整合多个嵌入表示\n   - 通过注意力机制加权平均不同表示\n   - 生成更高质量的特征表示\n\n2. **自注意力机制**\n   - 学习特征向量之间的关系\n   - 通过注意力改进特征表示\n   - 常用于Transformer架构中\n\n## 应用领域\n\n3.1节讨论的特征相关注意力机制在多个领域有广泛应用：\n- 医学数据分析(多特征协同注意力)\n- 推荐系统(多层级注意力)\n- 情感分析(旋转注意力)\n- 文档分类(层级注意力)\n- 多语言处理(多表示注意力)\n\n## 总结\n\n3.1节系统性地分类了基于输入特征特性的注意力机制变体，为研究者提供了清晰的框架来选择适合特定任务和数据类型的最佳注意力机制。这些机制通过充分利用输入特征的多重性、层级结构和表示多样性，显著提升了模型在各种任务上的表现。\n\n\n\n图3展示了完整的注意力机制分类体系，其中3.1节讨论的特征相关注意力机制是该体系的重要组成部分。\n\n$$\n\\underset{n\\_{f}^{(1)}\\times n\\_{f}^{(2)}}{A}=\\operatorname{act}(\\underset{n\\_{f}^{(1)}\\times d\\_{k}^{(1)}}{\\begin{array}{c}K^{(1)^{T}}\\end{array}}\\times\\underset{d\\_{k}^{(1)}\\times d\\_{k}^{(2)}}{\\begin{array}{c}W\\_{A}\\end{array}}\\times\\underset{d\\_{k}^{(2)}\\times n\\_{f}^{(2)}}{\\begin{array}{c}K^{(2)}\\end{array}})\n$$\n\n# 📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\n\n<a href=\"/paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf\" target=\"_blank\">📄 Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a>\n\n{% post_link Attention %}","source":"_posts/Feature-Related-Attention.md","raw":"---\ntitle: Feature-Related Attention\ndate: 2025-07-14 22:30:57\ncategories:\n  - CDR\n  - model\n  - attention\n  - feature related\ntags:\n  - CDR\n  - model\n  - Basic\n  - deep learning\n  - 还没写完捏\n---\n\n# 在输入特征上做文章的Attention\n\n本文将接着详细说明一种基于输入特征分类Attention的方式，并介绍在这种分类方式下关注到的不同的Attention的架构。\n\n具体来说，本文主要探讨了基于输入特征特性的注意力机制变体。本节根据输入特征的不同特性，将特征相关的注意力机制分为三类：特征多重性(Multiplicity of Features)、特征层级(Levels of Features)和特征表示(Feature Representations)。\n\n在阅读这篇博客前请先阅读 {% post_link Attention %}\n\n<!-- more -->\n\n## 特征多重性(Multiplicity of Features)\n\n这部分讨论了如何处理多个输入源的特征，主要分为单一特征注意力和多特征注意力机制。\n\n### 单一特征注意力(Singular Features Attention)\n\n大多数任务模型只处理单一输入(如图像、句子或声音序列)，使用单一特征注意力机制。这种机制直接对单个输入的特征向量进行注意力计算。\n\n### 多特征注意力机制\n\n当模型需要同时处理多个输入源时，需要特殊的多特征注意力机制：\n\n**协同注意力(Co-attention)**\n   - 分为 **粗科粒度(Coarse-grained)** 和 **细颗粒度(Fine-grained)** 两种\n   - **粗颗粒度协同**注意力使用一个输入的*紧凑表示*作为查询来关注另一个输入\n   - **细颗粒度协同**注意力使用一个输入的所有特征向量作为查询\n\n#### 粗颗粒度协同\n\n论文给出的粗颗粒度协同的实例是**alternating co-attention**\n\n##### alternating co-attention\n\n<img src=\"/img/Attention/AlternatingCo-Attention.png\" alt=\"alternating co-attention\" width=\"60%\" height=\"auto\">\n\n如上图所示，这是alternating co-attention的架构图，该机制交替使用两个输入的特征矩阵，先计算第一个输入的注意力，将其上下文向量作为查询计算第二个输入的注意力，然后再用第二个输入的上下文向量重新计算第一个输入的注意力。\n\n这里现给出他的score函数\n\n对于有序列输入的Attention：\n\n$$\n\\mathrm{score}(\\underset{d\\_{q}\\times1}{\\boldsymbol{q}},\\underset{d\\_{k}\\times1}{\\boldsymbol{k}\\_{l}})=\\underset{1\\times d\\_{w}}{\\boldsymbol{w}^{T}}\\times\\mathrm{act}(\\underset{d\\_{w}\\times d\\_{q}}{\\boldsymbol{W}\\_{1}}\\times\\underset{d\\_{q}\\times1}{\\boldsymbol{q}}+\\underset{d\\_{w}\\times d\\_{k}}{\\boldsymbol{W}\\_{2}}\\times\\underset{d\\_{k}\\times1}{\\boldsymbol{k}\\_{l}}+\\underset{d\\_{w}\\times1}{\\boldsymbol{b}})\n$$\n\n对于无序列输入的Attention ~~（这是一种自注意力机制，后面会提到）~~ ：\n\n$$\n\\underset{1\\times1}{e\\_{l}^{(0)}}=\\underset{1\\times d\\_{w}}{\\boldsymbol{w}^{(1)T}}\\times\\operatorname{act}(\\underset{d\\_{w}\\times d\\_{k}^{(1)}}{\\boldsymbol{W}^{(1)}}\\times\\underset{d\\_{k}^{(1)}\\times1}{\\boldsymbol{k}\\_{l}^{(1)}}+\\underset{d\\_{w}\\times1}{\\boldsymbol{b}^{(1)}})\n$$\n\n对于第二层Attention：\n\n$$\n\\underset{1\\times1}{e\\_{l}^{(2)}}=\\mathrm{score}(\\underset{d\\_{v}^{(1)}\\times 1}{\\boldsymbol{c}^{(0)}},\\underset{d\\_{k}^{(2)}\\times1}{\\boldsymbol{k}\\_{l}^{(2)}})\n$$\n\n对于第三层Attention：\n\n$$\n\\underset{1\\times1}{e\\_{l}^{(1)}}=\\mathrm{score}(\\underset{d\\_{v}^{(2)}\\times 1}{\\boldsymbol{c}^{(2)}},\\underset{d\\_{k}^{(1)}\\times1}{\\boldsymbol{k}\\_{l}^{(1)}})\n$$\n\n生成的上下文向量$\\boldsymbol{c}^{(1)}$和$\\boldsymbol{c}^{(2)}$被连接起来，并在输出模型中用于预测。交替协同注意力由于需要一个接一个地计算上下文向量，因此本质上包含了*一种顺序性*。这可能会带来计算上的劣势，因为*无法并行*化。 \n\n##### interactive co-attention\n\n   - 并行计算两个输入的注意力\n   - 使用未加权平均的关键向量作为查询\n   - 计算效率更高，可以并行处理\n\n\n\n##### 并行协同注意力(Parallel Co-attention)\n   - 同时计算两个输入的注意力\n   - 使用亲和矩阵(Affinity Matrix)转换关键向量空间\n   - 通过聚合形式计算注意力分数\n\n\n\n##### 旋转注意力(Rotatory Attention)\n   - 主要用于情感分析任务\n   - 处理三个输入：目标短语、左上下文和右上下文\n   - 通过注意力机制迭代改进表示\n\n## 特征层级(Levels of Features)\n\n这部分讨论了如何处理具有层级结构的特征，主要分为单层级注意力和多层级注意力机制。\n\n### 单层级注意力(Single-Level Attention)\n\n传统注意力机制通常在单一层级上处理特征，如只关注单词级别或句子级别。\n\n### 多层级注意力机制\n\n1. **注意力叠加(Attention-via-Attention)**\n   - 同时处理字符级和词级特征\n   - 先计算词级注意力，用其上下文向量辅助计算字符级注意力\n   - 最终拼接两个层级的上下文向量\n\n\n\n2. **层级注意力(Hierarchical Attention)**\n   - 从最低层级开始，逐步构建高层级表示\n   - 常用于文档分类：词→句→文档\n   - 每个层级通过注意力机制生成摘要表示\n\n\n\n## 特征表示(Feature Representations)\n\n这部分讨论了特征表示方式的注意力机制变体，主要分为单一表示注意力和多表示注意力。\n\n### 单一表示注意力(Single-Representational Attention)\n\n传统方法使用单一嵌入或表示模型生成特征表示。\n\n### 多表示注意力(Multi-Representational Attention)\n\n1. **元嵌入(Meta-embeddings)**\n   - 整合多个嵌入表示\n   - 通过注意力机制加权平均不同表示\n   - 生成更高质量的特征表示\n\n2. **自注意力机制**\n   - 学习特征向量之间的关系\n   - 通过注意力改进特征表示\n   - 常用于Transformer架构中\n\n## 应用领域\n\n3.1节讨论的特征相关注意力机制在多个领域有广泛应用：\n- 医学数据分析(多特征协同注意力)\n- 推荐系统(多层级注意力)\n- 情感分析(旋转注意力)\n- 文档分类(层级注意力)\n- 多语言处理(多表示注意力)\n\n## 总结\n\n3.1节系统性地分类了基于输入特征特性的注意力机制变体，为研究者提供了清晰的框架来选择适合特定任务和数据类型的最佳注意力机制。这些机制通过充分利用输入特征的多重性、层级结构和表示多样性，显著提升了模型在各种任务上的表现。\n\n\n\n图3展示了完整的注意力机制分类体系，其中3.1节讨论的特征相关注意力机制是该体系的重要组成部分。\n\n$$\n\\underset{n\\_{f}^{(1)}\\times n\\_{f}^{(2)}}{A}=\\operatorname{act}(\\underset{n\\_{f}^{(1)}\\times d\\_{k}^{(1)}}{\\begin{array}{c}K^{(1)^{T}}\\end{array}}\\times\\underset{d\\_{k}^{(1)}\\times d\\_{k}^{(2)}}{\\begin{array}{c}W\\_{A}\\end{array}}\\times\\underset{d\\_{k}^{(2)}\\times n\\_{f}^{(2)}}{\\begin{array}{c}K^{(2)}\\end{array}})\n$$\n\n# 📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\n\n<a href=\"/paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf\" target=\"_blank\">📄 Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a>\n\n{% post_link Attention %}","slug":"Feature-Related-Attention","published":1,"updated":"2025-07-14T15:40:28.077Z","comments":1,"layout":"post","photos":[],"_id":"cmd39u3jg0007qk410x0u0c1w","content":"<h1 id=\"在输入特征上做文章的Attention\"><a href=\"#在输入特征上做文章的Attention\" class=\"headerlink\" title=\"在输入特征上做文章的Attention\"></a>在输入特征上做文章的Attention</h1><p>本文将接着详细说明一种基于输入特征分类Attention的方式，并介绍在这种分类方式下关注到的不同的Attention的架构。</p>\n<p>具体来说，本文主要探讨了基于输入特征特性的注意力机制变体。本节根据输入特征的不同特性，将特征相关的注意力机制分为三类：特征多重性(Multiplicity of Features)、特征层级(Levels of Features)和特征表示(Feature Representations)。</p>\n<p>在阅读这篇博客前请先阅读 <a href=\"/2025/07/10/Attention/\" title=\"Attention Overview\">Attention Overview</a></p>\n<span id=\"more\"></span>\n\n<h2 id=\"特征多重性-Multiplicity-of-Features\"><a href=\"#特征多重性-Multiplicity-of-Features\" class=\"headerlink\" title=\"特征多重性(Multiplicity of Features)\"></a>特征多重性(Multiplicity of Features)</h2><p>这部分讨论了如何处理多个输入源的特征，主要分为单一特征注意力和多特征注意力机制。</p>\n<h3 id=\"单一特征注意力-Singular-Features-Attention\"><a href=\"#单一特征注意力-Singular-Features-Attention\" class=\"headerlink\" title=\"单一特征注意力(Singular Features Attention)\"></a>单一特征注意力(Singular Features Attention)</h3><p>大多数任务模型只处理单一输入(如图像、句子或声音序列)，使用单一特征注意力机制。这种机制直接对单个输入的特征向量进行注意力计算。</p>\n<h3 id=\"多特征注意力机制\"><a href=\"#多特征注意力机制\" class=\"headerlink\" title=\"多特征注意力机制\"></a>多特征注意力机制</h3><p>当模型需要同时处理多个输入源时，需要特殊的多特征注意力机制：</p>\n<p><strong>协同注意力(Co-attention)</strong></p>\n<ul>\n<li>分为 <strong>粗科粒度(Coarse-grained)</strong> 和 <strong>细颗粒度(Fine-grained)</strong> 两种</li>\n<li><strong>粗颗粒度协同</strong>注意力使用一个输入的<em>紧凑表示</em>作为查询来关注另一个输入</li>\n<li><strong>细颗粒度协同</strong>注意力使用一个输入的所有特征向量作为查询</li>\n</ul>\n<h4 id=\"粗颗粒度协同\"><a href=\"#粗颗粒度协同\" class=\"headerlink\" title=\"粗颗粒度协同\"></a>粗颗粒度协同</h4><p>论文给出的粗颗粒度协同的实例是<strong>alternating co-attention</strong></p>\n<h5 id=\"alternating-co-attention\"><a href=\"#alternating-co-attention\" class=\"headerlink\" title=\"alternating co-attention\"></a>alternating co-attention</h5><img src=\"/img/Attention/AlternatingCo-Attention.png\" alt=\"alternating co-attention\" width=\"60%\" height=\"auto\">\n\n<p>如上图所示，这是alternating co-attention的架构图，该机制交替使用两个输入的特征矩阵，先计算第一个输入的注意力，将其上下文向量作为查询计算第二个输入的注意力，然后再用第二个输入的上下文向量重新计算第一个输入的注意力。</p>\n<p>这里现给出他的score函数</p>\n<p>对于有序列输入的Attention：</p>\n<p>$$<br>\\mathrm{score}(\\underset{d_{q}\\times1}{\\boldsymbol{q}},\\underset{d_{k}\\times1}{\\boldsymbol{k}_{l}})&#x3D;\\underset{1\\times d_{w}}{\\boldsymbol{w}^{T}}\\times\\mathrm{act}(\\underset{d_{w}\\times d_{q}}{\\boldsymbol{W}_{1}}\\times\\underset{d_{q}\\times1}{\\boldsymbol{q}}+\\underset{d_{w}\\times d_{k}}{\\boldsymbol{W}_{2}}\\times\\underset{d_{k}\\times1}{\\boldsymbol{k}_{l}}+\\underset{d_{w}\\times1}{\\boldsymbol{b}})<br>$$</p>\n<p>对于无序列输入的Attention <del>（这是一种自注意力机制，后面会提到）</del> ：</p>\n<p>$$<br>\\underset{1\\times1}{e_{l}^{(0)}}&#x3D;\\underset{1\\times d_{w}}{\\boldsymbol{w}^{(1)T}}\\times\\operatorname{act}(\\underset{d_{w}\\times d_{k}^{(1)}}{\\boldsymbol{W}^{(1)}}\\times\\underset{d_{k}^{(1)}\\times1}{\\boldsymbol{k}_{l}^{(1)}}+\\underset{d_{w}\\times1}{\\boldsymbol{b}^{(1)}})<br>$$</p>\n<p>对于第二层Attention：</p>\n<p>$$<br>\\underset{1\\times1}{e_{l}^{(2)}}&#x3D;\\mathrm{score}(\\underset{d_{v}^{(1)}\\times 1}{\\boldsymbol{c}^{(0)}},\\underset{d_{k}^{(2)}\\times1}{\\boldsymbol{k}_{l}^{(2)}})<br>$$</p>\n<p>对于第三层Attention：</p>\n<p>$$<br>\\underset{1\\times1}{e_{l}^{(1)}}&#x3D;\\mathrm{score}(\\underset{d_{v}^{(2)}\\times 1}{\\boldsymbol{c}^{(2)}},\\underset{d_{k}^{(1)}\\times1}{\\boldsymbol{k}_{l}^{(1)}})<br>$$</p>\n<p>生成的上下文向量$\\boldsymbol{c}^{(1)}$和$\\boldsymbol{c}^{(2)}$被连接起来，并在输出模型中用于预测。交替协同注意力由于需要一个接一个地计算上下文向量，因此本质上包含了<em>一种顺序性</em>。这可能会带来计算上的劣势，因为<em>无法并行</em>化。 </p>\n<h5 id=\"interactive-co-attention\"><a href=\"#interactive-co-attention\" class=\"headerlink\" title=\"interactive co-attention\"></a>interactive co-attention</h5><ul>\n<li>并行计算两个输入的注意力</li>\n<li>使用未加权平均的关键向量作为查询</li>\n<li>计算效率更高，可以并行处理</li>\n</ul>\n<h5 id=\"并行协同注意力-Parallel-Co-attention\"><a href=\"#并行协同注意力-Parallel-Co-attention\" class=\"headerlink\" title=\"并行协同注意力(Parallel Co-attention)\"></a>并行协同注意力(Parallel Co-attention)</h5><ul>\n<li>同时计算两个输入的注意力</li>\n<li>使用亲和矩阵(Affinity Matrix)转换关键向量空间</li>\n<li>通过聚合形式计算注意力分数</li>\n</ul>\n<h5 id=\"旋转注意力-Rotatory-Attention\"><a href=\"#旋转注意力-Rotatory-Attention\" class=\"headerlink\" title=\"旋转注意力(Rotatory Attention)\"></a>旋转注意力(Rotatory Attention)</h5><ul>\n<li>主要用于情感分析任务</li>\n<li>处理三个输入：目标短语、左上下文和右上下文</li>\n<li>通过注意力机制迭代改进表示</li>\n</ul>\n<h2 id=\"特征层级-Levels-of-Features\"><a href=\"#特征层级-Levels-of-Features\" class=\"headerlink\" title=\"特征层级(Levels of Features)\"></a>特征层级(Levels of Features)</h2><p>这部分讨论了如何处理具有层级结构的特征，主要分为单层级注意力和多层级注意力机制。</p>\n<h3 id=\"单层级注意力-Single-Level-Attention\"><a href=\"#单层级注意力-Single-Level-Attention\" class=\"headerlink\" title=\"单层级注意力(Single-Level Attention)\"></a>单层级注意力(Single-Level Attention)</h3><p>传统注意力机制通常在单一层级上处理特征，如只关注单词级别或句子级别。</p>\n<h3 id=\"多层级注意力机制\"><a href=\"#多层级注意力机制\" class=\"headerlink\" title=\"多层级注意力机制\"></a>多层级注意力机制</h3><ol>\n<li><p><strong>注意力叠加(Attention-via-Attention)</strong></p>\n<ul>\n<li>同时处理字符级和词级特征</li>\n<li>先计算词级注意力，用其上下文向量辅助计算字符级注意力</li>\n<li>最终拼接两个层级的上下文向量</li>\n</ul>\n</li>\n<li><p><strong>层级注意力(Hierarchical Attention)</strong></p>\n<ul>\n<li>从最低层级开始，逐步构建高层级表示</li>\n<li>常用于文档分类：词→句→文档</li>\n<li>每个层级通过注意力机制生成摘要表示</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"特征表示-Feature-Representations\"><a href=\"#特征表示-Feature-Representations\" class=\"headerlink\" title=\"特征表示(Feature Representations)\"></a>特征表示(Feature Representations)</h2><p>这部分讨论了特征表示方式的注意力机制变体，主要分为单一表示注意力和多表示注意力。</p>\n<h3 id=\"单一表示注意力-Single-Representational-Attention\"><a href=\"#单一表示注意力-Single-Representational-Attention\" class=\"headerlink\" title=\"单一表示注意力(Single-Representational Attention)\"></a>单一表示注意力(Single-Representational Attention)</h3><p>传统方法使用单一嵌入或表示模型生成特征表示。</p>\n<h3 id=\"多表示注意力-Multi-Representational-Attention\"><a href=\"#多表示注意力-Multi-Representational-Attention\" class=\"headerlink\" title=\"多表示注意力(Multi-Representational Attention)\"></a>多表示注意力(Multi-Representational Attention)</h3><ol>\n<li><p><strong>元嵌入(Meta-embeddings)</strong></p>\n<ul>\n<li>整合多个嵌入表示</li>\n<li>通过注意力机制加权平均不同表示</li>\n<li>生成更高质量的特征表示</li>\n</ul>\n</li>\n<li><p><strong>自注意力机制</strong></p>\n<ul>\n<li>学习特征向量之间的关系</li>\n<li>通过注意力改进特征表示</li>\n<li>常用于Transformer架构中</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"应用领域\"><a href=\"#应用领域\" class=\"headerlink\" title=\"应用领域\"></a>应用领域</h2><p>3.1节讨论的特征相关注意力机制在多个领域有广泛应用：</p>\n<ul>\n<li>医学数据分析(多特征协同注意力)</li>\n<li>推荐系统(多层级注意力)</li>\n<li>情感分析(旋转注意力)</li>\n<li>文档分类(层级注意力)</li>\n<li>多语言处理(多表示注意力)</li>\n</ul>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>3.1节系统性地分类了基于输入特征特性的注意力机制变体，为研究者提供了清晰的框架来选择适合特定任务和数据类型的最佳注意力机制。这些机制通过充分利用输入特征的多重性、层级结构和表示多样性，显著提升了模型在各种任务上的表现。</p>\n<p>图3展示了完整的注意力机制分类体系，其中3.1节讨论的特征相关注意力机制是该体系的重要组成部分。</p>\n<p>$$<br>\\underset{n_{f}^{(1)}\\times n_{f}^{(2)}}{A}&#x3D;\\operatorname{act}(\\underset{n_{f}^{(1)}\\times d_{k}^{(1)}}{\\begin{array}{c}K^{(1)^{T}}\\end{array}}\\times\\underset{d_{k}^{(1)}\\times d_{k}^{(2)}}{\\begin{array}{c}W_{A}\\end{array}}\\times\\underset{d_{k}^{(2)}\\times n_{f}^{(2)}}{\\begin{array}{c}K^{(2)}\\end{array}})<br>$$</p>\n<h1 id=\"📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"><a href=\"#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\" class=\"headerlink\" title=\"📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1><p><a href=\"/paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf\" target=\"_blank\">📄 Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a></p>\n<a href=\"/2025/07/10/Attention/\" title=\"Attention Overview\">Attention Overview</a>","length":1650,"excerpt":"<h1 id=\"在输入特征上做文章的Attention\"><a href=\"#在输入特征上做文章的Attention\" class=\"headerlink\" title=\"在输入特征上做文章的Attention\"></a>在输入特征上做文章的Attention</h1><p>本文将接着详细说明一种基于输入特征分类Attention的方式，并介绍在这种分类方式下关注到的不同的Attention的架构。</p>\n<p>具体来说，本文主要探讨了基于输入特征特性的注意力机制变体。本节根据输入特征的不同特性，将特征相关的注意力机制分为三类：特征多重性(Multiplicity of Features)、特征层级(Levels of Features)和特征表示(Feature Representations)。</p>\n<p>在阅读这篇博客前请先阅读 <a href=\"/2025/07/10/Attention/\" title=\"Attention Overview\">Attention Overview</a></p>","more":"<h2 id=\"特征多重性-Multiplicity-of-Features\"><a href=\"#特征多重性-Multiplicity-of-Features\" class=\"headerlink\" title=\"特征多重性(Multiplicity of Features)\"></a>特征多重性(Multiplicity of Features)</h2><p>这部分讨论了如何处理多个输入源的特征，主要分为单一特征注意力和多特征注意力机制。</p>\n<h3 id=\"单一特征注意力-Singular-Features-Attention\"><a href=\"#单一特征注意力-Singular-Features-Attention\" class=\"headerlink\" title=\"单一特征注意力(Singular Features Attention)\"></a>单一特征注意力(Singular Features Attention)</h3><p>大多数任务模型只处理单一输入(如图像、句子或声音序列)，使用单一特征注意力机制。这种机制直接对单个输入的特征向量进行注意力计算。</p>\n<h3 id=\"多特征注意力机制\"><a href=\"#多特征注意力机制\" class=\"headerlink\" title=\"多特征注意力机制\"></a>多特征注意力机制</h3><p>当模型需要同时处理多个输入源时，需要特殊的多特征注意力机制：</p>\n<p><strong>协同注意力(Co-attention)</strong></p>\n<ul>\n<li>分为 <strong>粗科粒度(Coarse-grained)</strong> 和 <strong>细颗粒度(Fine-grained)</strong> 两种</li>\n<li><strong>粗颗粒度协同</strong>注意力使用一个输入的<em>紧凑表示</em>作为查询来关注另一个输入</li>\n<li><strong>细颗粒度协同</strong>注意力使用一个输入的所有特征向量作为查询</li>\n</ul>\n<h4 id=\"粗颗粒度协同\"><a href=\"#粗颗粒度协同\" class=\"headerlink\" title=\"粗颗粒度协同\"></a>粗颗粒度协同</h4><p>论文给出的粗颗粒度协同的实例是<strong>alternating co-attention</strong></p>\n<h5 id=\"alternating-co-attention\"><a href=\"#alternating-co-attention\" class=\"headerlink\" title=\"alternating co-attention\"></a>alternating co-attention</h5><img src=\"/img/Attention/AlternatingCo-Attention.png\" alt=\"alternating co-attention\" width=\"60%\" height=\"auto\">\n\n<p>如上图所示，这是alternating co-attention的架构图，该机制交替使用两个输入的特征矩阵，先计算第一个输入的注意力，将其上下文向量作为查询计算第二个输入的注意力，然后再用第二个输入的上下文向量重新计算第一个输入的注意力。</p>\n<p>这里现给出他的score函数</p>\n<p>对于有序列输入的Attention：</p>\n<p>$$<br>\\mathrm{score}(\\underset{d_{q}\\times1}{\\boldsymbol{q}},\\underset{d_{k}\\times1}{\\boldsymbol{k}_{l}})&#x3D;\\underset{1\\times d_{w}}{\\boldsymbol{w}^{T}}\\times\\mathrm{act}(\\underset{d_{w}\\times d_{q}}{\\boldsymbol{W}_{1}}\\times\\underset{d_{q}\\times1}{\\boldsymbol{q}}+\\underset{d_{w}\\times d_{k}}{\\boldsymbol{W}_{2}}\\times\\underset{d_{k}\\times1}{\\boldsymbol{k}_{l}}+\\underset{d_{w}\\times1}{\\boldsymbol{b}})<br>$$</p>\n<p>对于无序列输入的Attention <del>（这是一种自注意力机制，后面会提到）</del> ：</p>\n<p>$$<br>\\underset{1\\times1}{e_{l}^{(0)}}&#x3D;\\underset{1\\times d_{w}}{\\boldsymbol{w}^{(1)T}}\\times\\operatorname{act}(\\underset{d_{w}\\times d_{k}^{(1)}}{\\boldsymbol{W}^{(1)}}\\times\\underset{d_{k}^{(1)}\\times1}{\\boldsymbol{k}_{l}^{(1)}}+\\underset{d_{w}\\times1}{\\boldsymbol{b}^{(1)}})<br>$$</p>\n<p>对于第二层Attention：</p>\n<p>$$<br>\\underset{1\\times1}{e_{l}^{(2)}}&#x3D;\\mathrm{score}(\\underset{d_{v}^{(1)}\\times 1}{\\boldsymbol{c}^{(0)}},\\underset{d_{k}^{(2)}\\times1}{\\boldsymbol{k}_{l}^{(2)}})<br>$$</p>\n<p>对于第三层Attention：</p>\n<p>$$<br>\\underset{1\\times1}{e_{l}^{(1)}}&#x3D;\\mathrm{score}(\\underset{d_{v}^{(2)}\\times 1}{\\boldsymbol{c}^{(2)}},\\underset{d_{k}^{(1)}\\times1}{\\boldsymbol{k}_{l}^{(1)}})<br>$$</p>\n<p>生成的上下文向量$\\boldsymbol{c}^{(1)}$和$\\boldsymbol{c}^{(2)}$被连接起来，并在输出模型中用于预测。交替协同注意力由于需要一个接一个地计算上下文向量，因此本质上包含了<em>一种顺序性</em>。这可能会带来计算上的劣势，因为<em>无法并行</em>化。 </p>\n<h5 id=\"interactive-co-attention\"><a href=\"#interactive-co-attention\" class=\"headerlink\" title=\"interactive co-attention\"></a>interactive co-attention</h5><ul>\n<li>并行计算两个输入的注意力</li>\n<li>使用未加权平均的关键向量作为查询</li>\n<li>计算效率更高，可以并行处理</li>\n</ul>\n<h5 id=\"并行协同注意力-Parallel-Co-attention\"><a href=\"#并行协同注意力-Parallel-Co-attention\" class=\"headerlink\" title=\"并行协同注意力(Parallel Co-attention)\"></a>并行协同注意力(Parallel Co-attention)</h5><ul>\n<li>同时计算两个输入的注意力</li>\n<li>使用亲和矩阵(Affinity Matrix)转换关键向量空间</li>\n<li>通过聚合形式计算注意力分数</li>\n</ul>\n<h5 id=\"旋转注意力-Rotatory-Attention\"><a href=\"#旋转注意力-Rotatory-Attention\" class=\"headerlink\" title=\"旋转注意力(Rotatory Attention)\"></a>旋转注意力(Rotatory Attention)</h5><ul>\n<li>主要用于情感分析任务</li>\n<li>处理三个输入：目标短语、左上下文和右上下文</li>\n<li>通过注意力机制迭代改进表示</li>\n</ul>\n<h2 id=\"特征层级-Levels-of-Features\"><a href=\"#特征层级-Levels-of-Features\" class=\"headerlink\" title=\"特征层级(Levels of Features)\"></a>特征层级(Levels of Features)</h2><p>这部分讨论了如何处理具有层级结构的特征，主要分为单层级注意力和多层级注意力机制。</p>\n<h3 id=\"单层级注意力-Single-Level-Attention\"><a href=\"#单层级注意力-Single-Level-Attention\" class=\"headerlink\" title=\"单层级注意力(Single-Level Attention)\"></a>单层级注意力(Single-Level Attention)</h3><p>传统注意力机制通常在单一层级上处理特征，如只关注单词级别或句子级别。</p>\n<h3 id=\"多层级注意力机制\"><a href=\"#多层级注意力机制\" class=\"headerlink\" title=\"多层级注意力机制\"></a>多层级注意力机制</h3><ol>\n<li><p><strong>注意力叠加(Attention-via-Attention)</strong></p>\n<ul>\n<li>同时处理字符级和词级特征</li>\n<li>先计算词级注意力，用其上下文向量辅助计算字符级注意力</li>\n<li>最终拼接两个层级的上下文向量</li>\n</ul>\n</li>\n<li><p><strong>层级注意力(Hierarchical Attention)</strong></p>\n<ul>\n<li>从最低层级开始，逐步构建高层级表示</li>\n<li>常用于文档分类：词→句→文档</li>\n<li>每个层级通过注意力机制生成摘要表示</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"特征表示-Feature-Representations\"><a href=\"#特征表示-Feature-Representations\" class=\"headerlink\" title=\"特征表示(Feature Representations)\"></a>特征表示(Feature Representations)</h2><p>这部分讨论了特征表示方式的注意力机制变体，主要分为单一表示注意力和多表示注意力。</p>\n<h3 id=\"单一表示注意力-Single-Representational-Attention\"><a href=\"#单一表示注意力-Single-Representational-Attention\" class=\"headerlink\" title=\"单一表示注意力(Single-Representational Attention)\"></a>单一表示注意力(Single-Representational Attention)</h3><p>传统方法使用单一嵌入或表示模型生成特征表示。</p>\n<h3 id=\"多表示注意力-Multi-Representational-Attention\"><a href=\"#多表示注意力-Multi-Representational-Attention\" class=\"headerlink\" title=\"多表示注意力(Multi-Representational Attention)\"></a>多表示注意力(Multi-Representational Attention)</h3><ol>\n<li><p><strong>元嵌入(Meta-embeddings)</strong></p>\n<ul>\n<li>整合多个嵌入表示</li>\n<li>通过注意力机制加权平均不同表示</li>\n<li>生成更高质量的特征表示</li>\n</ul>\n</li>\n<li><p><strong>自注意力机制</strong></p>\n<ul>\n<li>学习特征向量之间的关系</li>\n<li>通过注意力改进特征表示</li>\n<li>常用于Transformer架构中</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"应用领域\"><a href=\"#应用领域\" class=\"headerlink\" title=\"应用领域\"></a>应用领域</h2><p>3.1节讨论的特征相关注意力机制在多个领域有广泛应用：</p>\n<ul>\n<li>医学数据分析(多特征协同注意力)</li>\n<li>推荐系统(多层级注意力)</li>\n<li>情感分析(旋转注意力)</li>\n<li>文档分类(层级注意力)</li>\n<li>多语言处理(多表示注意力)</li>\n</ul>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>3.1节系统性地分类了基于输入特征特性的注意力机制变体，为研究者提供了清晰的框架来选择适合特定任务和数据类型的最佳注意力机制。这些机制通过充分利用输入特征的多重性、层级结构和表示多样性，显著提升了模型在各种任务上的表现。</p>\n<p>图3展示了完整的注意力机制分类体系，其中3.1节讨论的特征相关注意力机制是该体系的重要组成部分。</p>\n<p>$$<br>\\underset{n_{f}^{(1)}\\times n_{f}^{(2)}}{A}&#x3D;\\operatorname{act}(\\underset{n_{f}^{(1)}\\times d_{k}^{(1)}}{\\begin{array}{c}K^{(1)^{T}}\\end{array}}\\times\\underset{d_{k}^{(1)}\\times d_{k}^{(2)}}{\\begin{array}{c}W_{A}\\end{array}}\\times\\underset{d_{k}^{(2)}\\times n_{f}^{(2)}}{\\begin{array}{c}K^{(2)}\\end{array}})<br>$$</p>\n<h1 id=\"📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"><a href=\"#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\" class=\"headerlink\" title=\"📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1><p><a href=\"/paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf\" target=\"_blank\">📄 Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a></p>\n<a href=\"/2025/07/10/Attention/\" title=\"Attention Overview\">Attention Overview</a>"},{"title":"GAT","date":"2025-07-14T14:26:58.000Z","_content":"\n# \n\n<!-- more -->\n\n### 图注意力机制\n\n#### GAT（Graph Attention Networks）\n\nGAT通过引入注意力机制来加权邻居节点的特征。对于节点i，其更新公式为：\n\n$$h\\_i^{(l+1)} = \\sigma(\\sum\\_{j \\in \\mathcal{N}\\_i} \\alpha\\_{ij}W^{(l)}h\\_j^{(l)})$$\n\n其中注意力系数$\\alpha_{ij}$的计算：\n\n$$\\alpha_{ij} = \\frac{exp(LeakyReLU(a^T[Wh_i || Wh_j]))}{\\sum_{k \\in \\mathcal{N}_i} exp(LeakyReLU(a^T[Wh_i || Wh_k]))}$$\n\n#### 多头注意力\n\n为了提高模型的稳定性和表达能力，GAT使用了多头注意力机制：\n\n$$h\\_i^{(l+1)} = \\sigma(\\frac{1}{K} \\sum\\_{k=1}^K \\sum\\_{j \\in \\mathcal{N}\\_i} \\alpha\\_{ij}^k W^k h\\_j^{(l)})$$\n\n### 变体与扩展\n\n#### 边注意力\n\n除了节点之间的注意力，一些模型还引入了边注意力机制：\n\n$$e_{ij} = a^T[Wh_i || Wh_j || We_{ij}]$$\n\n其中$e_{ij}$是边的特征。\n\n#### 全局注意力\n\n通过引入全局节点或池化操作，可以实现全局注意力：\n\n$$g = \\sum_{i \\in V} \\beta_i h_i$$\n\n其中$\\beta_i$是全局注意力权重。\n\n## 💻 实现细节\n\n### PyTorch实现的GAT层\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass GATLayer(nn.Module):\n    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n        super(GATLayer, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.dropout = dropout\n        self.alpha = alpha\n        self.concat = concat\n\n        # 变换矩阵\n        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n        \n        # 注意力向量\n        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n\n        self.leakyrelu = nn.LeakyReLU(self.alpha)\n\n    def forward(self, x, adj):\n        # x: 节点特征矩阵 [N, in_features]\n        # adj: 邻接矩阵 [N, N]\n        \n        # 线性变换\n        h = torch.mm(x, self.W)  # [N, out_features]\n        N = h.size()[0]\n\n        # 计算注意力分数\n        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1)\n        a_input = a_input.view(N, N, 2 * self.out_features)\n        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n\n        # 掩码机制\n        zero_vec = -9e15 * torch.ones_like(e)\n        attention = torch.where(adj > 0, e, zero_vec)\n        attention = F.softmax(attention, dim=1)\n        attention = F.dropout(attention, self.dropout, training=self.training)\n\n        # 聚合特征\n        h_prime = torch.matmul(attention, h)\n\n        if self.concat:\n            return F.elu(h_prime)\n        else:\n            return h_prime\n\nclass GAT(nn.Module):\n    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n        super(GAT, self).__init__()\n        self.dropout = dropout\n        \n        # 多头注意力层\n        self.attentions = nn.ModuleList([\n            GATLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) \n            for _ in range(nheads)\n        ])\n        \n        # 输出层\n        self.out_att = GATLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n\n    def forward(self, x, adj):\n        x = F.dropout(x, self.dropout, training=self.training)\n        # 多头注意力\n        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = self.out_att(x, adj)\n        return F.log_softmax(x, dim=1)\n```\n\n### 实际应用示例\n\n```python\n# 模型初始化\nmodel = GAT(nfeat=input_dim,\n           nhid=8,\n           nclass=num_classes,\n           dropout=0.6,\n           alpha=0.2,\n           nheads=8)\n\n# 优化器\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n\n# 训练循环\ndef train():\n    model.train()\n    optimizer.zero_grad()\n    output = model(features, adj)\n    loss = F.nll_loss(output[idx_train], labels[idx_train])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n```\n\n## 🔍 注意事项与最佳实践\n\n\n## 📈 未来展望\n\n# 📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\n\n<a href=\"/paper/Lee 等 - 2018 - Attention Models in Graphs A Survey.pdf\" target=\"_blank\">📄 Lee 等 - 2018 - Attention Models in Graphs A Survey</a>\n<a href=\"https://github.com/xmu-xiaoma666/External-Attention-pytorch\" target=\"_blank\">github: External-Attention-pytorch</a> ","source":"_posts/GAT.md","raw":"---\ntitle: GAT\ndate: 2025-07-14 22:26:58\ncategories:\ntags:\n---\n\n# \n\n<!-- more -->\n\n### 图注意力机制\n\n#### GAT（Graph Attention Networks）\n\nGAT通过引入注意力机制来加权邻居节点的特征。对于节点i，其更新公式为：\n\n$$h\\_i^{(l+1)} = \\sigma(\\sum\\_{j \\in \\mathcal{N}\\_i} \\alpha\\_{ij}W^{(l)}h\\_j^{(l)})$$\n\n其中注意力系数$\\alpha_{ij}$的计算：\n\n$$\\alpha_{ij} = \\frac{exp(LeakyReLU(a^T[Wh_i || Wh_j]))}{\\sum_{k \\in \\mathcal{N}_i} exp(LeakyReLU(a^T[Wh_i || Wh_k]))}$$\n\n#### 多头注意力\n\n为了提高模型的稳定性和表达能力，GAT使用了多头注意力机制：\n\n$$h\\_i^{(l+1)} = \\sigma(\\frac{1}{K} \\sum\\_{k=1}^K \\sum\\_{j \\in \\mathcal{N}\\_i} \\alpha\\_{ij}^k W^k h\\_j^{(l)})$$\n\n### 变体与扩展\n\n#### 边注意力\n\n除了节点之间的注意力，一些模型还引入了边注意力机制：\n\n$$e_{ij} = a^T[Wh_i || Wh_j || We_{ij}]$$\n\n其中$e_{ij}$是边的特征。\n\n#### 全局注意力\n\n通过引入全局节点或池化操作，可以实现全局注意力：\n\n$$g = \\sum_{i \\in V} \\beta_i h_i$$\n\n其中$\\beta_i$是全局注意力权重。\n\n## 💻 实现细节\n\n### PyTorch实现的GAT层\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass GATLayer(nn.Module):\n    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n        super(GATLayer, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.dropout = dropout\n        self.alpha = alpha\n        self.concat = concat\n\n        # 变换矩阵\n        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n        \n        # 注意力向量\n        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n\n        self.leakyrelu = nn.LeakyReLU(self.alpha)\n\n    def forward(self, x, adj):\n        # x: 节点特征矩阵 [N, in_features]\n        # adj: 邻接矩阵 [N, N]\n        \n        # 线性变换\n        h = torch.mm(x, self.W)  # [N, out_features]\n        N = h.size()[0]\n\n        # 计算注意力分数\n        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1)\n        a_input = a_input.view(N, N, 2 * self.out_features)\n        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n\n        # 掩码机制\n        zero_vec = -9e15 * torch.ones_like(e)\n        attention = torch.where(adj > 0, e, zero_vec)\n        attention = F.softmax(attention, dim=1)\n        attention = F.dropout(attention, self.dropout, training=self.training)\n\n        # 聚合特征\n        h_prime = torch.matmul(attention, h)\n\n        if self.concat:\n            return F.elu(h_prime)\n        else:\n            return h_prime\n\nclass GAT(nn.Module):\n    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n        super(GAT, self).__init__()\n        self.dropout = dropout\n        \n        # 多头注意力层\n        self.attentions = nn.ModuleList([\n            GATLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) \n            for _ in range(nheads)\n        ])\n        \n        # 输出层\n        self.out_att = GATLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n\n    def forward(self, x, adj):\n        x = F.dropout(x, self.dropout, training=self.training)\n        # 多头注意力\n        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = self.out_att(x, adj)\n        return F.log_softmax(x, dim=1)\n```\n\n### 实际应用示例\n\n```python\n# 模型初始化\nmodel = GAT(nfeat=input_dim,\n           nhid=8,\n           nclass=num_classes,\n           dropout=0.6,\n           alpha=0.2,\n           nheads=8)\n\n# 优化器\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n\n# 训练循环\ndef train():\n    model.train()\n    optimizer.zero_grad()\n    output = model(features, adj)\n    loss = F.nll_loss(output[idx_train], labels[idx_train])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n```\n\n## 🔍 注意事项与最佳实践\n\n\n## 📈 未来展望\n\n# 📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\n\n<a href=\"/paper/Lee 等 - 2018 - Attention Models in Graphs A Survey.pdf\" target=\"_blank\">📄 Lee 等 - 2018 - Attention Models in Graphs A Survey</a>\n<a href=\"https://github.com/xmu-xiaoma666/External-Attention-pytorch\" target=\"_blank\">github: External-Attention-pytorch</a> ","slug":"GAT","published":1,"updated":"2025-07-14T14:27:44.356Z","comments":1,"layout":"post","photos":[],"_id":"cmd39u3ji0009qk418slt6ytx","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><span id=\"more\"></span>\n\n<h3 id=\"图注意力机制\"><a href=\"#图注意力机制\" class=\"headerlink\" title=\"图注意力机制\"></a>图注意力机制</h3><h4 id=\"GAT（Graph-Attention-Networks）\"><a href=\"#GAT（Graph-Attention-Networks）\" class=\"headerlink\" title=\"GAT（Graph Attention Networks）\"></a>GAT（Graph Attention Networks）</h4><p>GAT通过引入注意力机制来加权邻居节点的特征。对于节点i，其更新公式为：</p>\n<p>$$h_i^{(l+1)} &#x3D; \\sigma(\\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij}W^{(l)}h_j^{(l)})$$</p>\n<p>其中注意力系数$\\alpha_{ij}$的计算：</p>\n<p>$$\\alpha_{ij} &#x3D; \\frac{exp(LeakyReLU(a^T[Wh_i || Wh_j]))}{\\sum_{k \\in \\mathcal{N}_i} exp(LeakyReLU(a^T[Wh_i || Wh_k]))}$$</p>\n<h4 id=\"多头注意力\"><a href=\"#多头注意力\" class=\"headerlink\" title=\"多头注意力\"></a>多头注意力</h4><p>为了提高模型的稳定性和表达能力，GAT使用了多头注意力机制：</p>\n<p>$$h_i^{(l+1)} &#x3D; \\sigma(\\frac{1}{K} \\sum_{k&#x3D;1}^K \\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij}^k W^k h_j^{(l)})$$</p>\n<h3 id=\"变体与扩展\"><a href=\"#变体与扩展\" class=\"headerlink\" title=\"变体与扩展\"></a>变体与扩展</h3><h4 id=\"边注意力\"><a href=\"#边注意力\" class=\"headerlink\" title=\"边注意力\"></a>边注意力</h4><p>除了节点之间的注意力，一些模型还引入了边注意力机制：</p>\n<p>$$e_{ij} &#x3D; a^T[Wh_i || Wh_j || We_{ij}]$$</p>\n<p>其中$e_{ij}$是边的特征。</p>\n<h4 id=\"全局注意力\"><a href=\"#全局注意力\" class=\"headerlink\" title=\"全局注意力\"></a>全局注意力</h4><p>通过引入全局节点或池化操作，可以实现全局注意力：</p>\n<p>$$g &#x3D; \\sum_{i \\in V} \\beta_i h_i$$</p>\n<p>其中$\\beta_i$是全局注意力权重。</p>\n<h2 id=\"💻-实现细节\"><a href=\"#💻-实现细节\" class=\"headerlink\" title=\"💻 实现细节\"></a>💻 实现细节</h2><h3 id=\"PyTorch实现的GAT层\"><a href=\"#PyTorch实现的GAT层\" class=\"headerlink\" title=\"PyTorch实现的GAT层\"></a>PyTorch实现的GAT层</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">GATLayer</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, in_features, out_features, dropout, alpha, concat=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(GATLayer, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.in_features = in_features</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.out_features = out_features</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = dropout</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.alpha = alpha</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.concat = concat</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 变换矩阵</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))</span><br><span class=\"line\">        nn.init.xavier_uniform_(<span class=\"variable language_\">self</span>.W.data, gain=<span class=\"number\">1.414</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 注意力向量</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.a = nn.Parameter(torch.zeros(size=(<span class=\"number\">2</span>*out_features, <span class=\"number\">1</span>)))</span><br><span class=\"line\">        nn.init.xavier_uniform_(<span class=\"variable language_\">self</span>.a.data, gain=<span class=\"number\">1.414</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.leakyrelu = nn.LeakyReLU(<span class=\"variable language_\">self</span>.alpha)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, adj</span>):</span><br><span class=\"line\">        <span class=\"comment\"># x: 节点特征矩阵 [N, in_features]</span></span><br><span class=\"line\">        <span class=\"comment\"># adj: 邻接矩阵 [N, N]</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 线性变换</span></span><br><span class=\"line\">        h = torch.mm(x, <span class=\"variable language_\">self</span>.W)  <span class=\"comment\"># [N, out_features]</span></span><br><span class=\"line\">        N = h.size()[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 计算注意力分数</span></span><br><span class=\"line\">        a_input = torch.cat([h.repeat(<span class=\"number\">1</span>, N).view(N * N, -<span class=\"number\">1</span>), h.repeat(N, <span class=\"number\">1</span>)], dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        a_input = a_input.view(N, N, <span class=\"number\">2</span> * <span class=\"variable language_\">self</span>.out_features)</span><br><span class=\"line\">        e = <span class=\"variable language_\">self</span>.leakyrelu(torch.matmul(a_input, <span class=\"variable language_\">self</span>.a).squeeze(<span class=\"number\">2</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 掩码机制</span></span><br><span class=\"line\">        zero_vec = -<span class=\"number\">9e15</span> * torch.ones_like(e)</span><br><span class=\"line\">        attention = torch.where(adj &gt; <span class=\"number\">0</span>, e, zero_vec)</span><br><span class=\"line\">        attention = F.softmax(attention, dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        attention = F.dropout(attention, <span class=\"variable language_\">self</span>.dropout, training=<span class=\"variable language_\">self</span>.training)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 聚合特征</span></span><br><span class=\"line\">        h_prime = torch.matmul(attention, h)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"variable language_\">self</span>.concat:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> F.elu(h_prime)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> h_prime</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">GAT</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, nfeat, nhid, nclass, dropout, alpha, nheads</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(GAT, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = dropout</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 多头注意力层</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.attentions = nn.ModuleList([</span><br><span class=\"line\">            GATLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=<span class=\"literal\">True</span>) </span><br><span class=\"line\">            <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(nheads)</span><br><span class=\"line\">        ])</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 输出层</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.out_att = GATLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, adj</span>):</span><br><span class=\"line\">        x = F.dropout(x, <span class=\"variable language_\">self</span>.dropout, training=<span class=\"variable language_\">self</span>.training)</span><br><span class=\"line\">        <span class=\"comment\"># 多头注意力</span></span><br><span class=\"line\">        x = torch.cat([att(x, adj) <span class=\"keyword\">for</span> att <span class=\"keyword\">in</span> <span class=\"variable language_\">self</span>.attentions], dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        x = F.dropout(x, <span class=\"variable language_\">self</span>.dropout, training=<span class=\"variable language_\">self</span>.training)</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.out_att(x, adj)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.log_softmax(x, dim=<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"实际应用示例\"><a href=\"#实际应用示例\" class=\"headerlink\" title=\"实际应用示例\"></a>实际应用示例</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 模型初始化</span></span><br><span class=\"line\">model = GAT(nfeat=input_dim,</span><br><span class=\"line\">           nhid=<span class=\"number\">8</span>,</span><br><span class=\"line\">           nclass=num_classes,</span><br><span class=\"line\">           dropout=<span class=\"number\">0.6</span>,</span><br><span class=\"line\">           alpha=<span class=\"number\">0.2</span>,</span><br><span class=\"line\">           nheads=<span class=\"number\">8</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 优化器</span></span><br><span class=\"line\">optimizer = torch.optim.Adam(model.parameters(), lr=<span class=\"number\">0.005</span>, weight_decay=<span class=\"number\">5e-4</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 训练循环</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>():</span><br><span class=\"line\">    model.train()</span><br><span class=\"line\">    optimizer.zero_grad()</span><br><span class=\"line\">    output = model(features, adj)</span><br><span class=\"line\">    loss = F.nll_loss(output[idx_train], labels[idx_train])</span><br><span class=\"line\">    loss.backward()</span><br><span class=\"line\">    optimizer.step()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> loss.item()</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"🔍-注意事项与最佳实践\"><a href=\"#🔍-注意事项与最佳实践\" class=\"headerlink\" title=\"🔍 注意事项与最佳实践\"></a>🔍 注意事项与最佳实践</h2><h2 id=\"📈-未来展望\"><a href=\"#📈-未来展望\" class=\"headerlink\" title=\"📈 未来展望\"></a>📈 未来展望</h2><h1 id=\"📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"><a href=\"#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\" class=\"headerlink\" title=\"📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1><p><a href=\"/paper/Lee 等 - 2018 - Attention Models in Graphs A Survey.pdf\" target=\"_blank\">📄 Lee 等 - 2018 - Attention Models in Graphs A Survey</a><br><a href=\"https://github.com/xmu-xiaoma666/External-Attention-pytorch\" target=\"_blank\">github: External-Attention-pytorch</a> </p>\n","length":642,"excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1>","more":"<h3 id=\"图注意力机制\"><a href=\"#图注意力机制\" class=\"headerlink\" title=\"图注意力机制\"></a>图注意力机制</h3><h4 id=\"GAT（Graph-Attention-Networks）\"><a href=\"#GAT（Graph-Attention-Networks）\" class=\"headerlink\" title=\"GAT（Graph Attention Networks）\"></a>GAT（Graph Attention Networks）</h4><p>GAT通过引入注意力机制来加权邻居节点的特征。对于节点i，其更新公式为：</p>\n<p>$$h_i^{(l+1)} &#x3D; \\sigma(\\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij}W^{(l)}h_j^{(l)})$$</p>\n<p>其中注意力系数$\\alpha_{ij}$的计算：</p>\n<p>$$\\alpha_{ij} &#x3D; \\frac{exp(LeakyReLU(a^T[Wh_i || Wh_j]))}{\\sum_{k \\in \\mathcal{N}_i} exp(LeakyReLU(a^T[Wh_i || Wh_k]))}$$</p>\n<h4 id=\"多头注意力\"><a href=\"#多头注意力\" class=\"headerlink\" title=\"多头注意力\"></a>多头注意力</h4><p>为了提高模型的稳定性和表达能力，GAT使用了多头注意力机制：</p>\n<p>$$h_i^{(l+1)} &#x3D; \\sigma(\\frac{1}{K} \\sum_{k&#x3D;1}^K \\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij}^k W^k h_j^{(l)})$$</p>\n<h3 id=\"变体与扩展\"><a href=\"#变体与扩展\" class=\"headerlink\" title=\"变体与扩展\"></a>变体与扩展</h3><h4 id=\"边注意力\"><a href=\"#边注意力\" class=\"headerlink\" title=\"边注意力\"></a>边注意力</h4><p>除了节点之间的注意力，一些模型还引入了边注意力机制：</p>\n<p>$$e_{ij} &#x3D; a^T[Wh_i || Wh_j || We_{ij}]$$</p>\n<p>其中$e_{ij}$是边的特征。</p>\n<h4 id=\"全局注意力\"><a href=\"#全局注意力\" class=\"headerlink\" title=\"全局注意力\"></a>全局注意力</h4><p>通过引入全局节点或池化操作，可以实现全局注意力：</p>\n<p>$$g &#x3D; \\sum_{i \\in V} \\beta_i h_i$$</p>\n<p>其中$\\beta_i$是全局注意力权重。</p>\n<h2 id=\"💻-实现细节\"><a href=\"#💻-实现细节\" class=\"headerlink\" title=\"💻 实现细节\"></a>💻 实现细节</h2><h3 id=\"PyTorch实现的GAT层\"><a href=\"#PyTorch实现的GAT层\" class=\"headerlink\" title=\"PyTorch实现的GAT层\"></a>PyTorch实现的GAT层</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">GATLayer</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, in_features, out_features, dropout, alpha, concat=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(GATLayer, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.in_features = in_features</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.out_features = out_features</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = dropout</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.alpha = alpha</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.concat = concat</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 变换矩阵</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))</span><br><span class=\"line\">        nn.init.xavier_uniform_(<span class=\"variable language_\">self</span>.W.data, gain=<span class=\"number\">1.414</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 注意力向量</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.a = nn.Parameter(torch.zeros(size=(<span class=\"number\">2</span>*out_features, <span class=\"number\">1</span>)))</span><br><span class=\"line\">        nn.init.xavier_uniform_(<span class=\"variable language_\">self</span>.a.data, gain=<span class=\"number\">1.414</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.leakyrelu = nn.LeakyReLU(<span class=\"variable language_\">self</span>.alpha)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, adj</span>):</span><br><span class=\"line\">        <span class=\"comment\"># x: 节点特征矩阵 [N, in_features]</span></span><br><span class=\"line\">        <span class=\"comment\"># adj: 邻接矩阵 [N, N]</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 线性变换</span></span><br><span class=\"line\">        h = torch.mm(x, <span class=\"variable language_\">self</span>.W)  <span class=\"comment\"># [N, out_features]</span></span><br><span class=\"line\">        N = h.size()[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 计算注意力分数</span></span><br><span class=\"line\">        a_input = torch.cat([h.repeat(<span class=\"number\">1</span>, N).view(N * N, -<span class=\"number\">1</span>), h.repeat(N, <span class=\"number\">1</span>)], dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        a_input = a_input.view(N, N, <span class=\"number\">2</span> * <span class=\"variable language_\">self</span>.out_features)</span><br><span class=\"line\">        e = <span class=\"variable language_\">self</span>.leakyrelu(torch.matmul(a_input, <span class=\"variable language_\">self</span>.a).squeeze(<span class=\"number\">2</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 掩码机制</span></span><br><span class=\"line\">        zero_vec = -<span class=\"number\">9e15</span> * torch.ones_like(e)</span><br><span class=\"line\">        attention = torch.where(adj &gt; <span class=\"number\">0</span>, e, zero_vec)</span><br><span class=\"line\">        attention = F.softmax(attention, dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        attention = F.dropout(attention, <span class=\"variable language_\">self</span>.dropout, training=<span class=\"variable language_\">self</span>.training)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 聚合特征</span></span><br><span class=\"line\">        h_prime = torch.matmul(attention, h)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"variable language_\">self</span>.concat:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> F.elu(h_prime)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> h_prime</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">GAT</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, nfeat, nhid, nclass, dropout, alpha, nheads</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(GAT, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = dropout</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 多头注意力层</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.attentions = nn.ModuleList([</span><br><span class=\"line\">            GATLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=<span class=\"literal\">True</span>) </span><br><span class=\"line\">            <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(nheads)</span><br><span class=\"line\">        ])</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 输出层</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.out_att = GATLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, adj</span>):</span><br><span class=\"line\">        x = F.dropout(x, <span class=\"variable language_\">self</span>.dropout, training=<span class=\"variable language_\">self</span>.training)</span><br><span class=\"line\">        <span class=\"comment\"># 多头注意力</span></span><br><span class=\"line\">        x = torch.cat([att(x, adj) <span class=\"keyword\">for</span> att <span class=\"keyword\">in</span> <span class=\"variable language_\">self</span>.attentions], dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        x = F.dropout(x, <span class=\"variable language_\">self</span>.dropout, training=<span class=\"variable language_\">self</span>.training)</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.out_att(x, adj)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.log_softmax(x, dim=<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"实际应用示例\"><a href=\"#实际应用示例\" class=\"headerlink\" title=\"实际应用示例\"></a>实际应用示例</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 模型初始化</span></span><br><span class=\"line\">model = GAT(nfeat=input_dim,</span><br><span class=\"line\">           nhid=<span class=\"number\">8</span>,</span><br><span class=\"line\">           nclass=num_classes,</span><br><span class=\"line\">           dropout=<span class=\"number\">0.6</span>,</span><br><span class=\"line\">           alpha=<span class=\"number\">0.2</span>,</span><br><span class=\"line\">           nheads=<span class=\"number\">8</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 优化器</span></span><br><span class=\"line\">optimizer = torch.optim.Adam(model.parameters(), lr=<span class=\"number\">0.005</span>, weight_decay=<span class=\"number\">5e-4</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 训练循环</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>():</span><br><span class=\"line\">    model.train()</span><br><span class=\"line\">    optimizer.zero_grad()</span><br><span class=\"line\">    output = model(features, adj)</span><br><span class=\"line\">    loss = F.nll_loss(output[idx_train], labels[idx_train])</span><br><span class=\"line\">    loss.backward()</span><br><span class=\"line\">    optimizer.step()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> loss.item()</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"🔍-注意事项与最佳实践\"><a href=\"#🔍-注意事项与最佳实践\" class=\"headerlink\" title=\"🔍 注意事项与最佳实践\"></a>🔍 注意事项与最佳实践</h2><h2 id=\"📈-未来展望\"><a href=\"#📈-未来展望\" class=\"headerlink\" title=\"📈 未来展望\"></a>📈 未来展望</h2><h1 id=\"📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"><a href=\"#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\" class=\"headerlink\" title=\"📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1><p><a href=\"/paper/Lee 等 - 2018 - Attention Models in Graphs A Survey.pdf\" target=\"_blank\">📄 Lee 等 - 2018 - Attention Models in Graphs A Survey</a><br><a href=\"https://github.com/xmu-xiaoma666/External-Attention-pytorch\" target=\"_blank\">github: External-Attention-pytorch</a> </p>"},{"title":"What Is GNN and GCN ?","date":"2025-07-10T07:36:46.000Z","_content":"\n# GNN 与 GCN\n\n> 图神经网络（Graph Neural Networks, GNN）和图卷积网络（Graph Convolutional Networks, GCN）是处理图数据的强大工具。本文将从理论到实践，全面介绍这两种重要的深度学习模型。\n\n本文主要介绍了*GNN和GCN的大致原理*，*GCN在PyG和PyTorch的实现* 以及它们在*DRP中的应用*\n\n<!-- more -->\n\n## 🎯 Intro\n\n在深度学习领域，处理图结构数据一直是一个具有挑战性的任务。传统的深度学习模型（如CNN、RNN）在处理欧几里得空间中的数据表现出色，但对于图这种非欧几里得结构的数据却显得力不从心。GNN和GCN的出现，为我们提供了处理图数据的有力工具。\n\n而在DRP领域，由于涉及到大量的Embedding，GCN现在几乎已经成为了必不可少的模块。\n\n但在开始各种各样的奇形怪状的GCN之前，了解GNN和GCN本身的实现仍然是非常必要的。~~于鼠鼠而言~~大致有以下理由：\n1. 部分抽象的基于GCN的模块第三方库不一定支持\n2. 由于反应表示数据的不平衡，我们可以构建的模型的层数是非常有限的（因为会过平滑）。因此对层内的改造就显得非常必要了。而这一切的前提便是理解原理捏\n\n在这里强烈建议去看一下[Distill](https://distill.pub/)的两篇有关图神经网络的博客，非常易懂。\n\n---\n\n## 📚 理论基础\n\n### 图的基本概念\n\n在开始之前，我们需要理解图的基本表示：\n- 图 $G = (V, E)$，其中 $V$ 是节点集合，$E$ 是边集合\n- 邻接矩阵 $A \\in \\mathbb{R}^{n \\times n}$\n- 度矩阵 $D = diag(d_1,...,d_n)$，其中 $d_i = \\sum_j A_{ij}$\n- 节点特征矩阵 $X \\in \\mathbb{R}^{n \\times d}$\n\n### GNN框架\n\nGNN的基本框架遵循消息传递范式（Message Passing Neural Network, MPNN），可以用以下数学公式表示：\n\n1. **消息传递阶段**（Message Passing）：\n   \n   对于节点 $v$，从其邻居节点 $u \\in \\mathcal{N}(v)$ 收集信息：\n   \n   $$m_v^{(l)} = \\sum_{u \\in \\mathcal{N}(v)} M_l(h_v^{(l-1)}, h_u^{(l-1)}, e_{uv})$$\n\n   其中：\n   - $h_v^{(l-1)}$ 是节点 $v$ 在第 $l-1$ 层的特征\n   - $e_{uv}$ 是边 $(u,v)$ 的特征\n   - $M_l$ 是可学习的消息函数\n\n2. **消息聚合阶段**（Aggregation）：\n   \n   将收集到的消息进行聚合：\n\n   $$a_v^{(l)} = AGG(\\{m_v^{(l)} | u \\in \\mathcal{N}(v)\\})$$\n\n   常见的聚合函数包括：\n   - 求和：$AGG_{sum} = \\sum_{u \\in \\mathcal{N}(v)} m_u$\n   - 平均：$AGG_{mean} = \\frac{1}{|\\mathcal{N}(v)|} \\sum_{u \\in \\mathcal{N}(v)} m_u$\n   - 最大：$AGG_{max} = max_{u \\in \\mathcal{N}(v)} m_u$\n\n3. **节点更新阶段**（Update）：\n   \n   更新节点的表示：\n\n   $$h_v^{(l)} = U_l(h_v^{(l-1)}, a_v^{(l)})$$\n\n   其中 $U_l$ 是可学习的更新函数，通常是MLP或其他神经网络。\n\n### GCN实现\n\n#### 拉普拉斯矩阵 🔍\n\n拉普拉斯矩阵是图信号处理中的核心概念，有多种形式：\n\n1. **组合拉普拉斯矩阵**：$L = D - A$\n\n2. **标准化拉普拉斯矩阵**：$L_{sym} = D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}} = I - D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$\n\n3. **随机游走拉普拉斯矩阵**：$L_{rw} = D^{-1}L = I - D^{-1}A$\n\n拉普拉斯矩阵的特性：\n- 对称性：$L = L^T$\n- 半正定性：所有特征值非负\n- 最小特征值为0，对应的特征向量是常数向量\n- 特征值的重数对应图的连通分量数\n\n#### 从传统卷积到图卷积 🔄\n\n##### 传统卷积回顾\n\n在欧几里得空间中，卷积操作定义为：\n\n$$(f * g)(p) = \\sum_{q \\in \\mathcal{N}(p)} f(q) \\cdot g(p-q)$$\n\n这里的关键特点是：\n- 平移不变性\n- 局部性\n- 参数共享\n\n##### 图上的卷积定义\n\n在图域中，我们需要重新定义这些特性：\n\n1. **空间域卷积**：\n   $$h_v = \\sum_{u \\in \\mathcal{N}(v)} W(e_{u,v})h_u$$\n   其中 $W(e_{u,v})$ 是边的权重函数\n\n2. **谱域卷积**：\n   $$g_\\theta * x = Ug_\\theta U^T x$$\n   其中 $U$ 是拉普拉斯矩阵的特征向量矩阵\n\n#### GCN的数学推导 ⚙️\n\nKipf & Welling提出的GCN模型中，单层传播规则为：\n\n$$H^{(l+1)} = \\sigma(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)})$$\n\n其中：\n- $\\tilde{A} = A + I_N$ 是添加了自环的邻接矩阵\n- $\\tilde{D}\\_{ii} = \\sum\\_{j} \\tilde{A}\\_{ij}$ 是对应的度矩阵\n- $H^{(l)}$ 是第 $l$ 层的激活值\n- $W^{(l)}$ 是可学习的权重矩阵\n- $\\sigma$ 是非线性激活函数\n\n~~一些自己的理解~~\n1. 引入$L_{sym} = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$作为聚合（AGG）部分\n   - 添加自环：$\\tilde{A} = A + I_N$\n   - 计算归一化系数：$\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$\n2. 特征变换：$H^{(l)}W^{(l)}$\n3. 邻域聚合：$\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)}$\n4. 非线性变换：$\\sigma(\\cdot)$\n\n---\n\n## 💻 实现细节\n\n基于这个理论框架的简单实现如下：\n\n```python\ndef message_passing(nodes, edges):\n    messages = {}\n    for edge in edges:\n        src, dst = edge\n        msg = compute_message(nodes[src], nodes[dst])\n        messages.setdefault(dst, []).append(msg)\n    return messages\n\ndef aggregate_messages(messages):\n    aggregated = {}\n    for node, msgs in messages.items():\n        aggregated[node] = sum(msgs) / len(msgs)  # 平均聚合\n    return aggregated\n\ndef update_nodes(nodes, aggregated):\n    updated = {}\n    for node, agg_msg in aggregated.items():\n        updated[node] = nodes[node] + agg_msg  # 残差连接\n    return updated\n```\n\n### PyTorch Geometric实现 🚀\n\n> 本节代码基于 PyTorch 2.1.0 和 PyTorch Geometric 2.4.0 版本\n\n使用PyTorch Geometric库的GCN实现：\n\n```python\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self, num_features, num_classes):\n        super(GCN, self).__init__()\n        self.conv1 = GCNConv(num_features, 16)\n        self.conv2 = GCNConv(16, num_classes)\n\n    def forward(self, x, edge_index):\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n        return F.log_softmax(x, dim=1)\n```\n\n### 原生PyTorch实现 🔧\n\n> 本节代码基于 PyTorch 2.1.0、NumPy 1.24.0 和 SciPy 1.11.0 版本\n\n不使用PyG，手动实现GCN~~主要是目前不太清楚主流的HGCN的实现方式捏~~：\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport scipy.sparse as sp\nimport numpy as np\n\nclass GCNLayer(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(GCNLayer, self).__init__()\n        self.W = nn.Parameter(torch.FloatTensor(in_features, out_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.W)\n\n    def forward(self, x, adj):\n        # adj: 归一化的邻接矩阵\n        support = torch.mm(x, self.W)\n        output = torch.sparse.mm(adj, support)\n        return output\n\nclass GCN(nn.Module):\n    def __init__(self, nfeat, nhid, nclass, dropout):\n        super(GCN, self).__init__()\n        self.gc1 = GCNLayer(nfeat, nhid)\n        self.gc2 = GCNLayer(nhid, nclass)\n        self.dropout = dropout\n\n    def forward(self, x, adj):\n        x = F.relu(self.gc1(x, adj))\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = self.gc2(x, adj)\n        return F.log_softmax(x, dim=1)\n\ndef normalize_adj(adj):\n    \"\"\"归一化邻接矩阵\"\"\"\n    adj = sp.coo_matrix(adj)\n    rowsum = np.array(adj.sum(1))\n    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n```\n\n---\n\n\n## 🎮 应用场景\n\n~~由于鼠鼠就是个臭写DRP的捏~~ 这里只给出GNN在DRP中的应用\n\n\n1. **药物表示**\n   - *分子图构建*：将药物SMILES字符串转换为图结构，节点表示原子（含原子类型、电荷等特征），边表示化学键（如键类型、距离）。  \n   - *GNN编码*：使用图卷积网络（GCN）、图注意力网络（GAT）或图同构网络（GIN）等层迭代聚合邻域信息，生成药物嵌入（embedding）。例如，GraTransDRP（2022）结合GAT和Transformer提升药物表征能力。\n\n2. **癌症表示**\n   - *生物网络构建*：基于基因互作（如STRING数据库的蛋白-蛋白互作）、基因共表达或通路信息构建异质图。例如，AGMI（2021）整合多组学数据和PPI网络，通过GNN学习癌症样本的联合表征。  \n   - *多组学融合*：部分模型（如TGSA）利用GNN整合基因组、转录组等数据，通过跨模态注意力机制增强特征交互。\n\n3. **异构图与联合建模**\n   - *细胞系-药物异构图*：如GraphCDR（2021）将细胞系和药物作为两类节点，通过边连接已知响应对，直接学习跨实体关系。  \n   - *知识增强*：预训练GNN于大规模生物化学属性预测（如Zhu et al., 2021），再迁移至DRP任务，提升泛化性。\n\n## 🎯 总结与展望\n\n- **动态图建模**：捕捉治疗过程中动态变化的生物网络。  \n- **三维分子图**：结合几何深度学习（如SchNet）提升立体化学感知。  \n- **基准测试**：需统一评估协议（如固定数据集和指标）以公平比较GNN与其他方法。\n\n~~之后应该会写一些具体模型的博客，有相关的会直接上链接的捏jrm~~\n\n# 📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\n<a href=\"/paper/1609.02907v4.pdf\" target=\"_blank\">📄 Thomas - SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</a>\n<a href=\"https://pytorch-geometric.readthedocs.io/\" target=\"_blank\">PyTorch Geometric 官方文档</a>\n<a href=\"https://distill.pub/2021/gnn-intro/\" target=\"_blank\">Distill: A Gentle Introduction to Graph Neural Networks</a>\n<a href=\"https://distill.pub/2021/understanding-gnns/\" target=\"_blank\">Distill: Understanding Convolutions on Graphs</a>\n<a href=\"https://www.zhihu.com/tardis/zm/art/107162772\" target=\"_blank\">知乎：图卷积网络（GCN）入门详解</a>\n<a href=\"https://github.com/tkipf/gcn\" target=\"_blank\">GCN 论文官方代码（GitHub）</a>","source":"_posts/GNN-and-GCN.md","raw":"---\ntitle: What Is GNN and GCN ?\ndate: 2025-07-10 15:36:46\ncategories:\n  - CDR\n  - model\n  - GNN\ntags:\n  - CDR\n  - model\n  - embedding\n  - PyTorch\n  - graph theory\n  - Basic\n---\n\n# GNN 与 GCN\n\n> 图神经网络（Graph Neural Networks, GNN）和图卷积网络（Graph Convolutional Networks, GCN）是处理图数据的强大工具。本文将从理论到实践，全面介绍这两种重要的深度学习模型。\n\n本文主要介绍了*GNN和GCN的大致原理*，*GCN在PyG和PyTorch的实现* 以及它们在*DRP中的应用*\n\n<!-- more -->\n\n## 🎯 Intro\n\n在深度学习领域，处理图结构数据一直是一个具有挑战性的任务。传统的深度学习模型（如CNN、RNN）在处理欧几里得空间中的数据表现出色，但对于图这种非欧几里得结构的数据却显得力不从心。GNN和GCN的出现，为我们提供了处理图数据的有力工具。\n\n而在DRP领域，由于涉及到大量的Embedding，GCN现在几乎已经成为了必不可少的模块。\n\n但在开始各种各样的奇形怪状的GCN之前，了解GNN和GCN本身的实现仍然是非常必要的。~~于鼠鼠而言~~大致有以下理由：\n1. 部分抽象的基于GCN的模块第三方库不一定支持\n2. 由于反应表示数据的不平衡，我们可以构建的模型的层数是非常有限的（因为会过平滑）。因此对层内的改造就显得非常必要了。而这一切的前提便是理解原理捏\n\n在这里强烈建议去看一下[Distill](https://distill.pub/)的两篇有关图神经网络的博客，非常易懂。\n\n---\n\n## 📚 理论基础\n\n### 图的基本概念\n\n在开始之前，我们需要理解图的基本表示：\n- 图 $G = (V, E)$，其中 $V$ 是节点集合，$E$ 是边集合\n- 邻接矩阵 $A \\in \\mathbb{R}^{n \\times n}$\n- 度矩阵 $D = diag(d_1,...,d_n)$，其中 $d_i = \\sum_j A_{ij}$\n- 节点特征矩阵 $X \\in \\mathbb{R}^{n \\times d}$\n\n### GNN框架\n\nGNN的基本框架遵循消息传递范式（Message Passing Neural Network, MPNN），可以用以下数学公式表示：\n\n1. **消息传递阶段**（Message Passing）：\n   \n   对于节点 $v$，从其邻居节点 $u \\in \\mathcal{N}(v)$ 收集信息：\n   \n   $$m_v^{(l)} = \\sum_{u \\in \\mathcal{N}(v)} M_l(h_v^{(l-1)}, h_u^{(l-1)}, e_{uv})$$\n\n   其中：\n   - $h_v^{(l-1)}$ 是节点 $v$ 在第 $l-1$ 层的特征\n   - $e_{uv}$ 是边 $(u,v)$ 的特征\n   - $M_l$ 是可学习的消息函数\n\n2. **消息聚合阶段**（Aggregation）：\n   \n   将收集到的消息进行聚合：\n\n   $$a_v^{(l)} = AGG(\\{m_v^{(l)} | u \\in \\mathcal{N}(v)\\})$$\n\n   常见的聚合函数包括：\n   - 求和：$AGG_{sum} = \\sum_{u \\in \\mathcal{N}(v)} m_u$\n   - 平均：$AGG_{mean} = \\frac{1}{|\\mathcal{N}(v)|} \\sum_{u \\in \\mathcal{N}(v)} m_u$\n   - 最大：$AGG_{max} = max_{u \\in \\mathcal{N}(v)} m_u$\n\n3. **节点更新阶段**（Update）：\n   \n   更新节点的表示：\n\n   $$h_v^{(l)} = U_l(h_v^{(l-1)}, a_v^{(l)})$$\n\n   其中 $U_l$ 是可学习的更新函数，通常是MLP或其他神经网络。\n\n### GCN实现\n\n#### 拉普拉斯矩阵 🔍\n\n拉普拉斯矩阵是图信号处理中的核心概念，有多种形式：\n\n1. **组合拉普拉斯矩阵**：$L = D - A$\n\n2. **标准化拉普拉斯矩阵**：$L_{sym} = D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}} = I - D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$\n\n3. **随机游走拉普拉斯矩阵**：$L_{rw} = D^{-1}L = I - D^{-1}A$\n\n拉普拉斯矩阵的特性：\n- 对称性：$L = L^T$\n- 半正定性：所有特征值非负\n- 最小特征值为0，对应的特征向量是常数向量\n- 特征值的重数对应图的连通分量数\n\n#### 从传统卷积到图卷积 🔄\n\n##### 传统卷积回顾\n\n在欧几里得空间中，卷积操作定义为：\n\n$$(f * g)(p) = \\sum_{q \\in \\mathcal{N}(p)} f(q) \\cdot g(p-q)$$\n\n这里的关键特点是：\n- 平移不变性\n- 局部性\n- 参数共享\n\n##### 图上的卷积定义\n\n在图域中，我们需要重新定义这些特性：\n\n1. **空间域卷积**：\n   $$h_v = \\sum_{u \\in \\mathcal{N}(v)} W(e_{u,v})h_u$$\n   其中 $W(e_{u,v})$ 是边的权重函数\n\n2. **谱域卷积**：\n   $$g_\\theta * x = Ug_\\theta U^T x$$\n   其中 $U$ 是拉普拉斯矩阵的特征向量矩阵\n\n#### GCN的数学推导 ⚙️\n\nKipf & Welling提出的GCN模型中，单层传播规则为：\n\n$$H^{(l+1)} = \\sigma(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)})$$\n\n其中：\n- $\\tilde{A} = A + I_N$ 是添加了自环的邻接矩阵\n- $\\tilde{D}\\_{ii} = \\sum\\_{j} \\tilde{A}\\_{ij}$ 是对应的度矩阵\n- $H^{(l)}$ 是第 $l$ 层的激活值\n- $W^{(l)}$ 是可学习的权重矩阵\n- $\\sigma$ 是非线性激活函数\n\n~~一些自己的理解~~\n1. 引入$L_{sym} = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$作为聚合（AGG）部分\n   - 添加自环：$\\tilde{A} = A + I_N$\n   - 计算归一化系数：$\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$\n2. 特征变换：$H^{(l)}W^{(l)}$\n3. 邻域聚合：$\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)}$\n4. 非线性变换：$\\sigma(\\cdot)$\n\n---\n\n## 💻 实现细节\n\n基于这个理论框架的简单实现如下：\n\n```python\ndef message_passing(nodes, edges):\n    messages = {}\n    for edge in edges:\n        src, dst = edge\n        msg = compute_message(nodes[src], nodes[dst])\n        messages.setdefault(dst, []).append(msg)\n    return messages\n\ndef aggregate_messages(messages):\n    aggregated = {}\n    for node, msgs in messages.items():\n        aggregated[node] = sum(msgs) / len(msgs)  # 平均聚合\n    return aggregated\n\ndef update_nodes(nodes, aggregated):\n    updated = {}\n    for node, agg_msg in aggregated.items():\n        updated[node] = nodes[node] + agg_msg  # 残差连接\n    return updated\n```\n\n### PyTorch Geometric实现 🚀\n\n> 本节代码基于 PyTorch 2.1.0 和 PyTorch Geometric 2.4.0 版本\n\n使用PyTorch Geometric库的GCN实现：\n\n```python\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self, num_features, num_classes):\n        super(GCN, self).__init__()\n        self.conv1 = GCNConv(num_features, 16)\n        self.conv2 = GCNConv(16, num_classes)\n\n    def forward(self, x, edge_index):\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n        return F.log_softmax(x, dim=1)\n```\n\n### 原生PyTorch实现 🔧\n\n> 本节代码基于 PyTorch 2.1.0、NumPy 1.24.0 和 SciPy 1.11.0 版本\n\n不使用PyG，手动实现GCN~~主要是目前不太清楚主流的HGCN的实现方式捏~~：\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport scipy.sparse as sp\nimport numpy as np\n\nclass GCNLayer(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(GCNLayer, self).__init__()\n        self.W = nn.Parameter(torch.FloatTensor(in_features, out_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.W)\n\n    def forward(self, x, adj):\n        # adj: 归一化的邻接矩阵\n        support = torch.mm(x, self.W)\n        output = torch.sparse.mm(adj, support)\n        return output\n\nclass GCN(nn.Module):\n    def __init__(self, nfeat, nhid, nclass, dropout):\n        super(GCN, self).__init__()\n        self.gc1 = GCNLayer(nfeat, nhid)\n        self.gc2 = GCNLayer(nhid, nclass)\n        self.dropout = dropout\n\n    def forward(self, x, adj):\n        x = F.relu(self.gc1(x, adj))\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = self.gc2(x, adj)\n        return F.log_softmax(x, dim=1)\n\ndef normalize_adj(adj):\n    \"\"\"归一化邻接矩阵\"\"\"\n    adj = sp.coo_matrix(adj)\n    rowsum = np.array(adj.sum(1))\n    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n```\n\n---\n\n\n## 🎮 应用场景\n\n~~由于鼠鼠就是个臭写DRP的捏~~ 这里只给出GNN在DRP中的应用\n\n\n1. **药物表示**\n   - *分子图构建*：将药物SMILES字符串转换为图结构，节点表示原子（含原子类型、电荷等特征），边表示化学键（如键类型、距离）。  \n   - *GNN编码*：使用图卷积网络（GCN）、图注意力网络（GAT）或图同构网络（GIN）等层迭代聚合邻域信息，生成药物嵌入（embedding）。例如，GraTransDRP（2022）结合GAT和Transformer提升药物表征能力。\n\n2. **癌症表示**\n   - *生物网络构建*：基于基因互作（如STRING数据库的蛋白-蛋白互作）、基因共表达或通路信息构建异质图。例如，AGMI（2021）整合多组学数据和PPI网络，通过GNN学习癌症样本的联合表征。  \n   - *多组学融合*：部分模型（如TGSA）利用GNN整合基因组、转录组等数据，通过跨模态注意力机制增强特征交互。\n\n3. **异构图与联合建模**\n   - *细胞系-药物异构图*：如GraphCDR（2021）将细胞系和药物作为两类节点，通过边连接已知响应对，直接学习跨实体关系。  \n   - *知识增强*：预训练GNN于大规模生物化学属性预测（如Zhu et al., 2021），再迁移至DRP任务，提升泛化性。\n\n## 🎯 总结与展望\n\n- **动态图建模**：捕捉治疗过程中动态变化的生物网络。  \n- **三维分子图**：结合几何深度学习（如SchNet）提升立体化学感知。  \n- **基准测试**：需统一评估协议（如固定数据集和指标）以公平比较GNN与其他方法。\n\n~~之后应该会写一些具体模型的博客，有相关的会直接上链接的捏jrm~~\n\n# 📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\n<a href=\"/paper/1609.02907v4.pdf\" target=\"_blank\">📄 Thomas - SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</a>\n<a href=\"https://pytorch-geometric.readthedocs.io/\" target=\"_blank\">PyTorch Geometric 官方文档</a>\n<a href=\"https://distill.pub/2021/gnn-intro/\" target=\"_blank\">Distill: A Gentle Introduction to Graph Neural Networks</a>\n<a href=\"https://distill.pub/2021/understanding-gnns/\" target=\"_blank\">Distill: Understanding Convolutions on Graphs</a>\n<a href=\"https://www.zhihu.com/tardis/zm/art/107162772\" target=\"_blank\">知乎：图卷积网络（GCN）入门详解</a>\n<a href=\"https://github.com/tkipf/gcn\" target=\"_blank\">GCN 论文官方代码（GitHub）</a>","slug":"GNN-and-GCN","published":1,"updated":"2025-07-12T06:52:36.342Z","comments":1,"layout":"post","photos":[],"_id":"cmd39u3jp001kqk41c4oc2684","content":"<h1 id=\"GNN-与-GCN\"><a href=\"#GNN-与-GCN\" class=\"headerlink\" title=\"GNN 与 GCN\"></a>GNN 与 GCN</h1><blockquote>\n<p>图神经网络（Graph Neural Networks, GNN）和图卷积网络（Graph Convolutional Networks, GCN）是处理图数据的强大工具。本文将从理论到实践，全面介绍这两种重要的深度学习模型。</p>\n</blockquote>\n<p>本文主要介绍了<em>GNN和GCN的大致原理</em>，<em>GCN在PyG和PyTorch的实现</em> 以及它们在<em>DRP中的应用</em></p>\n<span id=\"more\"></span>\n\n<h2 id=\"🎯-Intro\"><a href=\"#🎯-Intro\" class=\"headerlink\" title=\"🎯 Intro\"></a>🎯 Intro</h2><p>在深度学习领域，处理图结构数据一直是一个具有挑战性的任务。传统的深度学习模型（如CNN、RNN）在处理欧几里得空间中的数据表现出色，但对于图这种非欧几里得结构的数据却显得力不从心。GNN和GCN的出现，为我们提供了处理图数据的有力工具。</p>\n<p>而在DRP领域，由于涉及到大量的Embedding，GCN现在几乎已经成为了必不可少的模块。</p>\n<p>但在开始各种各样的奇形怪状的GCN之前，了解GNN和GCN本身的实现仍然是非常必要的。<del>于鼠鼠而言</del>大致有以下理由：</p>\n<ol>\n<li>部分抽象的基于GCN的模块第三方库不一定支持</li>\n<li>由于反应表示数据的不平衡，我们可以构建的模型的层数是非常有限的（因为会过平滑）。因此对层内的改造就显得非常必要了。而这一切的前提便是理解原理捏</li>\n</ol>\n<p>在这里强烈建议去看一下<a href=\"https://distill.pub/\">Distill</a>的两篇有关图神经网络的博客，非常易懂。</p>\n<hr>\n<h2 id=\"📚-理论基础\"><a href=\"#📚-理论基础\" class=\"headerlink\" title=\"📚 理论基础\"></a>📚 理论基础</h2><h3 id=\"图的基本概念\"><a href=\"#图的基本概念\" class=\"headerlink\" title=\"图的基本概念\"></a>图的基本概念</h3><p>在开始之前，我们需要理解图的基本表示：</p>\n<ul>\n<li>图 $G &#x3D; (V, E)$，其中 $V$ 是节点集合，$E$ 是边集合</li>\n<li>邻接矩阵 $A \\in \\mathbb{R}^{n \\times n}$</li>\n<li>度矩阵 $D &#x3D; diag(d_1,…,d_n)$，其中 $d_i &#x3D; \\sum_j A_{ij}$</li>\n<li>节点特征矩阵 $X \\in \\mathbb{R}^{n \\times d}$</li>\n</ul>\n<h3 id=\"GNN框架\"><a href=\"#GNN框架\" class=\"headerlink\" title=\"GNN框架\"></a>GNN框架</h3><p>GNN的基本框架遵循消息传递范式（Message Passing Neural Network, MPNN），可以用以下数学公式表示：</p>\n<ol>\n<li><p><strong>消息传递阶段</strong>（Message Passing）：</p>\n<p>对于节点 $v$，从其邻居节点 $u \\in \\mathcal{N}(v)$ 收集信息：</p>\n<p>$$m_v^{(l)} &#x3D; \\sum_{u \\in \\mathcal{N}(v)} M_l(h_v^{(l-1)}, h_u^{(l-1)}, e_{uv})$$</p>\n<p>其中：</p>\n<ul>\n<li>$h_v^{(l-1)}$ 是节点 $v$ 在第 $l-1$ 层的特征</li>\n<li>$e_{uv}$ 是边 $(u,v)$ 的特征</li>\n<li>$M_l$ 是可学习的消息函数</li>\n</ul>\n</li>\n<li><p><strong>消息聚合阶段</strong>（Aggregation）：</p>\n<p>将收集到的消息进行聚合：</p>\n<p>$$a_v^{(l)} &#x3D; AGG({m_v^{(l)} | u \\in \\mathcal{N}(v)})$$</p>\n<p>常见的聚合函数包括：</p>\n<ul>\n<li>求和：$AGG_{sum} &#x3D; \\sum_{u \\in \\mathcal{N}(v)} m_u$</li>\n<li>平均：$AGG_{mean} &#x3D; \\frac{1}{|\\mathcal{N}(v)|} \\sum_{u \\in \\mathcal{N}(v)} m_u$</li>\n<li>最大：$AGG_{max} &#x3D; max_{u \\in \\mathcal{N}(v)} m_u$</li>\n</ul>\n</li>\n<li><p><strong>节点更新阶段</strong>（Update）：</p>\n<p>更新节点的表示：</p>\n<p>$$h_v^{(l)} &#x3D; U_l(h_v^{(l-1)}, a_v^{(l)})$$</p>\n<p>其中 $U_l$ 是可学习的更新函数，通常是MLP或其他神经网络。</p>\n</li>\n</ol>\n<h3 id=\"GCN实现\"><a href=\"#GCN实现\" class=\"headerlink\" title=\"GCN实现\"></a>GCN实现</h3><h4 id=\"拉普拉斯矩阵-🔍\"><a href=\"#拉普拉斯矩阵-🔍\" class=\"headerlink\" title=\"拉普拉斯矩阵 🔍\"></a>拉普拉斯矩阵 🔍</h4><p>拉普拉斯矩阵是图信号处理中的核心概念，有多种形式：</p>\n<ol>\n<li><p><strong>组合拉普拉斯矩阵</strong>：$L &#x3D; D - A$</p>\n</li>\n<li><p><strong>标准化拉普拉斯矩阵</strong>：$L_{sym} &#x3D; D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}} &#x3D; I - D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$</p>\n</li>\n<li><p><strong>随机游走拉普拉斯矩阵</strong>：$L_{rw} &#x3D; D^{-1}L &#x3D; I - D^{-1}A$</p>\n</li>\n</ol>\n<p>拉普拉斯矩阵的特性：</p>\n<ul>\n<li>对称性：$L &#x3D; L^T$</li>\n<li>半正定性：所有特征值非负</li>\n<li>最小特征值为0，对应的特征向量是常数向量</li>\n<li>特征值的重数对应图的连通分量数</li>\n</ul>\n<h4 id=\"从传统卷积到图卷积-🔄\"><a href=\"#从传统卷积到图卷积-🔄\" class=\"headerlink\" title=\"从传统卷积到图卷积 🔄\"></a>从传统卷积到图卷积 🔄</h4><h5 id=\"传统卷积回顾\"><a href=\"#传统卷积回顾\" class=\"headerlink\" title=\"传统卷积回顾\"></a>传统卷积回顾</h5><p>在欧几里得空间中，卷积操作定义为：</p>\n<p>$$(f * g)(p) &#x3D; \\sum_{q \\in \\mathcal{N}(p)} f(q) \\cdot g(p-q)$$</p>\n<p>这里的关键特点是：</p>\n<ul>\n<li>平移不变性</li>\n<li>局部性</li>\n<li>参数共享</li>\n</ul>\n<h5 id=\"图上的卷积定义\"><a href=\"#图上的卷积定义\" class=\"headerlink\" title=\"图上的卷积定义\"></a>图上的卷积定义</h5><p>在图域中，我们需要重新定义这些特性：</p>\n<ol>\n<li><p><strong>空间域卷积</strong>：<br>$$h_v &#x3D; \\sum_{u \\in \\mathcal{N}(v)} W(e_{u,v})h_u$$<br>其中 $W(e_{u,v})$ 是边的权重函数</p>\n</li>\n<li><p><strong>谱域卷积</strong>：<br>$$g_\\theta * x &#x3D; Ug_\\theta U^T x$$<br>其中 $U$ 是拉普拉斯矩阵的特征向量矩阵</p>\n</li>\n</ol>\n<h4 id=\"GCN的数学推导-⚙️\"><a href=\"#GCN的数学推导-⚙️\" class=\"headerlink\" title=\"GCN的数学推导 ⚙️\"></a>GCN的数学推导 ⚙️</h4><p>Kipf &amp; Welling提出的GCN模型中，单层传播规则为：</p>\n<p>$$H^{(l+1)} &#x3D; \\sigma(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)})$$</p>\n<p>其中：</p>\n<ul>\n<li>$\\tilde{A} &#x3D; A + I_N$ 是添加了自环的邻接矩阵</li>\n<li>$\\tilde{D}_{ii} &#x3D; \\sum_{j} \\tilde{A}_{ij}$ 是对应的度矩阵</li>\n<li>$H^{(l)}$ 是第 $l$ 层的激活值</li>\n<li>$W^{(l)}$ 是可学习的权重矩阵</li>\n<li>$\\sigma$ 是非线性激活函数</li>\n</ul>\n<p><del>一些自己的理解</del></p>\n<ol>\n<li>引入$L_{sym} &#x3D; \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$作为聚合（AGG）部分<ul>\n<li>添加自环：$\\tilde{A} &#x3D; A + I_N$</li>\n<li>计算归一化系数：$\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$</li>\n</ul>\n</li>\n<li>特征变换：$H^{(l)}W^{(l)}$</li>\n<li>邻域聚合：$\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)}$</li>\n<li>非线性变换：$\\sigma(\\cdot)$</li>\n</ol>\n<hr>\n<h2 id=\"💻-实现细节\"><a href=\"#💻-实现细节\" class=\"headerlink\" title=\"💻 实现细节\"></a>💻 实现细节</h2><p>基于这个理论框架的简单实现如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">message_passing</span>(<span class=\"params\">nodes, edges</span>):</span><br><span class=\"line\">    messages = &#123;&#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> edge <span class=\"keyword\">in</span> edges:</span><br><span class=\"line\">        src, dst = edge</span><br><span class=\"line\">        msg = compute_message(nodes[src], nodes[dst])</span><br><span class=\"line\">        messages.setdefault(dst, []).append(msg)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> messages</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">aggregate_messages</span>(<span class=\"params\">messages</span>):</span><br><span class=\"line\">    aggregated = &#123;&#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> node, msgs <span class=\"keyword\">in</span> messages.items():</span><br><span class=\"line\">        aggregated[node] = <span class=\"built_in\">sum</span>(msgs) / <span class=\"built_in\">len</span>(msgs)  <span class=\"comment\"># 平均聚合</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> aggregated</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">update_nodes</span>(<span class=\"params\">nodes, aggregated</span>):</span><br><span class=\"line\">    updated = &#123;&#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> node, agg_msg <span class=\"keyword\">in</span> aggregated.items():</span><br><span class=\"line\">        updated[node] = nodes[node] + agg_msg  <span class=\"comment\"># 残差连接</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> updated</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"PyTorch-Geometric实现-🚀\"><a href=\"#PyTorch-Geometric实现-🚀\" class=\"headerlink\" title=\"PyTorch Geometric实现 🚀\"></a>PyTorch Geometric实现 🚀</h3><blockquote>\n<p>本节代码基于 PyTorch 2.1.0 和 PyTorch Geometric 2.4.0 版本</p>\n</blockquote>\n<p>使用PyTorch Geometric库的GCN实现：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch_geometric.nn <span class=\"keyword\">import</span> GCNConv</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">GCN</span>(torch.nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, num_features, num_classes</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(GCN, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.conv1 = GCNConv(num_features, <span class=\"number\">16</span>)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.conv2 = GCNConv(<span class=\"number\">16</span>, num_classes)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, edge_index</span>):</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.conv1(x, edge_index)</span><br><span class=\"line\">        x = F.relu(x)</span><br><span class=\"line\">        x = F.dropout(x, training=<span class=\"variable language_\">self</span>.training)</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.conv2(x, edge_index)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.log_softmax(x, dim=<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"原生PyTorch实现-🔧\"><a href=\"#原生PyTorch实现-🔧\" class=\"headerlink\" title=\"原生PyTorch实现 🔧\"></a>原生PyTorch实现 🔧</h3><blockquote>\n<p>本节代码基于 PyTorch 2.1.0、NumPy 1.24.0 和 SciPy 1.11.0 版本</p>\n</blockquote>\n<p>不使用PyG，手动实现GCN<del>主要是目前不太清楚主流的HGCN的实现方式捏</del>：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">import</span> scipy.sparse <span class=\"keyword\">as</span> sp</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">GCNLayer</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, in_features, out_features</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(GCNLayer, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.W = nn.Parameter(torch.FloatTensor(in_features, out_features))</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.reset_parameters()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">reset_parameters</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        nn.init.kaiming_uniform_(<span class=\"variable language_\">self</span>.W)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, adj</span>):</span><br><span class=\"line\">        <span class=\"comment\"># adj: 归一化的邻接矩阵</span></span><br><span class=\"line\">        support = torch.mm(x, <span class=\"variable language_\">self</span>.W)</span><br><span class=\"line\">        output = torch.sparse.mm(adj, support)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">GCN</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, nfeat, nhid, nclass, dropout</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(GCN, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.gc1 = GCNLayer(nfeat, nhid)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.gc2 = GCNLayer(nhid, nclass)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = dropout</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, adj</span>):</span><br><span class=\"line\">        x = F.relu(<span class=\"variable language_\">self</span>.gc1(x, adj))</span><br><span class=\"line\">        x = F.dropout(x, <span class=\"variable language_\">self</span>.dropout, training=<span class=\"variable language_\">self</span>.training)</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.gc2(x, adj)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.log_softmax(x, dim=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">normalize_adj</span>(<span class=\"params\">adj</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;归一化邻接矩阵&quot;&quot;&quot;</span></span><br><span class=\"line\">    adj = sp.coo_matrix(adj)</span><br><span class=\"line\">    rowsum = np.array(adj.<span class=\"built_in\">sum</span>(<span class=\"number\">1</span>))</span><br><span class=\"line\">    d_inv_sqrt = np.power(rowsum, -<span class=\"number\">0.5</span>).flatten()</span><br><span class=\"line\">    d_inv_sqrt[np.isinf(d_inv_sqrt)] = <span class=\"number\">0.</span></span><br><span class=\"line\">    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()</span><br></pre></td></tr></table></figure>\n\n<hr>\n<h2 id=\"🎮-应用场景\"><a href=\"#🎮-应用场景\" class=\"headerlink\" title=\"🎮 应用场景\"></a>🎮 应用场景</h2><p><del>由于鼠鼠就是个臭写DRP的捏</del> 这里只给出GNN在DRP中的应用</p>\n<ol>\n<li><p><strong>药物表示</strong></p>\n<ul>\n<li><em>分子图构建</em>：将药物SMILES字符串转换为图结构，节点表示原子（含原子类型、电荷等特征），边表示化学键（如键类型、距离）。  </li>\n<li><em>GNN编码</em>：使用图卷积网络（GCN）、图注意力网络（GAT）或图同构网络（GIN）等层迭代聚合邻域信息，生成药物嵌入（embedding）。例如，GraTransDRP（2022）结合GAT和Transformer提升药物表征能力。</li>\n</ul>\n</li>\n<li><p><strong>癌症表示</strong></p>\n<ul>\n<li><em>生物网络构建</em>：基于基因互作（如STRING数据库的蛋白-蛋白互作）、基因共表达或通路信息构建异质图。例如，AGMI（2021）整合多组学数据和PPI网络，通过GNN学习癌症样本的联合表征。  </li>\n<li><em>多组学融合</em>：部分模型（如TGSA）利用GNN整合基因组、转录组等数据，通过跨模态注意力机制增强特征交互。</li>\n</ul>\n</li>\n<li><p><strong>异构图与联合建模</strong></p>\n<ul>\n<li><em>细胞系-药物异构图</em>：如GraphCDR（2021）将细胞系和药物作为两类节点，通过边连接已知响应对，直接学习跨实体关系。  </li>\n<li><em>知识增强</em>：预训练GNN于大规模生物化学属性预测（如Zhu et al., 2021），再迁移至DRP任务，提升泛化性。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"🎯-总结与展望\"><a href=\"#🎯-总结与展望\" class=\"headerlink\" title=\"🎯 总结与展望\"></a>🎯 总结与展望</h2><ul>\n<li><strong>动态图建模</strong>：捕捉治疗过程中动态变化的生物网络。  </li>\n<li><strong>三维分子图</strong>：结合几何深度学习（如SchNet）提升立体化学感知。  </li>\n<li><strong>基准测试</strong>：需统一评估协议（如固定数据集和指标）以公平比较GNN与其他方法。</li>\n</ul>\n<p><del>之后应该会写一些具体模型的博客，有相关的会直接上链接的捏jrm</del></p>\n<h1 id=\"📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"><a href=\"#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\" class=\"headerlink\" title=\"📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1><p><a href=\"/paper/1609.02907v4.pdf\" target=\"_blank\">📄 Thomas - SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</a><br><a href=\"https://pytorch-geometric.readthedocs.io/\" target=\"_blank\">PyTorch Geometric 官方文档</a><br><a href=\"https://distill.pub/2021/gnn-intro/\" target=\"_blank\">Distill: A Gentle Introduction to Graph Neural Networks</a><br><a href=\"https://distill.pub/2021/understanding-gnns/\" target=\"_blank\">Distill: Understanding Convolutions on Graphs</a><br><a href=\"https://www.zhihu.com/tardis/zm/art/107162772\" target=\"_blank\">知乎：图卷积网络（GCN）入门详解</a><br><a href=\"https://github.com/tkipf/gcn\" target=\"_blank\">GCN 论文官方代码（GitHub）</a></p>\n","length":2131,"excerpt":"<h1 id=\"GNN-与-GCN\"><a href=\"#GNN-与-GCN\" class=\"headerlink\" title=\"GNN 与 GCN\"></a>GNN 与 GCN</h1><blockquote>\n<p>图神经网络（Graph Neural Networks, GNN）和图卷积网络（Graph Convolutional Networks, GCN）是处理图数据的强大工具。本文将从理论到实践，全面介绍这两种重要的深度学习模型。</p>\n</blockquote>\n<p>本文主要介绍了<em>GNN和GCN的大致原理</em>，<em>GCN在PyG和PyTorch的实现</em> 以及它们在<em>DRP中的应用</em></p>","more":"<h2 id=\"🎯-Intro\"><a href=\"#🎯-Intro\" class=\"headerlink\" title=\"🎯 Intro\"></a>🎯 Intro</h2><p>在深度学习领域，处理图结构数据一直是一个具有挑战性的任务。传统的深度学习模型（如CNN、RNN）在处理欧几里得空间中的数据表现出色，但对于图这种非欧几里得结构的数据却显得力不从心。GNN和GCN的出现，为我们提供了处理图数据的有力工具。</p>\n<p>而在DRP领域，由于涉及到大量的Embedding，GCN现在几乎已经成为了必不可少的模块。</p>\n<p>但在开始各种各样的奇形怪状的GCN之前，了解GNN和GCN本身的实现仍然是非常必要的。<del>于鼠鼠而言</del>大致有以下理由：</p>\n<ol>\n<li>部分抽象的基于GCN的模块第三方库不一定支持</li>\n<li>由于反应表示数据的不平衡，我们可以构建的模型的层数是非常有限的（因为会过平滑）。因此对层内的改造就显得非常必要了。而这一切的前提便是理解原理捏</li>\n</ol>\n<p>在这里强烈建议去看一下<a href=\"https://distill.pub/\">Distill</a>的两篇有关图神经网络的博客，非常易懂。</p>\n<hr>\n<h2 id=\"📚-理论基础\"><a href=\"#📚-理论基础\" class=\"headerlink\" title=\"📚 理论基础\"></a>📚 理论基础</h2><h3 id=\"图的基本概念\"><a href=\"#图的基本概念\" class=\"headerlink\" title=\"图的基本概念\"></a>图的基本概念</h3><p>在开始之前，我们需要理解图的基本表示：</p>\n<ul>\n<li>图 $G &#x3D; (V, E)$，其中 $V$ 是节点集合，$E$ 是边集合</li>\n<li>邻接矩阵 $A \\in \\mathbb{R}^{n \\times n}$</li>\n<li>度矩阵 $D &#x3D; diag(d_1,…,d_n)$，其中 $d_i &#x3D; \\sum_j A_{ij}$</li>\n<li>节点特征矩阵 $X \\in \\mathbb{R}^{n \\times d}$</li>\n</ul>\n<h3 id=\"GNN框架\"><a href=\"#GNN框架\" class=\"headerlink\" title=\"GNN框架\"></a>GNN框架</h3><p>GNN的基本框架遵循消息传递范式（Message Passing Neural Network, MPNN），可以用以下数学公式表示：</p>\n<ol>\n<li><p><strong>消息传递阶段</strong>（Message Passing）：</p>\n<p>对于节点 $v$，从其邻居节点 $u \\in \\mathcal{N}(v)$ 收集信息：</p>\n<p>$$m_v^{(l)} &#x3D; \\sum_{u \\in \\mathcal{N}(v)} M_l(h_v^{(l-1)}, h_u^{(l-1)}, e_{uv})$$</p>\n<p>其中：</p>\n<ul>\n<li>$h_v^{(l-1)}$ 是节点 $v$ 在第 $l-1$ 层的特征</li>\n<li>$e_{uv}$ 是边 $(u,v)$ 的特征</li>\n<li>$M_l$ 是可学习的消息函数</li>\n</ul>\n</li>\n<li><p><strong>消息聚合阶段</strong>（Aggregation）：</p>\n<p>将收集到的消息进行聚合：</p>\n<p>$$a_v^{(l)} &#x3D; AGG({m_v^{(l)} | u \\in \\mathcal{N}(v)})$$</p>\n<p>常见的聚合函数包括：</p>\n<ul>\n<li>求和：$AGG_{sum} &#x3D; \\sum_{u \\in \\mathcal{N}(v)} m_u$</li>\n<li>平均：$AGG_{mean} &#x3D; \\frac{1}{|\\mathcal{N}(v)|} \\sum_{u \\in \\mathcal{N}(v)} m_u$</li>\n<li>最大：$AGG_{max} &#x3D; max_{u \\in \\mathcal{N}(v)} m_u$</li>\n</ul>\n</li>\n<li><p><strong>节点更新阶段</strong>（Update）：</p>\n<p>更新节点的表示：</p>\n<p>$$h_v^{(l)} &#x3D; U_l(h_v^{(l-1)}, a_v^{(l)})$$</p>\n<p>其中 $U_l$ 是可学习的更新函数，通常是MLP或其他神经网络。</p>\n</li>\n</ol>\n<h3 id=\"GCN实现\"><a href=\"#GCN实现\" class=\"headerlink\" title=\"GCN实现\"></a>GCN实现</h3><h4 id=\"拉普拉斯矩阵-🔍\"><a href=\"#拉普拉斯矩阵-🔍\" class=\"headerlink\" title=\"拉普拉斯矩阵 🔍\"></a>拉普拉斯矩阵 🔍</h4><p>拉普拉斯矩阵是图信号处理中的核心概念，有多种形式：</p>\n<ol>\n<li><p><strong>组合拉普拉斯矩阵</strong>：$L &#x3D; D - A$</p>\n</li>\n<li><p><strong>标准化拉普拉斯矩阵</strong>：$L_{sym} &#x3D; D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}} &#x3D; I - D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$</p>\n</li>\n<li><p><strong>随机游走拉普拉斯矩阵</strong>：$L_{rw} &#x3D; D^{-1}L &#x3D; I - D^{-1}A$</p>\n</li>\n</ol>\n<p>拉普拉斯矩阵的特性：</p>\n<ul>\n<li>对称性：$L &#x3D; L^T$</li>\n<li>半正定性：所有特征值非负</li>\n<li>最小特征值为0，对应的特征向量是常数向量</li>\n<li>特征值的重数对应图的连通分量数</li>\n</ul>\n<h4 id=\"从传统卷积到图卷积-🔄\"><a href=\"#从传统卷积到图卷积-🔄\" class=\"headerlink\" title=\"从传统卷积到图卷积 🔄\"></a>从传统卷积到图卷积 🔄</h4><h5 id=\"传统卷积回顾\"><a href=\"#传统卷积回顾\" class=\"headerlink\" title=\"传统卷积回顾\"></a>传统卷积回顾</h5><p>在欧几里得空间中，卷积操作定义为：</p>\n<p>$$(f * g)(p) &#x3D; \\sum_{q \\in \\mathcal{N}(p)} f(q) \\cdot g(p-q)$$</p>\n<p>这里的关键特点是：</p>\n<ul>\n<li>平移不变性</li>\n<li>局部性</li>\n<li>参数共享</li>\n</ul>\n<h5 id=\"图上的卷积定义\"><a href=\"#图上的卷积定义\" class=\"headerlink\" title=\"图上的卷积定义\"></a>图上的卷积定义</h5><p>在图域中，我们需要重新定义这些特性：</p>\n<ol>\n<li><p><strong>空间域卷积</strong>：<br>$$h_v &#x3D; \\sum_{u \\in \\mathcal{N}(v)} W(e_{u,v})h_u$$<br>其中 $W(e_{u,v})$ 是边的权重函数</p>\n</li>\n<li><p><strong>谱域卷积</strong>：<br>$$g_\\theta * x &#x3D; Ug_\\theta U^T x$$<br>其中 $U$ 是拉普拉斯矩阵的特征向量矩阵</p>\n</li>\n</ol>\n<h4 id=\"GCN的数学推导-⚙️\"><a href=\"#GCN的数学推导-⚙️\" class=\"headerlink\" title=\"GCN的数学推导 ⚙️\"></a>GCN的数学推导 ⚙️</h4><p>Kipf &amp; Welling提出的GCN模型中，单层传播规则为：</p>\n<p>$$H^{(l+1)} &#x3D; \\sigma(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)})$$</p>\n<p>其中：</p>\n<ul>\n<li>$\\tilde{A} &#x3D; A + I_N$ 是添加了自环的邻接矩阵</li>\n<li>$\\tilde{D}_{ii} &#x3D; \\sum_{j} \\tilde{A}_{ij}$ 是对应的度矩阵</li>\n<li>$H^{(l)}$ 是第 $l$ 层的激活值</li>\n<li>$W^{(l)}$ 是可学习的权重矩阵</li>\n<li>$\\sigma$ 是非线性激活函数</li>\n</ul>\n<p><del>一些自己的理解</del></p>\n<ol>\n<li>引入$L_{sym} &#x3D; \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$作为聚合（AGG）部分<ul>\n<li>添加自环：$\\tilde{A} &#x3D; A + I_N$</li>\n<li>计算归一化系数：$\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$</li>\n</ul>\n</li>\n<li>特征变换：$H^{(l)}W^{(l)}$</li>\n<li>邻域聚合：$\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)}$</li>\n<li>非线性变换：$\\sigma(\\cdot)$</li>\n</ol>\n<hr>\n<h2 id=\"💻-实现细节\"><a href=\"#💻-实现细节\" class=\"headerlink\" title=\"💻 实现细节\"></a>💻 实现细节</h2><p>基于这个理论框架的简单实现如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">message_passing</span>(<span class=\"params\">nodes, edges</span>):</span><br><span class=\"line\">    messages = &#123;&#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> edge <span class=\"keyword\">in</span> edges:</span><br><span class=\"line\">        src, dst = edge</span><br><span class=\"line\">        msg = compute_message(nodes[src], nodes[dst])</span><br><span class=\"line\">        messages.setdefault(dst, []).append(msg)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> messages</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">aggregate_messages</span>(<span class=\"params\">messages</span>):</span><br><span class=\"line\">    aggregated = &#123;&#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> node, msgs <span class=\"keyword\">in</span> messages.items():</span><br><span class=\"line\">        aggregated[node] = <span class=\"built_in\">sum</span>(msgs) / <span class=\"built_in\">len</span>(msgs)  <span class=\"comment\"># 平均聚合</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> aggregated</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">update_nodes</span>(<span class=\"params\">nodes, aggregated</span>):</span><br><span class=\"line\">    updated = &#123;&#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> node, agg_msg <span class=\"keyword\">in</span> aggregated.items():</span><br><span class=\"line\">        updated[node] = nodes[node] + agg_msg  <span class=\"comment\"># 残差连接</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> updated</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"PyTorch-Geometric实现-🚀\"><a href=\"#PyTorch-Geometric实现-🚀\" class=\"headerlink\" title=\"PyTorch Geometric实现 🚀\"></a>PyTorch Geometric实现 🚀</h3><blockquote>\n<p>本节代码基于 PyTorch 2.1.0 和 PyTorch Geometric 2.4.0 版本</p>\n</blockquote>\n<p>使用PyTorch Geometric库的GCN实现：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch_geometric.nn <span class=\"keyword\">import</span> GCNConv</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">GCN</span>(torch.nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, num_features, num_classes</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(GCN, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.conv1 = GCNConv(num_features, <span class=\"number\">16</span>)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.conv2 = GCNConv(<span class=\"number\">16</span>, num_classes)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, edge_index</span>):</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.conv1(x, edge_index)</span><br><span class=\"line\">        x = F.relu(x)</span><br><span class=\"line\">        x = F.dropout(x, training=<span class=\"variable language_\">self</span>.training)</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.conv2(x, edge_index)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.log_softmax(x, dim=<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"原生PyTorch实现-🔧\"><a href=\"#原生PyTorch实现-🔧\" class=\"headerlink\" title=\"原生PyTorch实现 🔧\"></a>原生PyTorch实现 🔧</h3><blockquote>\n<p>本节代码基于 PyTorch 2.1.0、NumPy 1.24.0 和 SciPy 1.11.0 版本</p>\n</blockquote>\n<p>不使用PyG，手动实现GCN<del>主要是目前不太清楚主流的HGCN的实现方式捏</del>：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">import</span> scipy.sparse <span class=\"keyword\">as</span> sp</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">GCNLayer</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, in_features, out_features</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(GCNLayer, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.W = nn.Parameter(torch.FloatTensor(in_features, out_features))</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.reset_parameters()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">reset_parameters</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        nn.init.kaiming_uniform_(<span class=\"variable language_\">self</span>.W)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, adj</span>):</span><br><span class=\"line\">        <span class=\"comment\"># adj: 归一化的邻接矩阵</span></span><br><span class=\"line\">        support = torch.mm(x, <span class=\"variable language_\">self</span>.W)</span><br><span class=\"line\">        output = torch.sparse.mm(adj, support)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">GCN</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, nfeat, nhid, nclass, dropout</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(GCN, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.gc1 = GCNLayer(nfeat, nhid)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.gc2 = GCNLayer(nhid, nclass)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = dropout</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, adj</span>):</span><br><span class=\"line\">        x = F.relu(<span class=\"variable language_\">self</span>.gc1(x, adj))</span><br><span class=\"line\">        x = F.dropout(x, <span class=\"variable language_\">self</span>.dropout, training=<span class=\"variable language_\">self</span>.training)</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.gc2(x, adj)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.log_softmax(x, dim=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">normalize_adj</span>(<span class=\"params\">adj</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;归一化邻接矩阵&quot;&quot;&quot;</span></span><br><span class=\"line\">    adj = sp.coo_matrix(adj)</span><br><span class=\"line\">    rowsum = np.array(adj.<span class=\"built_in\">sum</span>(<span class=\"number\">1</span>))</span><br><span class=\"line\">    d_inv_sqrt = np.power(rowsum, -<span class=\"number\">0.5</span>).flatten()</span><br><span class=\"line\">    d_inv_sqrt[np.isinf(d_inv_sqrt)] = <span class=\"number\">0.</span></span><br><span class=\"line\">    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()</span><br></pre></td></tr></table></figure>\n\n<hr>\n<h2 id=\"🎮-应用场景\"><a href=\"#🎮-应用场景\" class=\"headerlink\" title=\"🎮 应用场景\"></a>🎮 应用场景</h2><p><del>由于鼠鼠就是个臭写DRP的捏</del> 这里只给出GNN在DRP中的应用</p>\n<ol>\n<li><p><strong>药物表示</strong></p>\n<ul>\n<li><em>分子图构建</em>：将药物SMILES字符串转换为图结构，节点表示原子（含原子类型、电荷等特征），边表示化学键（如键类型、距离）。  </li>\n<li><em>GNN编码</em>：使用图卷积网络（GCN）、图注意力网络（GAT）或图同构网络（GIN）等层迭代聚合邻域信息，生成药物嵌入（embedding）。例如，GraTransDRP（2022）结合GAT和Transformer提升药物表征能力。</li>\n</ul>\n</li>\n<li><p><strong>癌症表示</strong></p>\n<ul>\n<li><em>生物网络构建</em>：基于基因互作（如STRING数据库的蛋白-蛋白互作）、基因共表达或通路信息构建异质图。例如，AGMI（2021）整合多组学数据和PPI网络，通过GNN学习癌症样本的联合表征。  </li>\n<li><em>多组学融合</em>：部分模型（如TGSA）利用GNN整合基因组、转录组等数据，通过跨模态注意力机制增强特征交互。</li>\n</ul>\n</li>\n<li><p><strong>异构图与联合建模</strong></p>\n<ul>\n<li><em>细胞系-药物异构图</em>：如GraphCDR（2021）将细胞系和药物作为两类节点，通过边连接已知响应对，直接学习跨实体关系。  </li>\n<li><em>知识增强</em>：预训练GNN于大规模生物化学属性预测（如Zhu et al., 2021），再迁移至DRP任务，提升泛化性。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"🎯-总结与展望\"><a href=\"#🎯-总结与展望\" class=\"headerlink\" title=\"🎯 总结与展望\"></a>🎯 总结与展望</h2><ul>\n<li><strong>动态图建模</strong>：捕捉治疗过程中动态变化的生物网络。  </li>\n<li><strong>三维分子图</strong>：结合几何深度学习（如SchNet）提升立体化学感知。  </li>\n<li><strong>基准测试</strong>：需统一评估协议（如固定数据集和指标）以公平比较GNN与其他方法。</li>\n</ul>\n<p><del>之后应该会写一些具体模型的博客，有相关的会直接上链接的捏jrm</del></p>\n<h1 id=\"📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"><a href=\"#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\" class=\"headerlink\" title=\"📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1><p><a href=\"/paper/1609.02907v4.pdf\" target=\"_blank\">📄 Thomas - SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</a><br><a href=\"https://pytorch-geometric.readthedocs.io/\" target=\"_blank\">PyTorch Geometric 官方文档</a><br><a href=\"https://distill.pub/2021/gnn-intro/\" target=\"_blank\">Distill: A Gentle Introduction to Graph Neural Networks</a><br><a href=\"https://distill.pub/2021/understanding-gnns/\" target=\"_blank\">Distill: Understanding Convolutions on Graphs</a><br><a href=\"https://www.zhihu.com/tardis/zm/art/107162772\" target=\"_blank\">知乎：图卷积网络（GCN）入门详解</a><br><a href=\"https://github.com/tkipf/gcn\" target=\"_blank\">GCN 论文官方代码（GitHub）</a></p>"},{"title":"hwadee","date":"2025-07-13T13:56:45.000Z","_content":"\n# \n先放在这里，懒得喷，以下是鼠鼠和室友在争论某问题时问的AI\n<!-- more -->\n\n# 甲醛有气味吗\nQus:\n甲醛有气味吗\nAns:\n- 引用 7 篇资料作为参考\n    1. [甲醛（有机化合物）_百度百科](https://baike.baidu.com/item/甲醛/167033?fr=aladdin)\n    2. [汞（是常温常压下唯一以液态存在的金属）_百度百科](http://baike.baidu.com/item/%E6%B0%B4%E9%93%B6/572967?fromModule=lemma_inlink)\n    3. [【甲醛有味吗】_徐信群_主任医师 - 大众养生网](https://www.cndzys.com/ylcore/wendanew_info/11es473224.html)\n    4. [装修完的新家闻不见味儿，就是没甲醛了？](http://mp.weixin.qq.com/s?__biz=MzA4ODA4MjY4MA==&mid=2650899715&idx=2&sn=19f245dad403038c8fcbd5cc242b82c6&chksm=8a413842f16d27e52c9809f4dc290c194d4bdd131d46559db9eb9591a59757398eea9bb2a724#rd)\n    5. [甲醛有味道吗，甲醛的气味像什么？](http://mp.weixin.qq.com/s?__biz=MzA5MDY2NTMzNg==&mid=2676835975&idx=3&sn=f0b45599c2c3427a8824a0f99159cc71&chksm=8bbb6131ad4acdd4f1b70ed654da5089241c06a940b56adc749d86160352f04e1d9f6b41e620#rd)\n    6. [甲醛有味还是无味-有来医生](https://m.youlai.cn/sjingbian/article/5B3495gV5qk.html)\n    7. [装修的味道是甲醛吗_齐家网](https://m.jia.com/zixun/article/1144566.html)\n甲醛的气味特性与其浓度密切相关，具体表现如下：\n### 1. 低浓度时通常无味\n- 甲醛在极低浓度（如低于0.08毫克/立方米）时，是一种无色无味的气体，难以通过嗅觉直接察觉。\n- 许多人误以为“没味道=没甲醛”，但实际上低浓度甲醛可能仍然存在，只是不易被闻到。\n### 2. 高浓度时有强烈刺激性气味\n- 当甲醛浓度超过0.1毫克/立方米时，会与空气中的水、氧气反应，生成具有刺鼻气味的化合物，常被描述为：\n    - 类似燃烧塑料或化学药品的味道；\n    - 锯木屑混合酸味（家具中甲醛）；\n    - 汽油味或烧焦塑料味（布料、窗帘中甲醛）；\n    - 部分人可能感知到淡淡的甜味，但主要仍是刺激性。\n### 3. 个体差异显著\n- 不同人对甲醛气味的敏感度差异较大，有些人可能在较低浓度就能察觉，而另一些人需要更高浓度才会感到不适。\n### 4. 与其他装修气味的区别\n- 装修后的刺鼻气味常被误认为全是甲醛，但实际上可能混合了**苯系物（油漆味）、氨气（厕所味）、TVOC（化学溶剂味）**等。\n- 甲醛的刺激性更偏向黏膜反应（如眼痒、喉咙痛），而苯系物等可能带有更明显的“化学香味”。\n### 5. 注意事项\n- 不能依赖嗅觉判断甲醛安全性：即使闻不到，甲醛仍可能超标（尤其来自人造板材的长期释放）。\n- 科学检测更可靠：家用检测仪易受环境干扰，建议通过专业机构检测。\n总结：甲醛在低浓度下无味，高浓度时呈现刺鼻气味，但气味并非判断其存在的唯一标准，需结合专业检测和通风措施保障健康。\n\n# 📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒","source":"_posts/hwadee.md","raw":"---\ntitle: hwadee\ndate: 2025-07-13 21:56:45\ncategories:\ntags:\n---\n\n# \n先放在这里，懒得喷，以下是鼠鼠和室友在争论某问题时问的AI\n<!-- more -->\n\n# 甲醛有气味吗\nQus:\n甲醛有气味吗\nAns:\n- 引用 7 篇资料作为参考\n    1. [甲醛（有机化合物）_百度百科](https://baike.baidu.com/item/甲醛/167033?fr=aladdin)\n    2. [汞（是常温常压下唯一以液态存在的金属）_百度百科](http://baike.baidu.com/item/%E6%B0%B4%E9%93%B6/572967?fromModule=lemma_inlink)\n    3. [【甲醛有味吗】_徐信群_主任医师 - 大众养生网](https://www.cndzys.com/ylcore/wendanew_info/11es473224.html)\n    4. [装修完的新家闻不见味儿，就是没甲醛了？](http://mp.weixin.qq.com/s?__biz=MzA4ODA4MjY4MA==&mid=2650899715&idx=2&sn=19f245dad403038c8fcbd5cc242b82c6&chksm=8a413842f16d27e52c9809f4dc290c194d4bdd131d46559db9eb9591a59757398eea9bb2a724#rd)\n    5. [甲醛有味道吗，甲醛的气味像什么？](http://mp.weixin.qq.com/s?__biz=MzA5MDY2NTMzNg==&mid=2676835975&idx=3&sn=f0b45599c2c3427a8824a0f99159cc71&chksm=8bbb6131ad4acdd4f1b70ed654da5089241c06a940b56adc749d86160352f04e1d9f6b41e620#rd)\n    6. [甲醛有味还是无味-有来医生](https://m.youlai.cn/sjingbian/article/5B3495gV5qk.html)\n    7. [装修的味道是甲醛吗_齐家网](https://m.jia.com/zixun/article/1144566.html)\n甲醛的气味特性与其浓度密切相关，具体表现如下：\n### 1. 低浓度时通常无味\n- 甲醛在极低浓度（如低于0.08毫克/立方米）时，是一种无色无味的气体，难以通过嗅觉直接察觉。\n- 许多人误以为“没味道=没甲醛”，但实际上低浓度甲醛可能仍然存在，只是不易被闻到。\n### 2. 高浓度时有强烈刺激性气味\n- 当甲醛浓度超过0.1毫克/立方米时，会与空气中的水、氧气反应，生成具有刺鼻气味的化合物，常被描述为：\n    - 类似燃烧塑料或化学药品的味道；\n    - 锯木屑混合酸味（家具中甲醛）；\n    - 汽油味或烧焦塑料味（布料、窗帘中甲醛）；\n    - 部分人可能感知到淡淡的甜味，但主要仍是刺激性。\n### 3. 个体差异显著\n- 不同人对甲醛气味的敏感度差异较大，有些人可能在较低浓度就能察觉，而另一些人需要更高浓度才会感到不适。\n### 4. 与其他装修气味的区别\n- 装修后的刺鼻气味常被误认为全是甲醛，但实际上可能混合了**苯系物（油漆味）、氨气（厕所味）、TVOC（化学溶剂味）**等。\n- 甲醛的刺激性更偏向黏膜反应（如眼痒、喉咙痛），而苯系物等可能带有更明显的“化学香味”。\n### 5. 注意事项\n- 不能依赖嗅觉判断甲醛安全性：即使闻不到，甲醛仍可能超标（尤其来自人造板材的长期释放）。\n- 科学检测更可靠：家用检测仪易受环境干扰，建议通过专业机构检测。\n总结：甲醛在低浓度下无味，高浓度时呈现刺鼻气味，但气味并非判断其存在的唯一标准，需结合专业检测和通风措施保障健康。\n\n# 📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒","slug":"hwadee","published":1,"updated":"2025-07-13T13:58:51.190Z","comments":1,"layout":"post","photos":[],"_id":"cmd39u3jp001lqk41aoy9b4bp","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>先放在这里，懒得喷，以下是鼠鼠和室友在争论某问题时问的AI</p>\n<span id=\"more\"></span>\n\n<h1 id=\"甲醛有气味吗\"><a href=\"#甲醛有气味吗\" class=\"headerlink\" title=\"甲醛有气味吗\"></a>甲醛有气味吗</h1><p>Qus:<br>甲醛有气味吗<br>Ans:</p>\n<ul>\n<li>引用 7 篇资料作为参考<ol>\n<li><a href=\"https://baike.baidu.com/item/%E7%94%B2%E9%86%9B/167033?fr=aladdin\">甲醛（有机化合物）_百度百科</a></li>\n<li><a href=\"http://baike.baidu.com/item/%E6%B0%B4%E9%93%B6/572967?fromModule=lemma_inlink\">汞（是常温常压下唯一以液态存在的金属）_百度百科</a></li>\n<li><a href=\"https://www.cndzys.com/ylcore/wendanew_info/11es473224.html\">【甲醛有味吗】_徐信群_主任医师 - 大众养生网</a></li>\n<li><a href=\"http://mp.weixin.qq.com/s?__biz=MzA4ODA4MjY4MA==&mid=2650899715&idx=2&sn=19f245dad403038c8fcbd5cc242b82c6&chksm=8a413842f16d27e52c9809f4dc290c194d4bdd131d46559db9eb9591a59757398eea9bb2a724#rd\">装修完的新家闻不见味儿，就是没甲醛了？</a></li>\n<li><a href=\"http://mp.weixin.qq.com/s?__biz=MzA5MDY2NTMzNg==&mid=2676835975&idx=3&sn=f0b45599c2c3427a8824a0f99159cc71&chksm=8bbb6131ad4acdd4f1b70ed654da5089241c06a940b56adc749d86160352f04e1d9f6b41e620#rd\">甲醛有味道吗，甲醛的气味像什么？</a></li>\n<li><a href=\"https://m.youlai.cn/sjingbian/article/5B3495gV5qk.html\">甲醛有味还是无味-有来医生</a></li>\n<li><a href=\"https://m.jia.com/zixun/article/1144566.html\">装修的味道是甲醛吗_齐家网</a><br>甲醛的气味特性与其浓度密切相关，具体表现如下：</li>\n</ol>\n</li>\n</ul>\n<h3 id=\"1-低浓度时通常无味\"><a href=\"#1-低浓度时通常无味\" class=\"headerlink\" title=\"1. 低浓度时通常无味\"></a>1. 低浓度时通常无味</h3><ul>\n<li>甲醛在极低浓度（如低于0.08毫克&#x2F;立方米）时，是一种无色无味的气体，难以通过嗅觉直接察觉。</li>\n<li>许多人误以为“没味道&#x3D;没甲醛”，但实际上低浓度甲醛可能仍然存在，只是不易被闻到。</li>\n</ul>\n<h3 id=\"2-高浓度时有强烈刺激性气味\"><a href=\"#2-高浓度时有强烈刺激性气味\" class=\"headerlink\" title=\"2. 高浓度时有强烈刺激性气味\"></a>2. 高浓度时有强烈刺激性气味</h3><ul>\n<li>当甲醛浓度超过0.1毫克&#x2F;立方米时，会与空气中的水、氧气反应，生成具有刺鼻气味的化合物，常被描述为：<ul>\n<li>类似燃烧塑料或化学药品的味道；</li>\n<li>锯木屑混合酸味（家具中甲醛）；</li>\n<li>汽油味或烧焦塑料味（布料、窗帘中甲醛）；</li>\n<li>部分人可能感知到淡淡的甜味，但主要仍是刺激性。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-个体差异显著\"><a href=\"#3-个体差异显著\" class=\"headerlink\" title=\"3. 个体差异显著\"></a>3. 个体差异显著</h3><ul>\n<li>不同人对甲醛气味的敏感度差异较大，有些人可能在较低浓度就能察觉，而另一些人需要更高浓度才会感到不适。</li>\n</ul>\n<h3 id=\"4-与其他装修气味的区别\"><a href=\"#4-与其他装修气味的区别\" class=\"headerlink\" title=\"4. 与其他装修气味的区别\"></a>4. 与其他装修气味的区别</h3><ul>\n<li>装修后的刺鼻气味常被误认为全是甲醛，但实际上可能混合了**苯系物（油漆味）、氨气（厕所味）、TVOC（化学溶剂味）**等。</li>\n<li>甲醛的刺激性更偏向黏膜反应（如眼痒、喉咙痛），而苯系物等可能带有更明显的“化学香味”。</li>\n</ul>\n<h3 id=\"5-注意事项\"><a href=\"#5-注意事项\" class=\"headerlink\" title=\"5. 注意事项\"></a>5. 注意事项</h3><ul>\n<li>不能依赖嗅觉判断甲醛安全性：即使闻不到，甲醛仍可能超标（尤其来自人造板材的长期释放）。</li>\n<li>科学检测更可靠：家用检测仪易受环境干扰，建议通过专业机构检测。<br>总结：甲醛在低浓度下无味，高浓度时呈现刺鼻气味，但气味并非判断其存在的唯一标准，需结合专业检测和通风措施保障健康。</li>\n</ul>\n<h1 id=\"📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"><a href=\"#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\" class=\"headerlink\" title=\"📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1>","length":650,"excerpt":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h1><p>先放在这里，懒得喷，以下是鼠鼠和室友在争论某问题时问的AI</p>","more":"<h1 id=\"甲醛有气味吗\"><a href=\"#甲醛有气味吗\" class=\"headerlink\" title=\"甲醛有气味吗\"></a>甲醛有气味吗</h1><p>Qus:<br>甲醛有气味吗<br>Ans:</p>\n<ul>\n<li>引用 7 篇资料作为参考<ol>\n<li><a href=\"https://baike.baidu.com/item/%E7%94%B2%E9%86%9B/167033?fr=aladdin\">甲醛（有机化合物）_百度百科</a></li>\n<li><a href=\"http://baike.baidu.com/item/%E6%B0%B4%E9%93%B6/572967?fromModule=lemma_inlink\">汞（是常温常压下唯一以液态存在的金属）_百度百科</a></li>\n<li><a href=\"https://www.cndzys.com/ylcore/wendanew_info/11es473224.html\">【甲醛有味吗】_徐信群_主任医师 - 大众养生网</a></li>\n<li><a href=\"http://mp.weixin.qq.com/s?__biz=MzA4ODA4MjY4MA==&mid=2650899715&idx=2&sn=19f245dad403038c8fcbd5cc242b82c6&chksm=8a413842f16d27e52c9809f4dc290c194d4bdd131d46559db9eb9591a59757398eea9bb2a724#rd\">装修完的新家闻不见味儿，就是没甲醛了？</a></li>\n<li><a href=\"http://mp.weixin.qq.com/s?__biz=MzA5MDY2NTMzNg==&mid=2676835975&idx=3&sn=f0b45599c2c3427a8824a0f99159cc71&chksm=8bbb6131ad4acdd4f1b70ed654da5089241c06a940b56adc749d86160352f04e1d9f6b41e620#rd\">甲醛有味道吗，甲醛的气味像什么？</a></li>\n<li><a href=\"https://m.youlai.cn/sjingbian/article/5B3495gV5qk.html\">甲醛有味还是无味-有来医生</a></li>\n<li><a href=\"https://m.jia.com/zixun/article/1144566.html\">装修的味道是甲醛吗_齐家网</a><br>甲醛的气味特性与其浓度密切相关，具体表现如下：</li>\n</ol>\n</li>\n</ul>\n<h3 id=\"1-低浓度时通常无味\"><a href=\"#1-低浓度时通常无味\" class=\"headerlink\" title=\"1. 低浓度时通常无味\"></a>1. 低浓度时通常无味</h3><ul>\n<li>甲醛在极低浓度（如低于0.08毫克&#x2F;立方米）时，是一种无色无味的气体，难以通过嗅觉直接察觉。</li>\n<li>许多人误以为“没味道&#x3D;没甲醛”，但实际上低浓度甲醛可能仍然存在，只是不易被闻到。</li>\n</ul>\n<h3 id=\"2-高浓度时有强烈刺激性气味\"><a href=\"#2-高浓度时有强烈刺激性气味\" class=\"headerlink\" title=\"2. 高浓度时有强烈刺激性气味\"></a>2. 高浓度时有强烈刺激性气味</h3><ul>\n<li>当甲醛浓度超过0.1毫克&#x2F;立方米时，会与空气中的水、氧气反应，生成具有刺鼻气味的化合物，常被描述为：<ul>\n<li>类似燃烧塑料或化学药品的味道；</li>\n<li>锯木屑混合酸味（家具中甲醛）；</li>\n<li>汽油味或烧焦塑料味（布料、窗帘中甲醛）；</li>\n<li>部分人可能感知到淡淡的甜味，但主要仍是刺激性。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-个体差异显著\"><a href=\"#3-个体差异显著\" class=\"headerlink\" title=\"3. 个体差异显著\"></a>3. 个体差异显著</h3><ul>\n<li>不同人对甲醛气味的敏感度差异较大，有些人可能在较低浓度就能察觉，而另一些人需要更高浓度才会感到不适。</li>\n</ul>\n<h3 id=\"4-与其他装修气味的区别\"><a href=\"#4-与其他装修气味的区别\" class=\"headerlink\" title=\"4. 与其他装修气味的区别\"></a>4. 与其他装修气味的区别</h3><ul>\n<li>装修后的刺鼻气味常被误认为全是甲醛，但实际上可能混合了**苯系物（油漆味）、氨气（厕所味）、TVOC（化学溶剂味）**等。</li>\n<li>甲醛的刺激性更偏向黏膜反应（如眼痒、喉咙痛），而苯系物等可能带有更明显的“化学香味”。</li>\n</ul>\n<h3 id=\"5-注意事项\"><a href=\"#5-注意事项\" class=\"headerlink\" title=\"5. 注意事项\"></a>5. 注意事项</h3><ul>\n<li>不能依赖嗅觉判断甲醛安全性：即使闻不到，甲醛仍可能超标（尤其来自人造板材的长期释放）。</li>\n<li>科学检测更可靠：家用检测仪易受环境干扰，建议通过专业机构检测。<br>总结：甲醛在低浓度下无味，高浓度时呈现刺鼻气味，但气味并非判断其存在的唯一标准，需结合专业检测和通风措施保障健康。</li>\n</ul>\n<h1 id=\"📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"><a href=\"#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\" class=\"headerlink\" title=\"📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1>"},{"title":"PEP 8","date":"2025-07-08T03:41:30.000Z","_content":"\n# Style Guide for Python Code\n\n> [PEP8](https://peps.python.org/pep-0008/) 是 Python 社群共通的風格指南，一開始是 Python 之父 Guido van Rossum 自己的撰碼風格，慢慢後來演變至今，目的在於幫助開發者寫出可讀性高且風格一致的程式。許多開源計畫，例如 Django 、 OpenStack 等都是以 PEP8 為基礎再加上自己的風格建議。\n\n这篇博客主要是为了在搭建自己的模型之前学习一下一些统一的规范是做的记录 ~~主要是目前读到的大多数论文的源码目命名没有规律~~ ，以加强之后搭建模型时代码的可读性\n\n另外，本博客只展示本人不太熟悉的捏\n\n<!-- more -->\n\n## 代码布局\n\n### 缩进\n\n**每个缩进级别使用 4 个空格**\n\n对于比较臭长的函数，可以使用*悬挂缩进*\n\n```Python\n# Correct:\n\n# Aligned with opening delimiter.\nfoo = long_function_name(var_one, var_two,\n                         var_three, var_four)\n\n# Add 4 spaces (an extra level of indentation) to distinguish arguments from the rest.\ndef long_function_name(\n        var_one, var_two, var_three,\n        var_four):\n    print(var_one)\n\n# Hanging indents should add a level.\nfoo = long_function_name(\n    var_one, var_two,\n    var_three, var_four)\n```\n\n```Python\n# Wrong:\n\n# Arguments on first line forbidden when not using vertical alignment.\nfoo = long_function_name(var_one, var_two,\n    var_three, var_four)\n\n# Further indentation required as indentation is not distinguishable.\ndef long_function_name(\n    var_one, var_two, var_three,\n    var_four):\n    print(var_one)\n```\n\n优先使用 _Tabs_ 进行缩进， _Tabs_ 和 _Spaces_ 不能混用\n\n### 每行最多字符数量\n\n**79** 个\n\n合理使用反斜杠\n\n```Python\nwith open('/path/to/some/file/you/want/to/read') as file_1, \\\n     open('/path/to/some/file/being/written', 'w') as file_2:\n    file_2.write(file_1.read())\n```\n\n### 二元运算符之前换行\n\n为了更好的确定该 `item` 采取的是什么运算\n\n```Python\n# Wrong:\n# operators sit far away from their operands\nincome = (gross_wages +\n          taxable_interest +\n          (dividends - qualified_dividends) -\n          ira_deduction -\n          student_loan_interest)\n```\n\n```Python\n# Correct:\n# easy to match operators with operands\nincome = (gross_wages\n          + taxable_interest\n          + (dividends - qualified_dividends)\n          - ira_deduction\n          - student_loan_interest)\n```\n\n### 如何空行（Blank Lines）\n\n_顶级函数_ 和 _类_ 之间空 **2** 行\n\n_类中的函数_ 空 **1** 行\n\n### import\n\n- 通常每一个库 **单独一行**（也有例外）\n\n```Python\nimport os\nimport sys\n\nfrom subprocess import Popen, PIPE\n```\n\n- 按以下顺序分组，每组间空行\n  1. **标准库**导入\n  2. **相关第三方库**导入\n  3. **特定的本地库**导入\n\n## 注释\n\n> Comments that contradict the code are worse than no comments.\n\n## 命名约定\n\n1. **类名** 用 **大驼峰**\n2. **函数名** 用 **小写下划线**\n3. 关于 _下划线_\n\n   - _单下划线_ 用于占位\n\n   ```Python\n   for _ in range(10):\n       print(random.randint(1, 100))\n   ```\n\n   - _单下划线_ 用于变量前表示该变量为 **弱私有** （语义上的 private），能调用但不能 import\n   - _双下划线_ 用于变量前表示该变量为 **强私有** （实际上也不能调用~~实现方式是重名名~~）\n     为了更好的说明这两点，给出以下两个测试程序\n\n   ```Python\n    \"\"\"\n    test_private_vars.py\n    This file is used to test the private variables in Python.\n    \"\"\"\n    class TestClass:\n        def __init__(self):\n            self.public_var = \"这是公有变量\"\n            self._weak_private = \"这是弱私有变量\"\n            self.__strong_private = \"这是强私有变量\"\n\n        def print_all_vars(self):\n            print(f\"从内部访问:\")\n            print(f\"公有变量: {self.public_var}\")\n            print(f\"弱私有变量: {self._weak_private}\")\n            print(f\"强私有变量: {self.__strong_private}\")\n\n    # 创建测试实例\n    test = TestClass()\n\n    # 1. 测试从类内部访问（通过方法）\n    print(\"\\n=== 测试1: 从类内部访问所有变量 ===\")\n    test.print_all_vars()\n\n    # 2. 测试从外部直接访问\n    print(\"\\n=== 测试2: 从外部访问变量 ===\")\n    print(f\"访问公有变量: {test.public_var}\")\n    print(f\"访问弱私有变量: {test._weak_private}\")  # 能访问，但IDE会警告\n    try:\n        print(f\"访问强私有变量: {test.__strong_private}\")\n    except AttributeError as e:\n        print(f\"访问强私有变量失败: {e}\")\n\n    # 3. 测试名称改写机制\n    print(\"\\n=== 测试3: 验证强私有变量的名称改写机制 ===\")\n    # 实际上Python会将__strong_private改写为_TestClass__strong_private\n    print(f\"通过改写后的名称访问强私有变量: {test._TestClass__strong_private}\")\n\n    # 4. 测试导入行为\n    if __name__ == \"__main__\":\n        print(\"\\n=== 测试4: 创建第二个文件并尝试导入 ===\")\n        print(\"请创建 test_import.py 并运行来测试导入行为\")\n   ```\n\n   ```Python\n   \"\"\"\n   test_import.py\n   This file is used to test the import of private variables in Python.\n   \"\"\"\n   from test_private_vars import TestClass\n\n   print(\"=== 测试导入后的访问行为 ===\")\n   test = TestClass()\n\n   # 测试访问公有变量\n   print(f\"访问公有变量: {test.public_var}\")\n\n   # 测试访问弱私有变量\n   try:\n       print(f\"访问弱私有变量: {test._weak_private}\")\n       print(\"注意：虽然能访问弱私有变量，但这违反了Python的约定\")\n   except AttributeError as e:\n       print(f\"访问弱私有变量失败: {e}\")\n\n   # 测试访问强私有变量\n   try:\n       print(f\"访问强私有变量: {test.__strong_private}\")\n   except AttributeError as e:\n       print(f\"访问强私有变量失败: {e}\")\n\n   # 测试通过名称改写访问强私有变量\n   try:\n       print(f\"通过改写后的名称访问强私有变量: {test._TestClass__strong_private}\")\n       print(\"注意：虽然能通过名称改写访问强私有变量，但这是一个不推荐的做法\")\n   except AttributeError as e:\n       print(f\"通过改写名称访问强私有变量失败: {e}\")\n   ```\n\n   以下是运行 `python test_private_vars.py` 的结果\n\n   ```bash\n   === 测试1: 从类内部访问所有变量 ===\n   从内部访问:\n   公有变量: 这是公有变量\n   弱私有变量: 这是弱私有变量\n   强私有变量: 这是强私有变量\n\n   === 测试2: 从外部访问变量 ===\n   访问公有变量: 这是公有变量\n   访问弱私有变量: 这是弱私有变量\n   访问强私有变量失败: 'TestClass' object has no attribute '__strong_private'\n\n   === 测试3: 验证强私有变量的名称改写机制 ===\n   通过改写后的名称访问强私有变量: 这是强私有变量\n\n   === 测试4: 创建第二个文件并尝试导入 ===\n   请创建 test_import.py 并运行来测试导入行为\n   ```\n\n   以下是运行 `python test_import.py` 的结果\n\n   ```bash\n   === 测试1: 从类内部访问所有变量 ===\n   从内部访问:\n   公有变量: 这是公有变量\n   弱私有变量: 这是弱私有变量\n   强私有变量: 这是强私有变量\n\n   === 测试2: 从外部访问变量 ===\n   访问公有变量: 这是公有变量\n   访问弱私有变量: 这是弱私有变量\n   访问强私有变量失败: 'TestClass' object has no attribute '__strong_private'\n\n   === 测试3: 验证强私有变量的名称改写机制 ===\n   通过改写后的名称访问强私有变量: 这是强私有变量\n   === 测试导入后的访问行为 ===\n   访问公有变量: 这是公有变量\n   访问弱私有变量: 这是弱私有变量\n   注意：虽然能访问弱私有变量，但这违反了Python的约定\n   访问强私有变量失败: 'TestClass' object has no attribute '__strong_private'\n   通过改写后的名称访问强私有变量: 这是强私有变量\n   注意：虽然能通过名称改写访问强私有变量，但这是一个不推荐的做法\n   ```\n","source":"_posts/PEP-8.md","raw":"---\ntitle: PEP 8\ndate: 2025-07-08 11:41:30\ntags:\n  - CDR\n  - Python\n  - PEP\n  - 闲🉐无聊\n  - 大概率没用\ncategories:\n  - Python\n---\n\n# Style Guide for Python Code\n\n> [PEP8](https://peps.python.org/pep-0008/) 是 Python 社群共通的風格指南，一開始是 Python 之父 Guido van Rossum 自己的撰碼風格，慢慢後來演變至今，目的在於幫助開發者寫出可讀性高且風格一致的程式。許多開源計畫，例如 Django 、 OpenStack 等都是以 PEP8 為基礎再加上自己的風格建議。\n\n这篇博客主要是为了在搭建自己的模型之前学习一下一些统一的规范是做的记录 ~~主要是目前读到的大多数论文的源码目命名没有规律~~ ，以加强之后搭建模型时代码的可读性\n\n另外，本博客只展示本人不太熟悉的捏\n\n<!-- more -->\n\n## 代码布局\n\n### 缩进\n\n**每个缩进级别使用 4 个空格**\n\n对于比较臭长的函数，可以使用*悬挂缩进*\n\n```Python\n# Correct:\n\n# Aligned with opening delimiter.\nfoo = long_function_name(var_one, var_two,\n                         var_three, var_four)\n\n# Add 4 spaces (an extra level of indentation) to distinguish arguments from the rest.\ndef long_function_name(\n        var_one, var_two, var_three,\n        var_four):\n    print(var_one)\n\n# Hanging indents should add a level.\nfoo = long_function_name(\n    var_one, var_two,\n    var_three, var_four)\n```\n\n```Python\n# Wrong:\n\n# Arguments on first line forbidden when not using vertical alignment.\nfoo = long_function_name(var_one, var_two,\n    var_three, var_four)\n\n# Further indentation required as indentation is not distinguishable.\ndef long_function_name(\n    var_one, var_two, var_three,\n    var_four):\n    print(var_one)\n```\n\n优先使用 _Tabs_ 进行缩进， _Tabs_ 和 _Spaces_ 不能混用\n\n### 每行最多字符数量\n\n**79** 个\n\n合理使用反斜杠\n\n```Python\nwith open('/path/to/some/file/you/want/to/read') as file_1, \\\n     open('/path/to/some/file/being/written', 'w') as file_2:\n    file_2.write(file_1.read())\n```\n\n### 二元运算符之前换行\n\n为了更好的确定该 `item` 采取的是什么运算\n\n```Python\n# Wrong:\n# operators sit far away from their operands\nincome = (gross_wages +\n          taxable_interest +\n          (dividends - qualified_dividends) -\n          ira_deduction -\n          student_loan_interest)\n```\n\n```Python\n# Correct:\n# easy to match operators with operands\nincome = (gross_wages\n          + taxable_interest\n          + (dividends - qualified_dividends)\n          - ira_deduction\n          - student_loan_interest)\n```\n\n### 如何空行（Blank Lines）\n\n_顶级函数_ 和 _类_ 之间空 **2** 行\n\n_类中的函数_ 空 **1** 行\n\n### import\n\n- 通常每一个库 **单独一行**（也有例外）\n\n```Python\nimport os\nimport sys\n\nfrom subprocess import Popen, PIPE\n```\n\n- 按以下顺序分组，每组间空行\n  1. **标准库**导入\n  2. **相关第三方库**导入\n  3. **特定的本地库**导入\n\n## 注释\n\n> Comments that contradict the code are worse than no comments.\n\n## 命名约定\n\n1. **类名** 用 **大驼峰**\n2. **函数名** 用 **小写下划线**\n3. 关于 _下划线_\n\n   - _单下划线_ 用于占位\n\n   ```Python\n   for _ in range(10):\n       print(random.randint(1, 100))\n   ```\n\n   - _单下划线_ 用于变量前表示该变量为 **弱私有** （语义上的 private），能调用但不能 import\n   - _双下划线_ 用于变量前表示该变量为 **强私有** （实际上也不能调用~~实现方式是重名名~~）\n     为了更好的说明这两点，给出以下两个测试程序\n\n   ```Python\n    \"\"\"\n    test_private_vars.py\n    This file is used to test the private variables in Python.\n    \"\"\"\n    class TestClass:\n        def __init__(self):\n            self.public_var = \"这是公有变量\"\n            self._weak_private = \"这是弱私有变量\"\n            self.__strong_private = \"这是强私有变量\"\n\n        def print_all_vars(self):\n            print(f\"从内部访问:\")\n            print(f\"公有变量: {self.public_var}\")\n            print(f\"弱私有变量: {self._weak_private}\")\n            print(f\"强私有变量: {self.__strong_private}\")\n\n    # 创建测试实例\n    test = TestClass()\n\n    # 1. 测试从类内部访问（通过方法）\n    print(\"\\n=== 测试1: 从类内部访问所有变量 ===\")\n    test.print_all_vars()\n\n    # 2. 测试从外部直接访问\n    print(\"\\n=== 测试2: 从外部访问变量 ===\")\n    print(f\"访问公有变量: {test.public_var}\")\n    print(f\"访问弱私有变量: {test._weak_private}\")  # 能访问，但IDE会警告\n    try:\n        print(f\"访问强私有变量: {test.__strong_private}\")\n    except AttributeError as e:\n        print(f\"访问强私有变量失败: {e}\")\n\n    # 3. 测试名称改写机制\n    print(\"\\n=== 测试3: 验证强私有变量的名称改写机制 ===\")\n    # 实际上Python会将__strong_private改写为_TestClass__strong_private\n    print(f\"通过改写后的名称访问强私有变量: {test._TestClass__strong_private}\")\n\n    # 4. 测试导入行为\n    if __name__ == \"__main__\":\n        print(\"\\n=== 测试4: 创建第二个文件并尝试导入 ===\")\n        print(\"请创建 test_import.py 并运行来测试导入行为\")\n   ```\n\n   ```Python\n   \"\"\"\n   test_import.py\n   This file is used to test the import of private variables in Python.\n   \"\"\"\n   from test_private_vars import TestClass\n\n   print(\"=== 测试导入后的访问行为 ===\")\n   test = TestClass()\n\n   # 测试访问公有变量\n   print(f\"访问公有变量: {test.public_var}\")\n\n   # 测试访问弱私有变量\n   try:\n       print(f\"访问弱私有变量: {test._weak_private}\")\n       print(\"注意：虽然能访问弱私有变量，但这违反了Python的约定\")\n   except AttributeError as e:\n       print(f\"访问弱私有变量失败: {e}\")\n\n   # 测试访问强私有变量\n   try:\n       print(f\"访问强私有变量: {test.__strong_private}\")\n   except AttributeError as e:\n       print(f\"访问强私有变量失败: {e}\")\n\n   # 测试通过名称改写访问强私有变量\n   try:\n       print(f\"通过改写后的名称访问强私有变量: {test._TestClass__strong_private}\")\n       print(\"注意：虽然能通过名称改写访问强私有变量，但这是一个不推荐的做法\")\n   except AttributeError as e:\n       print(f\"通过改写名称访问强私有变量失败: {e}\")\n   ```\n\n   以下是运行 `python test_private_vars.py` 的结果\n\n   ```bash\n   === 测试1: 从类内部访问所有变量 ===\n   从内部访问:\n   公有变量: 这是公有变量\n   弱私有变量: 这是弱私有变量\n   强私有变量: 这是强私有变量\n\n   === 测试2: 从外部访问变量 ===\n   访问公有变量: 这是公有变量\n   访问弱私有变量: 这是弱私有变量\n   访问强私有变量失败: 'TestClass' object has no attribute '__strong_private'\n\n   === 测试3: 验证强私有变量的名称改写机制 ===\n   通过改写后的名称访问强私有变量: 这是强私有变量\n\n   === 测试4: 创建第二个文件并尝试导入 ===\n   请创建 test_import.py 并运行来测试导入行为\n   ```\n\n   以下是运行 `python test_import.py` 的结果\n\n   ```bash\n   === 测试1: 从类内部访问所有变量 ===\n   从内部访问:\n   公有变量: 这是公有变量\n   弱私有变量: 这是弱私有变量\n   强私有变量: 这是强私有变量\n\n   === 测试2: 从外部访问变量 ===\n   访问公有变量: 这是公有变量\n   访问弱私有变量: 这是弱私有变量\n   访问强私有变量失败: 'TestClass' object has no attribute '__strong_private'\n\n   === 测试3: 验证强私有变量的名称改写机制 ===\n   通过改写后的名称访问强私有变量: 这是强私有变量\n   === 测试导入后的访问行为 ===\n   访问公有变量: 这是公有变量\n   访问弱私有变量: 这是弱私有变量\n   注意：虽然能访问弱私有变量，但这违反了Python的约定\n   访问强私有变量失败: 'TestClass' object has no attribute '__strong_private'\n   通过改写后的名称访问强私有变量: 这是强私有变量\n   注意：虽然能通过名称改写访问强私有变量，但这是一个不推荐的做法\n   ```\n","slug":"PEP-8","published":1,"updated":"2025-07-12T07:33:33.250Z","comments":1,"layout":"post","photos":[],"_id":"cmd39u3jq001nqk41bott3h5l","content":"<h1 id=\"Style-Guide-for-Python-Code\"><a href=\"#Style-Guide-for-Python-Code\" class=\"headerlink\" title=\"Style Guide for Python Code\"></a>Style Guide for Python Code</h1><blockquote>\n<p><a href=\"https://peps.python.org/pep-0008/\">PEP8</a> 是 Python 社群共通的風格指南，一開始是 Python 之父 Guido van Rossum 自己的撰碼風格，慢慢後來演變至今，目的在於幫助開發者寫出可讀性高且風格一致的程式。許多開源計畫，例如 Django 、 OpenStack 等都是以 PEP8 為基礎再加上自己的風格建議。</p>\n</blockquote>\n<p>这篇博客主要是为了在搭建自己的模型之前学习一下一些统一的规范是做的记录 <del>主要是目前读到的大多数论文的源码目命名没有规律</del> ，以加强之后搭建模型时代码的可读性</p>\n<p>另外，本博客只展示本人不太熟悉的捏</p>\n<span id=\"more\"></span>\n\n<h2 id=\"代码布局\"><a href=\"#代码布局\" class=\"headerlink\" title=\"代码布局\"></a>代码布局</h2><h3 id=\"缩进\"><a href=\"#缩进\" class=\"headerlink\" title=\"缩进\"></a>缩进</h3><p><strong>每个缩进级别使用 4 个空格</strong></p>\n<p>对于比较臭长的函数，可以使用<em>悬挂缩进</em></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Correct:</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Aligned with opening delimiter.</span></span><br><span class=\"line\">foo = long_function_name(var_one, var_two,</span><br><span class=\"line\">                         var_three, var_four)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Add 4 spaces (an extra level of indentation) to distinguish arguments from the rest.</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">long_function_name</span>(<span class=\"params\"></span></span><br><span class=\"line\"><span class=\"params\">        var_one, var_two, var_three,</span></span><br><span class=\"line\"><span class=\"params\">        var_four</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(var_one)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Hanging indents should add a level.</span></span><br><span class=\"line\">foo = long_function_name(</span><br><span class=\"line\">    var_one, var_two,</span><br><span class=\"line\">    var_three, var_four)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Wrong:</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Arguments on first line forbidden when not using vertical alignment.</span></span><br><span class=\"line\">foo = long_function_name(var_one, var_two,</span><br><span class=\"line\">    var_three, var_four)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Further indentation required as indentation is not distinguishable.</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">long_function_name</span>(<span class=\"params\"></span></span><br><span class=\"line\"><span class=\"params\">    var_one, var_two, var_three,</span></span><br><span class=\"line\"><span class=\"params\">    var_four</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(var_one)</span><br></pre></td></tr></table></figure>\n\n<p>优先使用 <em>Tabs</em> 进行缩进， <em>Tabs</em> 和 <em>Spaces</em> 不能混用</p>\n<h3 id=\"每行最多字符数量\"><a href=\"#每行最多字符数量\" class=\"headerlink\" title=\"每行最多字符数量\"></a>每行最多字符数量</h3><p><strong>79</strong> 个</p>\n<p>合理使用反斜杠</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;/path/to/some/file/you/want/to/read&#x27;</span>) <span class=\"keyword\">as</span> file_1, \\</span><br><span class=\"line\">     <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;/path/to/some/file/being/written&#x27;</span>, <span class=\"string\">&#x27;w&#x27;</span>) <span class=\"keyword\">as</span> file_2:</span><br><span class=\"line\">    file_2.write(file_1.read())</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"二元运算符之前换行\"><a href=\"#二元运算符之前换行\" class=\"headerlink\" title=\"二元运算符之前换行\"></a>二元运算符之前换行</h3><p>为了更好的确定该 <code>item</code> 采取的是什么运算</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Wrong:</span></span><br><span class=\"line\"><span class=\"comment\"># operators sit far away from their operands</span></span><br><span class=\"line\">income = (gross_wages +</span><br><span class=\"line\">          taxable_interest +</span><br><span class=\"line\">          (dividends - qualified_dividends) -</span><br><span class=\"line\">          ira_deduction -</span><br><span class=\"line\">          student_loan_interest)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Correct:</span></span><br><span class=\"line\"><span class=\"comment\"># easy to match operators with operands</span></span><br><span class=\"line\">income = (gross_wages</span><br><span class=\"line\">          + taxable_interest</span><br><span class=\"line\">          + (dividends - qualified_dividends)</span><br><span class=\"line\">          - ira_deduction</span><br><span class=\"line\">          - student_loan_interest)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"如何空行（Blank-Lines）\"><a href=\"#如何空行（Blank-Lines）\" class=\"headerlink\" title=\"如何空行（Blank Lines）\"></a>如何空行（Blank Lines）</h3><p><em>顶级函数</em> 和 <em>类</em> 之间空 <strong>2</strong> 行</p>\n<p><em>类中的函数</em> 空 <strong>1</strong> 行</p>\n<h3 id=\"import\"><a href=\"#import\" class=\"headerlink\" title=\"import\"></a>import</h3><ul>\n<li>通常每一个库 <strong>单独一行</strong>（也有例外）</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> subprocess <span class=\"keyword\">import</span> Popen, PIPE</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>按以下顺序分组，每组间空行<ol>\n<li><strong>标准库</strong>导入</li>\n<li><strong>相关第三方库</strong>导入</li>\n<li><strong>特定的本地库</strong>导入</li>\n</ol>\n</li>\n</ul>\n<h2 id=\"注释\"><a href=\"#注释\" class=\"headerlink\" title=\"注释\"></a>注释</h2><blockquote>\n<p>Comments that contradict the code are worse than no comments.</p>\n</blockquote>\n<h2 id=\"命名约定\"><a href=\"#命名约定\" class=\"headerlink\" title=\"命名约定\"></a>命名约定</h2><ol>\n<li><p><strong>类名</strong> 用 <strong>大驼峰</strong></p>\n</li>\n<li><p><strong>函数名</strong> 用 <strong>小写下划线</strong></p>\n</li>\n<li><p>关于 <em>下划线</em></p>\n<ul>\n<li><em>单下划线</em> 用于占位</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(random.randint(<span class=\"number\">1</span>, <span class=\"number\">100</span>))</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><em>单下划线</em> 用于变量前表示该变量为 <strong>弱私有</strong> （语义上的 private），能调用但不能 import</li>\n<li><em>双下划线</em> 用于变量前表示该变量为 <strong>强私有</strong> （实际上也不能调用<del>实现方式是重名名</del>）<br>为了更好的说明这两点，给出以下两个测试程序</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">test_private_vars.py</span></span><br><span class=\"line\"><span class=\"string\">This file is used to test the private variables in Python.</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">TestClass</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.public_var = <span class=\"string\">&quot;这是公有变量&quot;</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>._weak_private = <span class=\"string\">&quot;这是弱私有变量&quot;</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.__strong_private = <span class=\"string\">&quot;这是强私有变量&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">print_all_vars</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;从内部访问:&quot;</span>)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;公有变量: <span class=\"subst\">&#123;self.public_var&#125;</span>&quot;</span>)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;弱私有变量: <span class=\"subst\">&#123;self._weak_private&#125;</span>&quot;</span>)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;强私有变量: <span class=\"subst\">&#123;self.__strong_private&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建测试实例</span></span><br><span class=\"line\">test = TestClass()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 1. 测试从类内部访问（通过方法）</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n=== 测试1: 从类内部访问所有变量 ===&quot;</span>)</span><br><span class=\"line\">test.print_all_vars()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. 测试从外部直接访问</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n=== 测试2: 从外部访问变量 ===&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问公有变量: <span class=\"subst\">&#123;test.public_var&#125;</span>&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问弱私有变量: <span class=\"subst\">&#123;test._weak_private&#125;</span>&quot;</span>)  <span class=\"comment\"># 能访问，但IDE会警告</span></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问强私有变量: <span class=\"subst\">&#123;test.__strong_private&#125;</span>&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span> AttributeError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问强私有变量失败: <span class=\"subst\">&#123;e&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 3. 测试名称改写机制</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n=== 测试3: 验证强私有变量的名称改写机制 ===&quot;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 实际上Python会将__strong_private改写为_TestClass__strong_private</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;通过改写后的名称访问强私有变量: <span class=\"subst\">&#123;test._TestClass__strong_private&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 4. 测试导入行为</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&quot;__main__&quot;</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n=== 测试4: 创建第二个文件并尝试导入 ===&quot;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;请创建 test_import.py 并运行来测试导入行为&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">test_import.py</span></span><br><span class=\"line\"><span class=\"string\">This file is used to test the import of private variables in Python.</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> test_private_vars <span class=\"keyword\">import</span> TestClass</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;=== 测试导入后的访问行为 ===&quot;</span>)</span><br><span class=\"line\">test = TestClass()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 测试访问公有变量</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问公有变量: <span class=\"subst\">&#123;test.public_var&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 测试访问弱私有变量</span></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问弱私有变量: <span class=\"subst\">&#123;test._weak_private&#125;</span>&quot;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;注意：虽然能访问弱私有变量，但这违反了Python的约定&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span> AttributeError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问弱私有变量失败: <span class=\"subst\">&#123;e&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 测试访问强私有变量</span></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问强私有变量: <span class=\"subst\">&#123;test.__strong_private&#125;</span>&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span> AttributeError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问强私有变量失败: <span class=\"subst\">&#123;e&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 测试通过名称改写访问强私有变量</span></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;通过改写后的名称访问强私有变量: <span class=\"subst\">&#123;test._TestClass__strong_private&#125;</span>&quot;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;注意：虽然能通过名称改写访问强私有变量，但这是一个不推荐的做法&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span> AttributeError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;通过改写名称访问强私有变量失败: <span class=\"subst\">&#123;e&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<p>以下是运行 <code>python test_private_vars.py</code> 的结果</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">=== 测试1: 从类内部访问所有变量 ===</span><br><span class=\"line\">从内部访问:</span><br><span class=\"line\">公有变量: 这是公有变量</span><br><span class=\"line\">弱私有变量: 这是弱私有变量</span><br><span class=\"line\">强私有变量: 这是强私有变量</span><br><span class=\"line\"></span><br><span class=\"line\">=== 测试2: 从外部访问变量 ===</span><br><span class=\"line\">访问公有变量: 这是公有变量</span><br><span class=\"line\">访问弱私有变量: 这是弱私有变量</span><br><span class=\"line\">访问强私有变量失败: <span class=\"string\">&#x27;TestClass&#x27;</span> object has no attribute <span class=\"string\">&#x27;__strong_private&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">=== 测试3: 验证强私有变量的名称改写机制 ===</span><br><span class=\"line\">通过改写后的名称访问强私有变量: 这是强私有变量</span><br><span class=\"line\"></span><br><span class=\"line\">=== 测试4: 创建第二个文件并尝试导入 ===</span><br><span class=\"line\">请创建 test_import.py 并运行来测试导入行为</span><br></pre></td></tr></table></figure>\n\n<p>以下是运行 <code>python test_import.py</code> 的结果</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">=== 测试1: 从类内部访问所有变量 ===</span><br><span class=\"line\">从内部访问:</span><br><span class=\"line\">公有变量: 这是公有变量</span><br><span class=\"line\">弱私有变量: 这是弱私有变量</span><br><span class=\"line\">强私有变量: 这是强私有变量</span><br><span class=\"line\"></span><br><span class=\"line\">=== 测试2: 从外部访问变量 ===</span><br><span class=\"line\">访问公有变量: 这是公有变量</span><br><span class=\"line\">访问弱私有变量: 这是弱私有变量</span><br><span class=\"line\">访问强私有变量失败: <span class=\"string\">&#x27;TestClass&#x27;</span> object has no attribute <span class=\"string\">&#x27;__strong_private&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">=== 测试3: 验证强私有变量的名称改写机制 ===</span><br><span class=\"line\">通过改写后的名称访问强私有变量: 这是强私有变量</span><br><span class=\"line\">=== 测试导入后的访问行为 ===</span><br><span class=\"line\">访问公有变量: 这是公有变量</span><br><span class=\"line\">访问弱私有变量: 这是弱私有变量</span><br><span class=\"line\">注意：虽然能访问弱私有变量，但这违反了Python的约定</span><br><span class=\"line\">访问强私有变量失败: <span class=\"string\">&#x27;TestClass&#x27;</span> object has no attribute <span class=\"string\">&#x27;__strong_private&#x27;</span></span><br><span class=\"line\">通过改写后的名称访问强私有变量: 这是强私有变量</span><br><span class=\"line\">注意：虽然能通过名称改写访问强私有变量，但这是一个不推荐的做法</span><br></pre></td></tr></table></figure></li>\n</ol>\n","length":1625,"excerpt":"<h1 id=\"Style-Guide-for-Python-Code\"><a href=\"#Style-Guide-for-Python-Code\" class=\"headerlink\" title=\"Style Guide for Python Code\"></a>Style Guide for Python Code</h1><blockquote>\n<p><a href=\"https://peps.python.org/pep-0008/\">PEP8</a> 是 Python 社群共通的風格指南，一開始是 Python 之父 Guido van Rossum 自己的撰碼風格，慢慢後來演變至今，目的在於幫助開發者寫出可讀性高且風格一致的程式。許多開源計畫，例如 Django 、 OpenStack 等都是以 PEP8 為基礎再加上自己的風格建議。</p>\n</blockquote>\n<p>这篇博客主要是为了在搭建自己的模型之前学习一下一些统一的规范是做的记录 <del>主要是目前读到的大多数论文的源码目命名没有规律</del> ，以加强之后搭建模型时代码的可读性</p>\n<p>另外，本博客只展示本人不太熟悉的捏</p>","more":"<h2 id=\"代码布局\"><a href=\"#代码布局\" class=\"headerlink\" title=\"代码布局\"></a>代码布局</h2><h3 id=\"缩进\"><a href=\"#缩进\" class=\"headerlink\" title=\"缩进\"></a>缩进</h3><p><strong>每个缩进级别使用 4 个空格</strong></p>\n<p>对于比较臭长的函数，可以使用<em>悬挂缩进</em></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Correct:</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Aligned with opening delimiter.</span></span><br><span class=\"line\">foo = long_function_name(var_one, var_two,</span><br><span class=\"line\">                         var_three, var_four)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Add 4 spaces (an extra level of indentation) to distinguish arguments from the rest.</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">long_function_name</span>(<span class=\"params\"></span></span><br><span class=\"line\"><span class=\"params\">        var_one, var_two, var_three,</span></span><br><span class=\"line\"><span class=\"params\">        var_four</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(var_one)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Hanging indents should add a level.</span></span><br><span class=\"line\">foo = long_function_name(</span><br><span class=\"line\">    var_one, var_two,</span><br><span class=\"line\">    var_three, var_four)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Wrong:</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Arguments on first line forbidden when not using vertical alignment.</span></span><br><span class=\"line\">foo = long_function_name(var_one, var_two,</span><br><span class=\"line\">    var_three, var_four)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Further indentation required as indentation is not distinguishable.</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">long_function_name</span>(<span class=\"params\"></span></span><br><span class=\"line\"><span class=\"params\">    var_one, var_two, var_three,</span></span><br><span class=\"line\"><span class=\"params\">    var_four</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(var_one)</span><br></pre></td></tr></table></figure>\n\n<p>优先使用 <em>Tabs</em> 进行缩进， <em>Tabs</em> 和 <em>Spaces</em> 不能混用</p>\n<h3 id=\"每行最多字符数量\"><a href=\"#每行最多字符数量\" class=\"headerlink\" title=\"每行最多字符数量\"></a>每行最多字符数量</h3><p><strong>79</strong> 个</p>\n<p>合理使用反斜杠</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;/path/to/some/file/you/want/to/read&#x27;</span>) <span class=\"keyword\">as</span> file_1, \\</span><br><span class=\"line\">     <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;/path/to/some/file/being/written&#x27;</span>, <span class=\"string\">&#x27;w&#x27;</span>) <span class=\"keyword\">as</span> file_2:</span><br><span class=\"line\">    file_2.write(file_1.read())</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"二元运算符之前换行\"><a href=\"#二元运算符之前换行\" class=\"headerlink\" title=\"二元运算符之前换行\"></a>二元运算符之前换行</h3><p>为了更好的确定该 <code>item</code> 采取的是什么运算</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Wrong:</span></span><br><span class=\"line\"><span class=\"comment\"># operators sit far away from their operands</span></span><br><span class=\"line\">income = (gross_wages +</span><br><span class=\"line\">          taxable_interest +</span><br><span class=\"line\">          (dividends - qualified_dividends) -</span><br><span class=\"line\">          ira_deduction -</span><br><span class=\"line\">          student_loan_interest)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Correct:</span></span><br><span class=\"line\"><span class=\"comment\"># easy to match operators with operands</span></span><br><span class=\"line\">income = (gross_wages</span><br><span class=\"line\">          + taxable_interest</span><br><span class=\"line\">          + (dividends - qualified_dividends)</span><br><span class=\"line\">          - ira_deduction</span><br><span class=\"line\">          - student_loan_interest)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"如何空行（Blank-Lines）\"><a href=\"#如何空行（Blank-Lines）\" class=\"headerlink\" title=\"如何空行（Blank Lines）\"></a>如何空行（Blank Lines）</h3><p><em>顶级函数</em> 和 <em>类</em> 之间空 <strong>2</strong> 行</p>\n<p><em>类中的函数</em> 空 <strong>1</strong> 行</p>\n<h3 id=\"import\"><a href=\"#import\" class=\"headerlink\" title=\"import\"></a>import</h3><ul>\n<li>通常每一个库 <strong>单独一行</strong>（也有例外）</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> subprocess <span class=\"keyword\">import</span> Popen, PIPE</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>按以下顺序分组，每组间空行<ol>\n<li><strong>标准库</strong>导入</li>\n<li><strong>相关第三方库</strong>导入</li>\n<li><strong>特定的本地库</strong>导入</li>\n</ol>\n</li>\n</ul>\n<h2 id=\"注释\"><a href=\"#注释\" class=\"headerlink\" title=\"注释\"></a>注释</h2><blockquote>\n<p>Comments that contradict the code are worse than no comments.</p>\n</blockquote>\n<h2 id=\"命名约定\"><a href=\"#命名约定\" class=\"headerlink\" title=\"命名约定\"></a>命名约定</h2><ol>\n<li><p><strong>类名</strong> 用 <strong>大驼峰</strong></p>\n</li>\n<li><p><strong>函数名</strong> 用 <strong>小写下划线</strong></p>\n</li>\n<li><p>关于 <em>下划线</em></p>\n<ul>\n<li><em>单下划线</em> 用于占位</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(random.randint(<span class=\"number\">1</span>, <span class=\"number\">100</span>))</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><em>单下划线</em> 用于变量前表示该变量为 <strong>弱私有</strong> （语义上的 private），能调用但不能 import</li>\n<li><em>双下划线</em> 用于变量前表示该变量为 <strong>强私有</strong> （实际上也不能调用<del>实现方式是重名名</del>）<br>为了更好的说明这两点，给出以下两个测试程序</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">test_private_vars.py</span></span><br><span class=\"line\"><span class=\"string\">This file is used to test the private variables in Python.</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">TestClass</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.public_var = <span class=\"string\">&quot;这是公有变量&quot;</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>._weak_private = <span class=\"string\">&quot;这是弱私有变量&quot;</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.__strong_private = <span class=\"string\">&quot;这是强私有变量&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">print_all_vars</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;从内部访问:&quot;</span>)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;公有变量: <span class=\"subst\">&#123;self.public_var&#125;</span>&quot;</span>)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;弱私有变量: <span class=\"subst\">&#123;self._weak_private&#125;</span>&quot;</span>)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;强私有变量: <span class=\"subst\">&#123;self.__strong_private&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建测试实例</span></span><br><span class=\"line\">test = TestClass()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 1. 测试从类内部访问（通过方法）</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n=== 测试1: 从类内部访问所有变量 ===&quot;</span>)</span><br><span class=\"line\">test.print_all_vars()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. 测试从外部直接访问</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n=== 测试2: 从外部访问变量 ===&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问公有变量: <span class=\"subst\">&#123;test.public_var&#125;</span>&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问弱私有变量: <span class=\"subst\">&#123;test._weak_private&#125;</span>&quot;</span>)  <span class=\"comment\"># 能访问，但IDE会警告</span></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问强私有变量: <span class=\"subst\">&#123;test.__strong_private&#125;</span>&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span> AttributeError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问强私有变量失败: <span class=\"subst\">&#123;e&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 3. 测试名称改写机制</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n=== 测试3: 验证强私有变量的名称改写机制 ===&quot;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 实际上Python会将__strong_private改写为_TestClass__strong_private</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;通过改写后的名称访问强私有变量: <span class=\"subst\">&#123;test._TestClass__strong_private&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 4. 测试导入行为</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&quot;__main__&quot;</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n=== 测试4: 创建第二个文件并尝试导入 ===&quot;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;请创建 test_import.py 并运行来测试导入行为&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">test_import.py</span></span><br><span class=\"line\"><span class=\"string\">This file is used to test the import of private variables in Python.</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> test_private_vars <span class=\"keyword\">import</span> TestClass</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;=== 测试导入后的访问行为 ===&quot;</span>)</span><br><span class=\"line\">test = TestClass()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 测试访问公有变量</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问公有变量: <span class=\"subst\">&#123;test.public_var&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 测试访问弱私有变量</span></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问弱私有变量: <span class=\"subst\">&#123;test._weak_private&#125;</span>&quot;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;注意：虽然能访问弱私有变量，但这违反了Python的约定&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span> AttributeError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问弱私有变量失败: <span class=\"subst\">&#123;e&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 测试访问强私有变量</span></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问强私有变量: <span class=\"subst\">&#123;test.__strong_private&#125;</span>&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span> AttributeError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问强私有变量失败: <span class=\"subst\">&#123;e&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 测试通过名称改写访问强私有变量</span></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;通过改写后的名称访问强私有变量: <span class=\"subst\">&#123;test._TestClass__strong_private&#125;</span>&quot;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;注意：虽然能通过名称改写访问强私有变量，但这是一个不推荐的做法&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span> AttributeError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;通过改写名称访问强私有变量失败: <span class=\"subst\">&#123;e&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<p>以下是运行 <code>python test_private_vars.py</code> 的结果</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">=== 测试1: 从类内部访问所有变量 ===</span><br><span class=\"line\">从内部访问:</span><br><span class=\"line\">公有变量: 这是公有变量</span><br><span class=\"line\">弱私有变量: 这是弱私有变量</span><br><span class=\"line\">强私有变量: 这是强私有变量</span><br><span class=\"line\"></span><br><span class=\"line\">=== 测试2: 从外部访问变量 ===</span><br><span class=\"line\">访问公有变量: 这是公有变量</span><br><span class=\"line\">访问弱私有变量: 这是弱私有变量</span><br><span class=\"line\">访问强私有变量失败: <span class=\"string\">&#x27;TestClass&#x27;</span> object has no attribute <span class=\"string\">&#x27;__strong_private&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">=== 测试3: 验证强私有变量的名称改写机制 ===</span><br><span class=\"line\">通过改写后的名称访问强私有变量: 这是强私有变量</span><br><span class=\"line\"></span><br><span class=\"line\">=== 测试4: 创建第二个文件并尝试导入 ===</span><br><span class=\"line\">请创建 test_import.py 并运行来测试导入行为</span><br></pre></td></tr></table></figure>\n\n<p>以下是运行 <code>python test_import.py</code> 的结果</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">=== 测试1: 从类内部访问所有变量 ===</span><br><span class=\"line\">从内部访问:</span><br><span class=\"line\">公有变量: 这是公有变量</span><br><span class=\"line\">弱私有变量: 这是弱私有变量</span><br><span class=\"line\">强私有变量: 这是强私有变量</span><br><span class=\"line\"></span><br><span class=\"line\">=== 测试2: 从外部访问变量 ===</span><br><span class=\"line\">访问公有变量: 这是公有变量</span><br><span class=\"line\">访问弱私有变量: 这是弱私有变量</span><br><span class=\"line\">访问强私有变量失败: <span class=\"string\">&#x27;TestClass&#x27;</span> object has no attribute <span class=\"string\">&#x27;__strong_private&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">=== 测试3: 验证强私有变量的名称改写机制 ===</span><br><span class=\"line\">通过改写后的名称访问强私有变量: 这是强私有变量</span><br><span class=\"line\">=== 测试导入后的访问行为 ===</span><br><span class=\"line\">访问公有变量: 这是公有变量</span><br><span class=\"line\">访问弱私有变量: 这是弱私有变量</span><br><span class=\"line\">注意：虽然能访问弱私有变量，但这违反了Python的约定</span><br><span class=\"line\">访问强私有变量失败: <span class=\"string\">&#x27;TestClass&#x27;</span> object has no attribute <span class=\"string\">&#x27;__strong_private&#x27;</span></span><br><span class=\"line\">通过改写后的名称访问强私有变量: 这是强私有变量</span><br><span class=\"line\">注意：虽然能通过名称改写访问强私有变量，但这是一个不推荐的做法</span><br></pre></td></tr></table></figure></li>\n</ol>"}],"PostAsset":[],"PostCategory":[{"post_id":"cmd39u3jd0003qk41hr428qgq","category_id":"cmd39u3je0004qk419vht124d","_id":"cmd39u3jl000lqk41301fcaoz"},{"post_id":"cmd39u3jd0003qk41hr428qgq","category_id":"cmd39u3jl000gqk412sq3bjuq","_id":"cmd39u3jm000nqk41ge0g0r2e"},{"post_id":"cmd39u3jb0001qk41d8z59j9n","category_id":"cmd39u3je0004qk419vht124d","_id":"cmd39u3jm000tqk414li5cga6"},{"post_id":"cmd39u3jb0001qk41d8z59j9n","category_id":"cmd39u3jk000eqk417i6qd8c3","_id":"cmd39u3jn000xqk41hhae5tt4"},{"post_id":"cmd39u3jb0001qk41d8z59j9n","category_id":"cmd39u3jl000kqk41e9obb0lz","_id":"cmd39u3jn000zqk4165hn5c49"},{"post_id":"cmd39u3jg0007qk410x0u0c1w","category_id":"cmd39u3je0004qk419vht124d","_id":"cmd39u3jn0012qk410y4ecz11"},{"post_id":"cmd39u3jg0007qk410x0u0c1w","category_id":"cmd39u3jk000eqk417i6qd8c3","_id":"cmd39u3jn0015qk410zyo0elp"},{"post_id":"cmd39u3jg0007qk410x0u0c1w","category_id":"cmd39u3jl000kqk41e9obb0lz","_id":"cmd39u3jn0017qk417mxv79ml"},{"post_id":"cmd39u3jg0007qk410x0u0c1w","category_id":"cmd39u3jm000wqk41hrb5gubo","_id":"cmd39u3jn001aqk412d228pwt"},{"post_id":"cmd39u3jp001kqk41c4oc2684","category_id":"cmd39u3je0004qk419vht124d","_id":"cmd39u3jr001uqk41g9r14g33"},{"post_id":"cmd39u3jp001kqk41c4oc2684","category_id":"cmd39u3jk000eqk417i6qd8c3","_id":"cmd39u3jr001wqk41bznq4cgo"},{"post_id":"cmd39u3jp001kqk41c4oc2684","category_id":"cmd39u3jq001oqk411vcr28pt","_id":"cmd39u3jr001yqk4164cqh8gq"},{"post_id":"cmd39u3jq001nqk41bott3h5l","category_id":"cmd39u3jr001qqk41etvledb1","_id":"cmd39u3jr0021qk412vjfaiwr"}],"PostTag":[{"post_id":"cmd39u3jb0001qk41d8z59j9n","tag_id":"cmd39u3jg0005qk41deim3eq8","_id":"cmd39u3jm000oqk41aj3g8d83"},{"post_id":"cmd39u3jb0001qk41d8z59j9n","tag_id":"cmd39u3ji000bqk41f39zge2j","_id":"cmd39u3jm000qqk41e1y86226"},{"post_id":"cmd39u3jb0001qk41d8z59j9n","tag_id":"cmd39u3jk000dqk41djir001t","_id":"cmd39u3jm000sqk41fruhespq"},{"post_id":"cmd39u3jb0001qk41d8z59j9n","tag_id":"cmd39u3jk000fqk417sbngi6l","_id":"cmd39u3jm000uqk419yb9cn66"},{"post_id":"cmd39u3jb0001qk41d8z59j9n","tag_id":"cmd39u3jl000hqk417jq1gwgt","_id":"cmd39u3jn000yqk41gdlchvjq"},{"post_id":"cmd39u3jb0001qk41d8z59j9n","tag_id":"cmd39u3jl000jqk41bkmbdjbm","_id":"cmd39u3jn0010qk415l6rgbzc"},{"post_id":"cmd39u3jd0003qk41hr428qgq","tag_id":"cmd39u3jg0005qk41deim3eq8","_id":"cmd39u3jn0014qk4121k76bes"},{"post_id":"cmd39u3jd0003qk41hr428qgq","tag_id":"cmd39u3jm000rqk41760x0ke7","_id":"cmd39u3jn0016qk410kwjh07v"},{"post_id":"cmd39u3jd0003qk41hr428qgq","tag_id":"cmd39u3jm000vqk41956m5y0b","_id":"cmd39u3jn0019qk41epkmftu8"},{"post_id":"cmd39u3jd0003qk41hr428qgq","tag_id":"cmd39u3jn0011qk413ehj333g","_id":"cmd39u3jn001bqk4134510s4n"},{"post_id":"cmd39u3jg0007qk410x0u0c1w","tag_id":"cmd39u3jg0005qk41deim3eq8","_id":"cmd39u3jo001fqk419tp7ahs9"},{"post_id":"cmd39u3jg0007qk410x0u0c1w","tag_id":"cmd39u3ji000bqk41f39zge2j","_id":"cmd39u3jo001gqk414www0tuz"},{"post_id":"cmd39u3jg0007qk410x0u0c1w","tag_id":"cmd39u3jk000dqk41djir001t","_id":"cmd39u3jo001hqk413hfu3512"},{"post_id":"cmd39u3jg0007qk410x0u0c1w","tag_id":"cmd39u3jl000hqk417jq1gwgt","_id":"cmd39u3jo001iqk41gfvd5skl"},{"post_id":"cmd39u3jg0007qk410x0u0c1w","tag_id":"cmd39u3jk000fqk417sbngi6l","_id":"cmd39u3jo001jqk41344n66uq"},{"post_id":"cmd39u3jp001kqk41c4oc2684","tag_id":"cmd39u3jg0005qk41deim3eq8","_id":"cmd39u3jr001rqk413txzf5aa"},{"post_id":"cmd39u3jp001kqk41c4oc2684","tag_id":"cmd39u3ji000bqk41f39zge2j","_id":"cmd39u3jr001sqk4126e4du32"},{"post_id":"cmd39u3jp001kqk41c4oc2684","tag_id":"cmd39u3jq001mqk418kx02leq","_id":"cmd39u3jr001vqk41e8he8sbu"},{"post_id":"cmd39u3jp001kqk41c4oc2684","tag_id":"cmd39u3jl000jqk41bkmbdjbm","_id":"cmd39u3jr001xqk41gjbzbq0r"},{"post_id":"cmd39u3jp001kqk41c4oc2684","tag_id":"cmd39u3jn0011qk413ehj333g","_id":"cmd39u3jr0020qk41h2nmf0fl"},{"post_id":"cmd39u3jp001kqk41c4oc2684","tag_id":"cmd39u3jk000dqk41djir001t","_id":"cmd39u3jr0022qk417wdpa3ir"},{"post_id":"cmd39u3jq001nqk41bott3h5l","tag_id":"cmd39u3jg0005qk41deim3eq8","_id":"cmd39u3js0024qk41gxaa8vqu"},{"post_id":"cmd39u3jq001nqk41bott3h5l","tag_id":"cmd39u3jr001pqk41dgiyhzgi","_id":"cmd39u3js0025qk41fxhkhhcl"},{"post_id":"cmd39u3jq001nqk41bott3h5l","tag_id":"cmd39u3jr001tqk411qp6dh6i","_id":"cmd39u3js0026qk4164cy8o33"},{"post_id":"cmd39u3jq001nqk41bott3h5l","tag_id":"cmd39u3jr001zqk4193nx1o60","_id":"cmd39u3js0027qk41dv6069xj"},{"post_id":"cmd39u3jq001nqk41bott3h5l","tag_id":"cmd39u3jr0023qk41f7gga2ra","_id":"cmd39u3js0028qk41e9dlhkwk"}],"Tag":[{"name":"CDR","_id":"cmd39u3jg0005qk41deim3eq8"},{"name":"model","_id":"cmd39u3ji000bqk41f39zge2j"},{"name":"Basic","_id":"cmd39u3jk000dqk41djir001t"},{"name":"还没写完捏","_id":"cmd39u3jk000fqk417sbngi6l"},{"name":"deep learning","_id":"cmd39u3jl000hqk417jq1gwgt"},{"name":"PyTorch","_id":"cmd39u3jl000jqk41bkmbdjbm"},{"name":"Data Analysis","_id":"cmd39u3jm000rqk41760x0ke7"},{"name":"可能有点用","_id":"cmd39u3jm000vqk41956m5y0b"},{"name":"graph theory","_id":"cmd39u3jn0011qk413ehj333g"},{"name":"embedding","_id":"cmd39u3jq001mqk418kx02leq"},{"name":"Python","_id":"cmd39u3jr001pqk41dgiyhzgi"},{"name":"PEP","_id":"cmd39u3jr001tqk411qp6dh6i"},{"name":"闲🉐无聊","_id":"cmd39u3jr001zqk4193nx1o60"},{"name":"大概率没用","_id":"cmd39u3jr0023qk41f7gga2ra"}]}}