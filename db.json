{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/css/noscript.styl","path":"css/noscript.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon.jpg","path":"images/favicon.jpg","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo-algolia-nebula-blue-full.svg","path":"images/logo-algolia-nebula-blue-full.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/js/bookmark.js","path":"js/bookmark.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/comments-buttons.js","path":"js/comments-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/comments.js","path":"js/comments.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/config.js","path":"js/config.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/motion.js","path":"js/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/next-boot.js","path":"js/next-boot.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/pjax.js","path":"js/pjax.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/schedule.js","path":"js/schedule.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/sidebar.js","path":"js/sidebar.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/utils.js","path":"js/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/addtoany.js","path":"js/third-party/addtoany.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/fancybox.js","path":"js/third-party/fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/pace.js","path":"js/third-party/pace.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/quicklink.js","path":"js/third-party/quicklink.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/analytics/baidu-analytics.js","path":"js/third-party/analytics/baidu-analytics.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/analytics/google-analytics.js","path":"js/third-party/analytics/google-analytics.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/analytics/growingio.js","path":"js/third-party/analytics/growingio.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/analytics/matomo.js","path":"js/third-party/analytics/matomo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/chat/chatra.js","path":"js/third-party/chat/chatra.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/chat/tidio.js","path":"js/third-party/chat/tidio.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/comments/changyan.js","path":"js/third-party/comments/changyan.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/comments/disqus.js","path":"js/third-party/comments/disqus.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/comments/disqusjs.js","path":"js/third-party/comments/disqusjs.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/comments/gitalk.js","path":"js/third-party/comments/gitalk.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/comments/isso.js","path":"js/third-party/comments/isso.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/comments/livere.js","path":"js/third-party/comments/livere.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/comments/utterances.js","path":"js/third-party/comments/utterances.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/math/katex.js","path":"js/third-party/math/katex.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/math/mathjax.js","path":"js/third-party/math/mathjax.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/search/algolia-search.js","path":"js/third-party/search/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/search/local-search.js","path":"js/third-party/search/local-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/statistics/firestore.js","path":"js/third-party/statistics/firestore.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/statistics/lean-analytics.js","path":"js/third-party/statistics/lean-analytics.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/tags/mermaid.js","path":"js/third-party/tags/mermaid.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/tags/pdf.js","path":"js/third-party/tags/pdf.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/third-party/tags/wavedrom.js","path":"js/third-party/tags/wavedrom.js","modified":1,"renderable":1},{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"source/paper/1609.02907v4.pdf","path":"paper/1609.02907v4.pdf","modified":1,"renderable":0},{"_id":"source/paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf","path":"paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf","modified":1,"renderable":0},{"_id":"source/paper/Lee 等 - 2018 - Attention Models in Graphs A Survey.pdf","path":"paper/Lee 等 - 2018 - Attention Models in Graphs A Survey.pdf","modified":1,"renderable":0},{"_id":"source/paper/Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends.pdf","path":"paper/Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends.pdf","modified":1,"renderable":0},{"_id":"source/test/test-pdf.md","path":"test/test-pdf.md","modified":1,"renderable":0},{"_id":"source/test/test_nodeppt.md","path":"test/test_nodeppt.md","modified":1,"renderable":0},{"_id":"source/code/data_analysis/visualize_graph_analysis.py","path":"code/data_analysis/visualize_graph_analysis.py","modified":1,"renderable":0},{"_id":"source/nodeppt/test/test_nodeppt.html","path":"nodeppt/test/test_nodeppt.html","modified":1,"renderable":0},{"_id":"source/img/CDR-data-analysis/comprehensive_bipartite_analysis_ccle.png","path":"img/CDR-data-analysis/comprehensive_bipartite_analysis_ccle.png","modified":1,"renderable":0},{"_id":"source/img/CDR-data-analysis/comprehensive_bipartite_analysis_gdsc.png","path":"img/CDR-data-analysis/comprehensive_bipartite_analysis_gdsc.png","modified":1,"renderable":0},{"_id":"source/img/CDR-data-analysis/gene.png","path":"img/CDR-data-analysis/gene.png","modified":1,"renderable":0},{"_id":"source/img/Attention/AttentionViaAttention.png","path":"img/Attention/AttentionViaAttention.png","modified":1,"renderable":0},{"_id":"source/img/Attention/CapsuleAttention.png","path":"img/Attention/CapsuleAttention.png","modified":1,"renderable":0},{"_id":"source/img/Attention/HierarchicalAttention.png","path":"img/Attention/HierarchicalAttention.png","modified":1,"renderable":0},{"_id":"source/img/Attention/GeneralAttentionModule.png","path":"img/Attention/GeneralAttentionModule.png","modified":1,"renderable":0},{"_id":"source/img/Attention/InteractiveCo-Attention.png","path":"img/Attention/InteractiveCo-Attention.png","modified":1,"renderable":0},{"_id":"source/img/Attention/MultiheadAttention.png","path":"img/Attention/MultiheadAttention.png","modified":1,"renderable":0},{"_id":"source/img/Attention/ParallelCo-Attention.png","path":"img/Attention/ParallelCo-Attention.png","modified":1,"renderable":0},{"_id":"source/img/Attention/Taxonomy.png","path":"img/Attention/Taxonomy.png","modified":1,"renderable":0},{"_id":"source/img/Attention/TotalModel.png","path":"img/Attention/TotalModel.png","modified":1,"renderable":0},{"_id":"source/img/Attention/MultihopAttention.png","path":"img/Attention/MultihopAttention.png","modified":1,"renderable":0},{"_id":"source/nodeppt/test/css/chunk-vendors.4e4765ff.css","path":"nodeppt/test/css/chunk-vendors.4e4765ff.css","modified":1,"renderable":0},{"_id":"source/nodeppt/test/img/swipe.svg","path":"nodeppt/test/img/swipe.svg","modified":1,"renderable":0},{"_id":"source/nodeppt/test/js/chunk-vendors.js","path":"nodeppt/test/js/chunk-vendors.js","modified":1,"renderable":0},{"_id":"source/nodeppt/test/js/chunk-vendors.js.LICENSE.txt","path":"nodeppt/test/js/chunk-vendors.js.LICENSE.txt","modified":1,"renderable":0},{"_id":"source/nodeppt/test/js/test_nodeppt.js.LICENSE.txt","path":"nodeppt/test/js/test_nodeppt.js.LICENSE.txt","modified":1,"renderable":0},{"_id":"source/nodeppt/test/js/test_nodeppt.js","path":"nodeppt/test/js/test_nodeppt.js","modified":1,"renderable":0}],"Cache":[{"_id":"source/CNAME","hash":"9f72a1c93d67b273a3f1b776b7607fca8ecb9800","modified":1752044018736},{"_id":"source/_posts/Attention.md","hash":"0beb2b6c67a5582ce14021990e5e237ca11e111e","modified":1752216576852},{"_id":"source/_posts/CDR-data-analysis.md","hash":"8644bad2329b52211ff2cb500e250e8520faba0b","modified":1752154774966},{"_id":"source/_posts/PEP-8.md","hash":"67ffed8d063d9ee7af0646d49ff4327a7f4b9877","modified":1752154588887},{"_id":"source/_posts/GNN-and-GCN.md","hash":"d7cd4a7596b9b1ec21dae115592f48f53d639aec","modified":1752216576854},{"_id":"source/about/index.md","hash":"666abd9721ded05a09b8b0875470a4e20a6294d5","modified":1751891111694},{"_id":"source/categories/index.md","hash":"69053fbdbc41e7bcd0541f7cc76503991398e867","modified":1752148714280},{"_id":"source/test/test-pdf.md","hash":"f7c865a873ba3b2e3587ee1a07d688bcf7f62d49","modified":1752235218257},{"_id":"source/tags/index.md","hash":"1cad3eb82f7660de4695b271aa49becb4707b833","modified":1752148697163},{"_id":"source/test/test_nodeppt.md","hash":"8ab37cbb7157ffd36ad5b535369431872efc1d33","modified":1752220450176},{"_id":"source/nodeppt/test/test_nodeppt.html","hash":"7d62ca54b2ecc3f46dab96441629eefdfbef3149","modified":1752220305852},{"_id":"source/code/data_analysis/visualize_graph_analysis.py","hash":"f7ed9f01a38fe104b17464bf6e2dc7c5093ca7dc","modified":1752066963171},{"_id":"source/img/CDR-data-analysis/gene.png","hash":"dac1e1481add33e6e471f62deb153719ff929a3a","modified":1752130347412},{"_id":"source/nodeppt/test/js/chunk-vendors.js","hash":"b8512dadb9d4df8bbb3926e0afaf76a46b3b278c","modified":1752220305852},{"_id":"source/nodeppt/test/js/test_nodeppt.js.LICENSE.txt","hash":"b9fe24e9574b5f05a2fbb03f8234b998e6c8c12c","modified":1752220305853},{"_id":"source/nodeppt/test/img/swipe.svg","hash":"21c944bb622059b5f2974345dd35f082d01f3333","modified":1752220305852},{"_id":"source/nodeppt/test/js/chunk-vendors.js.LICENSE.txt","hash":"b9fe24e9574b5f05a2fbb03f8234b998e6c8c12c","modified":1752220305853},{"_id":"source/nodeppt/test/js/test_nodeppt.js","hash":"f6aa6c0b1cf60221d64032b8876e515aeae284f4","modified":1752220305852},{"_id":"source/img/Attention/AttentionViaAttention.png","hash":"eee60186ed279182176b247f00e6243b707c54a6","modified":1752219360264},{"_id":"source/img/Attention/InteractiveCo-Attention.png","hash":"d66ad9309b6851348104b591f7a265f55ba4aae4","modified":1752219361194},{"_id":"source/img/Attention/ParallelCo-Attention.png","hash":"1389f902e31640d6ff97a4375804b5b443706c02","modified":1752219361727},{"_id":"source/img/Attention/MultiheadAttention.png","hash":"1d61e6a59d33b5b70521acf9b1e8b191d04a7278","modified":1752219361392},{"_id":"source/img/Attention/TotalModel.png","hash":"b9c32980245dcf7bb4e9e54b04b59ccaa14bf265","modified":1752219362722},{"_id":"source/nodeppt/test/css/chunk-vendors.4e4765ff.css","hash":"abd0e586824cde74bd2348327b0d209e38d6e9b0","modified":1752220305852},{"_id":"source/img/Attention/GeneralAttentionModule.png","hash":"9e9b439faf1e0e38564fc8eab3364f8e6938cf6e","modified":1752219360789},{"_id":"source/img/Attention/HierarchicalAttention.png","hash":"ddad89428142f48353d6338f6d69402ec9b0d95e","modified":1752219360947},{"_id":"source/img/Attention/MultihopAttention.png","hash":"9bb72595c6b04f2157d01b069d004eaa94d71efa","modified":1752219361566},{"_id":"source/img/Attention/CapsuleAttention.png","hash":"8d29b9e962f65aea416727e5e0d321cf00e718f1","modified":1752219360562},{"_id":"themes/next/.editorconfig","hash":"731c650ddad6eb0fc7c3d4a91cad1698fe7ad311","modified":1751874689073},{"_id":"themes/next/.gitattributes","hash":"aeeca2f1e987d83232d7870d1435a4e3ed66b648","modified":1751874689073},{"_id":"themes/next/.gitignore","hash":"087b7677078303acb2acb47432165950e4d29b43","modified":1751874689090},{"_id":"themes/next/.stylelintrc","hash":"20f46858e47aba6b3fc47e9b43e9f7531fa66e33","modified":1751874689090},{"_id":"themes/next/LICENSE.md","hash":"8cfb03967dd4cbaf3b825271ffce0039aa3fc22a","modified":1751874689090},{"_id":"themes/next/_vendors.yml","hash":"0c3c1b4e3e5f89985aca0e24cc8dad496e737634","modified":1751874689094},{"_id":"themes/next/README.md","hash":"cfafa81c8df2c64b0db6a14c7f5e0bb014936b36","modified":1751874689093},{"_id":"themes/next/crowdin.yml","hash":"4a53f5985e545c635cb56b2a57ed290cb8cf8942","modified":1751874689094},{"_id":"themes/next/_config.yml","hash":"dc1b1942d463024642377d0c7c32ff7a493b7739","modified":1752217913629},{"_id":"themes/next/eslint.config.js","hash":"e35570c8e7ef9ea4adad7bafb3558dfd8e928e48","modified":1751874689098},{"_id":"themes/next/package.json","hash":"761cf8d5dd2a376b18a088d9e908ea75d9270b58","modified":1751874689163},{"_id":"themes/next/renovate.json","hash":"767b077c7b615e20af3cf865813cd64674a9bea6","modified":1751874689163},{"_id":"themes/next/.github/CODE_OF_CONDUCT.md","hash":"82a3d202da8fe6ce765715bd5b7571676e90364d","modified":1751874689077},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"2fdca1040427cabfe27cae6754ec5e027ec7092e","modified":1751874689078},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"610675119f83cdbf3f19d7996b17e33062e3b165","modified":1751874689083},{"_id":"themes/next/.github/issue_label_bot.yaml","hash":"533fbe6b2f87d7e7ec6949063bb7ea7eb4fbe52d","modified":1751874689084},{"_id":"themes/next/.github/config.yml","hash":"0956bf71b6f36632b63b14d26580458041a5abd2","modified":1751874689084},{"_id":"themes/next/.github/labeler.yml","hash":"35da1a37e9a6bdee1eeae30a8816cd75c9a04b3b","modified":1751874689086},{"_id":"themes/next/.github/label-commenter-config.yml","hash":"d403cbbbd0c554563e9e678ff82120ef9451d98a","modified":1751874689085},{"_id":"themes/next/.githooks/pre-commit","hash":"b69b9d0b51e27d5d4c87c3242f5067c2cda26e44","modified":1751874689076},{"_id":"themes/next/.github/release.yml","hash":"83b4dae3f8d76619e208d2110a247b3ccadd64d8","modified":1751874689086},{"_id":"themes/next/.githooks/install.js","hash":"72757c6827909a5f2c217ddbbdf6034ca6fab74a","modified":1751874689076},{"_id":"themes/next/docs/AGPL3.md","hash":"f463f95b169d64983f59fa6f3e4b6760290a0e6b","modified":1751874689094},{"_id":"themes/next/docs/AUTHORS.md","hash":"579014d47f45b27fd1618b9709f0efe9585c7449","modified":1751874689094},{"_id":"themes/next/docs/LICENSE.txt","hash":"d1cd5a8e83d3bbdb50f902d2b487813da95ddfd3","modified":1751874689098},{"_id":"themes/next/languages/ar.yml","hash":"c77fc13e0431e71eeb0f767a0a436284fc81df90","modified":1751874689098},{"_id":"themes/next/languages/README.md","hash":"b1c96465b3bc139bf5ba6200974b66581d8ff85a","modified":1751874689098},{"_id":"themes/next/languages/bn.yml","hash":"30ffd43588ddf2cd39432d964087242e1c3e5407","modified":1751874689098},{"_id":"themes/next/languages/de.yml","hash":"5101612f7cac38884206d28319532f1aab32fbe6","modified":1751874689104},{"_id":"themes/next/languages/default.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1751874689105},{"_id":"themes/next/languages/en.yml","hash":"5ca2c0b3c95dd87485f2d2b3477f88810ad1a63b","modified":1751874689106},{"_id":"themes/next/languages/eo.yml","hash":"7bd0124c2d1dcdbfde350fce335e130556ebcee8","modified":1751874689106},{"_id":"themes/next/languages/es.yml","hash":"31c2a47d14cd5b804cae0c0b44d43bece069d3aa","modified":1751874689107},{"_id":"themes/next/languages/fa.yml","hash":"cc6ef3144b954daa29ce7d42f08191eafe052fab","modified":1751874689107},{"_id":"themes/next/languages/fr.yml","hash":"3a7469446f77e5d1a12eee828ebc49ecb43b6c35","modified":1751874689107},{"_id":"themes/next/languages/it.yml","hash":"4969c4f17d88003135218de72ddf0bf8d894dc07","modified":1751874689108},{"_id":"themes/next/languages/id.yml","hash":"bd2c30def05a37c66653900ca07952d52a142961","modified":1751874689108},{"_id":"themes/next/languages/ja.yml","hash":"e660ec12b9eade103492aba7aa9b23ef06d353a9","modified":1751874689109},{"_id":"themes/next/languages/ko.yml","hash":"04927e331652ee13f9c8a110fe12d7f936a6b0e6","modified":1751874689109},{"_id":"themes/next/languages/nl.yml","hash":"4575f93c4ee4383bdfaca01ad6ffff5664cbbbae","modified":1751874689111},{"_id":"themes/next/languages/pt-BR.yml","hash":"ff9aa6a6549d07b7dc667094f34a06945e2c21b4","modified":1751874689111},{"_id":"themes/next/languages/pt.yml","hash":"2c86b3235de9539df2119e52e913d847ecf782c0","modified":1751874689111},{"_id":"themes/next/languages/ru.yml","hash":"d726265bc5cf8bd4fb64f85b2d07b3326439bb00","modified":1751874689112},{"_id":"themes/next/languages/si.yml","hash":"07f98d6ca99bceef35254c60fc0e4401ff608557","modified":1751874689112},{"_id":"themes/next/languages/th.yml","hash":"dc434da945d52dc7a8d26acc80b23e6922712f89","modified":1751874689112},{"_id":"themes/next/languages/tk.yml","hash":"7b9115e0520dc81a95a7c8e4f56a01fc84f74017","modified":1751874689112},{"_id":"themes/next/languages/tr.yml","hash":"822ec400a0a2bb2245abf31e9ed4f3af77b2814e","modified":1751874689114},{"_id":"themes/next/languages/uk.yml","hash":"10a2f2e7183410d1d10fa937e3e0198b46bf4930","modified":1751874689114},{"_id":"themes/next/languages/vi.yml","hash":"8344cb90ee92a21a398faff3317c025a43446db5","modified":1751874689116},{"_id":"themes/next/languages/zh-CN.yml","hash":"c06c95121b3e5bd5abe01848073fbe7f9d6dcda9","modified":1751874689116},{"_id":"themes/next/languages/zh-HK.yml","hash":"293009343786f9b460412b7032e694755e5edc8d","modified":1751874689117},{"_id":"themes/next/languages/zh-TW.yml","hash":"664ebfa09503d29d6f33a5ffadaa16989785e0fe","modified":1751874689117},{"_id":"themes/next/layout/_layout.njk","hash":"e8dd48352cbdb8ade0fb4f9380c5fe3dab0a995e","modified":1751874689118},{"_id":"themes/next/layout/archive.njk","hash":"aa491dba8f746e626c273a920effedf7d0b32170","modified":1751874689156},{"_id":"themes/next/layout/category.njk","hash":"82f541452cae76a94ee15cb8d8a888f44260a0fd","modified":1751874689156},{"_id":"themes/next/layout/index.njk","hash":"fa52c3049871e879980cb6abccdea3792ca4ce70","modified":1751874689160},{"_id":"themes/next/layout/page.njk","hash":"d24238eea91a347050a663c7a7de8cebaab4c595","modified":1751874689161},{"_id":"themes/next/layout/tag.njk","hash":"b6c017d30d08ddd30d66e9c6f3a71aa65d214eac","modified":1751874689161},{"_id":"themes/next/layout/post.njk","hash":"6767de9702a07a2a4e16a8a6bc9c5919536c1e3f","modified":1751874689161},{"_id":"themes/next/test/index.js","hash":"c0723bd61aed6872f184aa743b782dc0b9a172e1","modified":1751874689262},{"_id":"themes/next/.github/ISSUE_TEMPLATE/bug-report.yml","hash":"3f19cbf0c2b2fee6bf3788870b842c9ccc1425ca","modified":1751874689079},{"_id":"themes/next/.github/ISSUE_TEMPLATE/feature-request.yml","hash":"fbc3062cd4591c8329fab9db72803746f0d11920","modified":1751874689081},{"_id":"themes/next/.github/ISSUE_TEMPLATE/config.yml","hash":"daeedc5da2ee74ac31cf71846b766ca6499e9fc6","modified":1751874689079},{"_id":"themes/next/.github/ISSUE_TEMPLATE/other.yml","hash":"10eca518b91a19984f6a5a912d41222042f61d63","modified":1751874689081},{"_id":"themes/next/.github/workflows/codeql.yml","hash":"82a2df4c676ae45b2768c71c724b188475c88ca5","modified":1751874689087},{"_id":"themes/next/.github/workflows/label-commenter.yml","hash":"dc4809131c54a1d9f8e7392aeaf53b13c6befd90","modified":1751874689088},{"_id":"themes/next/.github/workflows/linter.yml","hash":"294660e1b6275a0fe4db2a44e5b6b679467d29fc","modified":1751874689089},{"_id":"themes/next/.github/workflows/labeler.yml","hash":"e8ca602587b3abe08b60fc1bdc1c2d62aeeb85bb","modified":1751874689088},{"_id":"themes/next/.github/workflows/lock.yml","hash":"80dc185dfaeb2927db72a44ff2382e05f47a2df5","modified":1751874689089},{"_id":"themes/next/.github/workflows/npm-publish.yml","hash":"4154828f8fcc37011e636fc1c3448f3b15350d44","modified":1751874689090},{"_id":"themes/next/docs/ru/README.md","hash":"2c22c6f8c77cd22d57b4a556826fa9cbd4034f85","modified":1751874689098},{"_id":"themes/next/.github/workflows/tester.yml","hash":"dba3f1d4bc4d2fab51d73d134ed5c7448a0976dc","modified":1751874689090},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"a09ceb82b45dd8b7da76c227f3d0bb7eebe7d5d1","modified":1751874689098},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"e077bebf4fb89b159b840b30f0230b95ff7db8a5","modified":1751874689098},{"_id":"themes/next/layout/_macro/post-collapse.njk","hash":"a60b386c7889f25f811182cae76e6474cb3254d1","modified":1751874689119},{"_id":"themes/next/docs/zh-CN/README.md","hash":"45481be96a710e53498721d7d1db1edb36318606","modified":1751874689098},{"_id":"themes/next/layout/_macro/sidebar.njk","hash":"b9fc0a6b02f0380de939d1ea2a7c7d2e6be9f462","modified":1751874689121},{"_id":"themes/next/layout/_macro/post.njk","hash":"775f264e8deb66653153117c8bd43854594cb36c","modified":1751874689119},{"_id":"themes/next/layout/_partials/comments.njk","hash":"60f4620dd479da6d86d1d493440e9e2a28b6e132","modified":1751874689121},{"_id":"themes/next/layout/_partials/footer.njk","hash":"cc51596980560ab60626da76260b7f5e83df8a33","modified":1751874689122},{"_id":"themes/next/layout/_partials/pagination.njk","hash":"c55167bc0dbe7e104d4f4d782e98fcabc7e07a35","modified":1751874689131},{"_id":"themes/next/layout/_partials/languages.njk","hash":"537026fc120adeef9148c98ebf074207e3810538","modified":1751874689127},{"_id":"themes/next/layout/_scripts/index.njk","hash":"6379fb7776ba2a93eb0220e5fa2adee1e3c5d9b0","modified":1751874689137},{"_id":"themes/next/layout/_partials/widgets.njk","hash":"bb5d32170b469dc018ceaa10a5b7cb892c9b85a4","modified":1751874689135},{"_id":"themes/next/layout/_scripts/vendors.njk","hash":"71691096a1a034f6af68403730a1589c0d7fabaa","modified":1751874689138},{"_id":"themes/next/layout/_third-party/addtoany.njk","hash":"9ef81adb0c35ebdb4499602155e87611aee0762a","modified":1751874689138},{"_id":"themes/next/layout/_third-party/index.njk","hash":"9bab629e2971b029545511b48f397445858ac7f0","modified":1751874689151},{"_id":"themes/next/layout/_third-party/fancybox.njk","hash":"53ad3c31762b74e5d29787b37d5e494cc4fded9b","modified":1751874689149},{"_id":"themes/next/layout/_third-party/quicklink.njk","hash":"73bc15a9c3c5c239ab90efa19a1e721f41f3cb93","modified":1751874689152},{"_id":"themes/next/layout/_third-party/pace.njk","hash":"13b2a77b4858a127f458ea092b6f713b052befac","modified":1751874689152},{"_id":"themes/next/scripts/events/index.js","hash":"8ae618d4436dab49928c2bebc0837e5310dbe7de","modified":1751874689165},{"_id":"themes/next/scripts/filters/default-injects.js","hash":"0c9a1fe9906672724dbf274154a37bac1915ca2c","modified":1751874689174},{"_id":"themes/next/scripts/filters/locals.js","hash":"87f3bff03fa9fd96cf0787a6442464a7ff57f76b","modified":1751874689174},{"_id":"themes/next/scripts/filters/minify.js","hash":"0f6b9b0eb41f6319e75ff86f9254cf1a9d3333b8","modified":1751874689175},{"_id":"themes/next/scripts/filters/post.js","hash":"e6cf4c94fc2291215a3345134ddbbc74d5091b00","modified":1751874689175},{"_id":"themes/next/scripts/helpers/engine.js","hash":"b69e324ae9750cb35a4609c41b73d52177cc94e9","modified":1751874689177},{"_id":"themes/next/scripts/helpers/font.js","hash":"dd76be4927a77c27292a4790bcc659c10518a219","modified":1751874689177},{"_id":"themes/next/scripts/helpers/navigation.js","hash":"82f6e81bacf23d10cac71b09cff8708e47891ef3","modified":1751874689177},{"_id":"themes/next/scripts/helpers/next-config.js","hash":"2a10b4b8879ccd7ea8b36253fe7a27f4161e6b82","modified":1751874689178},{"_id":"themes/next/scripts/helpers/next-paginator.js","hash":"2e8dcc12a52517bb4e00ea611720bcd298624112","modified":1751874689179},{"_id":"themes/next/scripts/helpers/next-url.js","hash":"ceff5e76564a5d6ed3e0222bccf581e106d80f3c","modified":1751874689179},{"_id":"themes/next/scripts/helpers/next-vendors.js","hash":"12814eb1845830204e3a398eb93a2c15e0ff8d94","modified":1751874689180},{"_id":"themes/next/scripts/tags/button.js","hash":"86c71c73a63744efbbbb367612871fede0d69529","modified":1751874689180},{"_id":"themes/next/scripts/tags/caniuse.js","hash":"8e912c715702addaf0cefe63e580e45b97ae8c3f","modified":1751874689181},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"b4d12e6fe29089be0f43bafc9eea736602cd16bf","modified":1751874689181},{"_id":"themes/next/scripts/tags/index.js","hash":"1a680bb01e14152905efe3ef624621dd74cf27c3","modified":1751874689183},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"652140814527db74bcde9110142824dc727be889","modified":1751874689182},{"_id":"themes/next/scripts/tags/label.js","hash":"c18b0e619a779ed40be7f014db92af18f45fbd5c","modified":1751874689183},{"_id":"themes/next/scripts/tags/link-grid.js","hash":"3f358bb78c5c6fdf45de287f3ead553e3a6a93c2","modified":1751874689183},{"_id":"themes/next/scripts/tags/mermaid.js","hash":"b139224ff2cc668f4d95bde184934833e05b29df","modified":1751874689184},{"_id":"themes/next/scripts/tags/note.js","hash":"a12fd53e421400836a3722ae69130969558d6ac0","modified":1751874689184},{"_id":"themes/next/scripts/tags/pdf.js","hash":"317ba4611020cc840854386dde098dbbe452777e","modified":1751874689184},{"_id":"themes/next/scripts/tags/tabs.js","hash":"e0ed5fe1bc9d2957952a1aacdf3252d6ef3f9743","modified":1751874689184},{"_id":"themes/next/scripts/tags/video.js","hash":"f6ad3f52779f0636251238d3cbdc5b6f91cc5aba","modified":1751874689186},{"_id":"themes/next/scripts/tags/wavedrom.js","hash":"188c1dd5d7dbc566cac00946da86aa76fff1c682","modified":1751874689186},{"_id":"themes/next/source/css/_colors.styl","hash":"9354013ea89c8e3874a44929c394db39ae04f36a","modified":1751874689188},{"_id":"themes/next/source/css/_mixins.styl","hash":"bb868086bf28029d2fb9f6d4abe620c382d66568","modified":1751874689223},{"_id":"themes/next/source/css/noscript.styl","hash":"64b378a4d2f0b36bf666fba13b57c59da3a8ac83","modified":1751874689237},{"_id":"themes/next/source/css/main.styl","hash":"c326550ce8b4deaa9b647bcfe3cdc04c100644e6","modified":1751874689236},{"_id":"themes/next/source/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1751874689238},{"_id":"themes/next/source/images/logo-algolia-nebula-blue-full.svg","hash":"5a81f1c5d66561b3bcb05dae48148088a3fb5c79","modified":1751874689239},{"_id":"themes/next/source/images/logo.svg","hash":"099e11ab995a2c8981427a85476d082609848c77","modified":1751874689241},{"_id":"themes/next/source/js/bookmark.js","hash":"e2fd71f2876531f4c8bf1f38828aae35dd82cca4","modified":1751874689241},{"_id":"themes/next/source/js/comments-buttons.js","hash":"81ea6cbcdf0357094753d7523919c1eafa38e79f","modified":1751874689242},{"_id":"themes/next/source/js/comments.js","hash":"0b4daf0ce610760bd52e95d423f61f3e1c72442a","modified":1751874689242},{"_id":"themes/next/source/js/config.js","hash":"211a9ab35205ccfa6b7c74394bade84da0d00af7","modified":1751874689243},{"_id":"themes/next/source/js/motion.js","hash":"a2abc7279723aee459b4c7a357fc84d5215affcf","modified":1751874689243},{"_id":"themes/next/source/js/next-boot.js","hash":"feafaa8cd1305b773f793bd36dee71fd832044a2","modified":1751874689244},{"_id":"themes/next/source/js/pjax.js","hash":"ca513ae9aa3ab3dfe46668f0717d9d37b571c5b7","modified":1751874689244},{"_id":"themes/next/source/js/schedule.js","hash":"4858233224756e3cadfabd8135f7c4b1e3673b44","modified":1751874689245},{"_id":"themes/next/source/js/sidebar.js","hash":"f3052996408a09233ffaf8d631fd930443ccd781","modified":1751874689245},{"_id":"themes/next/test/helpers/index.js","hash":"2fb58dca3df2fe53116ee2b1232fa26ebe7b2ce5","modified":1751874689262},{"_id":"themes/next/test/helpers/font.js","hash":"f61d3a1de5261728398d2de57e33452cf7d93383","modified":1751874689262},{"_id":"themes/next/source/js/utils.js","hash":"381223fa6bba40d973550deb05bc409c3962338b","modified":1751874689256},{"_id":"themes/next/test/tags/button.js","hash":"68b2f363286ce08e6b64d5d0d90aad6960e13a93","modified":1751874689264},{"_id":"themes/next/test/helpers/next-url.js","hash":"4f39453286113d3d1e64bcc1934b5f974a33102d","modified":1751874689262},{"_id":"themes/next/test/tags/caniuse.js","hash":"32f1ae9d49fcf0a1ebaa07f1f274aaf6f85b4699","modified":1751874689264},{"_id":"themes/next/test/tags/center-quote.js","hash":"c073b7ee0f72d1c304522030a8eea68878adfcba","modified":1751874689264},{"_id":"themes/next/test/tags/group-pictures.js","hash":"1232c69689eee53b5b5926beb66f402a3bec5581","modified":1751874689266},{"_id":"themes/next/test/tags/index.js","hash":"5cad001936a694bf32d59751cc2b68a66199f976","modified":1751874689266},{"_id":"themes/next/test/tags/link-grid.js","hash":"8b0ce804b53329c0e76259a94a24c60a66b9f77b","modified":1751874689268},{"_id":"themes/next/test/tags/label.js","hash":"11c90db85fd373b08fd4241c3adfb19057e34025","modified":1751874689268},{"_id":"themes/next/test/tags/mermaid.js","hash":"ff6751c3e77db4c80be9c03b896451b1fd1b185f","modified":1751874689268},{"_id":"themes/next/test/tags/note.js","hash":"2c4127a43850d89fe13064ae26e3b45bc72b525d","modified":1751874689269},{"_id":"themes/next/test/tags/pdf.js","hash":"4303175f1702adabd445aec58101d48df8c1a3ba","modified":1751874689269},{"_id":"themes/next/test/tags/video.js","hash":"6ab9deb1d702bda3132e6c45fa5e6b51c79a7151","modified":1751874689270},{"_id":"themes/next/test/tags/tabs.js","hash":"2f809f250a0445671912ae0fd54bbb6c0f5e9880","modified":1751874689270},{"_id":"themes/next/test/validate/index.js","hash":"9d44aff73371402574a8b61dda1037a58e382b52","modified":1751874689271},{"_id":"themes/next/layout/_partials/head/head-unique.njk","hash":"c2ee46d744afa78d95c6bca5dfcbb2fbc38474c2","modified":1751874689123},{"_id":"themes/next/layout/_partials/header/brand.njk","hash":"7bf2b6ab499775355bbf8f3ae597ff7bc00e89e4","modified":1751874689124},{"_id":"themes/next/layout/_partials/head/head.njk","hash":"e016c3f80db433f17781caf26e44f2089a71550a","modified":1751874689123},{"_id":"themes/next/layout/_partials/header/index.njk","hash":"1b2ae17f3c394ce310fe2d9ed5f4d07d8cc74ae7","modified":1751874689124},{"_id":"themes/next/layout/_partials/header/menu.njk","hash":"828700af2a2f273d3e3e0554ffc60706d889d539","modified":1751874689126},{"_id":"themes/next/layout/_partials/header/sub-menu.njk","hash":"20cb9c39bcdcfa3f710df7bc5838d6e62d4dd674","modified":1751874689126},{"_id":"themes/next/layout/_partials/header/menu-item.njk","hash":"62513f08e9e7f4abeaeedca91fd0af0861a2540f","modified":1751874689124},{"_id":"themes/next/layout/_partials/page/breadcrumb.njk","hash":"1fe44a1e156cd30e3e9fd8313e1011ad30970f83","modified":1751874689128},{"_id":"themes/next/layout/_partials/page/schedule.njk","hash":"6ec9c97e91c793cc2eb5ac0f7c3c36fdaaf637d1","modified":1751874689128},{"_id":"themes/next/layout/_partials/page/categories.njk","hash":"b352346dd2cb42f7eeaec5e39d9a2a353b029775","modified":1751874689128},{"_id":"themes/next/layout/_partials/page/tags.njk","hash":"752df7d12360a077c51a25609916a3ecc1763bb3","modified":1751874689128},{"_id":"themes/next/layout/_partials/page/page-header.njk","hash":"92553feb26f30f7fc9147bc4ef122908a9da06be","modified":1751874689128},{"_id":"themes/next/layout/_partials/post/post-copyright.njk","hash":"22c922214982c4bb78ea92fa2ff1de93d98216a3","modified":1751874689131},{"_id":"themes/next/layout/_partials/post/post-followme.njk","hash":"ebe45ab38ace03ea74eed3f676ec1cb5805a3c8a","modified":1751874689131},{"_id":"themes/next/layout/_partials/post/post-meta.njk","hash":"9a9c4fb7e7c4fe4b7d474bdfdb4ed2b0a5423df2","modified":1751874689131},{"_id":"themes/next/layout/_partials/post/post-related.njk","hash":"9a7eda45ad0753d5f624c51a56e00277ba97c93d","modified":1751874689131},{"_id":"themes/next/layout/_partials/post/post-reward.njk","hash":"fd6fde597ea2fb7b80cffd0a3553c8e73acb8ab2","modified":1751874689131},{"_id":"themes/next/layout/_partials/post/post-share.njk","hash":"8a6414987474592e82b3e57eeb0bd526ae42d235","modified":1751874689131},{"_id":"themes/next/layout/_partials/search/index.njk","hash":"10145ae7ef87b502d20bfa08beb2b899228f1419","modified":1751874689135},{"_id":"themes/next/layout/_partials/sidebar/site-overview.njk","hash":"8d76300e31178e790400a37adfaf9bc0e5f8fae7","modified":1751874689135},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.njk","hash":"3e80332f88b101141be69f2a07f54ed8c053eabb","modified":1751874689140},{"_id":"themes/next/layout/_third-party/analytics/cloudflare.njk","hash":"17173c45f0b740669ef45aaae1fe96eeb0a0ff52","modified":1751874689141},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.njk","hash":"52ad137450f7b3d6a330e16b3ed1c6174290f0eb","modified":1751874689141},{"_id":"themes/next/layout/_third-party/analytics/growingio.njk","hash":"9ff9ec05c2037beea229a6bb698f9e3546973220","modified":1751874689142},{"_id":"themes/next/layout/_third-party/analytics/matomo.njk","hash":"43238bc1bc2e88d707f8097814ef3ff830cbe641","modified":1751874689143},{"_id":"themes/next/layout/_third-party/analytics/index.njk","hash":"81abcf790cf97395cde9e3cd6d5d1aa5656bd134","modified":1751874689142},{"_id":"themes/next/layout/_third-party/analytics/microsoft-clarity.njk","hash":"2129a017a1b1ab751aca9c70e76461ad91b2a81e","modified":1751874689143},{"_id":"themes/next/layout/_third-party/analytics/plausible.njk","hash":"64ed6fecae79f9f9c61e956d2bd077792473e069","modified":1751874689143},{"_id":"themes/next/layout/_third-party/analytics/umami.njk","hash":"c8b8c10dcf5c4ffa4a8aec5ba5892842ec204ac0","modified":1751874689144},{"_id":"themes/next/layout/_third-party/comments/disqus.njk","hash":"b0828dd1b1fd66ecd612d9e886a08e7579e9a4f7","modified":1751874689147},{"_id":"themes/next/layout/_third-party/comments/changyan.njk","hash":"5f7967bd946060f4102263a552ddfbae9975e7ea","modified":1751874689146},{"_id":"themes/next/layout/_third-party/comments/disqusjs.njk","hash":"c5086b4c35f730f82c99c4a8317f2f153ebde869","modified":1751874689147},{"_id":"themes/next/layout/_third-party/comments/gitalk.njk","hash":"6fd4df5c21cfe530dbb0c012bc0b202f2c362b9c","modified":1751874689147},{"_id":"themes/next/layout/_third-party/comments/isso.njk","hash":"38badcc7624a13961381c2465478056b9602aee5","modified":1751874689148},{"_id":"themes/next/layout/_third-party/comments/livere.njk","hash":"b8e0d5de584cece5e05b03db5b86145aa1e422b4","modified":1751874689148},{"_id":"themes/next/layout/_third-party/comments/utterances.njk","hash":"a7921be7328e1509d33b435175f5333a9aada66f","modified":1751874689149},{"_id":"themes/next/layout/_third-party/chat/chatra.njk","hash":"09d2c9487d75894d45a823e3237ae9f90fd6ee01","modified":1751874689145},{"_id":"themes/next/layout/_third-party/chat/tidio.njk","hash":"3fbc72427c1211e5dcfd269af1a74852a7ba5c1a","modified":1751874689145},{"_id":"themes/next/layout/_third-party/math/katex.njk","hash":"1df65b1390add93b86ae3f9423d96a130b0ece04","modified":1751874689151},{"_id":"themes/next/layout/_third-party/math/mathjax.njk","hash":"a62aa1ed4e35b8d0451d83f341bf0a97538bc9a4","modified":1751874689152},{"_id":"themes/next/layout/_third-party/math/index.njk","hash":"1856c4b035c5b8e64300a11af0461b519dfc4cf4","modified":1751874689151},{"_id":"themes/next/layout/_third-party/search/algolia-search.njk","hash":"599f3f5e4385501d0010edc02ed9ca547d2467f7","modified":1751874689154},{"_id":"themes/next/layout/_third-party/search/localsearch.njk","hash":"210c32b654adae3d8076c4417d370b42af258cea","modified":1751874689155},{"_id":"themes/next/layout/_third-party/statistics/busuanzi-counter.njk","hash":"442df8a04f1967779cdd7599156496bdac8f1f23","modified":1751874689155},{"_id":"themes/next/layout/_third-party/statistics/firestore.njk","hash":"af5336e8bbdc4638435971da115bb7443d374ade","modified":1751874689156},{"_id":"themes/next/layout/_third-party/statistics/lean-analytics.njk","hash":"8703d1855bb8d251c9b7c2940b7e3be525e53000","modified":1751874689156},{"_id":"themes/next/layout/_third-party/statistics/index.njk","hash":"866ffa15a3250678eb8a90aa6f609fa965db90fd","modified":1751874689156},{"_id":"themes/next/layout/_third-party/tags/pdf.njk","hash":"0386c708975cc5faea4f782611c5d2c6b8ac2850","modified":1751874689156},{"_id":"themes/next/layout/_third-party/tags/mermaid.njk","hash":"dd8f963acd5a3685be46fd5319c06df0308d99b2","modified":1751874689156},{"_id":"themes/next/scripts/events/lib/config.js","hash":"b4944b3272edecceedc3935f8606e810f9ff237e","modified":1751874689166},{"_id":"themes/next/layout/_third-party/tags/wavedrom.njk","hash":"baec18165d767094ecb2dda7141ca3bd8c6f2eb9","modified":1751874689156},{"_id":"themes/next/scripts/events/lib/highlight.js","hash":"df360f546d59b1fee8926806268754433dfa5de7","modified":1751874689166},{"_id":"themes/next/scripts/events/lib/injects.js","hash":"1f1ea7b579a49f17574c31d78d663c54896133eb","modified":1751874689167},{"_id":"themes/next/scripts/events/lib/navigation.js","hash":"0a2df7d40d64c61ce7e730bb8269b848b03526d1","modified":1751874689167},{"_id":"themes/next/scripts/events/lib/utils.js","hash":"da22c6008fc8d214ba8561b412233380a06c40f9","modified":1751874689168},{"_id":"themes/next/scripts/events/lib/vendors.js","hash":"af3e7066a7a8d87448aadde3bc9c848077aae418","modified":1751874689168},{"_id":"themes/next/scripts/filters/comment/changyan.js","hash":"761760031e49edb77277bd4a8582774959240d6b","modified":1751874689169},{"_id":"themes/next/scripts/filters/comment/common.js","hash":"550cc7f57bc3ab3d093e34741e7860222851d4d4","modified":1751874689171},{"_id":"themes/next/scripts/filters/comment/default-config.js","hash":"1cb58aa6b88f7461c3c3f9605273686adcc30979","modified":1751874689171},{"_id":"themes/next/scripts/filters/comment/disqus.js","hash":"3283bdd6e5ac7d10376df8ddd5faaec5dc1bd667","modified":1751874689171},{"_id":"themes/next/scripts/filters/comment/disqusjs.js","hash":"4dbe9652ae53a181c86eb7e9005a5255f0540055","modified":1751874689171},{"_id":"themes/next/scripts/filters/comment/gitalk.js","hash":"96e58efba0dc76af409cc7d2db225f0fe4526ea8","modified":1751874689171},{"_id":"themes/next/scripts/filters/comment/isso.js","hash":"c22cbccd7d514947e084eeac6a3af1aa41ec857a","modified":1751874689171},{"_id":"themes/next/scripts/filters/comment/livere.js","hash":"bb8ebb541c40362c0cbbd8e83d3b777302bb6c40","modified":1751874689171},{"_id":"themes/next/scripts/filters/comment/utterances.js","hash":"a50718c081685fd35ff8ea9ca13682c284399ed8","modified":1751874689171},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"c4537fa2de33d98baff2c87a73801770414e0b69","modified":1751874689235},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"9cd228d5de7c2e49a231be072c9ef13af2dd95ff","modified":1751874689235},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"c42a1685b29650e0ab078a496b2bfe0a4483f922","modified":1751874689236},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"54c52744898eb5d2073b83f98bb1a61759da92dc","modified":1751874689236},{"_id":"themes/next/source/css/_variables/base.styl","hash":"c4fda1066a429e77828f457aea5d5b06be3eeda6","modified":1751874689236},{"_id":"themes/next/source/js/third-party/addtoany.js","hash":"5cff12a5c9fc55a68f0f57a291494cd738301ba4","modified":1751874689245},{"_id":"themes/next/source/js/third-party/pace.js","hash":"0ebee77b2307bf4b260afb06c060171ef42b7141","modified":1751874689256},{"_id":"themes/next/source/js/third-party/fancybox.js","hash":"e3022c2ea60409a82a3e2a0d3615e4a25c684551","modified":1751874689254},{"_id":"themes/next/source/js/third-party/quicklink.js","hash":"5b0197e061f57e00875be3636ba372a67693abe0","modified":1751874689256},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"f882924c7b99afc23106e39164516888b2972c7b","modified":1751874689188},{"_id":"themes/next/source/css/_common/components/index.styl","hash":"49c3063b46d65796003a822deed971abee41675e","modified":1751874689188},{"_id":"themes/next/source/css/_common/outline/index.styl","hash":"7782dfae7a0f8cd61b936fa8ac980440a7bbd3bb","modified":1751874689206},{"_id":"themes/next/source/css/_common/components/reading-progress.styl","hash":"f3defd56be33dba4866a695396d96c767ce63182","modified":1751874689199},{"_id":"themes/next/source/css/_common/outline/mobile.styl","hash":"883a4f42badb7b206604eb2805ae582c3a511403","modified":1751874689206},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"0951dfc96a5c4aca904142e7f1e57e2ad2dd2e10","modified":1751874689213},{"_id":"themes/next/source/css/_common/scaffolding/buttons.styl","hash":"f768ecb2fe3e9384777c1c115cd7409e9155edd7","modified":1751874689213},{"_id":"themes/next/source/css/_common/scaffolding/comments.styl","hash":"cf8446f4378dcab27b55ede1635c608ae6b8a5c8","modified":1751874689214},{"_id":"themes/next/source/css/_common/scaffolding/index.styl","hash":"43045d115f8fe95732c446aa45bf1c97609ff2a5","modified":1751874689215},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"b9388016f8d9274703e77e306a1feaad1b7b9d6c","modified":1751874689216},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"6d740699fb6a7640647a8fd77c4ea4992d8d6437","modified":1751874689216},{"_id":"themes/next/source/css/_common/scaffolding/pagination.styl","hash":"fd4a2d2303f107a021837aa6e3d977c2f9d14c42","modified":1751874689216},{"_id":"themes/next/source/css/_common/scaffolding/toggles.styl","hash":"b9322d644b2090d793521555646673322f2dd5e8","modified":1751874689223},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"ce25cb35779f29d01f7b7fc51ee09e68987ad318","modified":1751874689224},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"be9d0e9493802c22f16a0645a59d0c2b556c1c3c","modified":1751874689225},{"_id":"themes/next/source/css/_schemes/Mist/_layout.styl","hash":"2db5ada757f7ffb9a36b06400cf30d80ec039b6e","modified":1751874689225},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"0bf248d61593fbcdfd83b018b92199f4da3699b1","modified":1751874689226},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expand.styl","hash":"68f8d30071516913a7a958e01d010e8a93f7aa24","modified":1751874689226},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"89bf3f6b82cb0fafbbd483431df8f450857c5a0b","modified":1751874689228},{"_id":"themes/next/source/css/_schemes/Muse/_header.styl","hash":"2f19fe2aba8f63fc99641e50bcb96cc9a4feb8a4","modified":1751874689228},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"e6e8892fccb02bff163d9b574984e0440a00d756","modified":1751874689228},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"b26770e09de9b51c851bb90fae19b27860e7fc9f","modified":1751874689228},{"_id":"themes/next/source/css/_schemes/Muse/_sidebar.styl","hash":"2fb4b3f88e34a8544436a05634690217f9483419","modified":1751874689229},{"_id":"themes/next/source/css/_schemes/Muse/_sub-menu.styl","hash":"2d3e05015796a790abd9d68957a5c698c0c9f9b6","modified":1751874689230},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"25c2a7930da14f023329df20f38df2728057fb4d","modified":1751874689230},{"_id":"themes/next/source/css/_schemes/Pisces/_header.styl","hash":"36438f7b47d7ebd250a25f97c743fda484283046","modified":1751874689231},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"3ab87229f9a8357446f77f05a0c6854fb5363fd5","modified":1751874689231},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"7a3b5cd21bc35d92358abb07fc0986722dd10881","modified":1751874689231},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"b5c3dd08c520a16ee49f85fa12b4935e725ef261","modified":1751874689231},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"7125fb0dba920e89c3f7c1c07e2f7cce1fbdb703","modified":1751874689231},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"7905f428b46d100ac5928875cb1e2b99fa86fc0b","modified":1751874689233},{"_id":"themes/next/source/js/third-party/analytics/baidu-analytics.js","hash":"f9579a02599de063ccff336177ba964a2931a6e9","modified":1751874689246},{"_id":"themes/next/source/js/third-party/analytics/growingio.js","hash":"f755e8537ccbbb0bd84c26923f320d4e206e7428","modified":1751874689248},{"_id":"themes/next/source/js/third-party/analytics/google-analytics.js","hash":"27a27de3d4e0d33e9c647f7ae78e1dd6d36d2463","modified":1751874689248},{"_id":"themes/next/source/js/third-party/analytics/matomo.js","hash":"f24604710002234f18a7b5cfe9ccdf3ed6e725a8","modified":1751874689248},{"_id":"themes/next/source/js/third-party/chat/chatra.js","hash":"72e0766752b78a723fb30e92d533a8b353104e2d","modified":1751874689248},{"_id":"themes/next/source/js/third-party/chat/tidio.js","hash":"77c231bcd64f1c09bd9989909e9fee703b65f47f","modified":1751874689248},{"_id":"themes/next/source/js/third-party/comments/changyan.js","hash":"b4cb06fdf531292e2361398a98d75a4ca2b8473a","modified":1751874689251},{"_id":"themes/next/source/js/third-party/comments/disqus.js","hash":"07e0038b221b9a4fd8ccac75093de3dee1f8315e","modified":1751874689251},{"_id":"themes/next/source/js/third-party/comments/disqusjs.js","hash":"5673d28509a8e837a512da490a88b7bf5162fc49","modified":1751874689251},{"_id":"themes/next/source/js/third-party/comments/gitalk.js","hash":"1e8509356fb027d948d118ab220d9631f4d482fa","modified":1751874689253},{"_id":"themes/next/source/js/third-party/comments/isso.js","hash":"b9b9fd2f0e098a123b34a4932da912a9485ffe6c","modified":1751874689253},{"_id":"themes/next/source/js/third-party/comments/livere.js","hash":"68892d74ef5fc308c6e7e6b4f190826d79f3055d","modified":1751874689253},{"_id":"themes/next/source/js/third-party/comments/utterances.js","hash":"ec44d7f1c8b51b0aa3cccba099a78f3575ac828c","modified":1751874689253},{"_id":"themes/next/source/js/third-party/math/katex.js","hash":"5c63ec71458b4fe0cd98fd4a04e11c3746764f11","modified":1751874689254},{"_id":"themes/next/source/js/third-party/math/mathjax.js","hash":"d93556184b2c0aa1dbc4a6fb892d2f77b80d7d9f","modified":1751874689256},{"_id":"themes/next/source/js/third-party/search/algolia-search.js","hash":"9486f0ba64182ce1a5b2524c10d02e0b992c7300","modified":1751874689256},{"_id":"themes/next/source/js/third-party/search/local-search.js","hash":"9f07ea758044afea0a293d75e6585ad65888fb71","modified":1751874689256},{"_id":"themes/next/source/js/third-party/statistics/firestore.js","hash":"ce12b5007c9aa997738641c06a1c081c357e27bd","modified":1751874689256},{"_id":"themes/next/source/js/third-party/statistics/lean-analytics.js","hash":"7db3233f0b33870943ce1547326a67f9e628b411","modified":1751874689256},{"_id":"themes/next/source/js/third-party/tags/pdf.js","hash":"e109c2d6828f527f0289d5fa3bb02fce63ee6d93","modified":1751874689256},{"_id":"themes/next/source/js/third-party/tags/mermaid.js","hash":"5ef9da034f3ec3b5b66137101ddcff0b2687ed69","modified":1751874689256},{"_id":"themes/next/source/js/third-party/tags/wavedrom.js","hash":"9cafc8f751581d496f1f3c056b95f3b8e1ebbf4e","modified":1751874689256},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"fde10ce94e9ae21a03b60d41d532835b54abdcb1","modified":1751874689191},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"f9fe39bef5fb2565abbffcb26390868c13d18359","modified":1751874689191},{"_id":"themes/next/source/css/_common/components/pages/index.styl","hash":"6cf78a379bb656cc0abb4ab80fcae60152ce41ad","modified":1751874689192},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"091b8c763e43447d087c122a86538f290f83136a","modified":1751874689192},{"_id":"themes/next/source/css/_common/components/pages/tag-cloud.styl","hash":"56d719bcdcba3d725141c55bbd4b168f3942f912","modified":1751874689193},{"_id":"themes/next/source/css/_common/components/post/index.styl","hash":"aa04527f76de7646573ea327d4f6145200d6070d","modified":1751874689193},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"c524d5990a8e7ed9caf17978cf372bb0646f7b9f","modified":1751874689194},{"_id":"themes/next/source/css/_common/components/post/post-body.styl","hash":"9fe9eb46d11cc772e37c067fcffe1b8e7ae10c31","modified":1751874689194},{"_id":"themes/next/source/css/_common/components/post/post-followme.styl","hash":"a87245394f946b607852ddcffb6ef8d5ff9f6561","modified":1751874689196},{"_id":"themes/next/source/css/_common/components/post/post-footer.styl","hash":"8aff582c5b8e17b99ad61298192a9e6564409211","modified":1751874689196},{"_id":"themes/next/source/css/_common/components/post/post-header.styl","hash":"8798669728f20843b100dbea65c70247fe978efc","modified":1751874689197},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"c34936a17c3d8af6c0988ac6746d7509dc0b50eb","modified":1751874689197},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"69dff7cf231d01f85671758455726dd666664a73","modified":1751874689197},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"6ad0da4a5585508abbe78342a680607face98e19","modified":1751874689198},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"0a698c3adba896a46e26850967cb73295c521940","modified":1751874689198},{"_id":"themes/next/source/css/_common/components/third-party/disqusjs.styl","hash":"a2ffaa06c963514a79fda52665e707af6e8e2f5e","modified":1751874689199},{"_id":"themes/next/source/css/_common/components/third-party/gitalk.styl","hash":"406c03be3cf8a8411227bc1fecf942ca368f7e2a","modified":1751874689200},{"_id":"themes/next/source/css/_common/components/third-party/index.styl","hash":"8a7b70ef14c7110aaceb30ea2f7eb4b202f0f0cd","modified":1751874689200},{"_id":"themes/next/source/css/_common/components/third-party/math.styl","hash":"1e5776ad4c5c8bcf7596ac74dcabc30704b3f5a0","modified":1751874689200},{"_id":"themes/next/source/css/_common/components/third-party/utterances.styl","hash":"d28856f365a9373c4ae6fe1e5673d63df2dfd65f","modified":1751874689200},{"_id":"themes/next/source/css/_common/components/third-party/search.styl","hash":"9394674b2c1e256ba93235c0dae492a8a3e850bb","modified":1751874689200},{"_id":"themes/next/source/css/_common/outline/footer/index.styl","hash":"39066b4e2e914d85736c9c33fa51f21a3f86c0e4","modified":1751874689200},{"_id":"themes/next/source/css/_common/outline/header/bookmark.styl","hash":"c8648c8ea3105556be0068d9fb2735261d0d94bc","modified":1751874689200},{"_id":"themes/next/source/css/_common/outline/header/github-banner.styl","hash":"05af22f3edc2383a3d97ec4c05e9ac43b014bead","modified":1751874689200},{"_id":"themes/next/source/css/_common/outline/header/index.styl","hash":"038625515ba4760e4dda6792549bddc0db5a3d20","modified":1751874689200},{"_id":"themes/next/source/css/_common/outline/header/site-meta.styl","hash":"dce8ea62d7d7b08a0444fbb2f617be30bfcfc152","modified":1751874689205},{"_id":"themes/next/source/css/_common/outline/header/site-nav.styl","hash":"d9bc2b520636b9df7f946295cd430593df4118ff","modified":1751874689206},{"_id":"themes/next/source/css/_common/outline/header/menu.styl","hash":"f4de9eb94bebdf7790522e103de51205a64cae9e","modified":1751874689205},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"63fb6d36d9ea62c7e39274c666e102b12f64ff77","modified":1751874689209},{"_id":"themes/next/source/css/_common/outline/sidebar/index.styl","hash":"7a47adf10809dff5cbaa6732cf6aa273e4673fd0","modified":1751874689208},{"_id":"themes/next/source/css/_common/outline/sidebar/related-posts.styl","hash":"b3689beb90cda64cafecaf6b25981fe8a9525992","modified":1751874689208},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"d8a028f532d562e6a86bb3b9c7b992e4b6dbbb51","modified":1751874689209},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"69869f1e317d78c03b3ef3a13e553d2c4ad04caf","modified":1751874689210},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-copyright.styl","hash":"a6fbede106afe30a9a7918b52ea8233e020b1382","modified":1751874689210},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"f5c1a6afcd2d460ccd5987ef1af747fc99d3ea72","modified":1751874689210},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"8f82d5141a18f6319e0609a3f6e4fcca6d442203","modified":1751874689211},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"cbc6b0938a2e60f35a5df32210904fb16e4938da","modified":1751874689211},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"794eabec62d6070255fa10516487cff70e0030bf","modified":1751874689211},{"_id":"themes/next/source/css/_common/outline/sidebar/site-state.styl","hash":"2de038def2cb91da143b14696366c14a66e0e569","modified":1751874689211},{"_id":"themes/next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"4d674c513300187bd3c08ecaf60358107e91c41d","modified":1751874689215},{"_id":"themes/next/source/css/_common/scaffolding/highlight/fold.styl","hash":"41c0516f76513036e30a70509a6d33e8a79bdba2","modified":1751874689215},{"_id":"themes/next/source/css/_common/scaffolding/highlight/index.styl","hash":"79e246b65cef7e32bfe4b9d32516bad1f1eabda0","modified":1751874689215},{"_id":"themes/next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"8d9218980e185210ce034e9769ab639b9630fd88","modified":1751874689219},{"_id":"themes/next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"6b3680e0dbea8e14c1cec24ef63b7fae5e37f7ef","modified":1751874689218},{"_id":"themes/next/source/css/_common/scaffolding/tags/index.styl","hash":"1772ade171aa6bd806909d69003cfc498c3af019","modified":1751874689220},{"_id":"themes/next/source/css/_common/scaffolding/tags/label.styl","hash":"531daf2612c6217950677a2d03924459ce57c291","modified":1751874689220},{"_id":"themes/next/source/css/_common/scaffolding/tags/link-grid.styl","hash":"7efdc07cf0311108c34fb8815eb890954757d351","modified":1751874689220},{"_id":"themes/next/source/css/_common/scaffolding/tags/mermaid.styl","hash":"c7754dc6c866928b538f0863a05b96ec44b5e986","modified":1751874689221},{"_id":"themes/next/source/css/_common/scaffolding/tags/pdf.styl","hash":"77122986509a6b4968bae2729417b7016137534c","modified":1751874689222},{"_id":"themes/next/source/css/_common/scaffolding/tags/note.styl","hash":"63910d15fa2b1df112aee0c4629955f157a077a4","modified":1751874689221},{"_id":"themes/next/source/css/_common/scaffolding/tags/wavedrom.styl","hash":"2f4c791b5999b2e1a707bd473c1db7c21d206a27","modified":1751874689222},{"_id":"themes/next/source/css/_common/scaffolding/tags/tabs.styl","hash":"bcc0d3482f2faa4552d38de51480ea7f92f6c821","modified":1751874689222},{"_id":"themes/next/source/images/favicon.jpg","hash":"ef2f732cc5072d26347e115d45d3899a9524d1a4","modified":1751862651306},{"_id":"source/img/Attention/Taxonomy.png","hash":"66d9f2b6baff13a690fecf3304ecad76fbfd354c","modified":1752219362473},{"_id":"source/img/CDR-data-analysis/comprehensive_bipartite_analysis_ccle.png","hash":"368e93a9832443fc2cdeedea46465d1ec1c2b94e","modified":1752130347397},{"_id":"source/paper/1609.02907v4.pdf","hash":"1762baa638866a13dcc6d146fd5a49b36cbd9c30","modified":1752132332530},{"_id":"source/paper/Lee 等 - 2018 - Attention Models in Graphs A Survey.pdf","hash":"cc23c580b7d8063415fb6eb512053d1079b849de","modified":1746799543164},{"_id":"source/img/CDR-data-analysis/comprehensive_bipartite_analysis_gdsc.png","hash":"860912fd44247e81e0a8c54b1ff3844d3ef41e0f","modified":1752130347411},{"_id":"source/paper/Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends.pdf","hash":"0c926dd126aa2e32816348ce95767a274e55d8ef","modified":1752130347427},{"_id":"source/paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf","hash":"73f96bdf97da1cef4b43d9ee7e0d08a7900ab2fc","modified":1746689552539},{"_id":"public/about/index.html","hash":"b25e227204e362d71eb1880aa8bc1ed54bc61533","modified":1752235225158},{"_id":"public/categories/index.html","hash":"7aa420c54b25a4b7223382c83270a1fbaf2c1103","modified":1752235225158},{"_id":"public/tags/index.html","hash":"8eccee509b1c90234b6123ed37b2c02f39d982e3","modified":1752235225158},{"_id":"public/2025/07/10/Attention/index.html","hash":"34c12a4005a3ad56f09021132020787256df13a9","modified":1752235225158},{"_id":"public/2025/07/10/GNN-and-GCN/index.html","hash":"a12e909e9ae931c3e162ba0c497bec5b60322cd3","modified":1752235225158},{"_id":"public/2025/07/09/CDR-data-analysis/index.html","hash":"d1d9dac2e94b2c171b716d1ed2aba244e6c050b3","modified":1752235225158},{"_id":"public/2025/07/08/PEP-8/index.html","hash":"bef76b516e22eff2507e37470a6913d5f22b1479","modified":1752235225158},{"_id":"public/archives/index.html","hash":"d030e188d4216c7447995e350bbb6939240e3be2","modified":1752235225158},{"_id":"public/archives/2025/index.html","hash":"058841f81e45e66d9f8d205aa808d6ec7883fcb0","modified":1752235225158},{"_id":"public/index.html","hash":"7b8bb13d9748e3f9e3e8951a5473d1b7f82dfee4","modified":1752235225158},{"_id":"public/archives/2025/07/index.html","hash":"b1a0b81b5ee0474f6ff9e29eb5df014cf7e55031","modified":1752235225158},{"_id":"public/tags/CDR/index.html","hash":"9887a679b7911c1cf0f3240e1a59794c3a9260f0","modified":1752235225158},{"_id":"public/tags/model/index.html","hash":"50b478e7ced049c9f8d664da22aaa0a05f96a3cc","modified":1752235225158},{"_id":"public/tags/Basic/index.html","hash":"1058cba7a458952e787648d7a0a5b192a970cd04","modified":1752235225158},{"_id":"public/tags/PyTorch/index.html","hash":"b93b91ea2fdf2e25ecad71fda5907ce80ab08f14","modified":1752235225158},{"_id":"public/tags/还没写完捏/index.html","hash":"26b68f991ec51bdb43492234896b3987db86b7fb","modified":1752235225158},{"_id":"public/tags/graph-theory/index.html","hash":"cc2cf9660cf97b285988734fb635b034a5b72caa","modified":1752235225158},{"_id":"public/tags/可能有点用/index.html","hash":"0b4997f3a4ff0f568af7b8b489efaac60120b7d1","modified":1752235225158},{"_id":"public/tags/embedding/index.html","hash":"975692997305efab2060e487aef9dda43c27d062","modified":1752235225158},{"_id":"public/tags/Data-Analysis/index.html","hash":"c1007aace31b8a004cd484c2ad67703ee20e5ec0","modified":1752235225158},{"_id":"public/tags/PEP/index.html","hash":"1a69390c8bc5834fd03785e7c3060f794e0f884b","modified":1752235225158},{"_id":"public/tags/Python/index.html","hash":"d70f47e68ccb8e29cb3676ac9f4d0dd80bd42d07","modified":1752235225158},{"_id":"public/tags/闲🉐无聊/index.html","hash":"0c34d7ca80ae60537075cdb2bb85523378cf816b","modified":1752235225158},{"_id":"public/tags/大概率没用/index.html","hash":"ee50433626745b5ed0c04abc2bec59fc869e322f","modified":1752235225158},{"_id":"public/categories/CDR/index.html","hash":"12c8ece34edb453d67a2ecb1ae27801ead908ec4","modified":1752235225158},{"_id":"public/categories/CDR/Python/index.html","hash":"d98dcee789cedfd73542b88285f900963d3ce144","modified":1752235225158},{"_id":"public/categories/CDR/model/index.html","hash":"4823909b8d40cf838f0e8b4cbc74a4e61155db32","modified":1752235225158},{"_id":"public/categories/CDR/Data-Analysis/index.html","hash":"bd6be4b94b56c7b63d47bef5f27a13f152fcf7bf","modified":1752235225158},{"_id":"public/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1752235225158},{"_id":"public/images/logo.svg","hash":"099e11ab995a2c8981427a85476d082609848c77","modified":1752235225158},{"_id":"public/CNAME","hash":"9f72a1c93d67b273a3f1b776b7607fca8ecb9800","modified":1752235225158},{"_id":"public/test/test_nodeppt.md","hash":"8ab37cbb7157ffd36ad5b535369431872efc1d33","modified":1752235225158},{"_id":"public/images/logo-algolia-nebula-blue-full.svg","hash":"5a81f1c5d66561b3bcb05dae48148088a3fb5c79","modified":1752235225158},{"_id":"public/test/test-pdf.md","hash":"f7c865a873ba3b2e3587ee1a07d688bcf7f62d49","modified":1752235225158},{"_id":"public/code/data_analysis/visualize_graph_analysis.py","hash":"f7ed9f01a38fe104b17464bf6e2dc7c5093ca7dc","modified":1752235225158},{"_id":"public/nodeppt/test/test_nodeppt.html","hash":"7d62ca54b2ecc3f46dab96441629eefdfbef3149","modified":1752235225158},{"_id":"public/img/CDR-data-analysis/gene.png","hash":"dac1e1481add33e6e471f62deb153719ff929a3a","modified":1752235225158},{"_id":"public/images/favicon.jpg","hash":"ef2f732cc5072d26347e115d45d3899a9524d1a4","modified":1752235225158},{"_id":"public/img/Attention/AttentionViaAttention.png","hash":"eee60186ed279182176b247f00e6243b707c54a6","modified":1752235225158},{"_id":"public/css/main.css","hash":"637d977e5e51685f47e8cc0c1191e67ace964914","modified":1752235225158},{"_id":"public/css/noscript.css","hash":"4cd5301e478e0e0d4b176740ec314087ec5cb707","modified":1752235225158},{"_id":"public/js/bookmark.js","hash":"9ba4cceafd12c6d5ba8a6b986a046ef8319a7811","modified":1752235225158},{"_id":"public/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1752235225158},{"_id":"public/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1752235225158},{"_id":"public/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1752235225158},{"_id":"public/js/motion.js","hash":"6f751f5c9499a39d7c5e1d323db3260342dd9431","modified":1752235225158},{"_id":"public/js/next-boot.js","hash":"523bbaeda463e82ab0be428cc0005717038ec63e","modified":1752235225158},{"_id":"public/js/schedule.js","hash":"a1333258726caf84f368a8f8454639c7dc1626bb","modified":1752235225158},{"_id":"public/js/sidebar.js","hash":"2ee359ae48273b01ba1e0768704524e08702c7eb","modified":1752235225158},{"_id":"public/js/utils.js","hash":"345a8158e6c34e19245a07c778f5699c8673f1b1","modified":1752235225158},{"_id":"public/js/third-party/addtoany.js","hash":"5276c8f78ee562a8965216dc67d762e59cb4a9f2","modified":1752235225158},{"_id":"public/js/third-party/fancybox.js","hash":"819f382c561fe5ec23c67cc5fabd63dd1cc22dc1","modified":1752235225158},{"_id":"public/js/pjax.js","hash":"694b271819aab37ce473b15db9e6aded971d82e5","modified":1752235225158},{"_id":"public/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1752235225158},{"_id":"public/js/third-party/quicklink.js","hash":"eed02e6fced8e5a653077205d4d4d7834ca71472","modified":1752235225158},{"_id":"public/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1752235225158},{"_id":"public/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1752235225158},{"_id":"public/js/third-party/analytics/matomo.js","hash":"c6a25b26a1443caa70b47fd3dfa282271574deb5","modified":1752235225158},{"_id":"public/js/third-party/analytics/google-analytics.js","hash":"def07bcc7c17d8a0caad177fb1dd2f3a5e5b3536","modified":1752235225158},{"_id":"public/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1752235225158},{"_id":"public/js/third-party/comments/changyan.js","hash":"260d1a77d6a3bb33a579d3e4cca1997003e799b5","modified":1752235225158},{"_id":"public/js/third-party/comments/disqusjs.js","hash":"1e826dea3f684c0515f362dc1352447a1f0eae71","modified":1752235225158},{"_id":"public/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1752235225158},{"_id":"public/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1752235225158},{"_id":"public/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1752235225158},{"_id":"public/js/third-party/comments/disqus.js","hash":"da361917d65e5dca8362f8cdeb6c8cc0e8316cec","modified":1752235225158},{"_id":"public/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1752235225158},{"_id":"public/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1752235225158},{"_id":"public/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1752235225158},{"_id":"public/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1752235225158},{"_id":"public/js/third-party/search/algolia-search.js","hash":"1905978ef587bf08fe088ce4693a4c08db07cfbb","modified":1752235225158},{"_id":"public/js/third-party/statistics/lean-analytics.js","hash":"835cbf54c49ef1327f47df70ff2636ad36b6f57d","modified":1752235225158},{"_id":"public/js/third-party/search/local-search.js","hash":"3968d972f47b79acc6c3fe44028bad77c9c5aab7","modified":1752235225158},{"_id":"public/js/third-party/statistics/firestore.js","hash":"6e0682bb42170d61b13b786295f45f9c785f8b73","modified":1752235225158},{"_id":"public/js/third-party/tags/mermaid.js","hash":"df01075f52302873f7de36050b5408c8d1afb452","modified":1752235225158},{"_id":"public/js/third-party/tags/wavedrom.js","hash":"40dcd10df6edf124088c329346e0cc0bdac74ef1","modified":1752235225158},{"_id":"public/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1752235225158},{"_id":"public/nodeppt/test/js/test_nodeppt.js.LICENSE.txt","hash":"b9fe24e9574b5f05a2fbb03f8234b998e6c8c12c","modified":1752235225158},{"_id":"public/nodeppt/test/img/swipe.svg","hash":"21c944bb622059b5f2974345dd35f082d01f3333","modified":1752235225158},{"_id":"public/nodeppt/test/js/chunk-vendors.js.LICENSE.txt","hash":"b9fe24e9574b5f05a2fbb03f8234b998e6c8c12c","modified":1752235225158},{"_id":"public/nodeppt/test/js/chunk-vendors.js","hash":"b8512dadb9d4df8bbb3926e0afaf76a46b3b278c","modified":1752235225158},{"_id":"public/nodeppt/test/js/test_nodeppt.js","hash":"f6aa6c0b1cf60221d64032b8876e515aeae284f4","modified":1752235225158},{"_id":"public/img/Attention/InteractiveCo-Attention.png","hash":"d66ad9309b6851348104b591f7a265f55ba4aae4","modified":1752235225158},{"_id":"public/img/Attention/MultiheadAttention.png","hash":"1d61e6a59d33b5b70521acf9b1e8b191d04a7278","modified":1752235225158},{"_id":"public/img/Attention/ParallelCo-Attention.png","hash":"1389f902e31640d6ff97a4375804b5b443706c02","modified":1752235225158},{"_id":"public/nodeppt/test/css/chunk-vendors.4e4765ff.css","hash":"abd0e586824cde74bd2348327b0d209e38d6e9b0","modified":1752235225158},{"_id":"public/img/Attention/TotalModel.png","hash":"b9c32980245dcf7bb4e9e54b04b59ccaa14bf265","modified":1752235225158},{"_id":"public/img/Attention/HierarchicalAttention.png","hash":"ddad89428142f48353d6338f6d69402ec9b0d95e","modified":1752235225158},{"_id":"public/img/Attention/GeneralAttentionModule.png","hash":"9e9b439faf1e0e38564fc8eab3364f8e6938cf6e","modified":1752235225158},{"_id":"public/img/Attention/MultihopAttention.png","hash":"9bb72595c6b04f2157d01b069d004eaa94d71efa","modified":1752235225158},{"_id":"public/img/Attention/CapsuleAttention.png","hash":"8d29b9e962f65aea416727e5e0d321cf00e718f1","modified":1752235225158},{"_id":"public/img/Attention/Taxonomy.png","hash":"66d9f2b6baff13a690fecf3304ecad76fbfd354c","modified":1752235225158},{"_id":"public/img/CDR-data-analysis/comprehensive_bipartite_analysis_ccle.png","hash":"368e93a9832443fc2cdeedea46465d1ec1c2b94e","modified":1752235225158},{"_id":"public/paper/1609.02907v4.pdf","hash":"1762baa638866a13dcc6d146fd5a49b36cbd9c30","modified":1752235225158},{"_id":"public/paper/Lee 等 - 2018 - Attention Models in Graphs A Survey.pdf","hash":"cc23c580b7d8063415fb6eb512053d1079b849de","modified":1752235225158},{"_id":"public/img/CDR-data-analysis/comprehensive_bipartite_analysis_gdsc.png","hash":"860912fd44247e81e0a8c54b1ff3844d3ef41e0f","modified":1752235225158},{"_id":"public/paper/Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends.pdf","hash":"0c926dd126aa2e32816348ce95767a274e55d8ef","modified":1752235225158},{"_id":"public/paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf","hash":"73f96bdf97da1cef4b43d9ee7e0d08a7900ab2fc","modified":1752235225158}],"Category":[{"name":"CDR","_id":"cmcyrm1gg0005ugwa6mp6chkd"},{"name":"Python","parent":"cmcyrm1gg0005ugwa6mp6chkd","_id":"cmcyrm1gn000dugwa6ewa04ac"},{"name":"model","parent":"cmcyrm1gg0005ugwa6mp6chkd","_id":"cmcyrm1go000fugwac72jf1hw"},{"name":"Data Analysis","parent":"cmcyrm1gg0005ugwa6mp6chkd","_id":"cmcyrm1go000iugwa2i7agury"}],"Data":[],"Page":[{"title":"What is it","date":"2025-07-07T08:26:04.000Z","_content":"\n> ——为了纪念自己已经死去的表达能力\n\n这是一个什么都有可能往里面塞的博客，但本人的意愿大致如下：\n\n1. 记录自己的 *CDR奇妙生活*\n2. 发现有 *大病* 的东西记录以绷住\n3. 作为 *UESTCer* 记录一些难绷小事","source":"about/index.md","raw":"---\ntitle: What is it\ndate: 2025-07-07 16:26:04\n---\n\n> ——为了纪念自己已经死去的表达能力\n\n这是一个什么都有可能往里面塞的博客，但本人的意愿大致如下：\n\n1. 记录自己的 *CDR奇妙生活*\n2. 发现有 *大病* 的东西记录以绷住\n3. 作为 *UESTCer* 记录一些难绷小事","updated":"2025-07-07T12:25:11.694Z","path":"about/index.html","comments":1,"layout":"page","_id":"cmcyrm1g50000ugwaeefrbvlw","content":"<blockquote>\n<p>——为了纪念自己已经死去的表达能力</p>\n</blockquote>\n<p>这是一个什么都有可能往里面塞的博客，但本人的意愿大致如下：</p>\n<ol>\n<li>记录自己的 <em>CDR奇妙生活</em></li>\n<li>发现有 <em>大病</em> 的东西记录以绷住</li>\n<li>作为 <em>UESTCer</em> 记录一些难绷小事</li>\n</ol>\n","excerpt":"","more":"<blockquote>\n<p>——为了纪念自己已经死去的表达能力</p>\n</blockquote>\n<p>这是一个什么都有可能往里面塞的博客，但本人的意愿大致如下：</p>\n<ol>\n<li>记录自己的 <em>CDR奇妙生活</em></li>\n<li>发现有 <em>大病</em> 的东西记录以绷住</li>\n<li>作为 <em>UESTCer</em> 记录一些难绷小事</li>\n</ol>\n"},{"title":"categories","date":"2025-07-05T16:25:00.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2025-07-06 00:25:00\ntype: \"categories\"\n---\n","updated":"2025-07-10T11:58:34.280Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cmcyrm1g70001ugwa440z717w","content":"","excerpt":"","more":""},{"title":"tags","date":"2025-07-09T16:33:53.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2025-07-10 00:33:53\ntype: \"tags\"\n---\n","updated":"2025-07-10T11:58:17.163Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cmcyrm1g90002ugwag760bi7o","content":"","excerpt":"","more":""}],"Post":[{"title":"Attention in Graph","date":"2025-07-10T11:56:23.000Z","js":["https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.min.js"],"_content":"\n# 🌟 图中的注意力机制\n\n> 注意力机制在图神经网络中扮演着越来越重要的角色。本文将深入探讨注意力机制在图结构数据处理中的应用，从基础概念到实际实现。\n>\n$$\n\\underset{d\\_{k} \\times n\\_{f}}{\\boldsymbol{K}}=\\underset{d\\_{k} \\times d\\_{f}}{\\boldsymbol{W}\\_{K}} \\times \\underset{d\\_{f} \\times n\\_{f}}{\\boldsymbol{F}}, \\quad \\underset{d\\_{v} \\times n\\_{f}}{\\boldsymbol{V}}=\\underset{d\\_{v} \\times d\\_{f}}{\\boldsymbol{W}\\_{V}} \\times \\underset{d\\_{f} \\times n\\_{f}}{\\boldsymbol{F}} .\n$$\n<!-- more -->\n\n## 🎯 引言\n\n在深度学习领域，注意力机制已经成为一个革命性的创新，特别是在处理序列数据和图像数据方面取得了巨大成功。而在图神经网络中，注意力机制的引入不仅提高了模型的表现力，还增强了模型的可解释性。\n\n在图结构数据中应用注意力机制主要有以下优势：\n1. 自适应性：能够根据任务动态调整不同邻居节点的重要性\n2. 可解释性：通过注意力权重可以直观理解模型的决策过程\n3. 长程依赖：有效缓解了传统GNN中的过平滑问题\n4. 异质性处理：更好地处理异质图中的不同类型节点和边\n\n## 📚 理论基础\n\n### 注意力机制回顾\n\n#### 基本注意力机制\n\n最基本的注意力机制可以表示为：\n\n$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n\n其中：\n- $Q$：查询矩阵（Query）\n- $K$：键矩阵（Key）\n- $V$：值矩阵（Value）\n- $d_k$：键向量的维度\n\n#### 自注意力机制\n\n自注意力是一种特殊的注意力机制，其中Q、K、V都来自同一个源序列：\n\n$$SelfAttention(X) = Attention(XW_Q, XW_K, XW_V)$$\n\n### 图注意力机制\n\n#### GAT（Graph Attention Networks）\n\nGAT通过引入注意力机制来加权邻居节点的特征。对于节点i，其更新公式为：\n\n$$h\\_i^{(l+1)} = \\sigma(\\sum\\_{j \\in \\mathcal{N}\\_i} \\alpha\\_{ij}W^{(l)}h\\_j^{(l)})$$\n\n其中注意力系数$\\alpha_{ij}$的计算：\n\n$$\\alpha_{ij} = \\frac{exp(LeakyReLU(a^T[Wh_i || Wh_j]))}{\\sum_{k \\in \\mathcal{N}_i} exp(LeakyReLU(a^T[Wh_i || Wh_k]))}$$\n\n#### 多头注意力\n\n为了提高模型的稳定性和表达能力，GAT使用了多头注意力机制：\n\n$$h\\_i^{(l+1)} = \\sigma(\\frac{1}{K} \\sum\\_{k=1}^K \\sum\\_{j \\in \\mathcal{N}\\_i} \\alpha\\_{ij}^k W^k h\\_j^{(l)})$$\n\n### 变体与扩展\n\n#### 边注意力\n\n除了节点之间的注意力，一些模型还引入了边注意力机制：\n\n$$e_{ij} = a^T[Wh_i || Wh_j || We_{ij}]$$\n\n其中$e_{ij}$是边的特征。\n\n#### 全局注意力\n\n通过引入全局节点或池化操作，可以实现全局注意力：\n\n$$g = \\sum_{i \\in V} \\beta_i h_i$$\n\n其中$\\beta_i$是全局注意力权重。\n\n## 💻 实现细节\n\n### PyTorch实现的GAT层\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass GATLayer(nn.Module):\n    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n        super(GATLayer, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.dropout = dropout\n        self.alpha = alpha\n        self.concat = concat\n\n        # 变换矩阵\n        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n        \n        # 注意力向量\n        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n\n        self.leakyrelu = nn.LeakyReLU(self.alpha)\n\n    def forward(self, x, adj):\n        # x: 节点特征矩阵 [N, in_features]\n        # adj: 邻接矩阵 [N, N]\n        \n        # 线性变换\n        h = torch.mm(x, self.W)  # [N, out_features]\n        N = h.size()[0]\n\n        # 计算注意力分数\n        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1)\n        a_input = a_input.view(N, N, 2 * self.out_features)\n        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n\n        # 掩码机制\n        zero_vec = -9e15 * torch.ones_like(e)\n        attention = torch.where(adj > 0, e, zero_vec)\n        attention = F.softmax(attention, dim=1)\n        attention = F.dropout(attention, self.dropout, training=self.training)\n\n        # 聚合特征\n        h_prime = torch.matmul(attention, h)\n\n        if self.concat:\n            return F.elu(h_prime)\n        else:\n            return h_prime\n\nclass GAT(nn.Module):\n    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n        super(GAT, self).__init__()\n        self.dropout = dropout\n        \n        # 多头注意力层\n        self.attentions = nn.ModuleList([\n            GATLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) \n            for _ in range(nheads)\n        ])\n        \n        # 输出层\n        self.out_att = GATLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n\n    def forward(self, x, adj):\n        x = F.dropout(x, self.dropout, training=self.training)\n        # 多头注意力\n        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = self.out_att(x, adj)\n        return F.log_softmax(x, dim=1)\n```\n\n### 实际应用示例\n\n```python\n# 模型初始化\nmodel = GAT(nfeat=input_dim,\n           nhid=8,\n           nclass=num_classes,\n           dropout=0.6,\n           alpha=0.2,\n           nheads=8)\n\n# 优化器\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n\n# 训练循环\ndef train():\n    model.train()\n    optimizer.zero_grad()\n    output = model(features, adj)\n    loss = F.nll_loss(output[idx_train], labels[idx_train])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n```\n\n## 🔍 注意事项与最佳实践\n\n1. **注意力头数的选择**\n   - 通常在4-8之间\n   - 需要根据任务复杂度和计算资源调整\n\n2. **过拟合处理**\n   - 使用dropout\n   - 添加L2正则化\n   - 使用残差连接\n\n3. **计算效率**\n   - 对于大规模图，考虑使用稀疏注意力\n   - 可以使用邻居采样减少计算量\n\n4. **模型设计考虑**\n   - 注意力层的堆叠不宜过深\n   - 考虑添加跳跃连接\n   - 根据任务选择合适的聚合函数\n\n## 📈 未来展望\n\n1. **可扩展性改进**\n   - 研究更高效的注意力计算方法\n   - 探索稀疏注意力机制\n\n2. **理论研究**\n   - 深入理解注意力机制的工作原理\n   - 研究注意力机制与图的结构特性的关系\n\n3. **应用拓展**\n   - 在更多领域验证效果\n   - 结合其他先进技术（如Transformer）\n\n# 📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\n<a href=\"/paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf\" target=\"_blank\">📄 Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a>\n<a href=\"/paper/Lee 等 - 2018 - Attention Models in Graphs A Survey.pdf\" target=\"_blank\">📄 Lee 等 - 2018 - Attention Models in Graphs A Survey</a>","source":"_posts/Attention.md","raw":"---\ntitle: Attention in Graph\ndate: 2025-07-10 19:56:23\ncategories:\n  - CDR\n  - model\ntags:\n  - CDR\n  - model\n  - Basic\n  - 还没写完捏\n  - PyTorch\n  - graph theory\njs:\n  - https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.min.js\n---\n\n# 🌟 图中的注意力机制\n\n> 注意力机制在图神经网络中扮演着越来越重要的角色。本文将深入探讨注意力机制在图结构数据处理中的应用，从基础概念到实际实现。\n>\n$$\n\\underset{d\\_{k} \\times n\\_{f}}{\\boldsymbol{K}}=\\underset{d\\_{k} \\times d\\_{f}}{\\boldsymbol{W}\\_{K}} \\times \\underset{d\\_{f} \\times n\\_{f}}{\\boldsymbol{F}}, \\quad \\underset{d\\_{v} \\times n\\_{f}}{\\boldsymbol{V}}=\\underset{d\\_{v} \\times d\\_{f}}{\\boldsymbol{W}\\_{V}} \\times \\underset{d\\_{f} \\times n\\_{f}}{\\boldsymbol{F}} .\n$$\n<!-- more -->\n\n## 🎯 引言\n\n在深度学习领域，注意力机制已经成为一个革命性的创新，特别是在处理序列数据和图像数据方面取得了巨大成功。而在图神经网络中，注意力机制的引入不仅提高了模型的表现力，还增强了模型的可解释性。\n\n在图结构数据中应用注意力机制主要有以下优势：\n1. 自适应性：能够根据任务动态调整不同邻居节点的重要性\n2. 可解释性：通过注意力权重可以直观理解模型的决策过程\n3. 长程依赖：有效缓解了传统GNN中的过平滑问题\n4. 异质性处理：更好地处理异质图中的不同类型节点和边\n\n## 📚 理论基础\n\n### 注意力机制回顾\n\n#### 基本注意力机制\n\n最基本的注意力机制可以表示为：\n\n$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n\n其中：\n- $Q$：查询矩阵（Query）\n- $K$：键矩阵（Key）\n- $V$：值矩阵（Value）\n- $d_k$：键向量的维度\n\n#### 自注意力机制\n\n自注意力是一种特殊的注意力机制，其中Q、K、V都来自同一个源序列：\n\n$$SelfAttention(X) = Attention(XW_Q, XW_K, XW_V)$$\n\n### 图注意力机制\n\n#### GAT（Graph Attention Networks）\n\nGAT通过引入注意力机制来加权邻居节点的特征。对于节点i，其更新公式为：\n\n$$h\\_i^{(l+1)} = \\sigma(\\sum\\_{j \\in \\mathcal{N}\\_i} \\alpha\\_{ij}W^{(l)}h\\_j^{(l)})$$\n\n其中注意力系数$\\alpha_{ij}$的计算：\n\n$$\\alpha_{ij} = \\frac{exp(LeakyReLU(a^T[Wh_i || Wh_j]))}{\\sum_{k \\in \\mathcal{N}_i} exp(LeakyReLU(a^T[Wh_i || Wh_k]))}$$\n\n#### 多头注意力\n\n为了提高模型的稳定性和表达能力，GAT使用了多头注意力机制：\n\n$$h\\_i^{(l+1)} = \\sigma(\\frac{1}{K} \\sum\\_{k=1}^K \\sum\\_{j \\in \\mathcal{N}\\_i} \\alpha\\_{ij}^k W^k h\\_j^{(l)})$$\n\n### 变体与扩展\n\n#### 边注意力\n\n除了节点之间的注意力，一些模型还引入了边注意力机制：\n\n$$e_{ij} = a^T[Wh_i || Wh_j || We_{ij}]$$\n\n其中$e_{ij}$是边的特征。\n\n#### 全局注意力\n\n通过引入全局节点或池化操作，可以实现全局注意力：\n\n$$g = \\sum_{i \\in V} \\beta_i h_i$$\n\n其中$\\beta_i$是全局注意力权重。\n\n## 💻 实现细节\n\n### PyTorch实现的GAT层\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass GATLayer(nn.Module):\n    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n        super(GATLayer, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.dropout = dropout\n        self.alpha = alpha\n        self.concat = concat\n\n        # 变换矩阵\n        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n        \n        # 注意力向量\n        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n\n        self.leakyrelu = nn.LeakyReLU(self.alpha)\n\n    def forward(self, x, adj):\n        # x: 节点特征矩阵 [N, in_features]\n        # adj: 邻接矩阵 [N, N]\n        \n        # 线性变换\n        h = torch.mm(x, self.W)  # [N, out_features]\n        N = h.size()[0]\n\n        # 计算注意力分数\n        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1)\n        a_input = a_input.view(N, N, 2 * self.out_features)\n        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n\n        # 掩码机制\n        zero_vec = -9e15 * torch.ones_like(e)\n        attention = torch.where(adj > 0, e, zero_vec)\n        attention = F.softmax(attention, dim=1)\n        attention = F.dropout(attention, self.dropout, training=self.training)\n\n        # 聚合特征\n        h_prime = torch.matmul(attention, h)\n\n        if self.concat:\n            return F.elu(h_prime)\n        else:\n            return h_prime\n\nclass GAT(nn.Module):\n    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n        super(GAT, self).__init__()\n        self.dropout = dropout\n        \n        # 多头注意力层\n        self.attentions = nn.ModuleList([\n            GATLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) \n            for _ in range(nheads)\n        ])\n        \n        # 输出层\n        self.out_att = GATLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n\n    def forward(self, x, adj):\n        x = F.dropout(x, self.dropout, training=self.training)\n        # 多头注意力\n        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = self.out_att(x, adj)\n        return F.log_softmax(x, dim=1)\n```\n\n### 实际应用示例\n\n```python\n# 模型初始化\nmodel = GAT(nfeat=input_dim,\n           nhid=8,\n           nclass=num_classes,\n           dropout=0.6,\n           alpha=0.2,\n           nheads=8)\n\n# 优化器\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n\n# 训练循环\ndef train():\n    model.train()\n    optimizer.zero_grad()\n    output = model(features, adj)\n    loss = F.nll_loss(output[idx_train], labels[idx_train])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n```\n\n## 🔍 注意事项与最佳实践\n\n1. **注意力头数的选择**\n   - 通常在4-8之间\n   - 需要根据任务复杂度和计算资源调整\n\n2. **过拟合处理**\n   - 使用dropout\n   - 添加L2正则化\n   - 使用残差连接\n\n3. **计算效率**\n   - 对于大规模图，考虑使用稀疏注意力\n   - 可以使用邻居采样减少计算量\n\n4. **模型设计考虑**\n   - 注意力层的堆叠不宜过深\n   - 考虑添加跳跃连接\n   - 根据任务选择合适的聚合函数\n\n## 📈 未来展望\n\n1. **可扩展性改进**\n   - 研究更高效的注意力计算方法\n   - 探索稀疏注意力机制\n\n2. **理论研究**\n   - 深入理解注意力机制的工作原理\n   - 研究注意力机制与图的结构特性的关系\n\n3. **应用拓展**\n   - 在更多领域验证效果\n   - 结合其他先进技术（如Transformer）\n\n# 📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\n<a href=\"/paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf\" target=\"_blank\">📄 Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a>\n<a href=\"/paper/Lee 等 - 2018 - Attention Models in Graphs A Survey.pdf\" target=\"_blank\">📄 Lee 等 - 2018 - Attention Models in Graphs A Survey</a>","slug":"Attention","published":1,"updated":"2025-07-11T06:49:36.852Z","comments":1,"layout":"post","photos":[],"_id":"cmcyrm1gc0003ugwa6kff203e","content":"<h1 id=\"🌟-图中的注意力机制\"><a href=\"#🌟-图中的注意力机制\" class=\"headerlink\" title=\"🌟 图中的注意力机制\"></a>🌟 图中的注意力机制</h1><blockquote>\n<p>注意力机制在图神经网络中扮演着越来越重要的角色。本文将深入探讨注意力机制在图结构数据处理中的应用，从基础概念到实际实现。</p>\n</blockquote>\n<p>$$<br>\\underset{d_{k} \\times n_{f}}{\\boldsymbol{K}}&#x3D;\\underset{d_{k} \\times d_{f}}{\\boldsymbol{W}_{K}} \\times \\underset{d_{f} \\times n_{f}}{\\boldsymbol{F}}, \\quad \\underset{d_{v} \\times n_{f}}{\\boldsymbol{V}}&#x3D;\\underset{d_{v} \\times d_{f}}{\\boldsymbol{W}_{V}} \\times \\underset{d_{f} \\times n_{f}}{\\boldsymbol{F}} .<br>$$</p>\n<span id=\"more\"></span>\n\n<h2 id=\"🎯-引言\"><a href=\"#🎯-引言\" class=\"headerlink\" title=\"🎯 引言\"></a>🎯 引言</h2><p>在深度学习领域，注意力机制已经成为一个革命性的创新，特别是在处理序列数据和图像数据方面取得了巨大成功。而在图神经网络中，注意力机制的引入不仅提高了模型的表现力，还增强了模型的可解释性。</p>\n<p>在图结构数据中应用注意力机制主要有以下优势：</p>\n<ol>\n<li>自适应性：能够根据任务动态调整不同邻居节点的重要性</li>\n<li>可解释性：通过注意力权重可以直观理解模型的决策过程</li>\n<li>长程依赖：有效缓解了传统GNN中的过平滑问题</li>\n<li>异质性处理：更好地处理异质图中的不同类型节点和边</li>\n</ol>\n<h2 id=\"📚-理论基础\"><a href=\"#📚-理论基础\" class=\"headerlink\" title=\"📚 理论基础\"></a>📚 理论基础</h2><h3 id=\"注意力机制回顾\"><a href=\"#注意力机制回顾\" class=\"headerlink\" title=\"注意力机制回顾\"></a>注意力机制回顾</h3><h4 id=\"基本注意力机制\"><a href=\"#基本注意力机制\" class=\"headerlink\" title=\"基本注意力机制\"></a>基本注意力机制</h4><p>最基本的注意力机制可以表示为：</p>\n<p>$$Attention(Q, K, V) &#x3D; softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$</p>\n<p>其中：</p>\n<ul>\n<li>$Q$：查询矩阵（Query）</li>\n<li>$K$：键矩阵（Key）</li>\n<li>$V$：值矩阵（Value）</li>\n<li>$d_k$：键向量的维度</li>\n</ul>\n<h4 id=\"自注意力机制\"><a href=\"#自注意力机制\" class=\"headerlink\" title=\"自注意力机制\"></a>自注意力机制</h4><p>自注意力是一种特殊的注意力机制，其中Q、K、V都来自同一个源序列：</p>\n<p>$$SelfAttention(X) &#x3D; Attention(XW_Q, XW_K, XW_V)$$</p>\n<h3 id=\"图注意力机制\"><a href=\"#图注意力机制\" class=\"headerlink\" title=\"图注意力机制\"></a>图注意力机制</h3><h4 id=\"GAT（Graph-Attention-Networks）\"><a href=\"#GAT（Graph-Attention-Networks）\" class=\"headerlink\" title=\"GAT（Graph Attention Networks）\"></a>GAT（Graph Attention Networks）</h4><p>GAT通过引入注意力机制来加权邻居节点的特征。对于节点i，其更新公式为：</p>\n<p>$$h_i^{(l+1)} &#x3D; \\sigma(\\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij}W^{(l)}h_j^{(l)})$$</p>\n<p>其中注意力系数$\\alpha_{ij}$的计算：</p>\n<p>$$\\alpha_{ij} &#x3D; \\frac{exp(LeakyReLU(a^T[Wh_i || Wh_j]))}{\\sum_{k \\in \\mathcal{N}_i} exp(LeakyReLU(a^T[Wh_i || Wh_k]))}$$</p>\n<h4 id=\"多头注意力\"><a href=\"#多头注意力\" class=\"headerlink\" title=\"多头注意力\"></a>多头注意力</h4><p>为了提高模型的稳定性和表达能力，GAT使用了多头注意力机制：</p>\n<p>$$h_i^{(l+1)} &#x3D; \\sigma(\\frac{1}{K} \\sum_{k&#x3D;1}^K \\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij}^k W^k h_j^{(l)})$$</p>\n<h3 id=\"变体与扩展\"><a href=\"#变体与扩展\" class=\"headerlink\" title=\"变体与扩展\"></a>变体与扩展</h3><h4 id=\"边注意力\"><a href=\"#边注意力\" class=\"headerlink\" title=\"边注意力\"></a>边注意力</h4><p>除了节点之间的注意力，一些模型还引入了边注意力机制：</p>\n<p>$$e_{ij} &#x3D; a^T[Wh_i || Wh_j || We_{ij}]$$</p>\n<p>其中$e_{ij}$是边的特征。</p>\n<h4 id=\"全局注意力\"><a href=\"#全局注意力\" class=\"headerlink\" title=\"全局注意力\"></a>全局注意力</h4><p>通过引入全局节点或池化操作，可以实现全局注意力：</p>\n<p>$$g &#x3D; \\sum_{i \\in V} \\beta_i h_i$$</p>\n<p>其中$\\beta_i$是全局注意力权重。</p>\n<h2 id=\"💻-实现细节\"><a href=\"#💻-实现细节\" class=\"headerlink\" title=\"💻 实现细节\"></a>💻 实现细节</h2><h3 id=\"PyTorch实现的GAT层\"><a href=\"#PyTorch实现的GAT层\" class=\"headerlink\" title=\"PyTorch实现的GAT层\"></a>PyTorch实现的GAT层</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">GATLayer</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, in_features, out_features, dropout, alpha, concat=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(GATLayer, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.in_features = in_features</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.out_features = out_features</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = dropout</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.alpha = alpha</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.concat = concat</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 变换矩阵</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))</span><br><span class=\"line\">        nn.init.xavier_uniform_(<span class=\"variable language_\">self</span>.W.data, gain=<span class=\"number\">1.414</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 注意力向量</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.a = nn.Parameter(torch.zeros(size=(<span class=\"number\">2</span>*out_features, <span class=\"number\">1</span>)))</span><br><span class=\"line\">        nn.init.xavier_uniform_(<span class=\"variable language_\">self</span>.a.data, gain=<span class=\"number\">1.414</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.leakyrelu = nn.LeakyReLU(<span class=\"variable language_\">self</span>.alpha)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, adj</span>):</span><br><span class=\"line\">        <span class=\"comment\"># x: 节点特征矩阵 [N, in_features]</span></span><br><span class=\"line\">        <span class=\"comment\"># adj: 邻接矩阵 [N, N]</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 线性变换</span></span><br><span class=\"line\">        h = torch.mm(x, <span class=\"variable language_\">self</span>.W)  <span class=\"comment\"># [N, out_features]</span></span><br><span class=\"line\">        N = h.size()[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 计算注意力分数</span></span><br><span class=\"line\">        a_input = torch.cat([h.repeat(<span class=\"number\">1</span>, N).view(N * N, -<span class=\"number\">1</span>), h.repeat(N, <span class=\"number\">1</span>)], dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        a_input = a_input.view(N, N, <span class=\"number\">2</span> * <span class=\"variable language_\">self</span>.out_features)</span><br><span class=\"line\">        e = <span class=\"variable language_\">self</span>.leakyrelu(torch.matmul(a_input, <span class=\"variable language_\">self</span>.a).squeeze(<span class=\"number\">2</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 掩码机制</span></span><br><span class=\"line\">        zero_vec = -<span class=\"number\">9e15</span> * torch.ones_like(e)</span><br><span class=\"line\">        attention = torch.where(adj &gt; <span class=\"number\">0</span>, e, zero_vec)</span><br><span class=\"line\">        attention = F.softmax(attention, dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        attention = F.dropout(attention, <span class=\"variable language_\">self</span>.dropout, training=<span class=\"variable language_\">self</span>.training)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 聚合特征</span></span><br><span class=\"line\">        h_prime = torch.matmul(attention, h)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"variable language_\">self</span>.concat:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> F.elu(h_prime)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> h_prime</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">GAT</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, nfeat, nhid, nclass, dropout, alpha, nheads</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(GAT, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = dropout</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 多头注意力层</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.attentions = nn.ModuleList([</span><br><span class=\"line\">            GATLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=<span class=\"literal\">True</span>) </span><br><span class=\"line\">            <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(nheads)</span><br><span class=\"line\">        ])</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 输出层</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.out_att = GATLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, adj</span>):</span><br><span class=\"line\">        x = F.dropout(x, <span class=\"variable language_\">self</span>.dropout, training=<span class=\"variable language_\">self</span>.training)</span><br><span class=\"line\">        <span class=\"comment\"># 多头注意力</span></span><br><span class=\"line\">        x = torch.cat([att(x, adj) <span class=\"keyword\">for</span> att <span class=\"keyword\">in</span> <span class=\"variable language_\">self</span>.attentions], dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        x = F.dropout(x, <span class=\"variable language_\">self</span>.dropout, training=<span class=\"variable language_\">self</span>.training)</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.out_att(x, adj)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.log_softmax(x, dim=<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"实际应用示例\"><a href=\"#实际应用示例\" class=\"headerlink\" title=\"实际应用示例\"></a>实际应用示例</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 模型初始化</span></span><br><span class=\"line\">model = GAT(nfeat=input_dim,</span><br><span class=\"line\">           nhid=<span class=\"number\">8</span>,</span><br><span class=\"line\">           nclass=num_classes,</span><br><span class=\"line\">           dropout=<span class=\"number\">0.6</span>,</span><br><span class=\"line\">           alpha=<span class=\"number\">0.2</span>,</span><br><span class=\"line\">           nheads=<span class=\"number\">8</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 优化器</span></span><br><span class=\"line\">optimizer = torch.optim.Adam(model.parameters(), lr=<span class=\"number\">0.005</span>, weight_decay=<span class=\"number\">5e-4</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 训练循环</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>():</span><br><span class=\"line\">    model.train()</span><br><span class=\"line\">    optimizer.zero_grad()</span><br><span class=\"line\">    output = model(features, adj)</span><br><span class=\"line\">    loss = F.nll_loss(output[idx_train], labels[idx_train])</span><br><span class=\"line\">    loss.backward()</span><br><span class=\"line\">    optimizer.step()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> loss.item()</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"🔍-注意事项与最佳实践\"><a href=\"#🔍-注意事项与最佳实践\" class=\"headerlink\" title=\"🔍 注意事项与最佳实践\"></a>🔍 注意事项与最佳实践</h2><ol>\n<li><p><strong>注意力头数的选择</strong></p>\n<ul>\n<li>通常在4-8之间</li>\n<li>需要根据任务复杂度和计算资源调整</li>\n</ul>\n</li>\n<li><p><strong>过拟合处理</strong></p>\n<ul>\n<li>使用dropout</li>\n<li>添加L2正则化</li>\n<li>使用残差连接</li>\n</ul>\n</li>\n<li><p><strong>计算效率</strong></p>\n<ul>\n<li>对于大规模图，考虑使用稀疏注意力</li>\n<li>可以使用邻居采样减少计算量</li>\n</ul>\n</li>\n<li><p><strong>模型设计考虑</strong></p>\n<ul>\n<li>注意力层的堆叠不宜过深</li>\n<li>考虑添加跳跃连接</li>\n<li>根据任务选择合适的聚合函数</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"📈-未来展望\"><a href=\"#📈-未来展望\" class=\"headerlink\" title=\"📈 未来展望\"></a>📈 未来展望</h2><ol>\n<li><p><strong>可扩展性改进</strong></p>\n<ul>\n<li>研究更高效的注意力计算方法</li>\n<li>探索稀疏注意力机制</li>\n</ul>\n</li>\n<li><p><strong>理论研究</strong></p>\n<ul>\n<li>深入理解注意力机制的工作原理</li>\n<li>研究注意力机制与图的结构特性的关系</li>\n</ul>\n</li>\n<li><p><strong>应用拓展</strong></p>\n<ul>\n<li>在更多领域验证效果</li>\n<li>结合其他先进技术（如Transformer）</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"><a href=\"#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\" class=\"headerlink\" title=\"📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1><p><a href=\"/paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf\" target=\"_blank\">📄 Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a><br><a href=\"/paper/Lee 等 - 2018 - Attention Models in Graphs A Survey.pdf\" target=\"_blank\">📄 Lee 等 - 2018 - Attention Models in Graphs A Survey</a></p>\n","excerpt":"<h1 id=\"🌟-图中的注意力机制\"><a href=\"#🌟-图中的注意力机制\" class=\"headerlink\" title=\"🌟 图中的注意力机制\"></a>🌟 图中的注意力机制</h1><blockquote>\n<p>注意力机制在图神经网络中扮演着越来越重要的角色。本文将深入探讨注意力机制在图结构数据处理中的应用，从基础概念到实际实现。</p>\n</blockquote>\n<p>$$<br>\\underset{d_{k} \\times n_{f}}{\\boldsymbol{K}}&#x3D;\\underset{d_{k} \\times d_{f}}{\\boldsymbol{W}_{K}} \\times \\underset{d_{f} \\times n_{f}}{\\boldsymbol{F}}, \\quad \\underset{d_{v} \\times n_{f}}{\\boldsymbol{V}}&#x3D;\\underset{d_{v} \\times d_{f}}{\\boldsymbol{W}_{V}} \\times \\underset{d_{f} \\times n_{f}}{\\boldsymbol{F}} .<br>$$</p>","more":"<h2 id=\"🎯-引言\"><a href=\"#🎯-引言\" class=\"headerlink\" title=\"🎯 引言\"></a>🎯 引言</h2><p>在深度学习领域，注意力机制已经成为一个革命性的创新，特别是在处理序列数据和图像数据方面取得了巨大成功。而在图神经网络中，注意力机制的引入不仅提高了模型的表现力，还增强了模型的可解释性。</p>\n<p>在图结构数据中应用注意力机制主要有以下优势：</p>\n<ol>\n<li>自适应性：能够根据任务动态调整不同邻居节点的重要性</li>\n<li>可解释性：通过注意力权重可以直观理解模型的决策过程</li>\n<li>长程依赖：有效缓解了传统GNN中的过平滑问题</li>\n<li>异质性处理：更好地处理异质图中的不同类型节点和边</li>\n</ol>\n<h2 id=\"📚-理论基础\"><a href=\"#📚-理论基础\" class=\"headerlink\" title=\"📚 理论基础\"></a>📚 理论基础</h2><h3 id=\"注意力机制回顾\"><a href=\"#注意力机制回顾\" class=\"headerlink\" title=\"注意力机制回顾\"></a>注意力机制回顾</h3><h4 id=\"基本注意力机制\"><a href=\"#基本注意力机制\" class=\"headerlink\" title=\"基本注意力机制\"></a>基本注意力机制</h4><p>最基本的注意力机制可以表示为：</p>\n<p>$$Attention(Q, K, V) &#x3D; softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$</p>\n<p>其中：</p>\n<ul>\n<li>$Q$：查询矩阵（Query）</li>\n<li>$K$：键矩阵（Key）</li>\n<li>$V$：值矩阵（Value）</li>\n<li>$d_k$：键向量的维度</li>\n</ul>\n<h4 id=\"自注意力机制\"><a href=\"#自注意力机制\" class=\"headerlink\" title=\"自注意力机制\"></a>自注意力机制</h4><p>自注意力是一种特殊的注意力机制，其中Q、K、V都来自同一个源序列：</p>\n<p>$$SelfAttention(X) &#x3D; Attention(XW_Q, XW_K, XW_V)$$</p>\n<h3 id=\"图注意力机制\"><a href=\"#图注意力机制\" class=\"headerlink\" title=\"图注意力机制\"></a>图注意力机制</h3><h4 id=\"GAT（Graph-Attention-Networks）\"><a href=\"#GAT（Graph-Attention-Networks）\" class=\"headerlink\" title=\"GAT（Graph Attention Networks）\"></a>GAT（Graph Attention Networks）</h4><p>GAT通过引入注意力机制来加权邻居节点的特征。对于节点i，其更新公式为：</p>\n<p>$$h_i^{(l+1)} &#x3D; \\sigma(\\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij}W^{(l)}h_j^{(l)})$$</p>\n<p>其中注意力系数$\\alpha_{ij}$的计算：</p>\n<p>$$\\alpha_{ij} &#x3D; \\frac{exp(LeakyReLU(a^T[Wh_i || Wh_j]))}{\\sum_{k \\in \\mathcal{N}_i} exp(LeakyReLU(a^T[Wh_i || Wh_k]))}$$</p>\n<h4 id=\"多头注意力\"><a href=\"#多头注意力\" class=\"headerlink\" title=\"多头注意力\"></a>多头注意力</h4><p>为了提高模型的稳定性和表达能力，GAT使用了多头注意力机制：</p>\n<p>$$h_i^{(l+1)} &#x3D; \\sigma(\\frac{1}{K} \\sum_{k&#x3D;1}^K \\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij}^k W^k h_j^{(l)})$$</p>\n<h3 id=\"变体与扩展\"><a href=\"#变体与扩展\" class=\"headerlink\" title=\"变体与扩展\"></a>变体与扩展</h3><h4 id=\"边注意力\"><a href=\"#边注意力\" class=\"headerlink\" title=\"边注意力\"></a>边注意力</h4><p>除了节点之间的注意力，一些模型还引入了边注意力机制：</p>\n<p>$$e_{ij} &#x3D; a^T[Wh_i || Wh_j || We_{ij}]$$</p>\n<p>其中$e_{ij}$是边的特征。</p>\n<h4 id=\"全局注意力\"><a href=\"#全局注意力\" class=\"headerlink\" title=\"全局注意力\"></a>全局注意力</h4><p>通过引入全局节点或池化操作，可以实现全局注意力：</p>\n<p>$$g &#x3D; \\sum_{i \\in V} \\beta_i h_i$$</p>\n<p>其中$\\beta_i$是全局注意力权重。</p>\n<h2 id=\"💻-实现细节\"><a href=\"#💻-实现细节\" class=\"headerlink\" title=\"💻 实现细节\"></a>💻 实现细节</h2><h3 id=\"PyTorch实现的GAT层\"><a href=\"#PyTorch实现的GAT层\" class=\"headerlink\" title=\"PyTorch实现的GAT层\"></a>PyTorch实现的GAT层</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">GATLayer</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, in_features, out_features, dropout, alpha, concat=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(GATLayer, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.in_features = in_features</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.out_features = out_features</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = dropout</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.alpha = alpha</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.concat = concat</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 变换矩阵</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))</span><br><span class=\"line\">        nn.init.xavier_uniform_(<span class=\"variable language_\">self</span>.W.data, gain=<span class=\"number\">1.414</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 注意力向量</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.a = nn.Parameter(torch.zeros(size=(<span class=\"number\">2</span>*out_features, <span class=\"number\">1</span>)))</span><br><span class=\"line\">        nn.init.xavier_uniform_(<span class=\"variable language_\">self</span>.a.data, gain=<span class=\"number\">1.414</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.leakyrelu = nn.LeakyReLU(<span class=\"variable language_\">self</span>.alpha)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, adj</span>):</span><br><span class=\"line\">        <span class=\"comment\"># x: 节点特征矩阵 [N, in_features]</span></span><br><span class=\"line\">        <span class=\"comment\"># adj: 邻接矩阵 [N, N]</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 线性变换</span></span><br><span class=\"line\">        h = torch.mm(x, <span class=\"variable language_\">self</span>.W)  <span class=\"comment\"># [N, out_features]</span></span><br><span class=\"line\">        N = h.size()[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 计算注意力分数</span></span><br><span class=\"line\">        a_input = torch.cat([h.repeat(<span class=\"number\">1</span>, N).view(N * N, -<span class=\"number\">1</span>), h.repeat(N, <span class=\"number\">1</span>)], dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        a_input = a_input.view(N, N, <span class=\"number\">2</span> * <span class=\"variable language_\">self</span>.out_features)</span><br><span class=\"line\">        e = <span class=\"variable language_\">self</span>.leakyrelu(torch.matmul(a_input, <span class=\"variable language_\">self</span>.a).squeeze(<span class=\"number\">2</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 掩码机制</span></span><br><span class=\"line\">        zero_vec = -<span class=\"number\">9e15</span> * torch.ones_like(e)</span><br><span class=\"line\">        attention = torch.where(adj &gt; <span class=\"number\">0</span>, e, zero_vec)</span><br><span class=\"line\">        attention = F.softmax(attention, dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        attention = F.dropout(attention, <span class=\"variable language_\">self</span>.dropout, training=<span class=\"variable language_\">self</span>.training)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 聚合特征</span></span><br><span class=\"line\">        h_prime = torch.matmul(attention, h)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"variable language_\">self</span>.concat:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> F.elu(h_prime)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> h_prime</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">GAT</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, nfeat, nhid, nclass, dropout, alpha, nheads</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(GAT, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = dropout</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 多头注意力层</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.attentions = nn.ModuleList([</span><br><span class=\"line\">            GATLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=<span class=\"literal\">True</span>) </span><br><span class=\"line\">            <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(nheads)</span><br><span class=\"line\">        ])</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 输出层</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.out_att = GATLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, adj</span>):</span><br><span class=\"line\">        x = F.dropout(x, <span class=\"variable language_\">self</span>.dropout, training=<span class=\"variable language_\">self</span>.training)</span><br><span class=\"line\">        <span class=\"comment\"># 多头注意力</span></span><br><span class=\"line\">        x = torch.cat([att(x, adj) <span class=\"keyword\">for</span> att <span class=\"keyword\">in</span> <span class=\"variable language_\">self</span>.attentions], dim=<span class=\"number\">1</span>)</span><br><span class=\"line\">        x = F.dropout(x, <span class=\"variable language_\">self</span>.dropout, training=<span class=\"variable language_\">self</span>.training)</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.out_att(x, adj)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.log_softmax(x, dim=<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"实际应用示例\"><a href=\"#实际应用示例\" class=\"headerlink\" title=\"实际应用示例\"></a>实际应用示例</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 模型初始化</span></span><br><span class=\"line\">model = GAT(nfeat=input_dim,</span><br><span class=\"line\">           nhid=<span class=\"number\">8</span>,</span><br><span class=\"line\">           nclass=num_classes,</span><br><span class=\"line\">           dropout=<span class=\"number\">0.6</span>,</span><br><span class=\"line\">           alpha=<span class=\"number\">0.2</span>,</span><br><span class=\"line\">           nheads=<span class=\"number\">8</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 优化器</span></span><br><span class=\"line\">optimizer = torch.optim.Adam(model.parameters(), lr=<span class=\"number\">0.005</span>, weight_decay=<span class=\"number\">5e-4</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 训练循环</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>():</span><br><span class=\"line\">    model.train()</span><br><span class=\"line\">    optimizer.zero_grad()</span><br><span class=\"line\">    output = model(features, adj)</span><br><span class=\"line\">    loss = F.nll_loss(output[idx_train], labels[idx_train])</span><br><span class=\"line\">    loss.backward()</span><br><span class=\"line\">    optimizer.step()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> loss.item()</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"🔍-注意事项与最佳实践\"><a href=\"#🔍-注意事项与最佳实践\" class=\"headerlink\" title=\"🔍 注意事项与最佳实践\"></a>🔍 注意事项与最佳实践</h2><ol>\n<li><p><strong>注意力头数的选择</strong></p>\n<ul>\n<li>通常在4-8之间</li>\n<li>需要根据任务复杂度和计算资源调整</li>\n</ul>\n</li>\n<li><p><strong>过拟合处理</strong></p>\n<ul>\n<li>使用dropout</li>\n<li>添加L2正则化</li>\n<li>使用残差连接</li>\n</ul>\n</li>\n<li><p><strong>计算效率</strong></p>\n<ul>\n<li>对于大规模图，考虑使用稀疏注意力</li>\n<li>可以使用邻居采样减少计算量</li>\n</ul>\n</li>\n<li><p><strong>模型设计考虑</strong></p>\n<ul>\n<li>注意力层的堆叠不宜过深</li>\n<li>考虑添加跳跃连接</li>\n<li>根据任务选择合适的聚合函数</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"📈-未来展望\"><a href=\"#📈-未来展望\" class=\"headerlink\" title=\"📈 未来展望\"></a>📈 未来展望</h2><ol>\n<li><p><strong>可扩展性改进</strong></p>\n<ul>\n<li>研究更高效的注意力计算方法</li>\n<li>探索稀疏注意力机制</li>\n</ul>\n</li>\n<li><p><strong>理论研究</strong></p>\n<ul>\n<li>深入理解注意力机制的工作原理</li>\n<li>研究注意力机制与图的结构特性的关系</li>\n</ul>\n</li>\n<li><p><strong>应用拓展</strong></p>\n<ul>\n<li>在更多领域验证效果</li>\n<li>结合其他先进技术（如Transformer）</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"><a href=\"#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\" class=\"headerlink\" title=\"📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1><p><a href=\"/paper/Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf\" target=\"_blank\">📄 Brauwers和Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a><br><a href=\"/paper/Lee 等 - 2018 - Attention Models in Graphs A Survey.pdf\" target=\"_blank\">📄 Lee 等 - 2018 - Attention Models in Graphs A Survey</a></p>"},{"title":"CDR Input Data Analysis","date":"2025-07-09T13:24:58.000Z","_content":"\n# CDR 数据源分析\n\n本文主要是介绍一下 **深度学习** 在 _药物反应预测_ 中运用到的数据源。~~但由于本人比较捞~~ 本文主要从 **深度学习** 角度来看待这些数据源，对其在医学方面的意义~~（主要是鼠鼠也不会捏）~~不会有太多的描述\n\n<!-- more -->\n\n## CDR = Cancer Drug Response\n\n我们的数据源有三种：\n\n- _Cancer Representations_（癌症特征的表示）\n- _Representations of Drug Compounds_（药物特征的表示）\n- _Representations of Treatment Response_（治疗响应的表示）\n\n接下来会按顺序进行说明\n\n---\n\n### Cancer Representations\n\n癌症的特征是多组学的 ~~这不是理所应当吗~~\n\n#### 多组学类型\n\n通常基于以下四类组学数据：\n\n- 基因组（Genomic）\n\n  - 突变（Mutation）：体细胞突变（如单核苷酸变异 SNVs）可能驱动癌症进展，并影响药物靶点。\n  - 拷贝数变异（CNV）：基因拷贝数的增加或缺失可能影响药物敏感性（如 HER2 扩增与曲妥珠单抗疗效相关）。\n\n- 转录组（Transcriptomic）\n\n  - 基因表达（Gene Expression）：通过微阵列或 RNA 测序（RNA-Seq）量化基因的 mRNA 水平。例如，高表达的耐药基因可能预示治疗失败。\n\n- 表观组（Epigenomic）\n\n  - DNA 甲基化（Methylation）：启动子区域的甲基化可能沉默抑癌基因，影响药物反应。\n\n- 蛋白质组（Proteomic）\n  - 蛋白质表达（RPPA 等）：直接测量蛋白质丰度（如激酶活性），更接近功能表型。\n\n对于同一种组学数据，他们被表示成一组 **维数相同的向量**\n\n#### 预处理与整合\n\n1. 数据预处理\n\n- 包括标准化（normalization）、批次效应校正（batch effect correction）和质量控制（QC）。例如，RNA-Seq 数据需通过 RPKM 或 TPM 标准化。\n\n2. 多组学整合方法 ：\n   - 早期整合（Early Integration）：直接拼接不同组学特征为单一向量，但可能因维度灾难（curse of dimensionality）导致过拟合。\n   - 晚期整合（Late Integration）：通过独立子网络处理每组学数据（如 CNN 处理突变，GNN 处理表达数据），再融合特征。例如，MOLI 模型通过三重损失函数整合多组学数据，显著提升跨癌症模型的泛化能力。\n\n#### 基因特征具有优势及新兴趋势\n\n> 2014 年 NCI-DREAM 挑战赛表明， 基因表达数据在预测乳腺癌细胞系药物敏感性时最具预测力（优于突变或 CNV）。因此，约 90%的 DRP 模型使用基因表达（单独或联合其他组学）\n> <img src=\"/img/CDR-data-analysis/gene.png\" alt=\"gene\" width=\"50%\">\n\n##### 新兴趋势\n\n1. **结构生物学整合**：如利用蛋白质-蛋白质相互作用（PPI）网络（STRING 数据库）或通路信息（GSEA）构建生物网络，增强模型可解释性。\n2. **图神经网络（GNN）**：将基因视为节点、相互作用为边，学习拓扑特征（如 GraOmicDRP 模型）。\n\n---\n\n### Representations of Drug Compounds\n\n对药物的表示主要分为三种，一般只选取其中的一种 ~~虽然也有选用几种的 **创新** 方式~~。值得一提的是，在选定药物的表示方式后，之后的特征工程的方式目前来看非常的统一。接下来一一说明每一种表示方式。\n\n#### SMILES（简化分子输入行条目系统）\n\n1. _定义_：SMILES 是一种**线性字符串**表示法，通过符号编码分子结构（如`CCO`表示乙醇）。\n2. _优势_：\n   - 易于存储和处理，广泛用于化学信息学工具（如 RDKit）。\n   - 可直接用于序列模型（如 RNN、Transformer）或通过预处理转换为其他表示（如图结构）。\n\n#### 分子指纹（Fingerprints, FPs）和描述符（Descriptors）\n\n1. 分子指纹\n\n   - _定义_：**二进制向量**，表示分子中是否存在特定子结构（如药效团或官能团）。\n   - _常用类型_：\n     - **Morgan 指纹（ECFP）**：基于原子邻域的圆形拓扑指纹，长度通常为 512 或 1024 位。\n     - **RDKit 指纹**：开源工具生成的二进制指纹。\n   - _优势_：固定长度，适合传统机器学习模型（如随机森林）。\n\n2. 分子描述符\n\n   - _定义_：**数值向量**，编码物理化学性质（如分子量、疏水性、极性表面积等）。\n   - _工具_：PaDEL、Mordred、Dragon 等软件可自动计算数百至数千个描述符。\n\n#### 图结构表示（Graph-based Representations）\n\n1. _定义_ ：将分子表示为**图**，其中原子为**节点**，化学键为**边**，节点和边可附加属性（如原子类型、键类型）。\n2. _优势_ ：\n   - 更自然地表征分子拓扑结构，适合图神经网络（GNN）。\n   - 可捕捉局部和全局分子特征（如官能团相互作用）。\n\n---\n\n### Representations of Treatment Response\n\n从构造模型的角度出发，这是 DRP 的核心数据源\n\n- 它决定了模型最后完成的**任务类型**：训练连续值的**回归任务**和训练离散值的**分类任务**\n- 他的数据质量很大程度上决定了模型的结果的优劣，即对该数据源对模型的好坏影响很大\n\n此外，很少有从数据分析的角度出发分析这个数据源的文献，于是在这里给出简要的说明\n\n#### 连续值表示（Continuous Measures）\n\n1. **IC50**\n\n   - 半数抑制浓度，即抑制 50%细胞活力所需的药物浓度。\n   - _优势_：直观反映药物效力，广泛用于回归模型（如预测 IC50 的数值）。\n   - _局限性_：仅反映单一浓度点的效果，可能忽略剂量-反应曲线的整体形状。\n\n2. **AUC/AAC**\n   - 剂量-反应曲线下面积（Area Under the Curve）或曲线上面积（Activity Area）。\n   - _优势_：全局度量，综合所有浓度点的效果，对噪声更鲁棒。\n   - _应用_：如 DeepCDR 等模型使用 AUC 作为回归目标，实证表明其泛化性优于 IC50。\n\n#### 分类表示（Categorical Measures）\n\n1. **二分类（敏感/耐药）**\n\n   - 通过阈值（如瀑布算法、LOBICO）将连续反应（如 IC50）转化为离散标签。\n   - _优势_：更贴近临床决策需求（如选择敏感药物）。\n   - _示例_：Sharifi-Noghabi et al. (2021) 使用二分类训练深度神经网络，预测患者肿瘤的敏感性。\n\n2. **多分类**\n   - 如低/中/高反应性，适用于更细粒度的临床分级。\n\n#### 排序表示（Ranking）\n\n1. _目标_\n\n   - 为个性化治疗推荐药物排序（如 Top-k 最有效药物）。\n\n2. _方法_\n\n   - Prasse et al. (2022)：将 IC50 转化为相关性分数，设计可微排序损失函数。\n   - PPORank：利用强化学习动态优化排序，适应新增数据。\n\n3. _优势_\n   - 直接支持临床优先级排序，优于传统回归或分类。\n\n#### 数据分析\n\n由于本人大概率会做个分类模型，所以会将主要分析的是**分类表示**的数据在**图神经网络**中比较重视的几个指标，这里分析 _CCLE_ 和 _GDSC_ 两个数据集在选用主流阈值选取方法之后的表示。\n\n直接先看结果捏（这里画了两个小图）\n\n- CCLE\n\n<img src=\"/img/CDR-data-analysis/comprehensive_bipartite_analysis_ccle.png\" alt=\"CCLE\" style=\"max-width: 100%; height: auto;\">\n\n- GDSC\n\n<img src=\"/img/CDR-data-analysis/comprehensive_bipartite_analysis_gdsc.png\" alt=\"GDSC\" style=\"max-width: 100%; height: auto;\">\n\n<p>\n  👉 <a href=\"/code/data_analysis/visualize_graph_analysis.py\" target=\"_blank\">查看用于生成上述图表的本地 Python 脚本：visualize_graph_analysis.py</a>\n</p>\n\n##### 🔍 关键数据对比\n\n| 特征         | CCLE   | GDSC    | 倍数差异    |\n| ------------ | ------ | ------- | ----------- |\n| **数据规模** |\n| 总节点数     | 341    | 783     | 2.3×        |\n| 第一类节点   | 317    | 561     | 1.8×        |\n| 第二类节点   | 24     | 222     | 9.3×        |\n| 总边数       | 7,307  | 100,572 | 13.8×       |\n| **图结构**   |\n| 密度         | 0.9604 | 0.8075  | 0.84×       |\n| 稀疏性       | 0.0396 | 0.1925  | 4.9×        |\n| 平均度       | 42.86  | 256.89  | 6.0×        |\n| 图直径       | 3      | 4       | 1.3×        |\n| **边分布**   |\n| 正边数量     | 1,375  | 11,591  | 8.4×        |\n| 负边数量     | 5,932  | 88,981  | 15.0×       |\n| 正边比例     | 18.8%  | 11.5%   | 0.61×       |\n| 正负边比例   | 1:4.3  | 1:7.7   | 1.8× 不平衡 |\n\n##### 📊 GNN 训练挑战分析\n\n###### 过平滑风险评估\n\n- **CCLE**: ⚠️ 高风险 (平均度 42.86)\n- **GDSC**: 🚨 极高风险 (平均度 256.89)\n\n###### 样本不平衡程度\n\n- **CCLE**: 正负边比例 1:4.3 (中等不平衡)\n- **GDSC**: 正负边比例 1:7.7 (严重不平衡)\n\n###### 邻居相似度分析\n\n```python\n# 邻居重叠度对比\nCCLE_similarity = {\n    \"第一类节点\": 0.9374,  # 高度相似\n    \"第二类节点\": 0.9274   # 高度相似\n}\n\nGDSC_similarity = {\n    \"第一类节点\": 0.7659,  # 中等相似\n    \"第二类节点\": 0.7143   # 中等相似\n}\n```\n\n**结论**: CCLE 结构更均匀但多样性不足，GDSC 结构更复杂但多样性更好\n\n##### 🎯 GNN 架构建议对比\n\n###### 推荐架构优先级\n\n- CCLE 推荐架构\n\n  1. **Bipartite GNN** + Signed GCN\n  2. **简单异构图 GNN** (HetGNN)\n  3. **标准 GCN** + 强正则化\n\n- GDSC 推荐架构\n\n  1. **采样型 GNN** (GraphSAINT, FastGCN) + SGCN\n  2. **大规模异构图 GNN** (HGT, RGCN)\n  3. **图 Transformer** (处理复杂结构)\n\n###### 具体参数建议\n\n| 参数           | CCLE         | GDSC         | 原因                |\n| -------------- | ------------ | ------------ | ------------------- |\n| **网络深度**   | 2-3 层       | 严格 2 层    | GDSC 过平滑风险更高 |\n| **隐藏维度**   | 64-128       | 128-256      | GDSC 需要更大容量   |\n| **Dropout 率** | 0.3-0.5      | 0.5-0.7      | GDSC 需要更强正则化 |\n| **学习率**     | 0.001-0.01   | 0.0001-0.001 | GDSC 需要更保守训练 |\n| **批次大小**   | 32-64 个子图 | 16-32 个子图 | GDSC 内存限制       |\n| **采样策略**   | 可选         | 必须         | GDSC 无法全图训练   |\n\n# 📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\n\n<a href=\"/paper/Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends.pdf\" target=\"_blank\">📄 Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends</a>","source":"_posts/CDR-data-analysis.md","raw":"---\ntitle: CDR Input Data Analysis\ndate: 2025-07-09 21:24:58\ntags:\n  - CDR\n  - Data Analysis\n  - 可能有点用\n  - graph theory\ncategories:\n  - CDR\n  - Data Analysis\n---\n\n# CDR 数据源分析\n\n本文主要是介绍一下 **深度学习** 在 _药物反应预测_ 中运用到的数据源。~~但由于本人比较捞~~ 本文主要从 **深度学习** 角度来看待这些数据源，对其在医学方面的意义~~（主要是鼠鼠也不会捏）~~不会有太多的描述\n\n<!-- more -->\n\n## CDR = Cancer Drug Response\n\n我们的数据源有三种：\n\n- _Cancer Representations_（癌症特征的表示）\n- _Representations of Drug Compounds_（药物特征的表示）\n- _Representations of Treatment Response_（治疗响应的表示）\n\n接下来会按顺序进行说明\n\n---\n\n### Cancer Representations\n\n癌症的特征是多组学的 ~~这不是理所应当吗~~\n\n#### 多组学类型\n\n通常基于以下四类组学数据：\n\n- 基因组（Genomic）\n\n  - 突变（Mutation）：体细胞突变（如单核苷酸变异 SNVs）可能驱动癌症进展，并影响药物靶点。\n  - 拷贝数变异（CNV）：基因拷贝数的增加或缺失可能影响药物敏感性（如 HER2 扩增与曲妥珠单抗疗效相关）。\n\n- 转录组（Transcriptomic）\n\n  - 基因表达（Gene Expression）：通过微阵列或 RNA 测序（RNA-Seq）量化基因的 mRNA 水平。例如，高表达的耐药基因可能预示治疗失败。\n\n- 表观组（Epigenomic）\n\n  - DNA 甲基化（Methylation）：启动子区域的甲基化可能沉默抑癌基因，影响药物反应。\n\n- 蛋白质组（Proteomic）\n  - 蛋白质表达（RPPA 等）：直接测量蛋白质丰度（如激酶活性），更接近功能表型。\n\n对于同一种组学数据，他们被表示成一组 **维数相同的向量**\n\n#### 预处理与整合\n\n1. 数据预处理\n\n- 包括标准化（normalization）、批次效应校正（batch effect correction）和质量控制（QC）。例如，RNA-Seq 数据需通过 RPKM 或 TPM 标准化。\n\n2. 多组学整合方法 ：\n   - 早期整合（Early Integration）：直接拼接不同组学特征为单一向量，但可能因维度灾难（curse of dimensionality）导致过拟合。\n   - 晚期整合（Late Integration）：通过独立子网络处理每组学数据（如 CNN 处理突变，GNN 处理表达数据），再融合特征。例如，MOLI 模型通过三重损失函数整合多组学数据，显著提升跨癌症模型的泛化能力。\n\n#### 基因特征具有优势及新兴趋势\n\n> 2014 年 NCI-DREAM 挑战赛表明， 基因表达数据在预测乳腺癌细胞系药物敏感性时最具预测力（优于突变或 CNV）。因此，约 90%的 DRP 模型使用基因表达（单独或联合其他组学）\n> <img src=\"/img/CDR-data-analysis/gene.png\" alt=\"gene\" width=\"50%\">\n\n##### 新兴趋势\n\n1. **结构生物学整合**：如利用蛋白质-蛋白质相互作用（PPI）网络（STRING 数据库）或通路信息（GSEA）构建生物网络，增强模型可解释性。\n2. **图神经网络（GNN）**：将基因视为节点、相互作用为边，学习拓扑特征（如 GraOmicDRP 模型）。\n\n---\n\n### Representations of Drug Compounds\n\n对药物的表示主要分为三种，一般只选取其中的一种 ~~虽然也有选用几种的 **创新** 方式~~。值得一提的是，在选定药物的表示方式后，之后的特征工程的方式目前来看非常的统一。接下来一一说明每一种表示方式。\n\n#### SMILES（简化分子输入行条目系统）\n\n1. _定义_：SMILES 是一种**线性字符串**表示法，通过符号编码分子结构（如`CCO`表示乙醇）。\n2. _优势_：\n   - 易于存储和处理，广泛用于化学信息学工具（如 RDKit）。\n   - 可直接用于序列模型（如 RNN、Transformer）或通过预处理转换为其他表示（如图结构）。\n\n#### 分子指纹（Fingerprints, FPs）和描述符（Descriptors）\n\n1. 分子指纹\n\n   - _定义_：**二进制向量**，表示分子中是否存在特定子结构（如药效团或官能团）。\n   - _常用类型_：\n     - **Morgan 指纹（ECFP）**：基于原子邻域的圆形拓扑指纹，长度通常为 512 或 1024 位。\n     - **RDKit 指纹**：开源工具生成的二进制指纹。\n   - _优势_：固定长度，适合传统机器学习模型（如随机森林）。\n\n2. 分子描述符\n\n   - _定义_：**数值向量**，编码物理化学性质（如分子量、疏水性、极性表面积等）。\n   - _工具_：PaDEL、Mordred、Dragon 等软件可自动计算数百至数千个描述符。\n\n#### 图结构表示（Graph-based Representations）\n\n1. _定义_ ：将分子表示为**图**，其中原子为**节点**，化学键为**边**，节点和边可附加属性（如原子类型、键类型）。\n2. _优势_ ：\n   - 更自然地表征分子拓扑结构，适合图神经网络（GNN）。\n   - 可捕捉局部和全局分子特征（如官能团相互作用）。\n\n---\n\n### Representations of Treatment Response\n\n从构造模型的角度出发，这是 DRP 的核心数据源\n\n- 它决定了模型最后完成的**任务类型**：训练连续值的**回归任务**和训练离散值的**分类任务**\n- 他的数据质量很大程度上决定了模型的结果的优劣，即对该数据源对模型的好坏影响很大\n\n此外，很少有从数据分析的角度出发分析这个数据源的文献，于是在这里给出简要的说明\n\n#### 连续值表示（Continuous Measures）\n\n1. **IC50**\n\n   - 半数抑制浓度，即抑制 50%细胞活力所需的药物浓度。\n   - _优势_：直观反映药物效力，广泛用于回归模型（如预测 IC50 的数值）。\n   - _局限性_：仅反映单一浓度点的效果，可能忽略剂量-反应曲线的整体形状。\n\n2. **AUC/AAC**\n   - 剂量-反应曲线下面积（Area Under the Curve）或曲线上面积（Activity Area）。\n   - _优势_：全局度量，综合所有浓度点的效果，对噪声更鲁棒。\n   - _应用_：如 DeepCDR 等模型使用 AUC 作为回归目标，实证表明其泛化性优于 IC50。\n\n#### 分类表示（Categorical Measures）\n\n1. **二分类（敏感/耐药）**\n\n   - 通过阈值（如瀑布算法、LOBICO）将连续反应（如 IC50）转化为离散标签。\n   - _优势_：更贴近临床决策需求（如选择敏感药物）。\n   - _示例_：Sharifi-Noghabi et al. (2021) 使用二分类训练深度神经网络，预测患者肿瘤的敏感性。\n\n2. **多分类**\n   - 如低/中/高反应性，适用于更细粒度的临床分级。\n\n#### 排序表示（Ranking）\n\n1. _目标_\n\n   - 为个性化治疗推荐药物排序（如 Top-k 最有效药物）。\n\n2. _方法_\n\n   - Prasse et al. (2022)：将 IC50 转化为相关性分数，设计可微排序损失函数。\n   - PPORank：利用强化学习动态优化排序，适应新增数据。\n\n3. _优势_\n   - 直接支持临床优先级排序，优于传统回归或分类。\n\n#### 数据分析\n\n由于本人大概率会做个分类模型，所以会将主要分析的是**分类表示**的数据在**图神经网络**中比较重视的几个指标，这里分析 _CCLE_ 和 _GDSC_ 两个数据集在选用主流阈值选取方法之后的表示。\n\n直接先看结果捏（这里画了两个小图）\n\n- CCLE\n\n<img src=\"/img/CDR-data-analysis/comprehensive_bipartite_analysis_ccle.png\" alt=\"CCLE\" style=\"max-width: 100%; height: auto;\">\n\n- GDSC\n\n<img src=\"/img/CDR-data-analysis/comprehensive_bipartite_analysis_gdsc.png\" alt=\"GDSC\" style=\"max-width: 100%; height: auto;\">\n\n<p>\n  👉 <a href=\"/code/data_analysis/visualize_graph_analysis.py\" target=\"_blank\">查看用于生成上述图表的本地 Python 脚本：visualize_graph_analysis.py</a>\n</p>\n\n##### 🔍 关键数据对比\n\n| 特征         | CCLE   | GDSC    | 倍数差异    |\n| ------------ | ------ | ------- | ----------- |\n| **数据规模** |\n| 总节点数     | 341    | 783     | 2.3×        |\n| 第一类节点   | 317    | 561     | 1.8×        |\n| 第二类节点   | 24     | 222     | 9.3×        |\n| 总边数       | 7,307  | 100,572 | 13.8×       |\n| **图结构**   |\n| 密度         | 0.9604 | 0.8075  | 0.84×       |\n| 稀疏性       | 0.0396 | 0.1925  | 4.9×        |\n| 平均度       | 42.86  | 256.89  | 6.0×        |\n| 图直径       | 3      | 4       | 1.3×        |\n| **边分布**   |\n| 正边数量     | 1,375  | 11,591  | 8.4×        |\n| 负边数量     | 5,932  | 88,981  | 15.0×       |\n| 正边比例     | 18.8%  | 11.5%   | 0.61×       |\n| 正负边比例   | 1:4.3  | 1:7.7   | 1.8× 不平衡 |\n\n##### 📊 GNN 训练挑战分析\n\n###### 过平滑风险评估\n\n- **CCLE**: ⚠️ 高风险 (平均度 42.86)\n- **GDSC**: 🚨 极高风险 (平均度 256.89)\n\n###### 样本不平衡程度\n\n- **CCLE**: 正负边比例 1:4.3 (中等不平衡)\n- **GDSC**: 正负边比例 1:7.7 (严重不平衡)\n\n###### 邻居相似度分析\n\n```python\n# 邻居重叠度对比\nCCLE_similarity = {\n    \"第一类节点\": 0.9374,  # 高度相似\n    \"第二类节点\": 0.9274   # 高度相似\n}\n\nGDSC_similarity = {\n    \"第一类节点\": 0.7659,  # 中等相似\n    \"第二类节点\": 0.7143   # 中等相似\n}\n```\n\n**结论**: CCLE 结构更均匀但多样性不足，GDSC 结构更复杂但多样性更好\n\n##### 🎯 GNN 架构建议对比\n\n###### 推荐架构优先级\n\n- CCLE 推荐架构\n\n  1. **Bipartite GNN** + Signed GCN\n  2. **简单异构图 GNN** (HetGNN)\n  3. **标准 GCN** + 强正则化\n\n- GDSC 推荐架构\n\n  1. **采样型 GNN** (GraphSAINT, FastGCN) + SGCN\n  2. **大规模异构图 GNN** (HGT, RGCN)\n  3. **图 Transformer** (处理复杂结构)\n\n###### 具体参数建议\n\n| 参数           | CCLE         | GDSC         | 原因                |\n| -------------- | ------------ | ------------ | ------------------- |\n| **网络深度**   | 2-3 层       | 严格 2 层    | GDSC 过平滑风险更高 |\n| **隐藏维度**   | 64-128       | 128-256      | GDSC 需要更大容量   |\n| **Dropout 率** | 0.3-0.5      | 0.5-0.7      | GDSC 需要更强正则化 |\n| **学习率**     | 0.001-0.01   | 0.0001-0.001 | GDSC 需要更保守训练 |\n| **批次大小**   | 32-64 个子图 | 16-32 个子图 | GDSC 内存限制       |\n| **采样策略**   | 可选         | 必须         | GDSC 无法全图训练   |\n\n# 📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\n\n<a href=\"/paper/Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends.pdf\" target=\"_blank\">📄 Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends</a>","slug":"CDR-data-analysis","published":1,"updated":"2025-07-10T13:39:34.966Z","comments":1,"layout":"post","photos":[],"_id":"cmcyrm1ge0004ugwa8yo4g6py","content":"<h1 id=\"CDR-数据源分析\"><a href=\"#CDR-数据源分析\" class=\"headerlink\" title=\"CDR 数据源分析\"></a>CDR 数据源分析</h1><p>本文主要是介绍一下 <strong>深度学习</strong> 在 <em>药物反应预测</em> 中运用到的数据源。<del>但由于本人比较捞</del> 本文主要从 <strong>深度学习</strong> 角度来看待这些数据源，对其在医学方面的意义<del>（主要是鼠鼠也不会捏）</del>不会有太多的描述</p>\n<span id=\"more\"></span>\n\n<h2 id=\"CDR-Cancer-Drug-Response\"><a href=\"#CDR-Cancer-Drug-Response\" class=\"headerlink\" title=\"CDR &#x3D; Cancer Drug Response\"></a>CDR &#x3D; Cancer Drug Response</h2><p>我们的数据源有三种：</p>\n<ul>\n<li><em>Cancer Representations</em>（癌症特征的表示）</li>\n<li><em>Representations of Drug Compounds</em>（药物特征的表示）</li>\n<li><em>Representations of Treatment Response</em>（治疗响应的表示）</li>\n</ul>\n<p>接下来会按顺序进行说明</p>\n<hr>\n<h3 id=\"Cancer-Representations\"><a href=\"#Cancer-Representations\" class=\"headerlink\" title=\"Cancer Representations\"></a>Cancer Representations</h3><p>癌症的特征是多组学的 <del>这不是理所应当吗</del></p>\n<h4 id=\"多组学类型\"><a href=\"#多组学类型\" class=\"headerlink\" title=\"多组学类型\"></a>多组学类型</h4><p>通常基于以下四类组学数据：</p>\n<ul>\n<li><p>基因组（Genomic）</p>\n<ul>\n<li>突变（Mutation）：体细胞突变（如单核苷酸变异 SNVs）可能驱动癌症进展，并影响药物靶点。</li>\n<li>拷贝数变异（CNV）：基因拷贝数的增加或缺失可能影响药物敏感性（如 HER2 扩增与曲妥珠单抗疗效相关）。</li>\n</ul>\n</li>\n<li><p>转录组（Transcriptomic）</p>\n<ul>\n<li>基因表达（Gene Expression）：通过微阵列或 RNA 测序（RNA-Seq）量化基因的 mRNA 水平。例如，高表达的耐药基因可能预示治疗失败。</li>\n</ul>\n</li>\n<li><p>表观组（Epigenomic）</p>\n<ul>\n<li>DNA 甲基化（Methylation）：启动子区域的甲基化可能沉默抑癌基因，影响药物反应。</li>\n</ul>\n</li>\n<li><p>蛋白质组（Proteomic）</p>\n<ul>\n<li>蛋白质表达（RPPA 等）：直接测量蛋白质丰度（如激酶活性），更接近功能表型。</li>\n</ul>\n</li>\n</ul>\n<p>对于同一种组学数据，他们被表示成一组 <strong>维数相同的向量</strong></p>\n<h4 id=\"预处理与整合\"><a href=\"#预处理与整合\" class=\"headerlink\" title=\"预处理与整合\"></a>预处理与整合</h4><ol>\n<li>数据预处理</li>\n</ol>\n<ul>\n<li>包括标准化（normalization）、批次效应校正（batch effect correction）和质量控制（QC）。例如，RNA-Seq 数据需通过 RPKM 或 TPM 标准化。</li>\n</ul>\n<ol start=\"2\">\n<li>多组学整合方法 ：<ul>\n<li>早期整合（Early Integration）：直接拼接不同组学特征为单一向量，但可能因维度灾难（curse of dimensionality）导致过拟合。</li>\n<li>晚期整合（Late Integration）：通过独立子网络处理每组学数据（如 CNN 处理突变，GNN 处理表达数据），再融合特征。例如，MOLI 模型通过三重损失函数整合多组学数据，显著提升跨癌症模型的泛化能力。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"基因特征具有优势及新兴趋势\"><a href=\"#基因特征具有优势及新兴趋势\" class=\"headerlink\" title=\"基因特征具有优势及新兴趋势\"></a>基因特征具有优势及新兴趋势</h4><blockquote>\n<p>2014 年 NCI-DREAM 挑战赛表明， 基因表达数据在预测乳腺癌细胞系药物敏感性时最具预测力（优于突变或 CNV）。因此，约 90%的 DRP 模型使用基因表达（单独或联合其他组学）<br><img src=\"/img/CDR-data-analysis/gene.png\" alt=\"gene\" width=\"50%\"></p>\n</blockquote>\n<h5 id=\"新兴趋势\"><a href=\"#新兴趋势\" class=\"headerlink\" title=\"新兴趋势\"></a>新兴趋势</h5><ol>\n<li><strong>结构生物学整合</strong>：如利用蛋白质-蛋白质相互作用（PPI）网络（STRING 数据库）或通路信息（GSEA）构建生物网络，增强模型可解释性。</li>\n<li><strong>图神经网络（GNN）</strong>：将基因视为节点、相互作用为边，学习拓扑特征（如 GraOmicDRP 模型）。</li>\n</ol>\n<hr>\n<h3 id=\"Representations-of-Drug-Compounds\"><a href=\"#Representations-of-Drug-Compounds\" class=\"headerlink\" title=\"Representations of Drug Compounds\"></a>Representations of Drug Compounds</h3><p>对药物的表示主要分为三种，一般只选取其中的一种 <del>虽然也有选用几种的 <strong>创新</strong> 方式</del>。值得一提的是，在选定药物的表示方式后，之后的特征工程的方式目前来看非常的统一。接下来一一说明每一种表示方式。</p>\n<h4 id=\"SMILES（简化分子输入行条目系统）\"><a href=\"#SMILES（简化分子输入行条目系统）\" class=\"headerlink\" title=\"SMILES（简化分子输入行条目系统）\"></a>SMILES（简化分子输入行条目系统）</h4><ol>\n<li><em>定义</em>：SMILES 是一种<strong>线性字符串</strong>表示法，通过符号编码分子结构（如<code>CCO</code>表示乙醇）。</li>\n<li><em>优势</em>：<ul>\n<li>易于存储和处理，广泛用于化学信息学工具（如 RDKit）。</li>\n<li>可直接用于序列模型（如 RNN、Transformer）或通过预处理转换为其他表示（如图结构）。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"分子指纹（Fingerprints-FPs）和描述符（Descriptors）\"><a href=\"#分子指纹（Fingerprints-FPs）和描述符（Descriptors）\" class=\"headerlink\" title=\"分子指纹（Fingerprints, FPs）和描述符（Descriptors）\"></a>分子指纹（Fingerprints, FPs）和描述符（Descriptors）</h4><ol>\n<li><p>分子指纹</p>\n<ul>\n<li><em>定义</em>：<strong>二进制向量</strong>，表示分子中是否存在特定子结构（如药效团或官能团）。</li>\n<li><em>常用类型</em>：<ul>\n<li><strong>Morgan 指纹（ECFP）</strong>：基于原子邻域的圆形拓扑指纹，长度通常为 512 或 1024 位。</li>\n<li><strong>RDKit 指纹</strong>：开源工具生成的二进制指纹。</li>\n</ul>\n</li>\n<li><em>优势</em>：固定长度，适合传统机器学习模型（如随机森林）。</li>\n</ul>\n</li>\n<li><p>分子描述符</p>\n<ul>\n<li><em>定义</em>：<strong>数值向量</strong>，编码物理化学性质（如分子量、疏水性、极性表面积等）。</li>\n<li><em>工具</em>：PaDEL、Mordred、Dragon 等软件可自动计算数百至数千个描述符。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"图结构表示（Graph-based-Representations）\"><a href=\"#图结构表示（Graph-based-Representations）\" class=\"headerlink\" title=\"图结构表示（Graph-based Representations）\"></a>图结构表示（Graph-based Representations）</h4><ol>\n<li><em>定义</em> ：将分子表示为<strong>图</strong>，其中原子为<strong>节点</strong>，化学键为<strong>边</strong>，节点和边可附加属性（如原子类型、键类型）。</li>\n<li><em>优势</em> ：<ul>\n<li>更自然地表征分子拓扑结构，适合图神经网络（GNN）。</li>\n<li>可捕捉局部和全局分子特征（如官能团相互作用）。</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h3 id=\"Representations-of-Treatment-Response\"><a href=\"#Representations-of-Treatment-Response\" class=\"headerlink\" title=\"Representations of Treatment Response\"></a>Representations of Treatment Response</h3><p>从构造模型的角度出发，这是 DRP 的核心数据源</p>\n<ul>\n<li>它决定了模型最后完成的<strong>任务类型</strong>：训练连续值的<strong>回归任务</strong>和训练离散值的<strong>分类任务</strong></li>\n<li>他的数据质量很大程度上决定了模型的结果的优劣，即对该数据源对模型的好坏影响很大</li>\n</ul>\n<p>此外，很少有从数据分析的角度出发分析这个数据源的文献，于是在这里给出简要的说明</p>\n<h4 id=\"连续值表示（Continuous-Measures）\"><a href=\"#连续值表示（Continuous-Measures）\" class=\"headerlink\" title=\"连续值表示（Continuous Measures）\"></a>连续值表示（Continuous Measures）</h4><ol>\n<li><p><strong>IC50</strong></p>\n<ul>\n<li>半数抑制浓度，即抑制 50%细胞活力所需的药物浓度。</li>\n<li><em>优势</em>：直观反映药物效力，广泛用于回归模型（如预测 IC50 的数值）。</li>\n<li><em>局限性</em>：仅反映单一浓度点的效果，可能忽略剂量-反应曲线的整体形状。</li>\n</ul>\n</li>\n<li><p><strong>AUC&#x2F;AAC</strong></p>\n<ul>\n<li>剂量-反应曲线下面积（Area Under the Curve）或曲线上面积（Activity Area）。</li>\n<li><em>优势</em>：全局度量，综合所有浓度点的效果，对噪声更鲁棒。</li>\n<li><em>应用</em>：如 DeepCDR 等模型使用 AUC 作为回归目标，实证表明其泛化性优于 IC50。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"分类表示（Categorical-Measures）\"><a href=\"#分类表示（Categorical-Measures）\" class=\"headerlink\" title=\"分类表示（Categorical Measures）\"></a>分类表示（Categorical Measures）</h4><ol>\n<li><p><strong>二分类（敏感&#x2F;耐药）</strong></p>\n<ul>\n<li>通过阈值（如瀑布算法、LOBICO）将连续反应（如 IC50）转化为离散标签。</li>\n<li><em>优势</em>：更贴近临床决策需求（如选择敏感药物）。</li>\n<li><em>示例</em>：Sharifi-Noghabi et al. (2021) 使用二分类训练深度神经网络，预测患者肿瘤的敏感性。</li>\n</ul>\n</li>\n<li><p><strong>多分类</strong></p>\n<ul>\n<li>如低&#x2F;中&#x2F;高反应性，适用于更细粒度的临床分级。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"排序表示（Ranking）\"><a href=\"#排序表示（Ranking）\" class=\"headerlink\" title=\"排序表示（Ranking）\"></a>排序表示（Ranking）</h4><ol>\n<li><p><em>目标</em></p>\n<ul>\n<li>为个性化治疗推荐药物排序（如 Top-k 最有效药物）。</li>\n</ul>\n</li>\n<li><p><em>方法</em></p>\n<ul>\n<li>Prasse et al. (2022)：将 IC50 转化为相关性分数，设计可微排序损失函数。</li>\n<li>PPORank：利用强化学习动态优化排序，适应新增数据。</li>\n</ul>\n</li>\n<li><p><em>优势</em></p>\n<ul>\n<li>直接支持临床优先级排序，优于传统回归或分类。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"数据分析\"><a href=\"#数据分析\" class=\"headerlink\" title=\"数据分析\"></a>数据分析</h4><p>由于本人大概率会做个分类模型，所以会将主要分析的是<strong>分类表示</strong>的数据在<strong>图神经网络</strong>中比较重视的几个指标，这里分析 <em>CCLE</em> 和 <em>GDSC</em> 两个数据集在选用主流阈值选取方法之后的表示。</p>\n<p>直接先看结果捏（这里画了两个小图）</p>\n<ul>\n<li>CCLE</li>\n</ul>\n<img src=\"/img/CDR-data-analysis/comprehensive_bipartite_analysis_ccle.png\" alt=\"CCLE\" style=\"max-width: 100%; height: auto;\">\n\n<ul>\n<li>GDSC</li>\n</ul>\n<img src=\"/img/CDR-data-analysis/comprehensive_bipartite_analysis_gdsc.png\" alt=\"GDSC\" style=\"max-width: 100%; height: auto;\">\n\n<p>\n  👉 <a href=\"/code/data_analysis/visualize_graph_analysis.py\" target=\"_blank\">查看用于生成上述图表的本地 Python 脚本：visualize_graph_analysis.py</a>\n</p>\n\n<h5 id=\"🔍-关键数据对比\"><a href=\"#🔍-关键数据对比\" class=\"headerlink\" title=\"🔍 关键数据对比\"></a>🔍 关键数据对比</h5><table>\n<thead>\n<tr>\n<th>特征</th>\n<th>CCLE</th>\n<th>GDSC</th>\n<th>倍数差异</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>数据规模</strong></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>总节点数</td>\n<td>341</td>\n<td>783</td>\n<td>2.3×</td>\n</tr>\n<tr>\n<td>第一类节点</td>\n<td>317</td>\n<td>561</td>\n<td>1.8×</td>\n</tr>\n<tr>\n<td>第二类节点</td>\n<td>24</td>\n<td>222</td>\n<td>9.3×</td>\n</tr>\n<tr>\n<td>总边数</td>\n<td>7,307</td>\n<td>100,572</td>\n<td>13.8×</td>\n</tr>\n<tr>\n<td><strong>图结构</strong></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>密度</td>\n<td>0.9604</td>\n<td>0.8075</td>\n<td>0.84×</td>\n</tr>\n<tr>\n<td>稀疏性</td>\n<td>0.0396</td>\n<td>0.1925</td>\n<td>4.9×</td>\n</tr>\n<tr>\n<td>平均度</td>\n<td>42.86</td>\n<td>256.89</td>\n<td>6.0×</td>\n</tr>\n<tr>\n<td>图直径</td>\n<td>3</td>\n<td>4</td>\n<td>1.3×</td>\n</tr>\n<tr>\n<td><strong>边分布</strong></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>正边数量</td>\n<td>1,375</td>\n<td>11,591</td>\n<td>8.4×</td>\n</tr>\n<tr>\n<td>负边数量</td>\n<td>5,932</td>\n<td>88,981</td>\n<td>15.0×</td>\n</tr>\n<tr>\n<td>正边比例</td>\n<td>18.8%</td>\n<td>11.5%</td>\n<td>0.61×</td>\n</tr>\n<tr>\n<td>正负边比例</td>\n<td>1:4.3</td>\n<td>1:7.7</td>\n<td>1.8× 不平衡</td>\n</tr>\n</tbody></table>\n<h5 id=\"📊-GNN-训练挑战分析\"><a href=\"#📊-GNN-训练挑战分析\" class=\"headerlink\" title=\"📊 GNN 训练挑战分析\"></a>📊 GNN 训练挑战分析</h5><h6 id=\"过平滑风险评估\"><a href=\"#过平滑风险评估\" class=\"headerlink\" title=\"过平滑风险评估\"></a>过平滑风险评估</h6><ul>\n<li><strong>CCLE</strong>: ⚠️ 高风险 (平均度 42.86)</li>\n<li><strong>GDSC</strong>: 🚨 极高风险 (平均度 256.89)</li>\n</ul>\n<h6 id=\"样本不平衡程度\"><a href=\"#样本不平衡程度\" class=\"headerlink\" title=\"样本不平衡程度\"></a>样本不平衡程度</h6><ul>\n<li><strong>CCLE</strong>: 正负边比例 1:4.3 (中等不平衡)</li>\n<li><strong>GDSC</strong>: 正负边比例 1:7.7 (严重不平衡)</li>\n</ul>\n<h6 id=\"邻居相似度分析\"><a href=\"#邻居相似度分析\" class=\"headerlink\" title=\"邻居相似度分析\"></a>邻居相似度分析</h6><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 邻居重叠度对比</span></span><br><span class=\"line\">CCLE_similarity = &#123;</span><br><span class=\"line\">    <span class=\"string\">&quot;第一类节点&quot;</span>: <span class=\"number\">0.9374</span>,  <span class=\"comment\"># 高度相似</span></span><br><span class=\"line\">    <span class=\"string\">&quot;第二类节点&quot;</span>: <span class=\"number\">0.9274</span>   <span class=\"comment\"># 高度相似</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">GDSC_similarity = &#123;</span><br><span class=\"line\">    <span class=\"string\">&quot;第一类节点&quot;</span>: <span class=\"number\">0.7659</span>,  <span class=\"comment\"># 中等相似</span></span><br><span class=\"line\">    <span class=\"string\">&quot;第二类节点&quot;</span>: <span class=\"number\">0.7143</span>   <span class=\"comment\"># 中等相似</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p><strong>结论</strong>: CCLE 结构更均匀但多样性不足，GDSC 结构更复杂但多样性更好</p>\n<h5 id=\"🎯-GNN-架构建议对比\"><a href=\"#🎯-GNN-架构建议对比\" class=\"headerlink\" title=\"🎯 GNN 架构建议对比\"></a>🎯 GNN 架构建议对比</h5><h6 id=\"推荐架构优先级\"><a href=\"#推荐架构优先级\" class=\"headerlink\" title=\"推荐架构优先级\"></a>推荐架构优先级</h6><ul>\n<li><p>CCLE 推荐架构</p>\n<ol>\n<li><strong>Bipartite GNN</strong> + Signed GCN</li>\n<li><strong>简单异构图 GNN</strong> (HetGNN)</li>\n<li><strong>标准 GCN</strong> + 强正则化</li>\n</ol>\n</li>\n<li><p>GDSC 推荐架构</p>\n<ol>\n<li><strong>采样型 GNN</strong> (GraphSAINT, FastGCN) + SGCN</li>\n<li><strong>大规模异构图 GNN</strong> (HGT, RGCN)</li>\n<li><strong>图 Transformer</strong> (处理复杂结构)</li>\n</ol>\n</li>\n</ul>\n<h6 id=\"具体参数建议\"><a href=\"#具体参数建议\" class=\"headerlink\" title=\"具体参数建议\"></a>具体参数建议</h6><table>\n<thead>\n<tr>\n<th>参数</th>\n<th>CCLE</th>\n<th>GDSC</th>\n<th>原因</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>网络深度</strong></td>\n<td>2-3 层</td>\n<td>严格 2 层</td>\n<td>GDSC 过平滑风险更高</td>\n</tr>\n<tr>\n<td><strong>隐藏维度</strong></td>\n<td>64-128</td>\n<td>128-256</td>\n<td>GDSC 需要更大容量</td>\n</tr>\n<tr>\n<td><strong>Dropout 率</strong></td>\n<td>0.3-0.5</td>\n<td>0.5-0.7</td>\n<td>GDSC 需要更强正则化</td>\n</tr>\n<tr>\n<td><strong>学习率</strong></td>\n<td>0.001-0.01</td>\n<td>0.0001-0.001</td>\n<td>GDSC 需要更保守训练</td>\n</tr>\n<tr>\n<td><strong>批次大小</strong></td>\n<td>32-64 个子图</td>\n<td>16-32 个子图</td>\n<td>GDSC 内存限制</td>\n</tr>\n<tr>\n<td><strong>采样策略</strong></td>\n<td>可选</td>\n<td>必须</td>\n<td>GDSC 无法全图训练</td>\n</tr>\n</tbody></table>\n<h1 id=\"📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"><a href=\"#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\" class=\"headerlink\" title=\"📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1><p><a href=\"/paper/Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends.pdf\" target=\"_blank\">📄 Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends</a></p>\n","excerpt":"<h1 id=\"CDR-数据源分析\"><a href=\"#CDR-数据源分析\" class=\"headerlink\" title=\"CDR 数据源分析\"></a>CDR 数据源分析</h1><p>本文主要是介绍一下 <strong>深度学习</strong> 在 <em>药物反应预测</em> 中运用到的数据源。<del>但由于本人比较捞</del> 本文主要从 <strong>深度学习</strong> 角度来看待这些数据源，对其在医学方面的意义<del>（主要是鼠鼠也不会捏）</del>不会有太多的描述</p>","more":"<h2 id=\"CDR-Cancer-Drug-Response\"><a href=\"#CDR-Cancer-Drug-Response\" class=\"headerlink\" title=\"CDR &#x3D; Cancer Drug Response\"></a>CDR &#x3D; Cancer Drug Response</h2><p>我们的数据源有三种：</p>\n<ul>\n<li><em>Cancer Representations</em>（癌症特征的表示）</li>\n<li><em>Representations of Drug Compounds</em>（药物特征的表示）</li>\n<li><em>Representations of Treatment Response</em>（治疗响应的表示）</li>\n</ul>\n<p>接下来会按顺序进行说明</p>\n<hr>\n<h3 id=\"Cancer-Representations\"><a href=\"#Cancer-Representations\" class=\"headerlink\" title=\"Cancer Representations\"></a>Cancer Representations</h3><p>癌症的特征是多组学的 <del>这不是理所应当吗</del></p>\n<h4 id=\"多组学类型\"><a href=\"#多组学类型\" class=\"headerlink\" title=\"多组学类型\"></a>多组学类型</h4><p>通常基于以下四类组学数据：</p>\n<ul>\n<li><p>基因组（Genomic）</p>\n<ul>\n<li>突变（Mutation）：体细胞突变（如单核苷酸变异 SNVs）可能驱动癌症进展，并影响药物靶点。</li>\n<li>拷贝数变异（CNV）：基因拷贝数的增加或缺失可能影响药物敏感性（如 HER2 扩增与曲妥珠单抗疗效相关）。</li>\n</ul>\n</li>\n<li><p>转录组（Transcriptomic）</p>\n<ul>\n<li>基因表达（Gene Expression）：通过微阵列或 RNA 测序（RNA-Seq）量化基因的 mRNA 水平。例如，高表达的耐药基因可能预示治疗失败。</li>\n</ul>\n</li>\n<li><p>表观组（Epigenomic）</p>\n<ul>\n<li>DNA 甲基化（Methylation）：启动子区域的甲基化可能沉默抑癌基因，影响药物反应。</li>\n</ul>\n</li>\n<li><p>蛋白质组（Proteomic）</p>\n<ul>\n<li>蛋白质表达（RPPA 等）：直接测量蛋白质丰度（如激酶活性），更接近功能表型。</li>\n</ul>\n</li>\n</ul>\n<p>对于同一种组学数据，他们被表示成一组 <strong>维数相同的向量</strong></p>\n<h4 id=\"预处理与整合\"><a href=\"#预处理与整合\" class=\"headerlink\" title=\"预处理与整合\"></a>预处理与整合</h4><ol>\n<li>数据预处理</li>\n</ol>\n<ul>\n<li>包括标准化（normalization）、批次效应校正（batch effect correction）和质量控制（QC）。例如，RNA-Seq 数据需通过 RPKM 或 TPM 标准化。</li>\n</ul>\n<ol start=\"2\">\n<li>多组学整合方法 ：<ul>\n<li>早期整合（Early Integration）：直接拼接不同组学特征为单一向量，但可能因维度灾难（curse of dimensionality）导致过拟合。</li>\n<li>晚期整合（Late Integration）：通过独立子网络处理每组学数据（如 CNN 处理突变，GNN 处理表达数据），再融合特征。例如，MOLI 模型通过三重损失函数整合多组学数据，显著提升跨癌症模型的泛化能力。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"基因特征具有优势及新兴趋势\"><a href=\"#基因特征具有优势及新兴趋势\" class=\"headerlink\" title=\"基因特征具有优势及新兴趋势\"></a>基因特征具有优势及新兴趋势</h4><blockquote>\n<p>2014 年 NCI-DREAM 挑战赛表明， 基因表达数据在预测乳腺癌细胞系药物敏感性时最具预测力（优于突变或 CNV）。因此，约 90%的 DRP 模型使用基因表达（单独或联合其他组学）<br><img src=\"/img/CDR-data-analysis/gene.png\" alt=\"gene\" width=\"50%\"></p>\n</blockquote>\n<h5 id=\"新兴趋势\"><a href=\"#新兴趋势\" class=\"headerlink\" title=\"新兴趋势\"></a>新兴趋势</h5><ol>\n<li><strong>结构生物学整合</strong>：如利用蛋白质-蛋白质相互作用（PPI）网络（STRING 数据库）或通路信息（GSEA）构建生物网络，增强模型可解释性。</li>\n<li><strong>图神经网络（GNN）</strong>：将基因视为节点、相互作用为边，学习拓扑特征（如 GraOmicDRP 模型）。</li>\n</ol>\n<hr>\n<h3 id=\"Representations-of-Drug-Compounds\"><a href=\"#Representations-of-Drug-Compounds\" class=\"headerlink\" title=\"Representations of Drug Compounds\"></a>Representations of Drug Compounds</h3><p>对药物的表示主要分为三种，一般只选取其中的一种 <del>虽然也有选用几种的 <strong>创新</strong> 方式</del>。值得一提的是，在选定药物的表示方式后，之后的特征工程的方式目前来看非常的统一。接下来一一说明每一种表示方式。</p>\n<h4 id=\"SMILES（简化分子输入行条目系统）\"><a href=\"#SMILES（简化分子输入行条目系统）\" class=\"headerlink\" title=\"SMILES（简化分子输入行条目系统）\"></a>SMILES（简化分子输入行条目系统）</h4><ol>\n<li><em>定义</em>：SMILES 是一种<strong>线性字符串</strong>表示法，通过符号编码分子结构（如<code>CCO</code>表示乙醇）。</li>\n<li><em>优势</em>：<ul>\n<li>易于存储和处理，广泛用于化学信息学工具（如 RDKit）。</li>\n<li>可直接用于序列模型（如 RNN、Transformer）或通过预处理转换为其他表示（如图结构）。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"分子指纹（Fingerprints-FPs）和描述符（Descriptors）\"><a href=\"#分子指纹（Fingerprints-FPs）和描述符（Descriptors）\" class=\"headerlink\" title=\"分子指纹（Fingerprints, FPs）和描述符（Descriptors）\"></a>分子指纹（Fingerprints, FPs）和描述符（Descriptors）</h4><ol>\n<li><p>分子指纹</p>\n<ul>\n<li><em>定义</em>：<strong>二进制向量</strong>，表示分子中是否存在特定子结构（如药效团或官能团）。</li>\n<li><em>常用类型</em>：<ul>\n<li><strong>Morgan 指纹（ECFP）</strong>：基于原子邻域的圆形拓扑指纹，长度通常为 512 或 1024 位。</li>\n<li><strong>RDKit 指纹</strong>：开源工具生成的二进制指纹。</li>\n</ul>\n</li>\n<li><em>优势</em>：固定长度，适合传统机器学习模型（如随机森林）。</li>\n</ul>\n</li>\n<li><p>分子描述符</p>\n<ul>\n<li><em>定义</em>：<strong>数值向量</strong>，编码物理化学性质（如分子量、疏水性、极性表面积等）。</li>\n<li><em>工具</em>：PaDEL、Mordred、Dragon 等软件可自动计算数百至数千个描述符。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"图结构表示（Graph-based-Representations）\"><a href=\"#图结构表示（Graph-based-Representations）\" class=\"headerlink\" title=\"图结构表示（Graph-based Representations）\"></a>图结构表示（Graph-based Representations）</h4><ol>\n<li><em>定义</em> ：将分子表示为<strong>图</strong>，其中原子为<strong>节点</strong>，化学键为<strong>边</strong>，节点和边可附加属性（如原子类型、键类型）。</li>\n<li><em>优势</em> ：<ul>\n<li>更自然地表征分子拓扑结构，适合图神经网络（GNN）。</li>\n<li>可捕捉局部和全局分子特征（如官能团相互作用）。</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h3 id=\"Representations-of-Treatment-Response\"><a href=\"#Representations-of-Treatment-Response\" class=\"headerlink\" title=\"Representations of Treatment Response\"></a>Representations of Treatment Response</h3><p>从构造模型的角度出发，这是 DRP 的核心数据源</p>\n<ul>\n<li>它决定了模型最后完成的<strong>任务类型</strong>：训练连续值的<strong>回归任务</strong>和训练离散值的<strong>分类任务</strong></li>\n<li>他的数据质量很大程度上决定了模型的结果的优劣，即对该数据源对模型的好坏影响很大</li>\n</ul>\n<p>此外，很少有从数据分析的角度出发分析这个数据源的文献，于是在这里给出简要的说明</p>\n<h4 id=\"连续值表示（Continuous-Measures）\"><a href=\"#连续值表示（Continuous-Measures）\" class=\"headerlink\" title=\"连续值表示（Continuous Measures）\"></a>连续值表示（Continuous Measures）</h4><ol>\n<li><p><strong>IC50</strong></p>\n<ul>\n<li>半数抑制浓度，即抑制 50%细胞活力所需的药物浓度。</li>\n<li><em>优势</em>：直观反映药物效力，广泛用于回归模型（如预测 IC50 的数值）。</li>\n<li><em>局限性</em>：仅反映单一浓度点的效果，可能忽略剂量-反应曲线的整体形状。</li>\n</ul>\n</li>\n<li><p><strong>AUC&#x2F;AAC</strong></p>\n<ul>\n<li>剂量-反应曲线下面积（Area Under the Curve）或曲线上面积（Activity Area）。</li>\n<li><em>优势</em>：全局度量，综合所有浓度点的效果，对噪声更鲁棒。</li>\n<li><em>应用</em>：如 DeepCDR 等模型使用 AUC 作为回归目标，实证表明其泛化性优于 IC50。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"分类表示（Categorical-Measures）\"><a href=\"#分类表示（Categorical-Measures）\" class=\"headerlink\" title=\"分类表示（Categorical Measures）\"></a>分类表示（Categorical Measures）</h4><ol>\n<li><p><strong>二分类（敏感&#x2F;耐药）</strong></p>\n<ul>\n<li>通过阈值（如瀑布算法、LOBICO）将连续反应（如 IC50）转化为离散标签。</li>\n<li><em>优势</em>：更贴近临床决策需求（如选择敏感药物）。</li>\n<li><em>示例</em>：Sharifi-Noghabi et al. (2021) 使用二分类训练深度神经网络，预测患者肿瘤的敏感性。</li>\n</ul>\n</li>\n<li><p><strong>多分类</strong></p>\n<ul>\n<li>如低&#x2F;中&#x2F;高反应性，适用于更细粒度的临床分级。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"排序表示（Ranking）\"><a href=\"#排序表示（Ranking）\" class=\"headerlink\" title=\"排序表示（Ranking）\"></a>排序表示（Ranking）</h4><ol>\n<li><p><em>目标</em></p>\n<ul>\n<li>为个性化治疗推荐药物排序（如 Top-k 最有效药物）。</li>\n</ul>\n</li>\n<li><p><em>方法</em></p>\n<ul>\n<li>Prasse et al. (2022)：将 IC50 转化为相关性分数，设计可微排序损失函数。</li>\n<li>PPORank：利用强化学习动态优化排序，适应新增数据。</li>\n</ul>\n</li>\n<li><p><em>优势</em></p>\n<ul>\n<li>直接支持临床优先级排序，优于传统回归或分类。</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"数据分析\"><a href=\"#数据分析\" class=\"headerlink\" title=\"数据分析\"></a>数据分析</h4><p>由于本人大概率会做个分类模型，所以会将主要分析的是<strong>分类表示</strong>的数据在<strong>图神经网络</strong>中比较重视的几个指标，这里分析 <em>CCLE</em> 和 <em>GDSC</em> 两个数据集在选用主流阈值选取方法之后的表示。</p>\n<p>直接先看结果捏（这里画了两个小图）</p>\n<ul>\n<li>CCLE</li>\n</ul>\n<img src=\"/img/CDR-data-analysis/comprehensive_bipartite_analysis_ccle.png\" alt=\"CCLE\" style=\"max-width: 100%; height: auto;\">\n\n<ul>\n<li>GDSC</li>\n</ul>\n<img src=\"/img/CDR-data-analysis/comprehensive_bipartite_analysis_gdsc.png\" alt=\"GDSC\" style=\"max-width: 100%; height: auto;\">\n\n<p>\n  👉 <a href=\"/code/data_analysis/visualize_graph_analysis.py\" target=\"_blank\">查看用于生成上述图表的本地 Python 脚本：visualize_graph_analysis.py</a>\n</p>\n\n<h5 id=\"🔍-关键数据对比\"><a href=\"#🔍-关键数据对比\" class=\"headerlink\" title=\"🔍 关键数据对比\"></a>🔍 关键数据对比</h5><table>\n<thead>\n<tr>\n<th>特征</th>\n<th>CCLE</th>\n<th>GDSC</th>\n<th>倍数差异</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>数据规模</strong></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>总节点数</td>\n<td>341</td>\n<td>783</td>\n<td>2.3×</td>\n</tr>\n<tr>\n<td>第一类节点</td>\n<td>317</td>\n<td>561</td>\n<td>1.8×</td>\n</tr>\n<tr>\n<td>第二类节点</td>\n<td>24</td>\n<td>222</td>\n<td>9.3×</td>\n</tr>\n<tr>\n<td>总边数</td>\n<td>7,307</td>\n<td>100,572</td>\n<td>13.8×</td>\n</tr>\n<tr>\n<td><strong>图结构</strong></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>密度</td>\n<td>0.9604</td>\n<td>0.8075</td>\n<td>0.84×</td>\n</tr>\n<tr>\n<td>稀疏性</td>\n<td>0.0396</td>\n<td>0.1925</td>\n<td>4.9×</td>\n</tr>\n<tr>\n<td>平均度</td>\n<td>42.86</td>\n<td>256.89</td>\n<td>6.0×</td>\n</tr>\n<tr>\n<td>图直径</td>\n<td>3</td>\n<td>4</td>\n<td>1.3×</td>\n</tr>\n<tr>\n<td><strong>边分布</strong></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>正边数量</td>\n<td>1,375</td>\n<td>11,591</td>\n<td>8.4×</td>\n</tr>\n<tr>\n<td>负边数量</td>\n<td>5,932</td>\n<td>88,981</td>\n<td>15.0×</td>\n</tr>\n<tr>\n<td>正边比例</td>\n<td>18.8%</td>\n<td>11.5%</td>\n<td>0.61×</td>\n</tr>\n<tr>\n<td>正负边比例</td>\n<td>1:4.3</td>\n<td>1:7.7</td>\n<td>1.8× 不平衡</td>\n</tr>\n</tbody></table>\n<h5 id=\"📊-GNN-训练挑战分析\"><a href=\"#📊-GNN-训练挑战分析\" class=\"headerlink\" title=\"📊 GNN 训练挑战分析\"></a>📊 GNN 训练挑战分析</h5><h6 id=\"过平滑风险评估\"><a href=\"#过平滑风险评估\" class=\"headerlink\" title=\"过平滑风险评估\"></a>过平滑风险评估</h6><ul>\n<li><strong>CCLE</strong>: ⚠️ 高风险 (平均度 42.86)</li>\n<li><strong>GDSC</strong>: 🚨 极高风险 (平均度 256.89)</li>\n</ul>\n<h6 id=\"样本不平衡程度\"><a href=\"#样本不平衡程度\" class=\"headerlink\" title=\"样本不平衡程度\"></a>样本不平衡程度</h6><ul>\n<li><strong>CCLE</strong>: 正负边比例 1:4.3 (中等不平衡)</li>\n<li><strong>GDSC</strong>: 正负边比例 1:7.7 (严重不平衡)</li>\n</ul>\n<h6 id=\"邻居相似度分析\"><a href=\"#邻居相似度分析\" class=\"headerlink\" title=\"邻居相似度分析\"></a>邻居相似度分析</h6><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 邻居重叠度对比</span></span><br><span class=\"line\">CCLE_similarity = &#123;</span><br><span class=\"line\">    <span class=\"string\">&quot;第一类节点&quot;</span>: <span class=\"number\">0.9374</span>,  <span class=\"comment\"># 高度相似</span></span><br><span class=\"line\">    <span class=\"string\">&quot;第二类节点&quot;</span>: <span class=\"number\">0.9274</span>   <span class=\"comment\"># 高度相似</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">GDSC_similarity = &#123;</span><br><span class=\"line\">    <span class=\"string\">&quot;第一类节点&quot;</span>: <span class=\"number\">0.7659</span>,  <span class=\"comment\"># 中等相似</span></span><br><span class=\"line\">    <span class=\"string\">&quot;第二类节点&quot;</span>: <span class=\"number\">0.7143</span>   <span class=\"comment\"># 中等相似</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p><strong>结论</strong>: CCLE 结构更均匀但多样性不足，GDSC 结构更复杂但多样性更好</p>\n<h5 id=\"🎯-GNN-架构建议对比\"><a href=\"#🎯-GNN-架构建议对比\" class=\"headerlink\" title=\"🎯 GNN 架构建议对比\"></a>🎯 GNN 架构建议对比</h5><h6 id=\"推荐架构优先级\"><a href=\"#推荐架构优先级\" class=\"headerlink\" title=\"推荐架构优先级\"></a>推荐架构优先级</h6><ul>\n<li><p>CCLE 推荐架构</p>\n<ol>\n<li><strong>Bipartite GNN</strong> + Signed GCN</li>\n<li><strong>简单异构图 GNN</strong> (HetGNN)</li>\n<li><strong>标准 GCN</strong> + 强正则化</li>\n</ol>\n</li>\n<li><p>GDSC 推荐架构</p>\n<ol>\n<li><strong>采样型 GNN</strong> (GraphSAINT, FastGCN) + SGCN</li>\n<li><strong>大规模异构图 GNN</strong> (HGT, RGCN)</li>\n<li><strong>图 Transformer</strong> (处理复杂结构)</li>\n</ol>\n</li>\n</ul>\n<h6 id=\"具体参数建议\"><a href=\"#具体参数建议\" class=\"headerlink\" title=\"具体参数建议\"></a>具体参数建议</h6><table>\n<thead>\n<tr>\n<th>参数</th>\n<th>CCLE</th>\n<th>GDSC</th>\n<th>原因</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>网络深度</strong></td>\n<td>2-3 层</td>\n<td>严格 2 层</td>\n<td>GDSC 过平滑风险更高</td>\n</tr>\n<tr>\n<td><strong>隐藏维度</strong></td>\n<td>64-128</td>\n<td>128-256</td>\n<td>GDSC 需要更大容量</td>\n</tr>\n<tr>\n<td><strong>Dropout 率</strong></td>\n<td>0.3-0.5</td>\n<td>0.5-0.7</td>\n<td>GDSC 需要更强正则化</td>\n</tr>\n<tr>\n<td><strong>学习率</strong></td>\n<td>0.001-0.01</td>\n<td>0.0001-0.001</td>\n<td>GDSC 需要更保守训练</td>\n</tr>\n<tr>\n<td><strong>批次大小</strong></td>\n<td>32-64 个子图</td>\n<td>16-32 个子图</td>\n<td>GDSC 内存限制</td>\n</tr>\n<tr>\n<td><strong>采样策略</strong></td>\n<td>可选</td>\n<td>必须</td>\n<td>GDSC 无法全图训练</td>\n</tr>\n</tbody></table>\n<h1 id=\"📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"><a href=\"#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\" class=\"headerlink\" title=\"📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1><p><a href=\"/paper/Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends.pdf\" target=\"_blank\">📄 Partin - Deep learning methods for drug response prediction in cancer Predominant and emerging trends</a></p>"},{"title":"What Is GNN and GCN ?","date":"2025-07-10T07:36:46.000Z","_content":"\n# GNN 与 GCN\n\n> 图神经网络（Graph Neural Networks, GNN）和图卷积网络（Graph Convolutional Networks, GCN）是处理图数据的强大工具。本文将从理论到实践，全面介绍这两种重要的深度学习模型。\n\n本文主要介绍了*GNN和GCN的大致原理*，*GCN在PyG和PyTorch的实现* 以及它们在*DRP中的应用*\n\n<!-- more -->\n\n## 🎯 Intro\n\n在深度学习领域，处理图结构数据一直是一个具有挑战性的任务。传统的深度学习模型（如CNN、RNN）在处理欧几里得空间中的数据表现出色，但对于图这种非欧几里得结构的数据却显得力不从心。GNN和GCN的出现，为我们提供了处理图数据的有力工具。\n\n而在DRP领域，由于涉及到大量的Embedding，GCN现在几乎已经成为了必不可少的模块。\n\n但在开始各种各样的奇形怪状的GCN之前，了解GNN和GCN本身的实现仍然是非常必要的。~~于鼠鼠而言~~大致有以下理由：\n1. 部分抽象的基于GCN的模块第三方库不一定支持\n2. 由于反应表示数据的不平衡，我们可以构建的模型的层数是非常有限的（因为会过平滑）。因此对层内的改造就显得非常必要了。而这一切的前提便是理解原理捏\n\n在这里强烈建议去看一下[Distill](https://distill.pub/)的两篇有关图神经网络的博客，非常易懂。\n\n---\n\n## 📚 理论基础\n\n### 图的基本概念\n\n在开始之前，我们需要理解图的基本表示：\n- 图 $G = (V, E)$，其中 $V$ 是节点集合，$E$ 是边集合\n- 邻接矩阵 $A \\in \\mathbb{R}^{n \\times n}$\n- 度矩阵 $D = diag(d_1,...,d_n)$，其中 $d_i = \\sum_j A_{ij}$\n- 节点特征矩阵 $X \\in \\mathbb{R}^{n \\times d}$\n\n### GNN框架\n\nGNN的基本框架遵循消息传递范式（Message Passing Neural Network, MPNN），可以用以下数学公式表示：\n\n1. **消息传递阶段**（Message Passing）：\n   \n   对于节点 $v$，从其邻居节点 $u \\in \\mathcal{N}(v)$ 收集信息：\n   \n   $$m_v^{(l)} = \\sum_{u \\in \\mathcal{N}(v)} M_l(h_v^{(l-1)}, h_u^{(l-1)}, e_{uv})$$\n\n   其中：\n   - $h_v^{(l-1)}$ 是节点 $v$ 在第 $l-1$ 层的特征\n   - $e_{uv}$ 是边 $(u,v)$ 的特征\n   - $M_l$ 是可学习的消息函数\n\n2. **消息聚合阶段**（Aggregation）：\n   \n   将收集到的消息进行聚合：\n\n   $$a_v^{(l)} = AGG(\\{m_v^{(l)} | u \\in \\mathcal{N}(v)\\})$$\n\n   常见的聚合函数包括：\n   - 求和：$AGG_{sum} = \\sum_{u \\in \\mathcal{N}(v)} m_u$\n   - 平均：$AGG_{mean} = \\frac{1}{|\\mathcal{N}(v)|} \\sum_{u \\in \\mathcal{N}(v)} m_u$\n   - 最大：$AGG_{max} = max_{u \\in \\mathcal{N}(v)} m_u$\n\n3. **节点更新阶段**（Update）：\n   \n   更新节点的表示：\n\n   $$h_v^{(l)} = U_l(h_v^{(l-1)}, a_v^{(l)})$$\n\n   其中 $U_l$ 是可学习的更新函数，通常是MLP或其他神经网络。\n\n### GCN实现\n\n#### 拉普拉斯矩阵 🔍\n\n拉普拉斯矩阵是图信号处理中的核心概念，有多种形式：\n\n1. **组合拉普拉斯矩阵**：$L = D - A$\n\n2. **标准化拉普拉斯矩阵**：$L_{sym} = D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}} = I - D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$\n\n3. **随机游走拉普拉斯矩阵**：$L_{rw} = D^{-1}L = I - D^{-1}A$\n\n拉普拉斯矩阵的特性：\n- 对称性：$L = L^T$\n- 半正定性：所有特征值非负\n- 最小特征值为0，对应的特征向量是常数向量\n- 特征值的重数对应图的连通分量数\n\n#### 从传统卷积到图卷积 🔄\n\n##### 传统卷积回顾\n\n在欧几里得空间中，卷积操作定义为：\n\n$$(f * g)(p) = \\sum_{q \\in \\mathcal{N}(p)} f(q) \\cdot g(p-q)$$\n\n这里的关键特点是：\n- 平移不变性\n- 局部性\n- 参数共享\n\n##### 图上的卷积定义\n\n在图域中，我们需要重新定义这些特性：\n\n1. **空间域卷积**：\n   $$h_v = \\sum_{u \\in \\mathcal{N}(v)} W(e_{u,v})h_u$$\n   其中 $W(e_{u,v})$ 是边的权重函数\n\n2. **谱域卷积**：\n   $$g_\\theta * x = Ug_\\theta U^T x$$\n   其中 $U$ 是拉普拉斯矩阵的特征向量矩阵\n\n#### GCN的数学推导 ⚙️\n\nKipf & Welling提出的GCN模型中，单层传播规则为：\n\n$$H^{(l+1)} = \\sigma(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)})$$\n\n其中：\n- $\\tilde{A} = A + I_N$ 是添加了自环的邻接矩阵\n- $\\tilde{D}\\_{ii} = \\sum\\_{j} \\tilde{A}\\_{ij}$ 是对应的度矩阵\n- $H^{(l)}$ 是第 $l$ 层的激活值\n- $W^{(l)}$ 是可学习的权重矩阵\n- $\\sigma$ 是非线性激活函数\n\n~~一些自己的理解~~\n1. 引入$L_{sym} = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$作为聚合（AGG）部分\n   - 添加自环：$\\tilde{A} = A + I_N$\n   - 计算归一化系数：$\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$\n2. 特征变换：$H^{(l)}W^{(l)}$\n3. 邻域聚合：$\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)}$\n4. 非线性变换：$\\sigma(\\cdot)$\n\n---\n\n## 💻 实现细节\n\n基于这个理论框架的简单实现如下：\n\n```python\ndef message_passing(nodes, edges):\n    messages = {}\n    for edge in edges:\n        src, dst = edge\n        msg = compute_message(nodes[src], nodes[dst])\n        messages.setdefault(dst, []).append(msg)\n    return messages\n\ndef aggregate_messages(messages):\n    aggregated = {}\n    for node, msgs in messages.items():\n        aggregated[node] = sum(msgs) / len(msgs)  # 平均聚合\n    return aggregated\n\ndef update_nodes(nodes, aggregated):\n    updated = {}\n    for node, agg_msg in aggregated.items():\n        updated[node] = nodes[node] + agg_msg  # 残差连接\n    return updated\n```\n\n### PyTorch Geometric实现 🚀\n\n> 本节代码基于 PyTorch 2.1.0 和 PyTorch Geometric 2.4.0 版本\n\n使用PyTorch Geometric库的GCN实现：\n\n```python\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self, num_features, num_classes):\n        super(GCN, self).__init__()\n        self.conv1 = GCNConv(num_features, 16)\n        self.conv2 = GCNConv(16, num_classes)\n\n    def forward(self, x, edge_index):\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n        return F.log_softmax(x, dim=1)\n```\n\n### 原生PyTorch实现 🔧\n\n> 本节代码基于 PyTorch 2.1.0、NumPy 1.24.0 和 SciPy 1.11.0 版本\n\n不使用PyG，手动实现GCN~~主要是目前不太清楚主流的HGCN的实现方式捏~~：\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport scipy.sparse as sp\nimport numpy as np\n\nclass GCNLayer(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(GCNLayer, self).__init__()\n        self.W = nn.Parameter(torch.FloatTensor(in_features, out_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.W)\n\n    def forward(self, x, adj):\n        # adj: 归一化的邻接矩阵\n        support = torch.mm(x, self.W)\n        output = torch.sparse.mm(adj, support)\n        return output\n\nclass GCN(nn.Module):\n    def __init__(self, nfeat, nhid, nclass, dropout):\n        super(GCN, self).__init__()\n        self.gc1 = GCNLayer(nfeat, nhid)\n        self.gc2 = GCNLayer(nhid, nclass)\n        self.dropout = dropout\n\n    def forward(self, x, adj):\n        x = F.relu(self.gc1(x, adj))\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = self.gc2(x, adj)\n        return F.log_softmax(x, dim=1)\n\ndef normalize_adj(adj):\n    \"\"\"归一化邻接矩阵\"\"\"\n    adj = sp.coo_matrix(adj)\n    rowsum = np.array(adj.sum(1))\n    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n```\n\n---\n\n\n## 🎮 应用场景\n\n~~由于鼠鼠就是个臭写DRP的捏~~ 这里只给出GNN在DRP中的应用\n\n\n1. **药物表示**\n   - *分子图构建*：将药物SMILES字符串转换为图结构，节点表示原子（含原子类型、电荷等特征），边表示化学键（如键类型、距离）。  \n   - *GNN编码*：使用图卷积网络（GCN）、图注意力网络（GAT）或图同构网络（GIN）等层迭代聚合邻域信息，生成药物嵌入（embedding）。例如，GraTransDRP（2022）结合GAT和Transformer提升药物表征能力。\n\n2. **癌症表示**\n   - *生物网络构建*：基于基因互作（如STRING数据库的蛋白-蛋白互作）、基因共表达或通路信息构建异质图。例如，AGMI（2021）整合多组学数据和PPI网络，通过GNN学习癌症样本的联合表征。  \n   - *多组学融合*：部分模型（如TGSA）利用GNN整合基因组、转录组等数据，通过跨模态注意力机制增强特征交互。\n\n3. **异构图与联合建模**\n   - *细胞系-药物异构图*：如GraphCDR（2021）将细胞系和药物作为两类节点，通过边连接已知响应对，直接学习跨实体关系。  \n   - *知识增强*：预训练GNN于大规模生物化学属性预测（如Zhu et al., 2021），再迁移至DRP任务，提升泛化性。\n\n## 🎯 总结与展望\n\n- **动态图建模**：捕捉治疗过程中动态变化的生物网络。  \n- **三维分子图**：结合几何深度学习（如SchNet）提升立体化学感知。  \n- **基准测试**：需统一评估协议（如固定数据集和指标）以公平比较GNN与其他方法。\n\n~~之后应该会写一些具体模型的博客，有相关的会直接上链接的捏jrm~~\n\n# 📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\n<a href=\"/paper/1609.02907v4.pdf\" target=\"_blank\">📄 Thomas - SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</a>\n<a href=\"https://pytorch-geometric.readthedocs.io/\" target=\"_blank\">PyTorch Geometric 官方文档</a>\n<a href=\"https://distill.pub/2021/gnn-intro/\" target=\"_blank\">Distill: A Gentle Introduction to Graph Neural Networks</a>\n<a href=\"https://distill.pub/2021/understanding-gnns/\" target=\"_blank\">Distill: Understanding Convolutions on Graphs</a>\n<a href=\"https://www.zhihu.com/tardis/zm/art/107162772\" target=\"_blank\">知乎：图卷积网络（GCN）入门详解</a>\n<a href=\"https://github.com/tkipf/gcn\" target=\"_blank\">GCN 论文官方代码（GitHub）</a>","source":"_posts/GNN-and-GCN.md","raw":"---\ntitle: What Is GNN and GCN ?\ndate: 2025-07-10 15:36:46\ncategories:\n  - CDR\n  - model\ntags:\n  - CDR\n  - model\n  - embedding\n  - PyTorch\n  - graph theory\n  - Basic\n---\n\n# GNN 与 GCN\n\n> 图神经网络（Graph Neural Networks, GNN）和图卷积网络（Graph Convolutional Networks, GCN）是处理图数据的强大工具。本文将从理论到实践，全面介绍这两种重要的深度学习模型。\n\n本文主要介绍了*GNN和GCN的大致原理*，*GCN在PyG和PyTorch的实现* 以及它们在*DRP中的应用*\n\n<!-- more -->\n\n## 🎯 Intro\n\n在深度学习领域，处理图结构数据一直是一个具有挑战性的任务。传统的深度学习模型（如CNN、RNN）在处理欧几里得空间中的数据表现出色，但对于图这种非欧几里得结构的数据却显得力不从心。GNN和GCN的出现，为我们提供了处理图数据的有力工具。\n\n而在DRP领域，由于涉及到大量的Embedding，GCN现在几乎已经成为了必不可少的模块。\n\n但在开始各种各样的奇形怪状的GCN之前，了解GNN和GCN本身的实现仍然是非常必要的。~~于鼠鼠而言~~大致有以下理由：\n1. 部分抽象的基于GCN的模块第三方库不一定支持\n2. 由于反应表示数据的不平衡，我们可以构建的模型的层数是非常有限的（因为会过平滑）。因此对层内的改造就显得非常必要了。而这一切的前提便是理解原理捏\n\n在这里强烈建议去看一下[Distill](https://distill.pub/)的两篇有关图神经网络的博客，非常易懂。\n\n---\n\n## 📚 理论基础\n\n### 图的基本概念\n\n在开始之前，我们需要理解图的基本表示：\n- 图 $G = (V, E)$，其中 $V$ 是节点集合，$E$ 是边集合\n- 邻接矩阵 $A \\in \\mathbb{R}^{n \\times n}$\n- 度矩阵 $D = diag(d_1,...,d_n)$，其中 $d_i = \\sum_j A_{ij}$\n- 节点特征矩阵 $X \\in \\mathbb{R}^{n \\times d}$\n\n### GNN框架\n\nGNN的基本框架遵循消息传递范式（Message Passing Neural Network, MPNN），可以用以下数学公式表示：\n\n1. **消息传递阶段**（Message Passing）：\n   \n   对于节点 $v$，从其邻居节点 $u \\in \\mathcal{N}(v)$ 收集信息：\n   \n   $$m_v^{(l)} = \\sum_{u \\in \\mathcal{N}(v)} M_l(h_v^{(l-1)}, h_u^{(l-1)}, e_{uv})$$\n\n   其中：\n   - $h_v^{(l-1)}$ 是节点 $v$ 在第 $l-1$ 层的特征\n   - $e_{uv}$ 是边 $(u,v)$ 的特征\n   - $M_l$ 是可学习的消息函数\n\n2. **消息聚合阶段**（Aggregation）：\n   \n   将收集到的消息进行聚合：\n\n   $$a_v^{(l)} = AGG(\\{m_v^{(l)} | u \\in \\mathcal{N}(v)\\})$$\n\n   常见的聚合函数包括：\n   - 求和：$AGG_{sum} = \\sum_{u \\in \\mathcal{N}(v)} m_u$\n   - 平均：$AGG_{mean} = \\frac{1}{|\\mathcal{N}(v)|} \\sum_{u \\in \\mathcal{N}(v)} m_u$\n   - 最大：$AGG_{max} = max_{u \\in \\mathcal{N}(v)} m_u$\n\n3. **节点更新阶段**（Update）：\n   \n   更新节点的表示：\n\n   $$h_v^{(l)} = U_l(h_v^{(l-1)}, a_v^{(l)})$$\n\n   其中 $U_l$ 是可学习的更新函数，通常是MLP或其他神经网络。\n\n### GCN实现\n\n#### 拉普拉斯矩阵 🔍\n\n拉普拉斯矩阵是图信号处理中的核心概念，有多种形式：\n\n1. **组合拉普拉斯矩阵**：$L = D - A$\n\n2. **标准化拉普拉斯矩阵**：$L_{sym} = D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}} = I - D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$\n\n3. **随机游走拉普拉斯矩阵**：$L_{rw} = D^{-1}L = I - D^{-1}A$\n\n拉普拉斯矩阵的特性：\n- 对称性：$L = L^T$\n- 半正定性：所有特征值非负\n- 最小特征值为0，对应的特征向量是常数向量\n- 特征值的重数对应图的连通分量数\n\n#### 从传统卷积到图卷积 🔄\n\n##### 传统卷积回顾\n\n在欧几里得空间中，卷积操作定义为：\n\n$$(f * g)(p) = \\sum_{q \\in \\mathcal{N}(p)} f(q) \\cdot g(p-q)$$\n\n这里的关键特点是：\n- 平移不变性\n- 局部性\n- 参数共享\n\n##### 图上的卷积定义\n\n在图域中，我们需要重新定义这些特性：\n\n1. **空间域卷积**：\n   $$h_v = \\sum_{u \\in \\mathcal{N}(v)} W(e_{u,v})h_u$$\n   其中 $W(e_{u,v})$ 是边的权重函数\n\n2. **谱域卷积**：\n   $$g_\\theta * x = Ug_\\theta U^T x$$\n   其中 $U$ 是拉普拉斯矩阵的特征向量矩阵\n\n#### GCN的数学推导 ⚙️\n\nKipf & Welling提出的GCN模型中，单层传播规则为：\n\n$$H^{(l+1)} = \\sigma(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)})$$\n\n其中：\n- $\\tilde{A} = A + I_N$ 是添加了自环的邻接矩阵\n- $\\tilde{D}\\_{ii} = \\sum\\_{j} \\tilde{A}\\_{ij}$ 是对应的度矩阵\n- $H^{(l)}$ 是第 $l$ 层的激活值\n- $W^{(l)}$ 是可学习的权重矩阵\n- $\\sigma$ 是非线性激活函数\n\n~~一些自己的理解~~\n1. 引入$L_{sym} = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$作为聚合（AGG）部分\n   - 添加自环：$\\tilde{A} = A + I_N$\n   - 计算归一化系数：$\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$\n2. 特征变换：$H^{(l)}W^{(l)}$\n3. 邻域聚合：$\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)}$\n4. 非线性变换：$\\sigma(\\cdot)$\n\n---\n\n## 💻 实现细节\n\n基于这个理论框架的简单实现如下：\n\n```python\ndef message_passing(nodes, edges):\n    messages = {}\n    for edge in edges:\n        src, dst = edge\n        msg = compute_message(nodes[src], nodes[dst])\n        messages.setdefault(dst, []).append(msg)\n    return messages\n\ndef aggregate_messages(messages):\n    aggregated = {}\n    for node, msgs in messages.items():\n        aggregated[node] = sum(msgs) / len(msgs)  # 平均聚合\n    return aggregated\n\ndef update_nodes(nodes, aggregated):\n    updated = {}\n    for node, agg_msg in aggregated.items():\n        updated[node] = nodes[node] + agg_msg  # 残差连接\n    return updated\n```\n\n### PyTorch Geometric实现 🚀\n\n> 本节代码基于 PyTorch 2.1.0 和 PyTorch Geometric 2.4.0 版本\n\n使用PyTorch Geometric库的GCN实现：\n\n```python\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self, num_features, num_classes):\n        super(GCN, self).__init__()\n        self.conv1 = GCNConv(num_features, 16)\n        self.conv2 = GCNConv(16, num_classes)\n\n    def forward(self, x, edge_index):\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n        return F.log_softmax(x, dim=1)\n```\n\n### 原生PyTorch实现 🔧\n\n> 本节代码基于 PyTorch 2.1.0、NumPy 1.24.0 和 SciPy 1.11.0 版本\n\n不使用PyG，手动实现GCN~~主要是目前不太清楚主流的HGCN的实现方式捏~~：\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport scipy.sparse as sp\nimport numpy as np\n\nclass GCNLayer(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(GCNLayer, self).__init__()\n        self.W = nn.Parameter(torch.FloatTensor(in_features, out_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.W)\n\n    def forward(self, x, adj):\n        # adj: 归一化的邻接矩阵\n        support = torch.mm(x, self.W)\n        output = torch.sparse.mm(adj, support)\n        return output\n\nclass GCN(nn.Module):\n    def __init__(self, nfeat, nhid, nclass, dropout):\n        super(GCN, self).__init__()\n        self.gc1 = GCNLayer(nfeat, nhid)\n        self.gc2 = GCNLayer(nhid, nclass)\n        self.dropout = dropout\n\n    def forward(self, x, adj):\n        x = F.relu(self.gc1(x, adj))\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = self.gc2(x, adj)\n        return F.log_softmax(x, dim=1)\n\ndef normalize_adj(adj):\n    \"\"\"归一化邻接矩阵\"\"\"\n    adj = sp.coo_matrix(adj)\n    rowsum = np.array(adj.sum(1))\n    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n```\n\n---\n\n\n## 🎮 应用场景\n\n~~由于鼠鼠就是个臭写DRP的捏~~ 这里只给出GNN在DRP中的应用\n\n\n1. **药物表示**\n   - *分子图构建*：将药物SMILES字符串转换为图结构，节点表示原子（含原子类型、电荷等特征），边表示化学键（如键类型、距离）。  \n   - *GNN编码*：使用图卷积网络（GCN）、图注意力网络（GAT）或图同构网络（GIN）等层迭代聚合邻域信息，生成药物嵌入（embedding）。例如，GraTransDRP（2022）结合GAT和Transformer提升药物表征能力。\n\n2. **癌症表示**\n   - *生物网络构建*：基于基因互作（如STRING数据库的蛋白-蛋白互作）、基因共表达或通路信息构建异质图。例如，AGMI（2021）整合多组学数据和PPI网络，通过GNN学习癌症样本的联合表征。  \n   - *多组学融合*：部分模型（如TGSA）利用GNN整合基因组、转录组等数据，通过跨模态注意力机制增强特征交互。\n\n3. **异构图与联合建模**\n   - *细胞系-药物异构图*：如GraphCDR（2021）将细胞系和药物作为两类节点，通过边连接已知响应对，直接学习跨实体关系。  \n   - *知识增强*：预训练GNN于大规模生物化学属性预测（如Zhu et al., 2021），再迁移至DRP任务，提升泛化性。\n\n## 🎯 总结与展望\n\n- **动态图建模**：捕捉治疗过程中动态变化的生物网络。  \n- **三维分子图**：结合几何深度学习（如SchNet）提升立体化学感知。  \n- **基准测试**：需统一评估协议（如固定数据集和指标）以公平比较GNN与其他方法。\n\n~~之后应该会写一些具体模型的博客，有相关的会直接上链接的捏jrm~~\n\n# 📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\n<a href=\"/paper/1609.02907v4.pdf\" target=\"_blank\">📄 Thomas - SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</a>\n<a href=\"https://pytorch-geometric.readthedocs.io/\" target=\"_blank\">PyTorch Geometric 官方文档</a>\n<a href=\"https://distill.pub/2021/gnn-intro/\" target=\"_blank\">Distill: A Gentle Introduction to Graph Neural Networks</a>\n<a href=\"https://distill.pub/2021/understanding-gnns/\" target=\"_blank\">Distill: Understanding Convolutions on Graphs</a>\n<a href=\"https://www.zhihu.com/tardis/zm/art/107162772\" target=\"_blank\">知乎：图卷积网络（GCN）入门详解</a>\n<a href=\"https://github.com/tkipf/gcn\" target=\"_blank\">GCN 论文官方代码（GitHub）</a>","slug":"GNN-and-GCN","published":1,"updated":"2025-07-11T06:49:36.854Z","comments":1,"layout":"post","photos":[],"_id":"cmcyrm1gh0007ugwaddmmc030","content":"<h1 id=\"GNN-与-GCN\"><a href=\"#GNN-与-GCN\" class=\"headerlink\" title=\"GNN 与 GCN\"></a>GNN 与 GCN</h1><blockquote>\n<p>图神经网络（Graph Neural Networks, GNN）和图卷积网络（Graph Convolutional Networks, GCN）是处理图数据的强大工具。本文将从理论到实践，全面介绍这两种重要的深度学习模型。</p>\n</blockquote>\n<p>本文主要介绍了<em>GNN和GCN的大致原理</em>，<em>GCN在PyG和PyTorch的实现</em> 以及它们在<em>DRP中的应用</em></p>\n<span id=\"more\"></span>\n\n<h2 id=\"🎯-Intro\"><a href=\"#🎯-Intro\" class=\"headerlink\" title=\"🎯 Intro\"></a>🎯 Intro</h2><p>在深度学习领域，处理图结构数据一直是一个具有挑战性的任务。传统的深度学习模型（如CNN、RNN）在处理欧几里得空间中的数据表现出色，但对于图这种非欧几里得结构的数据却显得力不从心。GNN和GCN的出现，为我们提供了处理图数据的有力工具。</p>\n<p>而在DRP领域，由于涉及到大量的Embedding，GCN现在几乎已经成为了必不可少的模块。</p>\n<p>但在开始各种各样的奇形怪状的GCN之前，了解GNN和GCN本身的实现仍然是非常必要的。<del>于鼠鼠而言</del>大致有以下理由：</p>\n<ol>\n<li>部分抽象的基于GCN的模块第三方库不一定支持</li>\n<li>由于反应表示数据的不平衡，我们可以构建的模型的层数是非常有限的（因为会过平滑）。因此对层内的改造就显得非常必要了。而这一切的前提便是理解原理捏</li>\n</ol>\n<p>在这里强烈建议去看一下<a href=\"https://distill.pub/\">Distill</a>的两篇有关图神经网络的博客，非常易懂。</p>\n<hr>\n<h2 id=\"📚-理论基础\"><a href=\"#📚-理论基础\" class=\"headerlink\" title=\"📚 理论基础\"></a>📚 理论基础</h2><h3 id=\"图的基本概念\"><a href=\"#图的基本概念\" class=\"headerlink\" title=\"图的基本概念\"></a>图的基本概念</h3><p>在开始之前，我们需要理解图的基本表示：</p>\n<ul>\n<li>图 $G &#x3D; (V, E)$，其中 $V$ 是节点集合，$E$ 是边集合</li>\n<li>邻接矩阵 $A \\in \\mathbb{R}^{n \\times n}$</li>\n<li>度矩阵 $D &#x3D; diag(d_1,…,d_n)$，其中 $d_i &#x3D; \\sum_j A_{ij}$</li>\n<li>节点特征矩阵 $X \\in \\mathbb{R}^{n \\times d}$</li>\n</ul>\n<h3 id=\"GNN框架\"><a href=\"#GNN框架\" class=\"headerlink\" title=\"GNN框架\"></a>GNN框架</h3><p>GNN的基本框架遵循消息传递范式（Message Passing Neural Network, MPNN），可以用以下数学公式表示：</p>\n<ol>\n<li><p><strong>消息传递阶段</strong>（Message Passing）：</p>\n<p>对于节点 $v$，从其邻居节点 $u \\in \\mathcal{N}(v)$ 收集信息：</p>\n<p>$$m_v^{(l)} &#x3D; \\sum_{u \\in \\mathcal{N}(v)} M_l(h_v^{(l-1)}, h_u^{(l-1)}, e_{uv})$$</p>\n<p>其中：</p>\n<ul>\n<li>$h_v^{(l-1)}$ 是节点 $v$ 在第 $l-1$ 层的特征</li>\n<li>$e_{uv}$ 是边 $(u,v)$ 的特征</li>\n<li>$M_l$ 是可学习的消息函数</li>\n</ul>\n</li>\n<li><p><strong>消息聚合阶段</strong>（Aggregation）：</p>\n<p>将收集到的消息进行聚合：</p>\n<p>$$a_v^{(l)} &#x3D; AGG({m_v^{(l)} | u \\in \\mathcal{N}(v)})$$</p>\n<p>常见的聚合函数包括：</p>\n<ul>\n<li>求和：$AGG_{sum} &#x3D; \\sum_{u \\in \\mathcal{N}(v)} m_u$</li>\n<li>平均：$AGG_{mean} &#x3D; \\frac{1}{|\\mathcal{N}(v)|} \\sum_{u \\in \\mathcal{N}(v)} m_u$</li>\n<li>最大：$AGG_{max} &#x3D; max_{u \\in \\mathcal{N}(v)} m_u$</li>\n</ul>\n</li>\n<li><p><strong>节点更新阶段</strong>（Update）：</p>\n<p>更新节点的表示：</p>\n<p>$$h_v^{(l)} &#x3D; U_l(h_v^{(l-1)}, a_v^{(l)})$$</p>\n<p>其中 $U_l$ 是可学习的更新函数，通常是MLP或其他神经网络。</p>\n</li>\n</ol>\n<h3 id=\"GCN实现\"><a href=\"#GCN实现\" class=\"headerlink\" title=\"GCN实现\"></a>GCN实现</h3><h4 id=\"拉普拉斯矩阵-🔍\"><a href=\"#拉普拉斯矩阵-🔍\" class=\"headerlink\" title=\"拉普拉斯矩阵 🔍\"></a>拉普拉斯矩阵 🔍</h4><p>拉普拉斯矩阵是图信号处理中的核心概念，有多种形式：</p>\n<ol>\n<li><p><strong>组合拉普拉斯矩阵</strong>：$L &#x3D; D - A$</p>\n</li>\n<li><p><strong>标准化拉普拉斯矩阵</strong>：$L_{sym} &#x3D; D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}} &#x3D; I - D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$</p>\n</li>\n<li><p><strong>随机游走拉普拉斯矩阵</strong>：$L_{rw} &#x3D; D^{-1}L &#x3D; I - D^{-1}A$</p>\n</li>\n</ol>\n<p>拉普拉斯矩阵的特性：</p>\n<ul>\n<li>对称性：$L &#x3D; L^T$</li>\n<li>半正定性：所有特征值非负</li>\n<li>最小特征值为0，对应的特征向量是常数向量</li>\n<li>特征值的重数对应图的连通分量数</li>\n</ul>\n<h4 id=\"从传统卷积到图卷积-🔄\"><a href=\"#从传统卷积到图卷积-🔄\" class=\"headerlink\" title=\"从传统卷积到图卷积 🔄\"></a>从传统卷积到图卷积 🔄</h4><h5 id=\"传统卷积回顾\"><a href=\"#传统卷积回顾\" class=\"headerlink\" title=\"传统卷积回顾\"></a>传统卷积回顾</h5><p>在欧几里得空间中，卷积操作定义为：</p>\n<p>$$(f * g)(p) &#x3D; \\sum_{q \\in \\mathcal{N}(p)} f(q) \\cdot g(p-q)$$</p>\n<p>这里的关键特点是：</p>\n<ul>\n<li>平移不变性</li>\n<li>局部性</li>\n<li>参数共享</li>\n</ul>\n<h5 id=\"图上的卷积定义\"><a href=\"#图上的卷积定义\" class=\"headerlink\" title=\"图上的卷积定义\"></a>图上的卷积定义</h5><p>在图域中，我们需要重新定义这些特性：</p>\n<ol>\n<li><p><strong>空间域卷积</strong>：<br>$$h_v &#x3D; \\sum_{u \\in \\mathcal{N}(v)} W(e_{u,v})h_u$$<br>其中 $W(e_{u,v})$ 是边的权重函数</p>\n</li>\n<li><p><strong>谱域卷积</strong>：<br>$$g_\\theta * x &#x3D; Ug_\\theta U^T x$$<br>其中 $U$ 是拉普拉斯矩阵的特征向量矩阵</p>\n</li>\n</ol>\n<h4 id=\"GCN的数学推导-⚙️\"><a href=\"#GCN的数学推导-⚙️\" class=\"headerlink\" title=\"GCN的数学推导 ⚙️\"></a>GCN的数学推导 ⚙️</h4><p>Kipf &amp; Welling提出的GCN模型中，单层传播规则为：</p>\n<p>$$H^{(l+1)} &#x3D; \\sigma(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)})$$</p>\n<p>其中：</p>\n<ul>\n<li>$\\tilde{A} &#x3D; A + I_N$ 是添加了自环的邻接矩阵</li>\n<li>$\\tilde{D}_{ii} &#x3D; \\sum_{j} \\tilde{A}_{ij}$ 是对应的度矩阵</li>\n<li>$H^{(l)}$ 是第 $l$ 层的激活值</li>\n<li>$W^{(l)}$ 是可学习的权重矩阵</li>\n<li>$\\sigma$ 是非线性激活函数</li>\n</ul>\n<p><del>一些自己的理解</del></p>\n<ol>\n<li>引入$L_{sym} &#x3D; \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$作为聚合（AGG）部分<ul>\n<li>添加自环：$\\tilde{A} &#x3D; A + I_N$</li>\n<li>计算归一化系数：$\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$</li>\n</ul>\n</li>\n<li>特征变换：$H^{(l)}W^{(l)}$</li>\n<li>邻域聚合：$\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)}$</li>\n<li>非线性变换：$\\sigma(\\cdot)$</li>\n</ol>\n<hr>\n<h2 id=\"💻-实现细节\"><a href=\"#💻-实现细节\" class=\"headerlink\" title=\"💻 实现细节\"></a>💻 实现细节</h2><p>基于这个理论框架的简单实现如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">message_passing</span>(<span class=\"params\">nodes, edges</span>):</span><br><span class=\"line\">    messages = &#123;&#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> edge <span class=\"keyword\">in</span> edges:</span><br><span class=\"line\">        src, dst = edge</span><br><span class=\"line\">        msg = compute_message(nodes[src], nodes[dst])</span><br><span class=\"line\">        messages.setdefault(dst, []).append(msg)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> messages</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">aggregate_messages</span>(<span class=\"params\">messages</span>):</span><br><span class=\"line\">    aggregated = &#123;&#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> node, msgs <span class=\"keyword\">in</span> messages.items():</span><br><span class=\"line\">        aggregated[node] = <span class=\"built_in\">sum</span>(msgs) / <span class=\"built_in\">len</span>(msgs)  <span class=\"comment\"># 平均聚合</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> aggregated</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">update_nodes</span>(<span class=\"params\">nodes, aggregated</span>):</span><br><span class=\"line\">    updated = &#123;&#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> node, agg_msg <span class=\"keyword\">in</span> aggregated.items():</span><br><span class=\"line\">        updated[node] = nodes[node] + agg_msg  <span class=\"comment\"># 残差连接</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> updated</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"PyTorch-Geometric实现-🚀\"><a href=\"#PyTorch-Geometric实现-🚀\" class=\"headerlink\" title=\"PyTorch Geometric实现 🚀\"></a>PyTorch Geometric实现 🚀</h3><blockquote>\n<p>本节代码基于 PyTorch 2.1.0 和 PyTorch Geometric 2.4.0 版本</p>\n</blockquote>\n<p>使用PyTorch Geometric库的GCN实现：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch_geometric.nn <span class=\"keyword\">import</span> GCNConv</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">GCN</span>(torch.nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, num_features, num_classes</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(GCN, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.conv1 = GCNConv(num_features, <span class=\"number\">16</span>)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.conv2 = GCNConv(<span class=\"number\">16</span>, num_classes)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, edge_index</span>):</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.conv1(x, edge_index)</span><br><span class=\"line\">        x = F.relu(x)</span><br><span class=\"line\">        x = F.dropout(x, training=<span class=\"variable language_\">self</span>.training)</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.conv2(x, edge_index)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.log_softmax(x, dim=<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"原生PyTorch实现-🔧\"><a href=\"#原生PyTorch实现-🔧\" class=\"headerlink\" title=\"原生PyTorch实现 🔧\"></a>原生PyTorch实现 🔧</h3><blockquote>\n<p>本节代码基于 PyTorch 2.1.0、NumPy 1.24.0 和 SciPy 1.11.0 版本</p>\n</blockquote>\n<p>不使用PyG，手动实现GCN<del>主要是目前不太清楚主流的HGCN的实现方式捏</del>：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">import</span> scipy.sparse <span class=\"keyword\">as</span> sp</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">GCNLayer</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, in_features, out_features</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(GCNLayer, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.W = nn.Parameter(torch.FloatTensor(in_features, out_features))</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.reset_parameters()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">reset_parameters</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        nn.init.kaiming_uniform_(<span class=\"variable language_\">self</span>.W)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, adj</span>):</span><br><span class=\"line\">        <span class=\"comment\"># adj: 归一化的邻接矩阵</span></span><br><span class=\"line\">        support = torch.mm(x, <span class=\"variable language_\">self</span>.W)</span><br><span class=\"line\">        output = torch.sparse.mm(adj, support)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">GCN</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, nfeat, nhid, nclass, dropout</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(GCN, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.gc1 = GCNLayer(nfeat, nhid)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.gc2 = GCNLayer(nhid, nclass)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = dropout</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, adj</span>):</span><br><span class=\"line\">        x = F.relu(<span class=\"variable language_\">self</span>.gc1(x, adj))</span><br><span class=\"line\">        x = F.dropout(x, <span class=\"variable language_\">self</span>.dropout, training=<span class=\"variable language_\">self</span>.training)</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.gc2(x, adj)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.log_softmax(x, dim=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">normalize_adj</span>(<span class=\"params\">adj</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;归一化邻接矩阵&quot;&quot;&quot;</span></span><br><span class=\"line\">    adj = sp.coo_matrix(adj)</span><br><span class=\"line\">    rowsum = np.array(adj.<span class=\"built_in\">sum</span>(<span class=\"number\">1</span>))</span><br><span class=\"line\">    d_inv_sqrt = np.power(rowsum, -<span class=\"number\">0.5</span>).flatten()</span><br><span class=\"line\">    d_inv_sqrt[np.isinf(d_inv_sqrt)] = <span class=\"number\">0.</span></span><br><span class=\"line\">    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()</span><br></pre></td></tr></table></figure>\n\n<hr>\n<h2 id=\"🎮-应用场景\"><a href=\"#🎮-应用场景\" class=\"headerlink\" title=\"🎮 应用场景\"></a>🎮 应用场景</h2><p><del>由于鼠鼠就是个臭写DRP的捏</del> 这里只给出GNN在DRP中的应用</p>\n<ol>\n<li><p><strong>药物表示</strong></p>\n<ul>\n<li><em>分子图构建</em>：将药物SMILES字符串转换为图结构，节点表示原子（含原子类型、电荷等特征），边表示化学键（如键类型、距离）。  </li>\n<li><em>GNN编码</em>：使用图卷积网络（GCN）、图注意力网络（GAT）或图同构网络（GIN）等层迭代聚合邻域信息，生成药物嵌入（embedding）。例如，GraTransDRP（2022）结合GAT和Transformer提升药物表征能力。</li>\n</ul>\n</li>\n<li><p><strong>癌症表示</strong></p>\n<ul>\n<li><em>生物网络构建</em>：基于基因互作（如STRING数据库的蛋白-蛋白互作）、基因共表达或通路信息构建异质图。例如，AGMI（2021）整合多组学数据和PPI网络，通过GNN学习癌症样本的联合表征。  </li>\n<li><em>多组学融合</em>：部分模型（如TGSA）利用GNN整合基因组、转录组等数据，通过跨模态注意力机制增强特征交互。</li>\n</ul>\n</li>\n<li><p><strong>异构图与联合建模</strong></p>\n<ul>\n<li><em>细胞系-药物异构图</em>：如GraphCDR（2021）将细胞系和药物作为两类节点，通过边连接已知响应对，直接学习跨实体关系。  </li>\n<li><em>知识增强</em>：预训练GNN于大规模生物化学属性预测（如Zhu et al., 2021），再迁移至DRP任务，提升泛化性。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"🎯-总结与展望\"><a href=\"#🎯-总结与展望\" class=\"headerlink\" title=\"🎯 总结与展望\"></a>🎯 总结与展望</h2><ul>\n<li><strong>动态图建模</strong>：捕捉治疗过程中动态变化的生物网络。  </li>\n<li><strong>三维分子图</strong>：结合几何深度学习（如SchNet）提升立体化学感知。  </li>\n<li><strong>基准测试</strong>：需统一评估协议（如固定数据集和指标）以公平比较GNN与其他方法。</li>\n</ul>\n<p><del>之后应该会写一些具体模型的博客，有相关的会直接上链接的捏jrm</del></p>\n<h1 id=\"📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"><a href=\"#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\" class=\"headerlink\" title=\"📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1><p><a href=\"/paper/1609.02907v4.pdf\" target=\"_blank\">📄 Thomas - SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</a><br><a href=\"https://pytorch-geometric.readthedocs.io/\" target=\"_blank\">PyTorch Geometric 官方文档</a><br><a href=\"https://distill.pub/2021/gnn-intro/\" target=\"_blank\">Distill: A Gentle Introduction to Graph Neural Networks</a><br><a href=\"https://distill.pub/2021/understanding-gnns/\" target=\"_blank\">Distill: Understanding Convolutions on Graphs</a><br><a href=\"https://www.zhihu.com/tardis/zm/art/107162772\" target=\"_blank\">知乎：图卷积网络（GCN）入门详解</a><br><a href=\"https://github.com/tkipf/gcn\" target=\"_blank\">GCN 论文官方代码（GitHub）</a></p>\n","excerpt":"<h1 id=\"GNN-与-GCN\"><a href=\"#GNN-与-GCN\" class=\"headerlink\" title=\"GNN 与 GCN\"></a>GNN 与 GCN</h1><blockquote>\n<p>图神经网络（Graph Neural Networks, GNN）和图卷积网络（Graph Convolutional Networks, GCN）是处理图数据的强大工具。本文将从理论到实践，全面介绍这两种重要的深度学习模型。</p>\n</blockquote>\n<p>本文主要介绍了<em>GNN和GCN的大致原理</em>，<em>GCN在PyG和PyTorch的实现</em> 以及它们在<em>DRP中的应用</em></p>","more":"<h2 id=\"🎯-Intro\"><a href=\"#🎯-Intro\" class=\"headerlink\" title=\"🎯 Intro\"></a>🎯 Intro</h2><p>在深度学习领域，处理图结构数据一直是一个具有挑战性的任务。传统的深度学习模型（如CNN、RNN）在处理欧几里得空间中的数据表现出色，但对于图这种非欧几里得结构的数据却显得力不从心。GNN和GCN的出现，为我们提供了处理图数据的有力工具。</p>\n<p>而在DRP领域，由于涉及到大量的Embedding，GCN现在几乎已经成为了必不可少的模块。</p>\n<p>但在开始各种各样的奇形怪状的GCN之前，了解GNN和GCN本身的实现仍然是非常必要的。<del>于鼠鼠而言</del>大致有以下理由：</p>\n<ol>\n<li>部分抽象的基于GCN的模块第三方库不一定支持</li>\n<li>由于反应表示数据的不平衡，我们可以构建的模型的层数是非常有限的（因为会过平滑）。因此对层内的改造就显得非常必要了。而这一切的前提便是理解原理捏</li>\n</ol>\n<p>在这里强烈建议去看一下<a href=\"https://distill.pub/\">Distill</a>的两篇有关图神经网络的博客，非常易懂。</p>\n<hr>\n<h2 id=\"📚-理论基础\"><a href=\"#📚-理论基础\" class=\"headerlink\" title=\"📚 理论基础\"></a>📚 理论基础</h2><h3 id=\"图的基本概念\"><a href=\"#图的基本概念\" class=\"headerlink\" title=\"图的基本概念\"></a>图的基本概念</h3><p>在开始之前，我们需要理解图的基本表示：</p>\n<ul>\n<li>图 $G &#x3D; (V, E)$，其中 $V$ 是节点集合，$E$ 是边集合</li>\n<li>邻接矩阵 $A \\in \\mathbb{R}^{n \\times n}$</li>\n<li>度矩阵 $D &#x3D; diag(d_1,…,d_n)$，其中 $d_i &#x3D; \\sum_j A_{ij}$</li>\n<li>节点特征矩阵 $X \\in \\mathbb{R}^{n \\times d}$</li>\n</ul>\n<h3 id=\"GNN框架\"><a href=\"#GNN框架\" class=\"headerlink\" title=\"GNN框架\"></a>GNN框架</h3><p>GNN的基本框架遵循消息传递范式（Message Passing Neural Network, MPNN），可以用以下数学公式表示：</p>\n<ol>\n<li><p><strong>消息传递阶段</strong>（Message Passing）：</p>\n<p>对于节点 $v$，从其邻居节点 $u \\in \\mathcal{N}(v)$ 收集信息：</p>\n<p>$$m_v^{(l)} &#x3D; \\sum_{u \\in \\mathcal{N}(v)} M_l(h_v^{(l-1)}, h_u^{(l-1)}, e_{uv})$$</p>\n<p>其中：</p>\n<ul>\n<li>$h_v^{(l-1)}$ 是节点 $v$ 在第 $l-1$ 层的特征</li>\n<li>$e_{uv}$ 是边 $(u,v)$ 的特征</li>\n<li>$M_l$ 是可学习的消息函数</li>\n</ul>\n</li>\n<li><p><strong>消息聚合阶段</strong>（Aggregation）：</p>\n<p>将收集到的消息进行聚合：</p>\n<p>$$a_v^{(l)} &#x3D; AGG({m_v^{(l)} | u \\in \\mathcal{N}(v)})$$</p>\n<p>常见的聚合函数包括：</p>\n<ul>\n<li>求和：$AGG_{sum} &#x3D; \\sum_{u \\in \\mathcal{N}(v)} m_u$</li>\n<li>平均：$AGG_{mean} &#x3D; \\frac{1}{|\\mathcal{N}(v)|} \\sum_{u \\in \\mathcal{N}(v)} m_u$</li>\n<li>最大：$AGG_{max} &#x3D; max_{u \\in \\mathcal{N}(v)} m_u$</li>\n</ul>\n</li>\n<li><p><strong>节点更新阶段</strong>（Update）：</p>\n<p>更新节点的表示：</p>\n<p>$$h_v^{(l)} &#x3D; U_l(h_v^{(l-1)}, a_v^{(l)})$$</p>\n<p>其中 $U_l$ 是可学习的更新函数，通常是MLP或其他神经网络。</p>\n</li>\n</ol>\n<h3 id=\"GCN实现\"><a href=\"#GCN实现\" class=\"headerlink\" title=\"GCN实现\"></a>GCN实现</h3><h4 id=\"拉普拉斯矩阵-🔍\"><a href=\"#拉普拉斯矩阵-🔍\" class=\"headerlink\" title=\"拉普拉斯矩阵 🔍\"></a>拉普拉斯矩阵 🔍</h4><p>拉普拉斯矩阵是图信号处理中的核心概念，有多种形式：</p>\n<ol>\n<li><p><strong>组合拉普拉斯矩阵</strong>：$L &#x3D; D - A$</p>\n</li>\n<li><p><strong>标准化拉普拉斯矩阵</strong>：$L_{sym} &#x3D; D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}} &#x3D; I - D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$</p>\n</li>\n<li><p><strong>随机游走拉普拉斯矩阵</strong>：$L_{rw} &#x3D; D^{-1}L &#x3D; I - D^{-1}A$</p>\n</li>\n</ol>\n<p>拉普拉斯矩阵的特性：</p>\n<ul>\n<li>对称性：$L &#x3D; L^T$</li>\n<li>半正定性：所有特征值非负</li>\n<li>最小特征值为0，对应的特征向量是常数向量</li>\n<li>特征值的重数对应图的连通分量数</li>\n</ul>\n<h4 id=\"从传统卷积到图卷积-🔄\"><a href=\"#从传统卷积到图卷积-🔄\" class=\"headerlink\" title=\"从传统卷积到图卷积 🔄\"></a>从传统卷积到图卷积 🔄</h4><h5 id=\"传统卷积回顾\"><a href=\"#传统卷积回顾\" class=\"headerlink\" title=\"传统卷积回顾\"></a>传统卷积回顾</h5><p>在欧几里得空间中，卷积操作定义为：</p>\n<p>$$(f * g)(p) &#x3D; \\sum_{q \\in \\mathcal{N}(p)} f(q) \\cdot g(p-q)$$</p>\n<p>这里的关键特点是：</p>\n<ul>\n<li>平移不变性</li>\n<li>局部性</li>\n<li>参数共享</li>\n</ul>\n<h5 id=\"图上的卷积定义\"><a href=\"#图上的卷积定义\" class=\"headerlink\" title=\"图上的卷积定义\"></a>图上的卷积定义</h5><p>在图域中，我们需要重新定义这些特性：</p>\n<ol>\n<li><p><strong>空间域卷积</strong>：<br>$$h_v &#x3D; \\sum_{u \\in \\mathcal{N}(v)} W(e_{u,v})h_u$$<br>其中 $W(e_{u,v})$ 是边的权重函数</p>\n</li>\n<li><p><strong>谱域卷积</strong>：<br>$$g_\\theta * x &#x3D; Ug_\\theta U^T x$$<br>其中 $U$ 是拉普拉斯矩阵的特征向量矩阵</p>\n</li>\n</ol>\n<h4 id=\"GCN的数学推导-⚙️\"><a href=\"#GCN的数学推导-⚙️\" class=\"headerlink\" title=\"GCN的数学推导 ⚙️\"></a>GCN的数学推导 ⚙️</h4><p>Kipf &amp; Welling提出的GCN模型中，单层传播规则为：</p>\n<p>$$H^{(l+1)} &#x3D; \\sigma(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)})$$</p>\n<p>其中：</p>\n<ul>\n<li>$\\tilde{A} &#x3D; A + I_N$ 是添加了自环的邻接矩阵</li>\n<li>$\\tilde{D}_{ii} &#x3D; \\sum_{j} \\tilde{A}_{ij}$ 是对应的度矩阵</li>\n<li>$H^{(l)}$ 是第 $l$ 层的激活值</li>\n<li>$W^{(l)}$ 是可学习的权重矩阵</li>\n<li>$\\sigma$ 是非线性激活函数</li>\n</ul>\n<p><del>一些自己的理解</del></p>\n<ol>\n<li>引入$L_{sym} &#x3D; \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$作为聚合（AGG）部分<ul>\n<li>添加自环：$\\tilde{A} &#x3D; A + I_N$</li>\n<li>计算归一化系数：$\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$</li>\n</ul>\n</li>\n<li>特征变换：$H^{(l)}W^{(l)}$</li>\n<li>邻域聚合：$\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)}$</li>\n<li>非线性变换：$\\sigma(\\cdot)$</li>\n</ol>\n<hr>\n<h2 id=\"💻-实现细节\"><a href=\"#💻-实现细节\" class=\"headerlink\" title=\"💻 实现细节\"></a>💻 实现细节</h2><p>基于这个理论框架的简单实现如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">message_passing</span>(<span class=\"params\">nodes, edges</span>):</span><br><span class=\"line\">    messages = &#123;&#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> edge <span class=\"keyword\">in</span> edges:</span><br><span class=\"line\">        src, dst = edge</span><br><span class=\"line\">        msg = compute_message(nodes[src], nodes[dst])</span><br><span class=\"line\">        messages.setdefault(dst, []).append(msg)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> messages</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">aggregate_messages</span>(<span class=\"params\">messages</span>):</span><br><span class=\"line\">    aggregated = &#123;&#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> node, msgs <span class=\"keyword\">in</span> messages.items():</span><br><span class=\"line\">        aggregated[node] = <span class=\"built_in\">sum</span>(msgs) / <span class=\"built_in\">len</span>(msgs)  <span class=\"comment\"># 平均聚合</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> aggregated</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">update_nodes</span>(<span class=\"params\">nodes, aggregated</span>):</span><br><span class=\"line\">    updated = &#123;&#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> node, agg_msg <span class=\"keyword\">in</span> aggregated.items():</span><br><span class=\"line\">        updated[node] = nodes[node] + agg_msg  <span class=\"comment\"># 残差连接</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> updated</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"PyTorch-Geometric实现-🚀\"><a href=\"#PyTorch-Geometric实现-🚀\" class=\"headerlink\" title=\"PyTorch Geometric实现 🚀\"></a>PyTorch Geometric实现 🚀</h3><blockquote>\n<p>本节代码基于 PyTorch 2.1.0 和 PyTorch Geometric 2.4.0 版本</p>\n</blockquote>\n<p>使用PyTorch Geometric库的GCN实现：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch_geometric.nn <span class=\"keyword\">import</span> GCNConv</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">GCN</span>(torch.nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, num_features, num_classes</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(GCN, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.conv1 = GCNConv(num_features, <span class=\"number\">16</span>)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.conv2 = GCNConv(<span class=\"number\">16</span>, num_classes)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, edge_index</span>):</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.conv1(x, edge_index)</span><br><span class=\"line\">        x = F.relu(x)</span><br><span class=\"line\">        x = F.dropout(x, training=<span class=\"variable language_\">self</span>.training)</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.conv2(x, edge_index)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.log_softmax(x, dim=<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"原生PyTorch实现-🔧\"><a href=\"#原生PyTorch实现-🔧\" class=\"headerlink\" title=\"原生PyTorch实现 🔧\"></a>原生PyTorch实现 🔧</h3><blockquote>\n<p>本节代码基于 PyTorch 2.1.0、NumPy 1.24.0 和 SciPy 1.11.0 版本</p>\n</blockquote>\n<p>不使用PyG，手动实现GCN<del>主要是目前不太清楚主流的HGCN的实现方式捏</del>：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">import</span> scipy.sparse <span class=\"keyword\">as</span> sp</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">GCNLayer</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, in_features, out_features</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(GCNLayer, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.W = nn.Parameter(torch.FloatTensor(in_features, out_features))</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.reset_parameters()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">reset_parameters</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        nn.init.kaiming_uniform_(<span class=\"variable language_\">self</span>.W)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, adj</span>):</span><br><span class=\"line\">        <span class=\"comment\"># adj: 归一化的邻接矩阵</span></span><br><span class=\"line\">        support = torch.mm(x, <span class=\"variable language_\">self</span>.W)</span><br><span class=\"line\">        output = torch.sparse.mm(adj, support)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">GCN</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, nfeat, nhid, nclass, dropout</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(GCN, <span class=\"variable language_\">self</span>).__init__()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.gc1 = GCNLayer(nfeat, nhid)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.gc2 = GCNLayer(nhid, nclass)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.dropout = dropout</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, adj</span>):</span><br><span class=\"line\">        x = F.relu(<span class=\"variable language_\">self</span>.gc1(x, adj))</span><br><span class=\"line\">        x = F.dropout(x, <span class=\"variable language_\">self</span>.dropout, training=<span class=\"variable language_\">self</span>.training)</span><br><span class=\"line\">        x = <span class=\"variable language_\">self</span>.gc2(x, adj)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.log_softmax(x, dim=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">normalize_adj</span>(<span class=\"params\">adj</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;归一化邻接矩阵&quot;&quot;&quot;</span></span><br><span class=\"line\">    adj = sp.coo_matrix(adj)</span><br><span class=\"line\">    rowsum = np.array(adj.<span class=\"built_in\">sum</span>(<span class=\"number\">1</span>))</span><br><span class=\"line\">    d_inv_sqrt = np.power(rowsum, -<span class=\"number\">0.5</span>).flatten()</span><br><span class=\"line\">    d_inv_sqrt[np.isinf(d_inv_sqrt)] = <span class=\"number\">0.</span></span><br><span class=\"line\">    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()</span><br></pre></td></tr></table></figure>\n\n<hr>\n<h2 id=\"🎮-应用场景\"><a href=\"#🎮-应用场景\" class=\"headerlink\" title=\"🎮 应用场景\"></a>🎮 应用场景</h2><p><del>由于鼠鼠就是个臭写DRP的捏</del> 这里只给出GNN在DRP中的应用</p>\n<ol>\n<li><p><strong>药物表示</strong></p>\n<ul>\n<li><em>分子图构建</em>：将药物SMILES字符串转换为图结构，节点表示原子（含原子类型、电荷等特征），边表示化学键（如键类型、距离）。  </li>\n<li><em>GNN编码</em>：使用图卷积网络（GCN）、图注意力网络（GAT）或图同构网络（GIN）等层迭代聚合邻域信息，生成药物嵌入（embedding）。例如，GraTransDRP（2022）结合GAT和Transformer提升药物表征能力。</li>\n</ul>\n</li>\n<li><p><strong>癌症表示</strong></p>\n<ul>\n<li><em>生物网络构建</em>：基于基因互作（如STRING数据库的蛋白-蛋白互作）、基因共表达或通路信息构建异质图。例如，AGMI（2021）整合多组学数据和PPI网络，通过GNN学习癌症样本的联合表征。  </li>\n<li><em>多组学融合</em>：部分模型（如TGSA）利用GNN整合基因组、转录组等数据，通过跨模态注意力机制增强特征交互。</li>\n</ul>\n</li>\n<li><p><strong>异构图与联合建模</strong></p>\n<ul>\n<li><em>细胞系-药物异构图</em>：如GraphCDR（2021）将细胞系和药物作为两类节点，通过边连接已知响应对，直接学习跨实体关系。  </li>\n<li><em>知识增强</em>：预训练GNN于大规模生物化学属性预测（如Zhu et al., 2021），再迁移至DRP任务，提升泛化性。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"🎯-总结与展望\"><a href=\"#🎯-总结与展望\" class=\"headerlink\" title=\"🎯 总结与展望\"></a>🎯 总结与展望</h2><ul>\n<li><strong>动态图建模</strong>：捕捉治疗过程中动态变化的生物网络。  </li>\n<li><strong>三维分子图</strong>：结合几何深度学习（如SchNet）提升立体化学感知。  </li>\n<li><strong>基准测试</strong>：需统一评估协议（如固定数据集和指标）以公平比较GNN与其他方法。</li>\n</ul>\n<p><del>之后应该会写一些具体模型的博客，有相关的会直接上链接的捏jrm</del></p>\n<h1 id=\"📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"><a href=\"#📚-𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\" class=\"headerlink\" title=\"📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒\"></a>📚 𝒥𝑒𝒻𝑒𝓇𝑒𝓃𝒸𝑒</h1><p><a href=\"/paper/1609.02907v4.pdf\" target=\"_blank\">📄 Thomas - SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</a><br><a href=\"https://pytorch-geometric.readthedocs.io/\" target=\"_blank\">PyTorch Geometric 官方文档</a><br><a href=\"https://distill.pub/2021/gnn-intro/\" target=\"_blank\">Distill: A Gentle Introduction to Graph Neural Networks</a><br><a href=\"https://distill.pub/2021/understanding-gnns/\" target=\"_blank\">Distill: Understanding Convolutions on Graphs</a><br><a href=\"https://www.zhihu.com/tardis/zm/art/107162772\" target=\"_blank\">知乎：图卷积网络（GCN）入门详解</a><br><a href=\"https://github.com/tkipf/gcn\" target=\"_blank\">GCN 论文官方代码（GitHub）</a></p>"},{"title":"PEP 8","date":"2025-07-08T03:41:30.000Z","_content":"\n# Style Guide for Python Code\n\n> [PEP8](https://peps.python.org/pep-0008/) 是 Python 社群共通的風格指南，一開始是 Python 之父 Guido van Rossum 自己的撰碼風格，慢慢後來演變至今，目的在於幫助開發者寫出可讀性高且風格一致的程式。許多開源計畫，例如 Django 、 OpenStack 等都是以 PEP8 為基礎再加上自己的風格建議。\n\n这篇博客主要是为了在搭建自己的模型之前学习一下一些统一的规范是做的记录 ~~主要是目前读到的大多数论文的源码目命名没有规律~~ ，以加强之后搭建模型时代码的可读性\n\n另外，本博客只展示本人不太熟悉的捏\n\n<!-- more -->\n\n## 代码布局\n\n### 缩进\n\n**每个缩进级别使用 4 个空格**\n\n对于比较臭长的函数，可以使用*悬挂缩进*\n\n```Python\n# Correct:\n\n# Aligned with opening delimiter.\nfoo = long_function_name(var_one, var_two,\n                         var_three, var_four)\n\n# Add 4 spaces (an extra level of indentation) to distinguish arguments from the rest.\ndef long_function_name(\n        var_one, var_two, var_three,\n        var_four):\n    print(var_one)\n\n# Hanging indents should add a level.\nfoo = long_function_name(\n    var_one, var_two,\n    var_three, var_four)\n```\n\n```Python\n# Wrong:\n\n# Arguments on first line forbidden when not using vertical alignment.\nfoo = long_function_name(var_one, var_two,\n    var_three, var_four)\n\n# Further indentation required as indentation is not distinguishable.\ndef long_function_name(\n    var_one, var_two, var_three,\n    var_four):\n    print(var_one)\n```\n\n优先使用 _Tabs_ 进行缩进， _Tabs_ 和 _Spaces_ 不能混用\n\n### 每行最多字符数量\n\n**79** 个\n\n合理使用反斜杠\n\n```Python\nwith open('/path/to/some/file/you/want/to/read') as file_1, \\\n     open('/path/to/some/file/being/written', 'w') as file_2:\n    file_2.write(file_1.read())\n```\n\n### 二元运算符之前换行\n\n为了更好的确定该 `item` 采取的是什么运算\n\n```Python\n# Wrong:\n# operators sit far away from their operands\nincome = (gross_wages +\n          taxable_interest +\n          (dividends - qualified_dividends) -\n          ira_deduction -\n          student_loan_interest)\n```\n\n```Python\n# Correct:\n# easy to match operators with operands\nincome = (gross_wages\n          + taxable_interest\n          + (dividends - qualified_dividends)\n          - ira_deduction\n          - student_loan_interest)\n```\n\n### 如何空行（Blank Lines）\n\n_顶级函数_ 和 _类_ 之间空 **2** 行\n\n_类中的函数_ 空 **1** 行\n\n### import\n\n- 通常每一个库 **单独一行**（也有例外）\n\n```Python\nimport os\nimport sys\n\nfrom subprocess import Popen, PIPE\n```\n\n- 按以下顺序分组，每组间空行\n  1. **标准库**导入\n  2. **相关第三方库**导入\n  3. **特定的本地库**导入\n\n## 注释\n\n> Comments that contradict the code are worse than no comments.\n\n## 命名约定\n\n1. **类名** 用 **大驼峰**\n2. **函数名** 用 **小写下划线**\n3. 关于 _下划线_\n\n   - _单下划线_ 用于占位\n\n   ```Python\n   for _ in range(10):\n       print(random.randint(1, 100))\n   ```\n\n   - _单下划线_ 用于变量前表示该变量为 **弱私有** （语义上的 private），能调用但不能 import\n   - _双下划线_ 用于变量前表示该变量为 **强私有** （实际上也不能调用~~实现方式是重名名~~）\n     为了更好的说明这两点，给出以下两个测试程序\n\n   ```Python\n    \"\"\"\n    test_private_vars.py\n    This file is used to test the private variables in Python.\n    \"\"\"\n    class TestClass:\n        def __init__(self):\n            self.public_var = \"这是公有变量\"\n            self._weak_private = \"这是弱私有变量\"\n            self.__strong_private = \"这是强私有变量\"\n\n        def print_all_vars(self):\n            print(f\"从内部访问:\")\n            print(f\"公有变量: {self.public_var}\")\n            print(f\"弱私有变量: {self._weak_private}\")\n            print(f\"强私有变量: {self.__strong_private}\")\n\n    # 创建测试实例\n    test = TestClass()\n\n    # 1. 测试从类内部访问（通过方法）\n    print(\"\\n=== 测试1: 从类内部访问所有变量 ===\")\n    test.print_all_vars()\n\n    # 2. 测试从外部直接访问\n    print(\"\\n=== 测试2: 从外部访问变量 ===\")\n    print(f\"访问公有变量: {test.public_var}\")\n    print(f\"访问弱私有变量: {test._weak_private}\")  # 能访问，但IDE会警告\n    try:\n        print(f\"访问强私有变量: {test.__strong_private}\")\n    except AttributeError as e:\n        print(f\"访问强私有变量失败: {e}\")\n\n    # 3. 测试名称改写机制\n    print(\"\\n=== 测试3: 验证强私有变量的名称改写机制 ===\")\n    # 实际上Python会将__strong_private改写为_TestClass__strong_private\n    print(f\"通过改写后的名称访问强私有变量: {test._TestClass__strong_private}\")\n\n    # 4. 测试导入行为\n    if __name__ == \"__main__\":\n        print(\"\\n=== 测试4: 创建第二个文件并尝试导入 ===\")\n        print(\"请创建 test_import.py 并运行来测试导入行为\")\n   ```\n\n   ```Python\n   \"\"\"\n   test_import.py\n   This file is used to test the import of private variables in Python.\n   \"\"\"\n   from test_private_vars import TestClass\n\n   print(\"=== 测试导入后的访问行为 ===\")\n   test = TestClass()\n\n   # 测试访问公有变量\n   print(f\"访问公有变量: {test.public_var}\")\n\n   # 测试访问弱私有变量\n   try:\n       print(f\"访问弱私有变量: {test._weak_private}\")\n       print(\"注意：虽然能访问弱私有变量，但这违反了Python的约定\")\n   except AttributeError as e:\n       print(f\"访问弱私有变量失败: {e}\")\n\n   # 测试访问强私有变量\n   try:\n       print(f\"访问强私有变量: {test.__strong_private}\")\n   except AttributeError as e:\n       print(f\"访问强私有变量失败: {e}\")\n\n   # 测试通过名称改写访问强私有变量\n   try:\n       print(f\"通过改写后的名称访问强私有变量: {test._TestClass__strong_private}\")\n       print(\"注意：虽然能通过名称改写访问强私有变量，但这是一个不推荐的做法\")\n   except AttributeError as e:\n       print(f\"通过改写名称访问强私有变量失败: {e}\")\n   ```\n\n   以下是运行 `python test_private_vars.py` 的结果\n\n   ```bash\n   === 测试1: 从类内部访问所有变量 ===\n   从内部访问:\n   公有变量: 这是公有变量\n   弱私有变量: 这是弱私有变量\n   强私有变量: 这是强私有变量\n\n   === 测试2: 从外部访问变量 ===\n   访问公有变量: 这是公有变量\n   访问弱私有变量: 这是弱私有变量\n   访问强私有变量失败: 'TestClass' object has no attribute '__strong_private'\n\n   === 测试3: 验证强私有变量的名称改写机制 ===\n   通过改写后的名称访问强私有变量: 这是强私有变量\n\n   === 测试4: 创建第二个文件并尝试导入 ===\n   请创建 test_import.py 并运行来测试导入行为\n   ```\n\n   以下是运行 `python test_import.py` 的结果\n\n   ```bash\n   === 测试1: 从类内部访问所有变量 ===\n   从内部访问:\n   公有变量: 这是公有变量\n   弱私有变量: 这是弱私有变量\n   强私有变量: 这是强私有变量\n\n   === 测试2: 从外部访问变量 ===\n   访问公有变量: 这是公有变量\n   访问弱私有变量: 这是弱私有变量\n   访问强私有变量失败: 'TestClass' object has no attribute '__strong_private'\n\n   === 测试3: 验证强私有变量的名称改写机制 ===\n   通过改写后的名称访问强私有变量: 这是强私有变量\n   === 测试导入后的访问行为 ===\n   访问公有变量: 这是公有变量\n   访问弱私有变量: 这是弱私有变量\n   注意：虽然能访问弱私有变量，但这违反了Python的约定\n   访问强私有变量失败: 'TestClass' object has no attribute '__strong_private'\n   通过改写后的名称访问强私有变量: 这是强私有变量\n   注意：虽然能通过名称改写访问强私有变量，但这是一个不推荐的做法\n   ```\n","source":"_posts/PEP-8.md","raw":"---\ntitle: PEP 8\ndate: 2025-07-08 11:41:30\ntags:\n  - CDR\n  - Python\n  - PEP\n  - 闲🉐无聊\n  - 大概率没用\ncategories:\n  - CDR\n  - Python\n---\n\n# Style Guide for Python Code\n\n> [PEP8](https://peps.python.org/pep-0008/) 是 Python 社群共通的風格指南，一開始是 Python 之父 Guido van Rossum 自己的撰碼風格，慢慢後來演變至今，目的在於幫助開發者寫出可讀性高且風格一致的程式。許多開源計畫，例如 Django 、 OpenStack 等都是以 PEP8 為基礎再加上自己的風格建議。\n\n这篇博客主要是为了在搭建自己的模型之前学习一下一些统一的规范是做的记录 ~~主要是目前读到的大多数论文的源码目命名没有规律~~ ，以加强之后搭建模型时代码的可读性\n\n另外，本博客只展示本人不太熟悉的捏\n\n<!-- more -->\n\n## 代码布局\n\n### 缩进\n\n**每个缩进级别使用 4 个空格**\n\n对于比较臭长的函数，可以使用*悬挂缩进*\n\n```Python\n# Correct:\n\n# Aligned with opening delimiter.\nfoo = long_function_name(var_one, var_two,\n                         var_three, var_four)\n\n# Add 4 spaces (an extra level of indentation) to distinguish arguments from the rest.\ndef long_function_name(\n        var_one, var_two, var_three,\n        var_four):\n    print(var_one)\n\n# Hanging indents should add a level.\nfoo = long_function_name(\n    var_one, var_two,\n    var_three, var_four)\n```\n\n```Python\n# Wrong:\n\n# Arguments on first line forbidden when not using vertical alignment.\nfoo = long_function_name(var_one, var_two,\n    var_three, var_four)\n\n# Further indentation required as indentation is not distinguishable.\ndef long_function_name(\n    var_one, var_two, var_three,\n    var_four):\n    print(var_one)\n```\n\n优先使用 _Tabs_ 进行缩进， _Tabs_ 和 _Spaces_ 不能混用\n\n### 每行最多字符数量\n\n**79** 个\n\n合理使用反斜杠\n\n```Python\nwith open('/path/to/some/file/you/want/to/read') as file_1, \\\n     open('/path/to/some/file/being/written', 'w') as file_2:\n    file_2.write(file_1.read())\n```\n\n### 二元运算符之前换行\n\n为了更好的确定该 `item` 采取的是什么运算\n\n```Python\n# Wrong:\n# operators sit far away from their operands\nincome = (gross_wages +\n          taxable_interest +\n          (dividends - qualified_dividends) -\n          ira_deduction -\n          student_loan_interest)\n```\n\n```Python\n# Correct:\n# easy to match operators with operands\nincome = (gross_wages\n          + taxable_interest\n          + (dividends - qualified_dividends)\n          - ira_deduction\n          - student_loan_interest)\n```\n\n### 如何空行（Blank Lines）\n\n_顶级函数_ 和 _类_ 之间空 **2** 行\n\n_类中的函数_ 空 **1** 行\n\n### import\n\n- 通常每一个库 **单独一行**（也有例外）\n\n```Python\nimport os\nimport sys\n\nfrom subprocess import Popen, PIPE\n```\n\n- 按以下顺序分组，每组间空行\n  1. **标准库**导入\n  2. **相关第三方库**导入\n  3. **特定的本地库**导入\n\n## 注释\n\n> Comments that contradict the code are worse than no comments.\n\n## 命名约定\n\n1. **类名** 用 **大驼峰**\n2. **函数名** 用 **小写下划线**\n3. 关于 _下划线_\n\n   - _单下划线_ 用于占位\n\n   ```Python\n   for _ in range(10):\n       print(random.randint(1, 100))\n   ```\n\n   - _单下划线_ 用于变量前表示该变量为 **弱私有** （语义上的 private），能调用但不能 import\n   - _双下划线_ 用于变量前表示该变量为 **强私有** （实际上也不能调用~~实现方式是重名名~~）\n     为了更好的说明这两点，给出以下两个测试程序\n\n   ```Python\n    \"\"\"\n    test_private_vars.py\n    This file is used to test the private variables in Python.\n    \"\"\"\n    class TestClass:\n        def __init__(self):\n            self.public_var = \"这是公有变量\"\n            self._weak_private = \"这是弱私有变量\"\n            self.__strong_private = \"这是强私有变量\"\n\n        def print_all_vars(self):\n            print(f\"从内部访问:\")\n            print(f\"公有变量: {self.public_var}\")\n            print(f\"弱私有变量: {self._weak_private}\")\n            print(f\"强私有变量: {self.__strong_private}\")\n\n    # 创建测试实例\n    test = TestClass()\n\n    # 1. 测试从类内部访问（通过方法）\n    print(\"\\n=== 测试1: 从类内部访问所有变量 ===\")\n    test.print_all_vars()\n\n    # 2. 测试从外部直接访问\n    print(\"\\n=== 测试2: 从外部访问变量 ===\")\n    print(f\"访问公有变量: {test.public_var}\")\n    print(f\"访问弱私有变量: {test._weak_private}\")  # 能访问，但IDE会警告\n    try:\n        print(f\"访问强私有变量: {test.__strong_private}\")\n    except AttributeError as e:\n        print(f\"访问强私有变量失败: {e}\")\n\n    # 3. 测试名称改写机制\n    print(\"\\n=== 测试3: 验证强私有变量的名称改写机制 ===\")\n    # 实际上Python会将__strong_private改写为_TestClass__strong_private\n    print(f\"通过改写后的名称访问强私有变量: {test._TestClass__strong_private}\")\n\n    # 4. 测试导入行为\n    if __name__ == \"__main__\":\n        print(\"\\n=== 测试4: 创建第二个文件并尝试导入 ===\")\n        print(\"请创建 test_import.py 并运行来测试导入行为\")\n   ```\n\n   ```Python\n   \"\"\"\n   test_import.py\n   This file is used to test the import of private variables in Python.\n   \"\"\"\n   from test_private_vars import TestClass\n\n   print(\"=== 测试导入后的访问行为 ===\")\n   test = TestClass()\n\n   # 测试访问公有变量\n   print(f\"访问公有变量: {test.public_var}\")\n\n   # 测试访问弱私有变量\n   try:\n       print(f\"访问弱私有变量: {test._weak_private}\")\n       print(\"注意：虽然能访问弱私有变量，但这违反了Python的约定\")\n   except AttributeError as e:\n       print(f\"访问弱私有变量失败: {e}\")\n\n   # 测试访问强私有变量\n   try:\n       print(f\"访问强私有变量: {test.__strong_private}\")\n   except AttributeError as e:\n       print(f\"访问强私有变量失败: {e}\")\n\n   # 测试通过名称改写访问强私有变量\n   try:\n       print(f\"通过改写后的名称访问强私有变量: {test._TestClass__strong_private}\")\n       print(\"注意：虽然能通过名称改写访问强私有变量，但这是一个不推荐的做法\")\n   except AttributeError as e:\n       print(f\"通过改写名称访问强私有变量失败: {e}\")\n   ```\n\n   以下是运行 `python test_private_vars.py` 的结果\n\n   ```bash\n   === 测试1: 从类内部访问所有变量 ===\n   从内部访问:\n   公有变量: 这是公有变量\n   弱私有变量: 这是弱私有变量\n   强私有变量: 这是强私有变量\n\n   === 测试2: 从外部访问变量 ===\n   访问公有变量: 这是公有变量\n   访问弱私有变量: 这是弱私有变量\n   访问强私有变量失败: 'TestClass' object has no attribute '__strong_private'\n\n   === 测试3: 验证强私有变量的名称改写机制 ===\n   通过改写后的名称访问强私有变量: 这是强私有变量\n\n   === 测试4: 创建第二个文件并尝试导入 ===\n   请创建 test_import.py 并运行来测试导入行为\n   ```\n\n   以下是运行 `python test_import.py` 的结果\n\n   ```bash\n   === 测试1: 从类内部访问所有变量 ===\n   从内部访问:\n   公有变量: 这是公有变量\n   弱私有变量: 这是弱私有变量\n   强私有变量: 这是强私有变量\n\n   === 测试2: 从外部访问变量 ===\n   访问公有变量: 这是公有变量\n   访问弱私有变量: 这是弱私有变量\n   访问强私有变量失败: 'TestClass' object has no attribute '__strong_private'\n\n   === 测试3: 验证强私有变量的名称改写机制 ===\n   通过改写后的名称访问强私有变量: 这是强私有变量\n   === 测试导入后的访问行为 ===\n   访问公有变量: 这是公有变量\n   访问弱私有变量: 这是弱私有变量\n   注意：虽然能访问弱私有变量，但这违反了Python的约定\n   访问强私有变量失败: 'TestClass' object has no attribute '__strong_private'\n   通过改写后的名称访问强私有变量: 这是强私有变量\n   注意：虽然能通过名称改写访问强私有变量，但这是一个不推荐的做法\n   ```\n","slug":"PEP-8","published":1,"updated":"2025-07-10T13:36:28.887Z","comments":1,"layout":"post","photos":[],"_id":"cmcyrm1gi0008ugwa4o5ycj1a","content":"<h1 id=\"Style-Guide-for-Python-Code\"><a href=\"#Style-Guide-for-Python-Code\" class=\"headerlink\" title=\"Style Guide for Python Code\"></a>Style Guide for Python Code</h1><blockquote>\n<p><a href=\"https://peps.python.org/pep-0008/\">PEP8</a> 是 Python 社群共通的風格指南，一開始是 Python 之父 Guido van Rossum 自己的撰碼風格，慢慢後來演變至今，目的在於幫助開發者寫出可讀性高且風格一致的程式。許多開源計畫，例如 Django 、 OpenStack 等都是以 PEP8 為基礎再加上自己的風格建議。</p>\n</blockquote>\n<p>这篇博客主要是为了在搭建自己的模型之前学习一下一些统一的规范是做的记录 <del>主要是目前读到的大多数论文的源码目命名没有规律</del> ，以加强之后搭建模型时代码的可读性</p>\n<p>另外，本博客只展示本人不太熟悉的捏</p>\n<span id=\"more\"></span>\n\n<h2 id=\"代码布局\"><a href=\"#代码布局\" class=\"headerlink\" title=\"代码布局\"></a>代码布局</h2><h3 id=\"缩进\"><a href=\"#缩进\" class=\"headerlink\" title=\"缩进\"></a>缩进</h3><p><strong>每个缩进级别使用 4 个空格</strong></p>\n<p>对于比较臭长的函数，可以使用<em>悬挂缩进</em></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Correct:</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Aligned with opening delimiter.</span></span><br><span class=\"line\">foo = long_function_name(var_one, var_two,</span><br><span class=\"line\">                         var_three, var_four)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Add 4 spaces (an extra level of indentation) to distinguish arguments from the rest.</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">long_function_name</span>(<span class=\"params\"></span></span><br><span class=\"line\"><span class=\"params\">        var_one, var_two, var_three,</span></span><br><span class=\"line\"><span class=\"params\">        var_four</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(var_one)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Hanging indents should add a level.</span></span><br><span class=\"line\">foo = long_function_name(</span><br><span class=\"line\">    var_one, var_two,</span><br><span class=\"line\">    var_three, var_four)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Wrong:</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Arguments on first line forbidden when not using vertical alignment.</span></span><br><span class=\"line\">foo = long_function_name(var_one, var_two,</span><br><span class=\"line\">    var_three, var_four)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Further indentation required as indentation is not distinguishable.</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">long_function_name</span>(<span class=\"params\"></span></span><br><span class=\"line\"><span class=\"params\">    var_one, var_two, var_three,</span></span><br><span class=\"line\"><span class=\"params\">    var_four</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(var_one)</span><br></pre></td></tr></table></figure>\n\n<p>优先使用 <em>Tabs</em> 进行缩进， <em>Tabs</em> 和 <em>Spaces</em> 不能混用</p>\n<h3 id=\"每行最多字符数量\"><a href=\"#每行最多字符数量\" class=\"headerlink\" title=\"每行最多字符数量\"></a>每行最多字符数量</h3><p><strong>79</strong> 个</p>\n<p>合理使用反斜杠</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;/path/to/some/file/you/want/to/read&#x27;</span>) <span class=\"keyword\">as</span> file_1, \\</span><br><span class=\"line\">     <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;/path/to/some/file/being/written&#x27;</span>, <span class=\"string\">&#x27;w&#x27;</span>) <span class=\"keyword\">as</span> file_2:</span><br><span class=\"line\">    file_2.write(file_1.read())</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"二元运算符之前换行\"><a href=\"#二元运算符之前换行\" class=\"headerlink\" title=\"二元运算符之前换行\"></a>二元运算符之前换行</h3><p>为了更好的确定该 <code>item</code> 采取的是什么运算</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Wrong:</span></span><br><span class=\"line\"><span class=\"comment\"># operators sit far away from their operands</span></span><br><span class=\"line\">income = (gross_wages +</span><br><span class=\"line\">          taxable_interest +</span><br><span class=\"line\">          (dividends - qualified_dividends) -</span><br><span class=\"line\">          ira_deduction -</span><br><span class=\"line\">          student_loan_interest)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Correct:</span></span><br><span class=\"line\"><span class=\"comment\"># easy to match operators with operands</span></span><br><span class=\"line\">income = (gross_wages</span><br><span class=\"line\">          + taxable_interest</span><br><span class=\"line\">          + (dividends - qualified_dividends)</span><br><span class=\"line\">          - ira_deduction</span><br><span class=\"line\">          - student_loan_interest)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"如何空行（Blank-Lines）\"><a href=\"#如何空行（Blank-Lines）\" class=\"headerlink\" title=\"如何空行（Blank Lines）\"></a>如何空行（Blank Lines）</h3><p><em>顶级函数</em> 和 <em>类</em> 之间空 <strong>2</strong> 行</p>\n<p><em>类中的函数</em> 空 <strong>1</strong> 行</p>\n<h3 id=\"import\"><a href=\"#import\" class=\"headerlink\" title=\"import\"></a>import</h3><ul>\n<li>通常每一个库 <strong>单独一行</strong>（也有例外）</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> subprocess <span class=\"keyword\">import</span> Popen, PIPE</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>按以下顺序分组，每组间空行<ol>\n<li><strong>标准库</strong>导入</li>\n<li><strong>相关第三方库</strong>导入</li>\n<li><strong>特定的本地库</strong>导入</li>\n</ol>\n</li>\n</ul>\n<h2 id=\"注释\"><a href=\"#注释\" class=\"headerlink\" title=\"注释\"></a>注释</h2><blockquote>\n<p>Comments that contradict the code are worse than no comments.</p>\n</blockquote>\n<h2 id=\"命名约定\"><a href=\"#命名约定\" class=\"headerlink\" title=\"命名约定\"></a>命名约定</h2><ol>\n<li><p><strong>类名</strong> 用 <strong>大驼峰</strong></p>\n</li>\n<li><p><strong>函数名</strong> 用 <strong>小写下划线</strong></p>\n</li>\n<li><p>关于 <em>下划线</em></p>\n<ul>\n<li><em>单下划线</em> 用于占位</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(random.randint(<span class=\"number\">1</span>, <span class=\"number\">100</span>))</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><em>单下划线</em> 用于变量前表示该变量为 <strong>弱私有</strong> （语义上的 private），能调用但不能 import</li>\n<li><em>双下划线</em> 用于变量前表示该变量为 <strong>强私有</strong> （实际上也不能调用<del>实现方式是重名名</del>）<br>为了更好的说明这两点，给出以下两个测试程序</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">test_private_vars.py</span></span><br><span class=\"line\"><span class=\"string\">This file is used to test the private variables in Python.</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">TestClass</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.public_var = <span class=\"string\">&quot;这是公有变量&quot;</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>._weak_private = <span class=\"string\">&quot;这是弱私有变量&quot;</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.__strong_private = <span class=\"string\">&quot;这是强私有变量&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">print_all_vars</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;从内部访问:&quot;</span>)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;公有变量: <span class=\"subst\">&#123;self.public_var&#125;</span>&quot;</span>)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;弱私有变量: <span class=\"subst\">&#123;self._weak_private&#125;</span>&quot;</span>)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;强私有变量: <span class=\"subst\">&#123;self.__strong_private&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建测试实例</span></span><br><span class=\"line\">test = TestClass()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 1. 测试从类内部访问（通过方法）</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n=== 测试1: 从类内部访问所有变量 ===&quot;</span>)</span><br><span class=\"line\">test.print_all_vars()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. 测试从外部直接访问</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n=== 测试2: 从外部访问变量 ===&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问公有变量: <span class=\"subst\">&#123;test.public_var&#125;</span>&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问弱私有变量: <span class=\"subst\">&#123;test._weak_private&#125;</span>&quot;</span>)  <span class=\"comment\"># 能访问，但IDE会警告</span></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问强私有变量: <span class=\"subst\">&#123;test.__strong_private&#125;</span>&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span> AttributeError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问强私有变量失败: <span class=\"subst\">&#123;e&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 3. 测试名称改写机制</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n=== 测试3: 验证强私有变量的名称改写机制 ===&quot;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 实际上Python会将__strong_private改写为_TestClass__strong_private</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;通过改写后的名称访问强私有变量: <span class=\"subst\">&#123;test._TestClass__strong_private&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 4. 测试导入行为</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&quot;__main__&quot;</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n=== 测试4: 创建第二个文件并尝试导入 ===&quot;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;请创建 test_import.py 并运行来测试导入行为&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">test_import.py</span></span><br><span class=\"line\"><span class=\"string\">This file is used to test the import of private variables in Python.</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> test_private_vars <span class=\"keyword\">import</span> TestClass</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;=== 测试导入后的访问行为 ===&quot;</span>)</span><br><span class=\"line\">test = TestClass()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 测试访问公有变量</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问公有变量: <span class=\"subst\">&#123;test.public_var&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 测试访问弱私有变量</span></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问弱私有变量: <span class=\"subst\">&#123;test._weak_private&#125;</span>&quot;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;注意：虽然能访问弱私有变量，但这违反了Python的约定&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span> AttributeError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问弱私有变量失败: <span class=\"subst\">&#123;e&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 测试访问强私有变量</span></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问强私有变量: <span class=\"subst\">&#123;test.__strong_private&#125;</span>&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span> AttributeError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问强私有变量失败: <span class=\"subst\">&#123;e&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 测试通过名称改写访问强私有变量</span></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;通过改写后的名称访问强私有变量: <span class=\"subst\">&#123;test._TestClass__strong_private&#125;</span>&quot;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;注意：虽然能通过名称改写访问强私有变量，但这是一个不推荐的做法&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span> AttributeError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;通过改写名称访问强私有变量失败: <span class=\"subst\">&#123;e&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<p>以下是运行 <code>python test_private_vars.py</code> 的结果</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">=== 测试1: 从类内部访问所有变量 ===</span><br><span class=\"line\">从内部访问:</span><br><span class=\"line\">公有变量: 这是公有变量</span><br><span class=\"line\">弱私有变量: 这是弱私有变量</span><br><span class=\"line\">强私有变量: 这是强私有变量</span><br><span class=\"line\"></span><br><span class=\"line\">=== 测试2: 从外部访问变量 ===</span><br><span class=\"line\">访问公有变量: 这是公有变量</span><br><span class=\"line\">访问弱私有变量: 这是弱私有变量</span><br><span class=\"line\">访问强私有变量失败: <span class=\"string\">&#x27;TestClass&#x27;</span> object has no attribute <span class=\"string\">&#x27;__strong_private&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">=== 测试3: 验证强私有变量的名称改写机制 ===</span><br><span class=\"line\">通过改写后的名称访问强私有变量: 这是强私有变量</span><br><span class=\"line\"></span><br><span class=\"line\">=== 测试4: 创建第二个文件并尝试导入 ===</span><br><span class=\"line\">请创建 test_import.py 并运行来测试导入行为</span><br></pre></td></tr></table></figure>\n\n<p>以下是运行 <code>python test_import.py</code> 的结果</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">=== 测试1: 从类内部访问所有变量 ===</span><br><span class=\"line\">从内部访问:</span><br><span class=\"line\">公有变量: 这是公有变量</span><br><span class=\"line\">弱私有变量: 这是弱私有变量</span><br><span class=\"line\">强私有变量: 这是强私有变量</span><br><span class=\"line\"></span><br><span class=\"line\">=== 测试2: 从外部访问变量 ===</span><br><span class=\"line\">访问公有变量: 这是公有变量</span><br><span class=\"line\">访问弱私有变量: 这是弱私有变量</span><br><span class=\"line\">访问强私有变量失败: <span class=\"string\">&#x27;TestClass&#x27;</span> object has no attribute <span class=\"string\">&#x27;__strong_private&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">=== 测试3: 验证强私有变量的名称改写机制 ===</span><br><span class=\"line\">通过改写后的名称访问强私有变量: 这是强私有变量</span><br><span class=\"line\">=== 测试导入后的访问行为 ===</span><br><span class=\"line\">访问公有变量: 这是公有变量</span><br><span class=\"line\">访问弱私有变量: 这是弱私有变量</span><br><span class=\"line\">注意：虽然能访问弱私有变量，但这违反了Python的约定</span><br><span class=\"line\">访问强私有变量失败: <span class=\"string\">&#x27;TestClass&#x27;</span> object has no attribute <span class=\"string\">&#x27;__strong_private&#x27;</span></span><br><span class=\"line\">通过改写后的名称访问强私有变量: 这是强私有变量</span><br><span class=\"line\">注意：虽然能通过名称改写访问强私有变量，但这是一个不推荐的做法</span><br></pre></td></tr></table></figure></li>\n</ol>\n","excerpt":"<h1 id=\"Style-Guide-for-Python-Code\"><a href=\"#Style-Guide-for-Python-Code\" class=\"headerlink\" title=\"Style Guide for Python Code\"></a>Style Guide for Python Code</h1><blockquote>\n<p><a href=\"https://peps.python.org/pep-0008/\">PEP8</a> 是 Python 社群共通的風格指南，一開始是 Python 之父 Guido van Rossum 自己的撰碼風格，慢慢後來演變至今，目的在於幫助開發者寫出可讀性高且風格一致的程式。許多開源計畫，例如 Django 、 OpenStack 等都是以 PEP8 為基礎再加上自己的風格建議。</p>\n</blockquote>\n<p>这篇博客主要是为了在搭建自己的模型之前学习一下一些统一的规范是做的记录 <del>主要是目前读到的大多数论文的源码目命名没有规律</del> ，以加强之后搭建模型时代码的可读性</p>\n<p>另外，本博客只展示本人不太熟悉的捏</p>","more":"<h2 id=\"代码布局\"><a href=\"#代码布局\" class=\"headerlink\" title=\"代码布局\"></a>代码布局</h2><h3 id=\"缩进\"><a href=\"#缩进\" class=\"headerlink\" title=\"缩进\"></a>缩进</h3><p><strong>每个缩进级别使用 4 个空格</strong></p>\n<p>对于比较臭长的函数，可以使用<em>悬挂缩进</em></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Correct:</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Aligned with opening delimiter.</span></span><br><span class=\"line\">foo = long_function_name(var_one, var_two,</span><br><span class=\"line\">                         var_three, var_four)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Add 4 spaces (an extra level of indentation) to distinguish arguments from the rest.</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">long_function_name</span>(<span class=\"params\"></span></span><br><span class=\"line\"><span class=\"params\">        var_one, var_two, var_three,</span></span><br><span class=\"line\"><span class=\"params\">        var_four</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(var_one)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Hanging indents should add a level.</span></span><br><span class=\"line\">foo = long_function_name(</span><br><span class=\"line\">    var_one, var_two,</span><br><span class=\"line\">    var_three, var_four)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Wrong:</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Arguments on first line forbidden when not using vertical alignment.</span></span><br><span class=\"line\">foo = long_function_name(var_one, var_two,</span><br><span class=\"line\">    var_three, var_four)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Further indentation required as indentation is not distinguishable.</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">long_function_name</span>(<span class=\"params\"></span></span><br><span class=\"line\"><span class=\"params\">    var_one, var_two, var_three,</span></span><br><span class=\"line\"><span class=\"params\">    var_four</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(var_one)</span><br></pre></td></tr></table></figure>\n\n<p>优先使用 <em>Tabs</em> 进行缩进， <em>Tabs</em> 和 <em>Spaces</em> 不能混用</p>\n<h3 id=\"每行最多字符数量\"><a href=\"#每行最多字符数量\" class=\"headerlink\" title=\"每行最多字符数量\"></a>每行最多字符数量</h3><p><strong>79</strong> 个</p>\n<p>合理使用反斜杠</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;/path/to/some/file/you/want/to/read&#x27;</span>) <span class=\"keyword\">as</span> file_1, \\</span><br><span class=\"line\">     <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;/path/to/some/file/being/written&#x27;</span>, <span class=\"string\">&#x27;w&#x27;</span>) <span class=\"keyword\">as</span> file_2:</span><br><span class=\"line\">    file_2.write(file_1.read())</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"二元运算符之前换行\"><a href=\"#二元运算符之前换行\" class=\"headerlink\" title=\"二元运算符之前换行\"></a>二元运算符之前换行</h3><p>为了更好的确定该 <code>item</code> 采取的是什么运算</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Wrong:</span></span><br><span class=\"line\"><span class=\"comment\"># operators sit far away from their operands</span></span><br><span class=\"line\">income = (gross_wages +</span><br><span class=\"line\">          taxable_interest +</span><br><span class=\"line\">          (dividends - qualified_dividends) -</span><br><span class=\"line\">          ira_deduction -</span><br><span class=\"line\">          student_loan_interest)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Correct:</span></span><br><span class=\"line\"><span class=\"comment\"># easy to match operators with operands</span></span><br><span class=\"line\">income = (gross_wages</span><br><span class=\"line\">          + taxable_interest</span><br><span class=\"line\">          + (dividends - qualified_dividends)</span><br><span class=\"line\">          - ira_deduction</span><br><span class=\"line\">          - student_loan_interest)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"如何空行（Blank-Lines）\"><a href=\"#如何空行（Blank-Lines）\" class=\"headerlink\" title=\"如何空行（Blank Lines）\"></a>如何空行（Blank Lines）</h3><p><em>顶级函数</em> 和 <em>类</em> 之间空 <strong>2</strong> 行</p>\n<p><em>类中的函数</em> 空 <strong>1</strong> 行</p>\n<h3 id=\"import\"><a href=\"#import\" class=\"headerlink\" title=\"import\"></a>import</h3><ul>\n<li>通常每一个库 <strong>单独一行</strong>（也有例外）</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> subprocess <span class=\"keyword\">import</span> Popen, PIPE</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>按以下顺序分组，每组间空行<ol>\n<li><strong>标准库</strong>导入</li>\n<li><strong>相关第三方库</strong>导入</li>\n<li><strong>特定的本地库</strong>导入</li>\n</ol>\n</li>\n</ul>\n<h2 id=\"注释\"><a href=\"#注释\" class=\"headerlink\" title=\"注释\"></a>注释</h2><blockquote>\n<p>Comments that contradict the code are worse than no comments.</p>\n</blockquote>\n<h2 id=\"命名约定\"><a href=\"#命名约定\" class=\"headerlink\" title=\"命名约定\"></a>命名约定</h2><ol>\n<li><p><strong>类名</strong> 用 <strong>大驼峰</strong></p>\n</li>\n<li><p><strong>函数名</strong> 用 <strong>小写下划线</strong></p>\n</li>\n<li><p>关于 <em>下划线</em></p>\n<ul>\n<li><em>单下划线</em> 用于占位</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(random.randint(<span class=\"number\">1</span>, <span class=\"number\">100</span>))</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><em>单下划线</em> 用于变量前表示该变量为 <strong>弱私有</strong> （语义上的 private），能调用但不能 import</li>\n<li><em>双下划线</em> 用于变量前表示该变量为 <strong>强私有</strong> （实际上也不能调用<del>实现方式是重名名</del>）<br>为了更好的说明这两点，给出以下两个测试程序</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">test_private_vars.py</span></span><br><span class=\"line\"><span class=\"string\">This file is used to test the private variables in Python.</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">TestClass</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.public_var = <span class=\"string\">&quot;这是公有变量&quot;</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>._weak_private = <span class=\"string\">&quot;这是弱私有变量&quot;</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.__strong_private = <span class=\"string\">&quot;这是强私有变量&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">print_all_vars</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;从内部访问:&quot;</span>)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;公有变量: <span class=\"subst\">&#123;self.public_var&#125;</span>&quot;</span>)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;弱私有变量: <span class=\"subst\">&#123;self._weak_private&#125;</span>&quot;</span>)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;强私有变量: <span class=\"subst\">&#123;self.__strong_private&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建测试实例</span></span><br><span class=\"line\">test = TestClass()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 1. 测试从类内部访问（通过方法）</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n=== 测试1: 从类内部访问所有变量 ===&quot;</span>)</span><br><span class=\"line\">test.print_all_vars()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. 测试从外部直接访问</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n=== 测试2: 从外部访问变量 ===&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问公有变量: <span class=\"subst\">&#123;test.public_var&#125;</span>&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问弱私有变量: <span class=\"subst\">&#123;test._weak_private&#125;</span>&quot;</span>)  <span class=\"comment\"># 能访问，但IDE会警告</span></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问强私有变量: <span class=\"subst\">&#123;test.__strong_private&#125;</span>&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span> AttributeError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问强私有变量失败: <span class=\"subst\">&#123;e&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 3. 测试名称改写机制</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n=== 测试3: 验证强私有变量的名称改写机制 ===&quot;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 实际上Python会将__strong_private改写为_TestClass__strong_private</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;通过改写后的名称访问强私有变量: <span class=\"subst\">&#123;test._TestClass__strong_private&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 4. 测试导入行为</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&quot;__main__&quot;</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n=== 测试4: 创建第二个文件并尝试导入 ===&quot;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;请创建 test_import.py 并运行来测试导入行为&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">test_import.py</span></span><br><span class=\"line\"><span class=\"string\">This file is used to test the import of private variables in Python.</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> test_private_vars <span class=\"keyword\">import</span> TestClass</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;=== 测试导入后的访问行为 ===&quot;</span>)</span><br><span class=\"line\">test = TestClass()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 测试访问公有变量</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问公有变量: <span class=\"subst\">&#123;test.public_var&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 测试访问弱私有变量</span></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问弱私有变量: <span class=\"subst\">&#123;test._weak_private&#125;</span>&quot;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;注意：虽然能访问弱私有变量，但这违反了Python的约定&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span> AttributeError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问弱私有变量失败: <span class=\"subst\">&#123;e&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 测试访问强私有变量</span></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问强私有变量: <span class=\"subst\">&#123;test.__strong_private&#125;</span>&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span> AttributeError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;访问强私有变量失败: <span class=\"subst\">&#123;e&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 测试通过名称改写访问强私有变量</span></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;通过改写后的名称访问强私有变量: <span class=\"subst\">&#123;test._TestClass__strong_private&#125;</span>&quot;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;注意：虽然能通过名称改写访问强私有变量，但这是一个不推荐的做法&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">except</span> AttributeError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;通过改写名称访问强私有变量失败: <span class=\"subst\">&#123;e&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<p>以下是运行 <code>python test_private_vars.py</code> 的结果</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">=== 测试1: 从类内部访问所有变量 ===</span><br><span class=\"line\">从内部访问:</span><br><span class=\"line\">公有变量: 这是公有变量</span><br><span class=\"line\">弱私有变量: 这是弱私有变量</span><br><span class=\"line\">强私有变量: 这是强私有变量</span><br><span class=\"line\"></span><br><span class=\"line\">=== 测试2: 从外部访问变量 ===</span><br><span class=\"line\">访问公有变量: 这是公有变量</span><br><span class=\"line\">访问弱私有变量: 这是弱私有变量</span><br><span class=\"line\">访问强私有变量失败: <span class=\"string\">&#x27;TestClass&#x27;</span> object has no attribute <span class=\"string\">&#x27;__strong_private&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">=== 测试3: 验证强私有变量的名称改写机制 ===</span><br><span class=\"line\">通过改写后的名称访问强私有变量: 这是强私有变量</span><br><span class=\"line\"></span><br><span class=\"line\">=== 测试4: 创建第二个文件并尝试导入 ===</span><br><span class=\"line\">请创建 test_import.py 并运行来测试导入行为</span><br></pre></td></tr></table></figure>\n\n<p>以下是运行 <code>python test_import.py</code> 的结果</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">=== 测试1: 从类内部访问所有变量 ===</span><br><span class=\"line\">从内部访问:</span><br><span class=\"line\">公有变量: 这是公有变量</span><br><span class=\"line\">弱私有变量: 这是弱私有变量</span><br><span class=\"line\">强私有变量: 这是强私有变量</span><br><span class=\"line\"></span><br><span class=\"line\">=== 测试2: 从外部访问变量 ===</span><br><span class=\"line\">访问公有变量: 这是公有变量</span><br><span class=\"line\">访问弱私有变量: 这是弱私有变量</span><br><span class=\"line\">访问强私有变量失败: <span class=\"string\">&#x27;TestClass&#x27;</span> object has no attribute <span class=\"string\">&#x27;__strong_private&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">=== 测试3: 验证强私有变量的名称改写机制 ===</span><br><span class=\"line\">通过改写后的名称访问强私有变量: 这是强私有变量</span><br><span class=\"line\">=== 测试导入后的访问行为 ===</span><br><span class=\"line\">访问公有变量: 这是公有变量</span><br><span class=\"line\">访问弱私有变量: 这是弱私有变量</span><br><span class=\"line\">注意：虽然能访问弱私有变量，但这违反了Python的约定</span><br><span class=\"line\">访问强私有变量失败: <span class=\"string\">&#x27;TestClass&#x27;</span> object has no attribute <span class=\"string\">&#x27;__strong_private&#x27;</span></span><br><span class=\"line\">通过改写后的名称访问强私有变量: 这是强私有变量</span><br><span class=\"line\">注意：虽然能通过名称改写访问强私有变量，但这是一个不推荐的做法</span><br></pre></td></tr></table></figure></li>\n</ol>"}],"PostAsset":[],"PostCategory":[{"post_id":"cmcyrm1gi0008ugwa4o5ycj1a","category_id":"cmcyrm1gg0005ugwa6mp6chkd","_id":"cmcyrm1go000hugwa5maqdq0l"},{"post_id":"cmcyrm1gi0008ugwa4o5ycj1a","category_id":"cmcyrm1gn000dugwa6ewa04ac","_id":"cmcyrm1gp000kugwa4gg41gjj"},{"post_id":"cmcyrm1gc0003ugwa6kff203e","category_id":"cmcyrm1gg0005ugwa6mp6chkd","_id":"cmcyrm1gp000lugwa1jvqh9ni"},{"post_id":"cmcyrm1gc0003ugwa6kff203e","category_id":"cmcyrm1go000fugwac72jf1hw","_id":"cmcyrm1gp000ougwa68znd55w"},{"post_id":"cmcyrm1ge0004ugwa8yo4g6py","category_id":"cmcyrm1gg0005ugwa6mp6chkd","_id":"cmcyrm1gq000qugwaeqxvbz6a"},{"post_id":"cmcyrm1ge0004ugwa8yo4g6py","category_id":"cmcyrm1go000iugwa2i7agury","_id":"cmcyrm1gq000tugwa2z4ihjzt"},{"post_id":"cmcyrm1gh0007ugwaddmmc030","category_id":"cmcyrm1gg0005ugwa6mp6chkd","_id":"cmcyrm1gq000vugwa8lfkhh2i"},{"post_id":"cmcyrm1gh0007ugwaddmmc030","category_id":"cmcyrm1go000fugwac72jf1hw","_id":"cmcyrm1gr000yugwafkl4gfvj"}],"PostTag":[{"post_id":"cmcyrm1gc0003ugwa6kff203e","tag_id":"cmcyrm1gh0006ugwa9e436exx","_id":"cmcyrm1gq000pugwa0eik5gu7"},{"post_id":"cmcyrm1gc0003ugwa6kff203e","tag_id":"cmcyrm1gk000augwa952017jb","_id":"cmcyrm1gq000rugwa7bcz03d2"},{"post_id":"cmcyrm1gc0003ugwa6kff203e","tag_id":"cmcyrm1gm000cugwaajwf6eyr","_id":"cmcyrm1gq000uugwahaqr899h"},{"post_id":"cmcyrm1gc0003ugwa6kff203e","tag_id":"cmcyrm1gn000eugwa95n9foo6","_id":"cmcyrm1gq000wugwa3c08752i"},{"post_id":"cmcyrm1gc0003ugwa6kff203e","tag_id":"cmcyrm1go000gugwaa2nk4g6m","_id":"cmcyrm1gr000zugwa3ikhgukj"},{"post_id":"cmcyrm1gc0003ugwa6kff203e","tag_id":"cmcyrm1gp000jugwaax7i92q5","_id":"cmcyrm1gr0010ugwafcoy0qks"},{"post_id":"cmcyrm1ge0004ugwa8yo4g6py","tag_id":"cmcyrm1gh0006ugwa9e436exx","_id":"cmcyrm1gr0013ugwahnoq6plz"},{"post_id":"cmcyrm1ge0004ugwa8yo4g6py","tag_id":"cmcyrm1gq000sugwacclcdyuy","_id":"cmcyrm1gs0014ugwacp2sgj3u"},{"post_id":"cmcyrm1ge0004ugwa8yo4g6py","tag_id":"cmcyrm1gq000xugwaf9apb9yr","_id":"cmcyrm1gs0016ugwa5o4wbm7j"},{"post_id":"cmcyrm1ge0004ugwa8yo4g6py","tag_id":"cmcyrm1gp000jugwaax7i92q5","_id":"cmcyrm1gs0017ugwab2sg2mdw"},{"post_id":"cmcyrm1gh0007ugwaddmmc030","tag_id":"cmcyrm1gh0006ugwa9e436exx","_id":"cmcyrm1gt001dugwa4jemejir"},{"post_id":"cmcyrm1gh0007ugwaddmmc030","tag_id":"cmcyrm1gk000augwa952017jb","_id":"cmcyrm1gt001eugwacewd4u92"},{"post_id":"cmcyrm1gh0007ugwaddmmc030","tag_id":"cmcyrm1gs0018ugwa5mdpdbj4","_id":"cmcyrm1gt001gugwa63q79lhs"},{"post_id":"cmcyrm1gh0007ugwaddmmc030","tag_id":"cmcyrm1go000gugwaa2nk4g6m","_id":"cmcyrm1gt001hugwa70ju8zq7"},{"post_id":"cmcyrm1gh0007ugwaddmmc030","tag_id":"cmcyrm1gp000jugwaax7i92q5","_id":"cmcyrm1gu001jugwaf1oc02zi"},{"post_id":"cmcyrm1gh0007ugwaddmmc030","tag_id":"cmcyrm1gm000cugwaajwf6eyr","_id":"cmcyrm1gu001kugwa650daeso"},{"post_id":"cmcyrm1gi0008ugwa4o5ycj1a","tag_id":"cmcyrm1gh0006ugwa9e436exx","_id":"cmcyrm1gu001mugwa4129exjb"},{"post_id":"cmcyrm1gi0008ugwa4o5ycj1a","tag_id":"cmcyrm1gt001cugwahd7ncdy7","_id":"cmcyrm1gu001nugwa8r3l744c"},{"post_id":"cmcyrm1gi0008ugwa4o5ycj1a","tag_id":"cmcyrm1gt001fugwa5qgm03bo","_id":"cmcyrm1gu001ougwaeyb330qq"},{"post_id":"cmcyrm1gi0008ugwa4o5ycj1a","tag_id":"cmcyrm1gt001iugwa6fq45afr","_id":"cmcyrm1gu001pugwae68kfs7l"},{"post_id":"cmcyrm1gi0008ugwa4o5ycj1a","tag_id":"cmcyrm1gu001lugwad4r33c4t","_id":"cmcyrm1gu001qugwa7gaj6far"}],"Tag":[{"name":"CDR","_id":"cmcyrm1gh0006ugwa9e436exx"},{"name":"model","_id":"cmcyrm1gk000augwa952017jb"},{"name":"Basic","_id":"cmcyrm1gm000cugwaajwf6eyr"},{"name":"还没写完捏","_id":"cmcyrm1gn000eugwa95n9foo6"},{"name":"PyTorch","_id":"cmcyrm1go000gugwaa2nk4g6m"},{"name":"graph theory","_id":"cmcyrm1gp000jugwaax7i92q5"},{"name":"Data Analysis","_id":"cmcyrm1gq000sugwacclcdyuy"},{"name":"可能有点用","_id":"cmcyrm1gq000xugwaf9apb9yr"},{"name":"embedding","_id":"cmcyrm1gs0018ugwa5mdpdbj4"},{"name":"Python","_id":"cmcyrm1gt001cugwahd7ncdy7"},{"name":"PEP","_id":"cmcyrm1gt001fugwa5qgm03bo"},{"name":"闲🉐无聊","_id":"cmcyrm1gt001iugwa6fq45afr"},{"name":"大概率没用","_id":"cmcyrm1gu001lugwad4r33c4t"}]}}