---
title: GAT
date: 2025-07-14 22:26:58
categories:
tags:
---

# 

<!-- more -->

### å›¾æ³¨æ„åŠ›æœºåˆ¶

#### GATï¼ˆGraph Attention Networksï¼‰

GATé€šè¿‡å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶æ¥åŠ æƒé‚»å±…èŠ‚ç‚¹çš„ç‰¹å¾ã€‚å¯¹äºèŠ‚ç‚¹iï¼Œå…¶æ›´æ–°å…¬å¼ä¸ºï¼š

$$h\_i^{(l+1)} = \sigma(\sum\_{j \in \mathcal{N}\_i} \alpha\_{ij}W^{(l)}h\_j^{(l)})$$

å…¶ä¸­æ³¨æ„åŠ›ç³»æ•°$\alpha_{ij}$çš„è®¡ç®—ï¼š

$$\alpha_{ij} = \frac{exp(LeakyReLU(a^T[Wh_i || Wh_j]))}{\sum_{k \in \mathcal{N}_i} exp(LeakyReLU(a^T[Wh_i || Wh_k]))}$$

#### å¤šå¤´æ³¨æ„åŠ›

ä¸ºäº†æé«˜æ¨¡å‹çš„ç¨³å®šæ€§å’Œè¡¨è¾¾èƒ½åŠ›ï¼ŒGATä½¿ç”¨äº†å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼š

$$h\_i^{(l+1)} = \sigma(\frac{1}{K} \sum\_{k=1}^K \sum\_{j \in \mathcal{N}\_i} \alpha\_{ij}^k W^k h\_j^{(l)})$$

### å˜ä½“ä¸æ‰©å±•

#### è¾¹æ³¨æ„åŠ›

é™¤äº†èŠ‚ç‚¹ä¹‹é—´çš„æ³¨æ„åŠ›ï¼Œä¸€äº›æ¨¡å‹è¿˜å¼•å…¥äº†è¾¹æ³¨æ„åŠ›æœºåˆ¶ï¼š

$$e_{ij} = a^T[Wh_i || Wh_j || We_{ij}]$$

å…¶ä¸­$e_{ij}$æ˜¯è¾¹çš„ç‰¹å¾ã€‚

#### å…¨å±€æ³¨æ„åŠ›

é€šè¿‡å¼•å…¥å…¨å±€èŠ‚ç‚¹æˆ–æ± åŒ–æ“ä½œï¼Œå¯ä»¥å®ç°å…¨å±€æ³¨æ„åŠ›ï¼š

$$g = \sum_{i \in V} \beta_i h_i$$

å…¶ä¸­$\beta_i$æ˜¯å…¨å±€æ³¨æ„åŠ›æƒé‡ã€‚

## ğŸ’» å®ç°ç»†èŠ‚

### PyTorchå®ç°çš„GATå±‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class GATLayer(nn.Module):
    def __init__(self, in_features, out_features, dropout, alpha, concat=True):
        super(GATLayer, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.dropout = dropout
        self.alpha = alpha
        self.concat = concat

        # å˜æ¢çŸ©é˜µ
        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))
        nn.init.xavier_uniform_(self.W.data, gain=1.414)
        
        # æ³¨æ„åŠ›å‘é‡
        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))
        nn.init.xavier_uniform_(self.a.data, gain=1.414)

        self.leakyrelu = nn.LeakyReLU(self.alpha)

    def forward(self, x, adj):
        # x: èŠ‚ç‚¹ç‰¹å¾çŸ©é˜µ [N, in_features]
        # adj: é‚»æ¥çŸ©é˜µ [N, N]
        
        # çº¿æ€§å˜æ¢
        h = torch.mm(x, self.W)  # [N, out_features]
        N = h.size()[0]

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1)
        a_input = a_input.view(N, N, 2 * self.out_features)
        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))

        # æ©ç æœºåˆ¶
        zero_vec = -9e15 * torch.ones_like(e)
        attention = torch.where(adj > 0, e, zero_vec)
        attention = F.softmax(attention, dim=1)
        attention = F.dropout(attention, self.dropout, training=self.training)

        # èšåˆç‰¹å¾
        h_prime = torch.matmul(attention, h)

        if self.concat:
            return F.elu(h_prime)
        else:
            return h_prime

class GAT(nn.Module):
    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):
        super(GAT, self).__init__()
        self.dropout = dropout
        
        # å¤šå¤´æ³¨æ„åŠ›å±‚
        self.attentions = nn.ModuleList([
            GATLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) 
            for _ in range(nheads)
        ])
        
        # è¾“å‡ºå±‚
        self.out_att = GATLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)

    def forward(self, x, adj):
        x = F.dropout(x, self.dropout, training=self.training)
        # å¤šå¤´æ³¨æ„åŠ›
        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)
        x = F.dropout(x, self.dropout, training=self.training)
        x = self.out_att(x, adj)
        return F.log_softmax(x, dim=1)
```

### å®é™…åº”ç”¨ç¤ºä¾‹

```python
# æ¨¡å‹åˆå§‹åŒ–
model = GAT(nfeat=input_dim,
           nhid=8,
           nclass=num_classes,
           dropout=0.6,
           alpha=0.2,
           nheads=8)

# ä¼˜åŒ–å™¨
optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)

# è®­ç»ƒå¾ªç¯
def train():
    model.train()
    optimizer.zero_grad()
    output = model(features, adj)
    loss = F.nll_loss(output[idx_train], labels[idx_train])
    loss.backward()
    optimizer.step()
    return loss.item()
```

## ğŸ” æ³¨æ„äº‹é¡¹ä¸æœ€ä½³å®è·µ


## ğŸ“ˆ æœªæ¥å±•æœ›

# ğŸ“š ğ’¥ğ‘’ğ’»ğ‘’ğ“‡ğ‘’ğ“ƒğ’¸ğ‘’

<a href="/paper/Lee ç­‰ - 2018 - Attention Models in Graphs A Survey.pdf" target="_blank">ğŸ“„ Lee ç­‰ - 2018 - Attention Models in Graphs A Survey</a>
<a href="https://github.com/xmu-xiaoma666/External-Attention-pytorch" target="_blank">github: External-Attention-pytorch</a> 