---
title: Feature-Related Attention
date: 2025-07-14 22:30:57
categories:
  - CDR
  - model
  - attention
  - feature related
tags:
  - CDR
  - model
  - Basic
  - deep learning
  - è¿˜æ²¡å†™å®Œæ
---

# åœ¨è¾“å…¥ç‰¹å¾ä¸Šåšæ–‡ç« çš„Attention

æœ¬æ–‡å°†æ¥ç€è¯¦ç»†è¯´æ˜ä¸€ç§åŸºäºè¾“å…¥ç‰¹å¾åˆ†ç±»Attentionçš„æ–¹å¼ï¼Œå¹¶ä»‹ç»åœ¨è¿™ç§åˆ†ç±»æ–¹å¼ä¸‹å…³æ³¨åˆ°çš„ä¸åŒçš„Attentionçš„æ¶æ„ã€‚

å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡ä¸»è¦æ¢è®¨äº†åŸºäºè¾“å…¥ç‰¹å¾ç‰¹æ€§çš„æ³¨æ„åŠ›æœºåˆ¶å˜ä½“ã€‚æœ¬èŠ‚æ ¹æ®è¾“å…¥ç‰¹å¾çš„ä¸åŒç‰¹æ€§ï¼Œå°†ç‰¹å¾ç›¸å…³çš„æ³¨æ„åŠ›æœºåˆ¶åˆ†ä¸ºä¸‰ç±»ï¼šç‰¹å¾å¤šé‡æ€§(Multiplicity of Features)ã€ç‰¹å¾å±‚çº§(Levels of Features)å’Œç‰¹å¾è¡¨ç¤º(Feature Representations)ã€‚

åœ¨é˜…è¯»è¿™ç¯‡åšå®¢å‰è¯·å…ˆé˜…è¯» {% post_link Attention %}

<!-- more -->

## ç‰¹å¾å¤šé‡æ€§(Multiplicity of Features)

è¿™éƒ¨åˆ†è®¨è®ºäº†å¦‚ä½•å¤„ç†å¤šä¸ªè¾“å…¥æºçš„ç‰¹å¾ï¼Œä¸»è¦åˆ†ä¸ºå•ä¸€ç‰¹å¾æ³¨æ„åŠ›å’Œå¤šç‰¹å¾æ³¨æ„åŠ›æœºåˆ¶ã€‚

### å•ä¸€ç‰¹å¾æ³¨æ„åŠ›(Singular Features Attention)

å¤§å¤šæ•°ä»»åŠ¡æ¨¡å‹åªå¤„ç†å•ä¸€è¾“å…¥(å¦‚å›¾åƒã€å¥å­æˆ–å£°éŸ³åºåˆ—)ï¼Œä½¿ç”¨å•ä¸€ç‰¹å¾æ³¨æ„åŠ›æœºåˆ¶ã€‚è¿™ç§æœºåˆ¶ç›´æ¥å¯¹å•ä¸ªè¾“å…¥çš„ç‰¹å¾å‘é‡è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚

### å¤šç‰¹å¾æ³¨æ„åŠ›æœºåˆ¶

å½“æ¨¡å‹éœ€è¦åŒæ—¶å¤„ç†å¤šä¸ªè¾“å…¥æºæ—¶ï¼Œéœ€è¦ç‰¹æ®Šçš„å¤šç‰¹å¾æ³¨æ„åŠ›æœºåˆ¶ï¼š

**ååŒæ³¨æ„åŠ›(Co-attention)**
   - åˆ†ä¸º **ç²—ç§‘ç²’åº¦(Coarse-grained)** å’Œ **ç»†é¢—ç²’åº¦(Fine-grained)** ä¸¤ç§
   - **ç²—é¢—ç²’åº¦ååŒ**æ³¨æ„åŠ›ä½¿ç”¨ä¸€ä¸ªè¾“å…¥çš„*ç´§å‡‘è¡¨ç¤º*ä½œä¸ºæŸ¥è¯¢æ¥å…³æ³¨å¦ä¸€ä¸ªè¾“å…¥
   - **ç»†é¢—ç²’åº¦ååŒ**æ³¨æ„åŠ›ä½¿ç”¨ä¸€ä¸ªè¾“å…¥çš„æ‰€æœ‰ç‰¹å¾å‘é‡ä½œä¸ºæŸ¥è¯¢

#### ç²—é¢—ç²’åº¦ååŒ

è®ºæ–‡ç»™å‡ºçš„ç²—é¢—ç²’åº¦ååŒçš„å®ä¾‹æ˜¯**alternating co-attention**

##### alternating co-attention

<img src="/img/Attention/AlternatingCo-Attention.png" alt="alternating co-attention" width="60%" height="auto">

å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œè¿™æ˜¯alternating co-attentionçš„æ¶æ„å›¾ï¼Œè¯¥æœºåˆ¶äº¤æ›¿ä½¿ç”¨ä¸¤ä¸ªè¾“å…¥çš„ç‰¹å¾çŸ©é˜µï¼Œå…ˆè®¡ç®—ç¬¬ä¸€ä¸ªè¾“å…¥çš„æ³¨æ„åŠ›ï¼Œå°†å…¶ä¸Šä¸‹æ–‡å‘é‡ä½œä¸ºæŸ¥è¯¢è®¡ç®—ç¬¬äºŒä¸ªè¾“å…¥çš„æ³¨æ„åŠ›ï¼Œç„¶åå†ç”¨ç¬¬äºŒä¸ªè¾“å…¥çš„ä¸Šä¸‹æ–‡å‘é‡é‡æ–°è®¡ç®—ç¬¬ä¸€ä¸ªè¾“å…¥çš„æ³¨æ„åŠ›ã€‚

è¿™é‡Œç°ç»™å‡ºä»–çš„scoreå‡½æ•°

å¯¹äºæœ‰åºåˆ—è¾“å…¥çš„Attentionï¼š

$$
\mathrm{score}(\underset{d\_{q}\times1}{\boldsymbol{q}},\underset{d\_{k}\times1}{\boldsymbol{k}\_{l}})=\underset{1\times d\_{w}}{\boldsymbol{w}^{T}}\times\mathrm{act}(\underset{d\_{w}\times d\_{q}}{\boldsymbol{W}\_{1}}\times\underset{d\_{q}\times1}{\boldsymbol{q}}+\underset{d\_{w}\times d\_{k}}{\boldsymbol{W}\_{2}}\times\underset{d\_{k}\times1}{\boldsymbol{k}\_{l}}+\underset{d\_{w}\times1}{\boldsymbol{b}})
$$

å¯¹äºæ— åºåˆ—è¾“å…¥çš„Attention ~~ï¼ˆè¿™æ˜¯ä¸€ç§è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œåé¢ä¼šæåˆ°ï¼‰~~ ï¼š

$$
\underset{1\times1}{e\_{l}^{(0)}}=\underset{1\times d\_{w}}{\boldsymbol{w}^{(1)T}}\times\operatorname{act}(\underset{d\_{w}\times d\_{k}^{(1)}}{\boldsymbol{W}^{(1)}}\times\underset{d\_{k}^{(1)}\times1}{\boldsymbol{k}\_{l}^{(1)}}+\underset{d\_{w}\times1}{\boldsymbol{b}^{(1)}})
$$

å¯¹äºç¬¬äºŒå±‚Attentionï¼š

$$
\underset{1\times1}{e\_{l}^{(2)}}=\mathrm{score}(\underset{d\_{v}^{(1)}\times 1}{\boldsymbol{c}^{(0)}},\underset{d\_{k}^{(2)}\times1}{\boldsymbol{k}\_{l}^{(2)}})
$$

å¯¹äºç¬¬ä¸‰å±‚Attentionï¼š

$$
\underset{1\times1}{e\_{l}^{(1)}}=\mathrm{score}(\underset{d\_{v}^{(2)}\times 1}{\boldsymbol{c}^{(2)}},\underset{d\_{k}^{(1)}\times1}{\boldsymbol{k}\_{l}^{(1)}})
$$

ç”Ÿæˆçš„ä¸Šä¸‹æ–‡å‘é‡$\boldsymbol{c}^{(1)}$å’Œ$\boldsymbol{c}^{(2)}$è¢«è¿æ¥èµ·æ¥ï¼Œå¹¶åœ¨è¾“å‡ºæ¨¡å‹ä¸­ç”¨äºé¢„æµ‹ã€‚äº¤æ›¿ååŒæ³¨æ„åŠ›ç”±äºéœ€è¦ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡ï¼Œå› æ­¤æœ¬è´¨ä¸ŠåŒ…å«äº†*ä¸€ç§é¡ºåºæ€§*ã€‚è¿™å¯èƒ½ä¼šå¸¦æ¥è®¡ç®—ä¸Šçš„åŠ£åŠ¿ï¼Œå› ä¸º*æ— æ³•å¹¶è¡Œ*åŒ–ã€‚ 

##### interactive co-attention

   - å¹¶è¡Œè®¡ç®—ä¸¤ä¸ªè¾“å…¥çš„æ³¨æ„åŠ›
   - ä½¿ç”¨æœªåŠ æƒå¹³å‡çš„å…³é”®å‘é‡ä½œä¸ºæŸ¥è¯¢
   - è®¡ç®—æ•ˆç‡æ›´é«˜ï¼Œå¯ä»¥å¹¶è¡Œå¤„ç†



##### å¹¶è¡ŒååŒæ³¨æ„åŠ›(Parallel Co-attention)
   - åŒæ—¶è®¡ç®—ä¸¤ä¸ªè¾“å…¥çš„æ³¨æ„åŠ›
   - ä½¿ç”¨äº²å’ŒçŸ©é˜µ(Affinity Matrix)è½¬æ¢å…³é”®å‘é‡ç©ºé—´
   - é€šè¿‡èšåˆå½¢å¼è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°



##### æ—‹è½¬æ³¨æ„åŠ›(Rotatory Attention)
   - ä¸»è¦ç”¨äºæƒ…æ„Ÿåˆ†æä»»åŠ¡
   - å¤„ç†ä¸‰ä¸ªè¾“å…¥ï¼šç›®æ ‡çŸ­è¯­ã€å·¦ä¸Šä¸‹æ–‡å’Œå³ä¸Šä¸‹æ–‡
   - é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶è¿­ä»£æ”¹è¿›è¡¨ç¤º

## ç‰¹å¾å±‚çº§(Levels of Features)

è¿™éƒ¨åˆ†è®¨è®ºäº†å¦‚ä½•å¤„ç†å…·æœ‰å±‚çº§ç»“æ„çš„ç‰¹å¾ï¼Œä¸»è¦åˆ†ä¸ºå•å±‚çº§æ³¨æ„åŠ›å’Œå¤šå±‚çº§æ³¨æ„åŠ›æœºåˆ¶ã€‚

### å•å±‚çº§æ³¨æ„åŠ›(Single-Level Attention)

ä¼ ç»Ÿæ³¨æ„åŠ›æœºåˆ¶é€šå¸¸åœ¨å•ä¸€å±‚çº§ä¸Šå¤„ç†ç‰¹å¾ï¼Œå¦‚åªå…³æ³¨å•è¯çº§åˆ«æˆ–å¥å­çº§åˆ«ã€‚

### å¤šå±‚çº§æ³¨æ„åŠ›æœºåˆ¶

1. **æ³¨æ„åŠ›å åŠ (Attention-via-Attention)**
   - åŒæ—¶å¤„ç†å­—ç¬¦çº§å’Œè¯çº§ç‰¹å¾
   - å…ˆè®¡ç®—è¯çº§æ³¨æ„åŠ›ï¼Œç”¨å…¶ä¸Šä¸‹æ–‡å‘é‡è¾…åŠ©è®¡ç®—å­—ç¬¦çº§æ³¨æ„åŠ›
   - æœ€ç»ˆæ‹¼æ¥ä¸¤ä¸ªå±‚çº§çš„ä¸Šä¸‹æ–‡å‘é‡



2. **å±‚çº§æ³¨æ„åŠ›(Hierarchical Attention)**
   - ä»æœ€ä½å±‚çº§å¼€å§‹ï¼Œé€æ­¥æ„å»ºé«˜å±‚çº§è¡¨ç¤º
   - å¸¸ç”¨äºæ–‡æ¡£åˆ†ç±»ï¼šè¯â†’å¥â†’æ–‡æ¡£
   - æ¯ä¸ªå±‚çº§é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ç”Ÿæˆæ‘˜è¦è¡¨ç¤º



## ç‰¹å¾è¡¨ç¤º(Feature Representations)

è¿™éƒ¨åˆ†è®¨è®ºäº†ç‰¹å¾è¡¨ç¤ºæ–¹å¼çš„æ³¨æ„åŠ›æœºåˆ¶å˜ä½“ï¼Œä¸»è¦åˆ†ä¸ºå•ä¸€è¡¨ç¤ºæ³¨æ„åŠ›å’Œå¤šè¡¨ç¤ºæ³¨æ„åŠ›ã€‚

### å•ä¸€è¡¨ç¤ºæ³¨æ„åŠ›(Single-Representational Attention)

ä¼ ç»Ÿæ–¹æ³•ä½¿ç”¨å•ä¸€åµŒå…¥æˆ–è¡¨ç¤ºæ¨¡å‹ç”Ÿæˆç‰¹å¾è¡¨ç¤ºã€‚

### å¤šè¡¨ç¤ºæ³¨æ„åŠ›(Multi-Representational Attention)

1. **å…ƒåµŒå…¥(Meta-embeddings)**
   - æ•´åˆå¤šä¸ªåµŒå…¥è¡¨ç¤º
   - é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶åŠ æƒå¹³å‡ä¸åŒè¡¨ç¤º
   - ç”Ÿæˆæ›´é«˜è´¨é‡çš„ç‰¹å¾è¡¨ç¤º

2. **è‡ªæ³¨æ„åŠ›æœºåˆ¶**
   - å­¦ä¹ ç‰¹å¾å‘é‡ä¹‹é—´çš„å…³ç³»
   - é€šè¿‡æ³¨æ„åŠ›æ”¹è¿›ç‰¹å¾è¡¨ç¤º
   - å¸¸ç”¨äºTransformeræ¶æ„ä¸­

## åº”ç”¨é¢†åŸŸ

3.1èŠ‚è®¨è®ºçš„ç‰¹å¾ç›¸å…³æ³¨æ„åŠ›æœºåˆ¶åœ¨å¤šä¸ªé¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ï¼š
- åŒ»å­¦æ•°æ®åˆ†æ(å¤šç‰¹å¾ååŒæ³¨æ„åŠ›)
- æ¨èç³»ç»Ÿ(å¤šå±‚çº§æ³¨æ„åŠ›)
- æƒ…æ„Ÿåˆ†æ(æ—‹è½¬æ³¨æ„åŠ›)
- æ–‡æ¡£åˆ†ç±»(å±‚çº§æ³¨æ„åŠ›)
- å¤šè¯­è¨€å¤„ç†(å¤šè¡¨ç¤ºæ³¨æ„åŠ›)

## æ€»ç»“

3.1èŠ‚ç³»ç»Ÿæ€§åœ°åˆ†ç±»äº†åŸºäºè¾“å…¥ç‰¹å¾ç‰¹æ€§çš„æ³¨æ„åŠ›æœºåˆ¶å˜ä½“ï¼Œä¸ºç ”ç©¶è€…æä¾›äº†æ¸…æ™°çš„æ¡†æ¶æ¥é€‰æ‹©é€‚åˆç‰¹å®šä»»åŠ¡å’Œæ•°æ®ç±»å‹çš„æœ€ä½³æ³¨æ„åŠ›æœºåˆ¶ã€‚è¿™äº›æœºåˆ¶é€šè¿‡å……åˆ†åˆ©ç”¨è¾“å…¥ç‰¹å¾çš„å¤šé‡æ€§ã€å±‚çº§ç»“æ„å’Œè¡¨ç¤ºå¤šæ ·æ€§ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚



å›¾3å±•ç¤ºäº†å®Œæ•´çš„æ³¨æ„åŠ›æœºåˆ¶åˆ†ç±»ä½“ç³»ï¼Œå…¶ä¸­3.1èŠ‚è®¨è®ºçš„ç‰¹å¾ç›¸å…³æ³¨æ„åŠ›æœºåˆ¶æ˜¯è¯¥ä½“ç³»çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚

$$
\underset{n\_{f}^{(1)}\times n\_{f}^{(2)}}{A}=\operatorname{act}(\underset{n\_{f}^{(1)}\times d\_{k}^{(1)}}{\begin{array}{c}K^{(1)^{T}}\end{array}}\times\underset{d\_{k}^{(1)}\times d\_{k}^{(2)}}{\begin{array}{c}W\_{A}\end{array}}\times\underset{d\_{k}^{(2)}\times n\_{f}^{(2)}}{\begin{array}{c}K^{(2)}\end{array}})
$$

# ğŸ“š ğ’¥ğ‘’ğ’»ğ‘’ğ“‡ğ‘’ğ“ƒğ’¸ğ‘’

<a href="/paper/Brauwerså’ŒFrasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf" target="_blank">ğŸ“„ Brauwerså’ŒFrasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a>

{% post_link Attention %}