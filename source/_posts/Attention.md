---
title: Attention Overview
date: 2025-07-10 19:56:23
categories:
  - CDR
  - model
  - attention
tags:
  - CDR
  - model
  - Basic
  - è¿˜æ²¡å†™å®Œæ
  - PyTorch
---

# Is Attention All My Need ?

> æ³¨æ„åŠ›æœºåˆ¶åœ¨å›¾ç¥ç»ç½‘ç»œä¸­æ‰®æ¼”ç€è¶Šæ¥è¶Šé‡è¦çš„è§’è‰²ã€‚~~ä½†é¼ é¼ ç°åœ¨è¿æ­£å¸¸çš„Attentionæœ‰å“ªäº›éƒ½ä¸æ¸…æ¥šæ~~æœ¬æ–‡é¼ é¼ å°†ä»ä¸€èˆ¬çš„Attentionå‡ºå‘ï¼Œç»™å‡ºAttentionçš„æ€»ä½“ç»“æ„ï¼Œç„¶åæŒ‰åˆ†ç±»ä»‹ç»ç°æœ‰çš„ä¸»è¦çš„Attention

æœ¬æ–‡ä¸»è¦æ¥è‡ªäºä¸€ç¯‡è®ºæ–‡ï¼ŒåŸºæœ¬å¯ä»¥çœ‹ä½œ[é‚£ç¯‡è®ºæ–‡](/paper/Brauwerså’ŒFrasincar%20-%202023%20-%20A%20General%20Survey%20on%20Attention%20Mechanisms%20in%20Deep%20Learning.pdf)çš„é˜…è¯»ç¬”è®°

<!-- more -->

## ğŸ¯ å¼•è¨€

åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸï¼Œæ³¨æ„åŠ›æœºåˆ¶å·²ç»æˆä¸ºä¸€ä¸ªé©å‘½æ€§çš„åˆ›æ–°ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†åºåˆ—æ•°æ®å’Œå›¾åƒæ•°æ®æ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸã€‚è€Œåœ¨å›¾ç¥ç»ç½‘ç»œä¸­ï¼Œæ³¨æ„åŠ›æœºåˆ¶çš„å¼•å…¥ä¸ä»…æé«˜äº†æ¨¡å‹çš„è¡¨ç°åŠ›ï¼Œè¿˜å¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚

åœ¨å›¾ç»“æ„æ•°æ®ä¸­åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶ä¸»è¦æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š
1. è‡ªé€‚åº”æ€§ï¼šèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡åŠ¨æ€è°ƒæ•´ä¸åŒé‚»å±…èŠ‚ç‚¹çš„é‡è¦æ€§
2. å¯è§£é‡Šæ€§ï¼šé€šè¿‡æ³¨æ„åŠ›æƒé‡å¯ä»¥ç›´è§‚ç†è§£æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹
3. é•¿ç¨‹ä¾èµ–ï¼šæœ‰æ•ˆç¼“è§£äº†ä¼ ç»ŸGNNä¸­çš„è¿‡å¹³æ»‘é—®é¢˜
4. å¼‚è´¨æ€§å¤„ç†ï¼šæ›´å¥½åœ°å¤„ç†å¼‚è´¨å›¾ä¸­çš„ä¸åŒç±»å‹èŠ‚ç‚¹å’Œè¾¹

## ğŸ“š æ€»è§ˆAttention

æœ¬ç« èŠ‚ä¸»è¦å‚è€ƒäº†è®ºæ–‡[ğŸ“„ Brauwerså’ŒFrasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning](/paper/Brauwerså’ŒFrasincar%20-%202023%20-%20A%20General%20Survey%20on%20Attention%20Mechanisms%20in%20Deep%20Learning.pdf)æœ‰å…´è¶£çš„è¯å¯ä»¥çœ‹çœ‹åŸæ–‡æ

<embed src="/paper/Brauwerså’ŒFrasincar%20-%202023%20-%20A%20General%20Survey%20on%20Attention%20Mechanisms%20in%20Deep%20Learning.pdf" width="45%" height="400" type="application/pdf">

### Attentionçš„ä¸€èˆ¬ç»“æ„

<img src="/img/Attention/TotalModel.png" alt="TotalModel" width="60%" height="auto">

ä¸Šå›¾æ˜¯ä»æ€»ä½“ä¸Šçœ‹Attentionåœ¨æ•´ä¸ªä»»åŠ¡æ¨¡å‹æ¡†æ¶ä¸­çš„ä½ç½®

æ¡†æ¶åŒ…å«å››ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š
1. **ç‰¹å¾æ¨¡å‹**ï¼šè´Ÿè´£è¾“å…¥æ•°æ®çš„ç‰¹å¾æå–
2. **æŸ¥è¯¢æ¨¡å‹**ï¼šç”Ÿæˆæ³¨æ„åŠ›æŸ¥è¯¢å‘é‡
3. **æ³¨æ„åŠ›æ¨¡å‹**ï¼šè®¡ç®—æ³¨æ„åŠ›æƒé‡
4. **è¾“å‡ºæ¨¡å‹**ï¼šç”Ÿæˆæœ€ç»ˆé¢„æµ‹ç»“æœ

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¼šä» *è¾“å…¥* çš„è§’åº¦æ¥çœ‹**ç‰¹å¾æ¨¡å‹**å’Œ**æŸ¥è¯¢æ¨¡å‹**ï¼Œä» *è¾“å‡º* çš„è§’åº¦æ¥çœ‹**æ³¨æ„åŠ›æ¨¡å‹**å’Œ**è¾“å‡ºæ¨¡å‹**

#### è¾“å…¥å¤„ç†æœºåˆ¶

1. **ç‰¹å¾æ¨¡å‹**ï¼Œå³å°†ä»»åŠ¡çš„è¾“å…¥è¿›è¡Œembedding
   
    å¯¹äºè¾“å…¥çŸ©é˜µ$ X \in \mathbb{R}^{d_x \times n_x} $ï¼Œç‰¹å¾æ¨¡å‹æå–ç‰¹å¾å‘é‡ï¼š$\boldsymbol{F} = [f_1, \ldots, f_{n_f}] \in \mathbb{R}^{d_f \times n_f}$

2. **æŸ¥è¯¢æ¨¡å‹**ï¼ŒæŸ¥è¯¢æ¨¡å‹äº§ç”ŸæŸ¥è¯¢å‘é‡$ \boldsymbol{q} \in \mathbb{R}^{d_q} $ï¼Œç”¨ä»¥å‘Šè¯‰æ³¨æ„åŠ›æ¨¡å‹å“ªä¸€ä¸ªç‰¹å¾æ˜¯é‡è¦çš„

ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œè¿™ä¸¤ä¸ªæ¨¡å‹å¯ä»¥ç”¨CNNæˆ–RNN

#### è¾“å‡ºè®¡ç®—æœºåˆ¶

<img src="/img/Attention/GeneralAttentionModule.png" alt="GeneralAttentionModule" width="50%" height="auto">

ä¸Šå›¾æ˜¯Attentionæ¨¡å‹æ€»ä½“ç»“æ„çš„è¯´æ˜ï¼Œä¸‹é¢å¯¹è¿™å¼ å›¾è¿›è¡Œè¯¦ç»†çš„è¯´æ˜

1. ç‰¹å¾çŸ©é˜µ$\boldsymbol{F} = [\boldsymbol{f}\_1, \ldots, \boldsymbol{f}\_{n\_f}] \in \mathbb{R}^{d\_f \times n\_f}$ï¼Œé€šè¿‡*æŸäº›æ–¹æ³•*å°†å…¶åˆ†ä¸ºKeysçŸ©é˜µ$\boldsymbol{K} = [\boldsymbol{k}\_1, \ldots, \boldsymbol{k}\_{n\_f}] \in \mathbb{R}^{d\_k \times n\_f}$å’ŒValuesçŸ©é˜µ$\boldsymbol{V} = [\boldsymbol{v}_1, \ldots, \boldsymbol{v}\_{n\_f}] \in \mathbb{R}^{d\_v \times n\_f}$ï¼Œè¿™é‡Œçš„*æŸäº›æ–¹æ³•*ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ï¼ŒæŒ‰ä»¥ä¸‹çš„æ–¹å¼é€šè¿‡**çº¿æ€§å˜æ¢**å¾—åˆ°ï¼š

$$
\underset{d\_{k} \times n\_{f}}{\boldsymbol{K}}=\underset{d\_{k} \times d\_{f}}{\boldsymbol{W}\_{K}} \times \underset{d\_{f} \times n\_{f}}{\boldsymbol{F}}, \quad \underset{d\_{v} \times n\_{f}}{\boldsymbol{V}}=\underset{d\_{v} \times d\_{f}}{\boldsymbol{W}\_{V}} \times \underset{d\_{f} \times n\_{f}}{\boldsymbol{F}} .
$$

2. `Attention Scores`æ¨¡å—æ ¹æ® $\boldsymbol{q}$ è®¡ç®—æ¯ä¸€ä¸ªkeyå‘é‡å¯¹åº”çš„åˆ†æ•°$\boldsymbol{e} = [e_1, \ldots, e_{n_f}] \in \mathbb{R}^{n_f}$ï¼š

    $$
    \underset{1\times 1}{e\_l} = \text{score}(\underset{d\_q \times 1}{\boldsymbol{q}}, \underset{d\_k \times 1}{\boldsymbol{k}\_l}) 
    $$

    å¦‚å‰æ‰€è¿°ï¼ŒæŸ¥è¯¢è±¡å¾ç€å¯¹ä¿¡æ¯çš„è¯·æ±‚ã€‚æ³¨æ„åŠ›åˆ†æ•°$e_l$è¡¨ç¤ºæ ¹æ®æŸ¥è¯¢ï¼Œå…³é”®å‘é‡$\boldsymbol{k}_l$ä¸­åŒ…å«çš„ä¿¡æ¯çš„é‡è¦æ€§ã€‚å¦‚æœæŸ¥è¯¢å’Œå…³é”®å‘é‡çš„ç»´åº¦ç›¸åŒï¼Œåˆ™å¾—åˆ†å‡½æ•°çš„ä¸€ä¸ªä¾‹å­æ˜¯å–å‘é‡çš„ç‚¹ç§¯ã€‚

3. ç”±äºç»è¿‡è¿™ä¹ˆä¸€å †æ“ä½œä¹‹åï¼Œåˆ†æ•°æœ‰å¾ˆå¤§çš„å¯èƒ½å·²ç»é£èµ·æ¥äº†æï¼Œè¿™ä¸ªæ—¶å€™å°±éœ€è¦`Attention Alignment`æ¨¡å—å¯¹å…¶è¿›è¡Œ**å½’ä¸€åŒ–**ä¹‹ç±»çš„æ“ä½œäº†æ

    $$
    \underset{1\times 1}{a\_l} = \text{align}(\underset{d\_q \times 1}{\boldsymbol{e\_l}}, \underset{n\_f \times 1}{\boldsymbol{e}}) 
    $$

æ³¨æ„åŠ›æƒé‡$\boldsymbol{a} = [a_1, \ldots, a_{n_f}] \in \mathbb{R}^{n_f}$ä¸ºæ³¨æ„åŠ›æ¨¡å—æä¾›äº†ä¸€ä¸ªç›¸å½“ç›´è§‚çš„è§£é‡Šã€‚æ¯ä¸ªæƒé‡ç›´æ¥è¡¨æ˜äº†æ¯ä¸ªç‰¹å¾å‘é‡ç›¸å¯¹äºå…¶ä»–ç‰¹å¾å‘é‡å¯¹äºè¿™ä¸ªé—®é¢˜çš„é‡è¦æ€§ã€‚ 

4. åœ¨`Weight Average`æ¨¡å—å®Œæˆ**ä¸Šä¸‹æ–‡ç”Ÿæˆ**ï¼š

    $$
    \underset{d\_v \times 1}{\boldsymbol{c}} = \sum\_{l = 1}^{n\_f} \underset{1 \times 1}{a\_l}\times \underset{d\_v \times 1}{\boldsymbol{v}\_l}
    $$

5. è¾“å‡ºå¤„ç†å°±æƒ³æ€ä¹ˆæå°±æ€ä¹ˆæäº†æï¼Œä¾‹å¦‚ ç”¨äºåˆ†ç±»

    $$
    \underset{d\_y \times 1}{\hat{\boldsymbol{y}}} = \text{softmax}( \underset{d\_y \times d\_v}{\boldsymbol{W}\_c}\times \underset{d\_v \times 1}{\boldsymbol{c}} + \underset{d\_y \times 1}{\boldsymbol{b}\_c})
    $$

### Attentionåˆ†ç±»

<img src="/img/Attention/Taxonomy.png" style="max-width: 100%; height: auto;">

è®ºæ–‡æŒ‰ç…§ä¸Šå›¾çš„æ–¹å¼ç»™Attentionè¿›è¡Œäº†åˆ†ç±»


#### åŸºæœ¬æ³¨æ„åŠ›æœºåˆ¶

æœ€åŸºæœ¬çš„æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥è¡¨ç¤ºä¸ºï¼š

$$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$

å…¶ä¸­ï¼š
- $Q$ï¼šæŸ¥è¯¢çŸ©é˜µï¼ˆQueryï¼‰
- $K$ï¼šé”®çŸ©é˜µï¼ˆKeyï¼‰
- $V$ï¼šå€¼çŸ©é˜µï¼ˆValueï¼‰
- $d_k$ï¼šé”®å‘é‡çš„ç»´åº¦

#### è‡ªæ³¨æ„åŠ›æœºåˆ¶

è‡ªæ³¨æ„åŠ›æ˜¯ä¸€ç§ç‰¹æ®Šçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶ä¸­Qã€Kã€Véƒ½æ¥è‡ªåŒä¸€ä¸ªæºåºåˆ—ï¼š

$$SelfAttention(X) = Attention(XW_Q, XW_K, XW_V)$$

### å›¾æ³¨æ„åŠ›æœºåˆ¶

#### GATï¼ˆGraph Attention Networksï¼‰

GATé€šè¿‡å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶æ¥åŠ æƒé‚»å±…èŠ‚ç‚¹çš„ç‰¹å¾ã€‚å¯¹äºèŠ‚ç‚¹iï¼Œå…¶æ›´æ–°å…¬å¼ä¸ºï¼š

$$h\_i^{(l+1)} = \sigma(\sum\_{j \in \mathcal{N}\_i} \alpha\_{ij}W^{(l)}h\_j^{(l)})$$

å…¶ä¸­æ³¨æ„åŠ›ç³»æ•°$\alpha_{ij}$çš„è®¡ç®—ï¼š

$$\alpha_{ij} = \frac{exp(LeakyReLU(a^T[Wh_i || Wh_j]))}{\sum_{k \in \mathcal{N}_i} exp(LeakyReLU(a^T[Wh_i || Wh_k]))}$$

#### å¤šå¤´æ³¨æ„åŠ›

ä¸ºäº†æé«˜æ¨¡å‹çš„ç¨³å®šæ€§å’Œè¡¨è¾¾èƒ½åŠ›ï¼ŒGATä½¿ç”¨äº†å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼š

$$h\_i^{(l+1)} = \sigma(\frac{1}{K} \sum\_{k=1}^K \sum\_{j \in \mathcal{N}\_i} \alpha\_{ij}^k W^k h\_j^{(l)})$$

### å˜ä½“ä¸æ‰©å±•

#### è¾¹æ³¨æ„åŠ›

é™¤äº†èŠ‚ç‚¹ä¹‹é—´çš„æ³¨æ„åŠ›ï¼Œä¸€äº›æ¨¡å‹è¿˜å¼•å…¥äº†è¾¹æ³¨æ„åŠ›æœºåˆ¶ï¼š

$$e_{ij} = a^T[Wh_i || Wh_j || We_{ij}]$$

å…¶ä¸­$e_{ij}$æ˜¯è¾¹çš„ç‰¹å¾ã€‚

#### å…¨å±€æ³¨æ„åŠ›

é€šè¿‡å¼•å…¥å…¨å±€èŠ‚ç‚¹æˆ–æ± åŒ–æ“ä½œï¼Œå¯ä»¥å®ç°å…¨å±€æ³¨æ„åŠ›ï¼š

$$g = \sum_{i \in V} \beta_i h_i$$

å…¶ä¸­$\beta_i$æ˜¯å…¨å±€æ³¨æ„åŠ›æƒé‡ã€‚

## ğŸ’» å®ç°ç»†èŠ‚

### PyTorchå®ç°çš„GATå±‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class GATLayer(nn.Module):
    def __init__(self, in_features, out_features, dropout, alpha, concat=True):
        super(GATLayer, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.dropout = dropout
        self.alpha = alpha
        self.concat = concat

        # å˜æ¢çŸ©é˜µ
        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))
        nn.init.xavier_uniform_(self.W.data, gain=1.414)
        
        # æ³¨æ„åŠ›å‘é‡
        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))
        nn.init.xavier_uniform_(self.a.data, gain=1.414)

        self.leakyrelu = nn.LeakyReLU(self.alpha)

    def forward(self, x, adj):
        # x: èŠ‚ç‚¹ç‰¹å¾çŸ©é˜µ [N, in_features]
        # adj: é‚»æ¥çŸ©é˜µ [N, N]
        
        # çº¿æ€§å˜æ¢
        h = torch.mm(x, self.W)  # [N, out_features]
        N = h.size()[0]

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1)
        a_input = a_input.view(N, N, 2 * self.out_features)
        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))

        # æ©ç æœºåˆ¶
        zero_vec = -9e15 * torch.ones_like(e)
        attention = torch.where(adj > 0, e, zero_vec)
        attention = F.softmax(attention, dim=1)
        attention = F.dropout(attention, self.dropout, training=self.training)

        # èšåˆç‰¹å¾
        h_prime = torch.matmul(attention, h)

        if self.concat:
            return F.elu(h_prime)
        else:
            return h_prime

class GAT(nn.Module):
    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):
        super(GAT, self).__init__()
        self.dropout = dropout
        
        # å¤šå¤´æ³¨æ„åŠ›å±‚
        self.attentions = nn.ModuleList([
            GATLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) 
            for _ in range(nheads)
        ])
        
        # è¾“å‡ºå±‚
        self.out_att = GATLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)

    def forward(self, x, adj):
        x = F.dropout(x, self.dropout, training=self.training)
        # å¤šå¤´æ³¨æ„åŠ›
        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)
        x = F.dropout(x, self.dropout, training=self.training)
        x = self.out_att(x, adj)
        return F.log_softmax(x, dim=1)
```

### å®é™…åº”ç”¨ç¤ºä¾‹

```python
# æ¨¡å‹åˆå§‹åŒ–
model = GAT(nfeat=input_dim,
           nhid=8,
           nclass=num_classes,
           dropout=0.6,
           alpha=0.2,
           nheads=8)

# ä¼˜åŒ–å™¨
optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)

# è®­ç»ƒå¾ªç¯
def train():
    model.train()
    optimizer.zero_grad()
    output = model(features, adj)
    loss = F.nll_loss(output[idx_train], labels[idx_train])
    loss.backward()
    optimizer.step()
    return loss.item()
```

## ğŸ” æ³¨æ„äº‹é¡¹ä¸æœ€ä½³å®è·µ


## ğŸ“ˆ æœªæ¥å±•æœ›


# ğŸ“š ğ’¥ğ‘’ğ’»ğ‘’ğ“‡ğ‘’ğ“ƒğ’¸ğ‘’
<a href="/paper/Brauwerså’ŒFrasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf" target="_blank">ğŸ“„ Brauwerså’ŒFrasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a>
<a href="/paper/Lee ç­‰ - 2018 - Attention Models in Graphs A Survey.pdf" target="_blank">ğŸ“„ Lee ç­‰ - 2018 - Attention Models in Graphs A Survey</a>
<a href="https://github.com/xmu-xiaoma666/External-Attention-pytorch" target="_blank">github: External-Attention-pytorch</a>