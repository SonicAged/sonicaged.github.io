---
title: Attention in Graph
date: 2025-07-10 19:56:23
categories:
  - CDR
  - model
tags:
  - CDR
  - model
  - Basic
  - è¿˜æ²¡å†™å®Œæ
  - PyTorch
  - graph theory
---

# ğŸŒŸ å›¾ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶

> æ³¨æ„åŠ›æœºåˆ¶åœ¨å›¾ç¥ç»ç½‘ç»œä¸­æ‰®æ¼”ç€è¶Šæ¥è¶Šé‡è¦çš„è§’è‰²ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨æ³¨æ„åŠ›æœºåˆ¶åœ¨å›¾ç»“æ„æ•°æ®å¤„ç†ä¸­çš„åº”ç”¨ï¼Œä»åŸºç¡€æ¦‚å¿µåˆ°å®é™…å®ç°ã€‚
>
$$
\underset{d\_{k} \times n\_{f}}{\boldsymbol{K}}=\underset{d\_{k} \times d\_{f}}{\boldsymbol{W}\_{K}} \times \underset{d\_{f} \times n\_{f}}{\boldsymbol{F}}, \quad \underset{d\_{v} \times n\_{f}}{\boldsymbol{V}}=\underset{d\_{v} \times d\_{f}}{\boldsymbol{W}\_{V}} \times \underset{d\_{f} \times n\_{f}}{\boldsymbol{F}} .
$$
<!-- more -->

## ğŸ¯ å¼•è¨€

åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸï¼Œæ³¨æ„åŠ›æœºåˆ¶å·²ç»æˆä¸ºä¸€ä¸ªé©å‘½æ€§çš„åˆ›æ–°ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†åºåˆ—æ•°æ®å’Œå›¾åƒæ•°æ®æ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸã€‚è€Œåœ¨å›¾ç¥ç»ç½‘ç»œä¸­ï¼Œæ³¨æ„åŠ›æœºåˆ¶çš„å¼•å…¥ä¸ä»…æé«˜äº†æ¨¡å‹çš„è¡¨ç°åŠ›ï¼Œè¿˜å¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚

åœ¨å›¾ç»“æ„æ•°æ®ä¸­åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶ä¸»è¦æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š
1. è‡ªé€‚åº”æ€§ï¼šèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡åŠ¨æ€è°ƒæ•´ä¸åŒé‚»å±…èŠ‚ç‚¹çš„é‡è¦æ€§
2. å¯è§£é‡Šæ€§ï¼šé€šè¿‡æ³¨æ„åŠ›æƒé‡å¯ä»¥ç›´è§‚ç†è§£æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹
3. é•¿ç¨‹ä¾èµ–ï¼šæœ‰æ•ˆç¼“è§£äº†ä¼ ç»ŸGNNä¸­çš„è¿‡å¹³æ»‘é—®é¢˜
4. å¼‚è´¨æ€§å¤„ç†ï¼šæ›´å¥½åœ°å¤„ç†å¼‚è´¨å›¾ä¸­çš„ä¸åŒç±»å‹èŠ‚ç‚¹å’Œè¾¹

## ğŸ“š ç†è®ºåŸºç¡€

### æ³¨æ„åŠ›æœºåˆ¶å›é¡¾

#### åŸºæœ¬æ³¨æ„åŠ›æœºåˆ¶

æœ€åŸºæœ¬çš„æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥è¡¨ç¤ºä¸ºï¼š

$$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$

å…¶ä¸­ï¼š
- $Q$ï¼šæŸ¥è¯¢çŸ©é˜µï¼ˆQueryï¼‰
- $K$ï¼šé”®çŸ©é˜µï¼ˆKeyï¼‰
- $V$ï¼šå€¼çŸ©é˜µï¼ˆValueï¼‰
- $d_k$ï¼šé”®å‘é‡çš„ç»´åº¦

#### è‡ªæ³¨æ„åŠ›æœºåˆ¶

è‡ªæ³¨æ„åŠ›æ˜¯ä¸€ç§ç‰¹æ®Šçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶ä¸­Qã€Kã€Véƒ½æ¥è‡ªåŒä¸€ä¸ªæºåºåˆ—ï¼š

$$SelfAttention(X) = Attention(XW_Q, XW_K, XW_V)$$

### å›¾æ³¨æ„åŠ›æœºåˆ¶

#### GATï¼ˆGraph Attention Networksï¼‰

GATé€šè¿‡å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶æ¥åŠ æƒé‚»å±…èŠ‚ç‚¹çš„ç‰¹å¾ã€‚å¯¹äºèŠ‚ç‚¹iï¼Œå…¶æ›´æ–°å…¬å¼ä¸ºï¼š

$$h\_i^{(l+1)} = \sigma(\sum\_{j \in \mathcal{N}\_i} \alpha\_{ij}W^{(l)}h\_j^{(l)})$$

å…¶ä¸­æ³¨æ„åŠ›ç³»æ•°$\alpha_{ij}$çš„è®¡ç®—ï¼š

$$\alpha_{ij} = \frac{exp(LeakyReLU(a^T[Wh_i || Wh_j]))}{\sum_{k \in \mathcal{N}_i} exp(LeakyReLU(a^T[Wh_i || Wh_k]))}$$

#### å¤šå¤´æ³¨æ„åŠ›

ä¸ºäº†æé«˜æ¨¡å‹çš„ç¨³å®šæ€§å’Œè¡¨è¾¾èƒ½åŠ›ï¼ŒGATä½¿ç”¨äº†å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼š

$$h\_i^{(l+1)} = \sigma(\frac{1}{K} \sum\_{k=1}^K \sum\_{j \in \mathcal{N}\_i} \alpha\_{ij}^k W^k h\_j^{(l)})$$

### å˜ä½“ä¸æ‰©å±•

#### è¾¹æ³¨æ„åŠ›

é™¤äº†èŠ‚ç‚¹ä¹‹é—´çš„æ³¨æ„åŠ›ï¼Œä¸€äº›æ¨¡å‹è¿˜å¼•å…¥äº†è¾¹æ³¨æ„åŠ›æœºåˆ¶ï¼š

$$e_{ij} = a^T[Wh_i || Wh_j || We_{ij}]$$

å…¶ä¸­$e_{ij}$æ˜¯è¾¹çš„ç‰¹å¾ã€‚

#### å…¨å±€æ³¨æ„åŠ›

é€šè¿‡å¼•å…¥å…¨å±€èŠ‚ç‚¹æˆ–æ± åŒ–æ“ä½œï¼Œå¯ä»¥å®ç°å…¨å±€æ³¨æ„åŠ›ï¼š

$$g = \sum_{i \in V} \beta_i h_i$$

å…¶ä¸­$\beta_i$æ˜¯å…¨å±€æ³¨æ„åŠ›æƒé‡ã€‚

## ğŸ’» å®ç°ç»†èŠ‚

### PyTorchå®ç°çš„GATå±‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class GATLayer(nn.Module):
    def __init__(self, in_features, out_features, dropout, alpha, concat=True):
        super(GATLayer, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.dropout = dropout
        self.alpha = alpha
        self.concat = concat

        # å˜æ¢çŸ©é˜µ
        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))
        nn.init.xavier_uniform_(self.W.data, gain=1.414)
        
        # æ³¨æ„åŠ›å‘é‡
        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))
        nn.init.xavier_uniform_(self.a.data, gain=1.414)

        self.leakyrelu = nn.LeakyReLU(self.alpha)

    def forward(self, x, adj):
        # x: èŠ‚ç‚¹ç‰¹å¾çŸ©é˜µ [N, in_features]
        # adj: é‚»æ¥çŸ©é˜µ [N, N]
        
        # çº¿æ€§å˜æ¢
        h = torch.mm(x, self.W)  # [N, out_features]
        N = h.size()[0]

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1)
        a_input = a_input.view(N, N, 2 * self.out_features)
        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))

        # æ©ç æœºåˆ¶
        zero_vec = -9e15 * torch.ones_like(e)
        attention = torch.where(adj > 0, e, zero_vec)
        attention = F.softmax(attention, dim=1)
        attention = F.dropout(attention, self.dropout, training=self.training)

        # èšåˆç‰¹å¾
        h_prime = torch.matmul(attention, h)

        if self.concat:
            return F.elu(h_prime)
        else:
            return h_prime

class GAT(nn.Module):
    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):
        super(GAT, self).__init__()
        self.dropout = dropout
        
        # å¤šå¤´æ³¨æ„åŠ›å±‚
        self.attentions = nn.ModuleList([
            GATLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) 
            for _ in range(nheads)
        ])
        
        # è¾“å‡ºå±‚
        self.out_att = GATLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)

    def forward(self, x, adj):
        x = F.dropout(x, self.dropout, training=self.training)
        # å¤šå¤´æ³¨æ„åŠ›
        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)
        x = F.dropout(x, self.dropout, training=self.training)
        x = self.out_att(x, adj)
        return F.log_softmax(x, dim=1)
```

### å®é™…åº”ç”¨ç¤ºä¾‹

```python
# æ¨¡å‹åˆå§‹åŒ–
model = GAT(nfeat=input_dim,
           nhid=8,
           nclass=num_classes,
           dropout=0.6,
           alpha=0.2,
           nheads=8)

# ä¼˜åŒ–å™¨
optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)

# è®­ç»ƒå¾ªç¯
def train():
    model.train()
    optimizer.zero_grad()
    output = model(features, adj)
    loss = F.nll_loss(output[idx_train], labels[idx_train])
    loss.backward()
    optimizer.step()
    return loss.item()
```

## ğŸ” æ³¨æ„äº‹é¡¹ä¸æœ€ä½³å®è·µ

1. **æ³¨æ„åŠ›å¤´æ•°çš„é€‰æ‹©**
   - é€šå¸¸åœ¨4-8ä¹‹é—´
   - éœ€è¦æ ¹æ®ä»»åŠ¡å¤æ‚åº¦å’Œè®¡ç®—èµ„æºè°ƒæ•´

2. **è¿‡æ‹Ÿåˆå¤„ç†**
   - ä½¿ç”¨dropout
   - æ·»åŠ L2æ­£åˆ™åŒ–
   - ä½¿ç”¨æ®‹å·®è¿æ¥

3. **è®¡ç®—æ•ˆç‡**
   - å¯¹äºå¤§è§„æ¨¡å›¾ï¼Œè€ƒè™‘ä½¿ç”¨ç¨€ç–æ³¨æ„åŠ›
   - å¯ä»¥ä½¿ç”¨é‚»å±…é‡‡æ ·å‡å°‘è®¡ç®—é‡

4. **æ¨¡å‹è®¾è®¡è€ƒè™‘**
   - æ³¨æ„åŠ›å±‚çš„å †å ä¸å®œè¿‡æ·±
   - è€ƒè™‘æ·»åŠ è·³è·ƒè¿æ¥
   - æ ¹æ®ä»»åŠ¡é€‰æ‹©åˆé€‚çš„èšåˆå‡½æ•°

## ğŸ“ˆ æœªæ¥å±•æœ›

1. **å¯æ‰©å±•æ€§æ”¹è¿›**
   - ç ”ç©¶æ›´é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—æ–¹æ³•
   - æ¢ç´¢ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶

2. **ç†è®ºç ”ç©¶**
   - æ·±å…¥ç†è§£æ³¨æ„åŠ›æœºåˆ¶çš„å·¥ä½œåŸç†
   - ç ”ç©¶æ³¨æ„åŠ›æœºåˆ¶ä¸å›¾çš„ç»“æ„ç‰¹æ€§çš„å…³ç³»

3. **åº”ç”¨æ‹“å±•**
   - åœ¨æ›´å¤šé¢†åŸŸéªŒè¯æ•ˆæœ
   - ç»“åˆå…¶ä»–å…ˆè¿›æŠ€æœ¯ï¼ˆå¦‚Transformerï¼‰

# ğŸ“š ğ’¥ğ‘’ğ’»ğ‘’ğ“‡ğ‘’ğ“ƒğ’¸ğ‘’
<a href="/paper/Brauwerså’ŒFrasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf" target="_blank">ğŸ“„ Brauwerså’ŒFrasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a>
<a href="/paper/Lee ç­‰ - 2018 - Attention Models in Graphs A Survey.pdf" target="_blank">ğŸ“„ Lee ç­‰ - 2018 - Attention Models in Graphs A Survey</a>