---
title: Attention Overview
date: 2025-07-10 19:56:23
categories:
  - model
  - attention
tags:
  - CDR
  - model
  - Basic
  - deep learning
  - PyTorch
---

# Is Attention All My Need ?

> æ³¨æ„åŠ›æœºåˆ¶åœ¨å›¾ç¥ç»ç½‘ç»œä¸­æ‰®æ¼”ç€è¶Šæ¥è¶Šé‡è¦çš„è§’è‰²ã€‚~~ä½†é¼ é¼ ç°åœ¨è¿æ­£å¸¸çš„ Attention æœ‰å“ªäº›éƒ½ä¸æ¸…æ¥šæ~~æœ¬æ–‡é¼ é¼ å°†ä»ä¸€èˆ¬çš„ Attention å‡ºå‘ï¼Œç»™å‡º Attention çš„æ€»ä½“ç»“æ„ï¼Œç„¶åæŒ‰åˆ†ç±»ä»‹ç»ç°æœ‰çš„ä¸»è¦çš„ Attention

æœ¬æ–‡ä¸»è¦æ¥è‡ªäºä¸€ç¯‡è®ºæ–‡ï¼ŒåŸºæœ¬å¯ä»¥çœ‹ä½œ[é‚£ç¯‡è®ºæ–‡](/paper/Brauwerså’ŒFrasincar%20-%202023%20-%20A%20General%20Survey%20on%20Attention%20Mechanisms%20in%20Deep%20Learning.pdf)çš„é˜…è¯»ç¬”è®°

<!-- more -->

## ğŸ¯ å¼•è¨€

åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸï¼Œæ³¨æ„åŠ›æœºåˆ¶å·²ç»æˆä¸ºä¸€ä¸ªé©å‘½æ€§çš„åˆ›æ–°ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†åºåˆ—æ•°æ®å’Œå›¾åƒæ•°æ®æ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸã€‚è€Œåœ¨å›¾ç¥ç»ç½‘ç»œä¸­ï¼Œæ³¨æ„åŠ›æœºåˆ¶çš„å¼•å…¥ä¸ä»…æé«˜äº†æ¨¡å‹çš„è¡¨ç°åŠ›ï¼Œè¿˜å¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚

åœ¨å›¾ç»“æ„æ•°æ®ä¸­åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶ä¸»è¦æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š

1. è‡ªé€‚åº”æ€§ï¼šèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡åŠ¨æ€è°ƒæ•´ä¸åŒé‚»å±…èŠ‚ç‚¹çš„é‡è¦æ€§
2. å¯è§£é‡Šæ€§ï¼šé€šè¿‡æ³¨æ„åŠ›æƒé‡å¯ä»¥ç›´è§‚ç†è§£æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹
3. é•¿ç¨‹ä¾èµ–ï¼šæœ‰æ•ˆç¼“è§£äº†ä¼ ç»Ÿ GNN ä¸­çš„è¿‡å¹³æ»‘é—®é¢˜
4. å¼‚è´¨æ€§å¤„ç†ï¼šæ›´å¥½åœ°å¤„ç†å¼‚è´¨å›¾ä¸­çš„ä¸åŒç±»å‹èŠ‚ç‚¹å’Œè¾¹

## ğŸ“š æ€»è§ˆ Attention

æœ¬ç« èŠ‚ä¸»è¦å‚è€ƒäº†è®ºæ–‡[ğŸ“„ Brauwers å’Œ Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning](/paper/Brauwerså’ŒFrasincar%20-%202023%20-%20A%20General%20Survey%20on%20Attention%20Mechanisms%20in%20Deep%20Learning.pdf)æœ‰å…´è¶£çš„è¯å¯ä»¥çœ‹çœ‹åŸæ–‡æ

<embed src="/paper/Brauwerså’ŒFrasincar%20-%202023%20-%20A%20General%20Survey%20on%20Attention%20Mechanisms%20in%20Deep%20Learning.pdf" width="45%" height="400" type="application/pdf">

### Attention çš„ä¸€èˆ¬ç»“æ„

<img src="/img/Attention/TotalModel.png" alt="TotalModel" width="60%" height="auto">

ä¸Šå›¾æ˜¯ä»æ€»ä½“ä¸Šçœ‹ Attention åœ¨æ•´ä¸ªä»»åŠ¡æ¨¡å‹æ¡†æ¶ä¸­çš„ä½ç½®

æ¡†æ¶åŒ…å«å››ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š

1. **ç‰¹å¾æ¨¡å‹**ï¼šè´Ÿè´£è¾“å…¥æ•°æ®çš„ç‰¹å¾æå–
2. **æŸ¥è¯¢æ¨¡å‹**ï¼šç”Ÿæˆæ³¨æ„åŠ›æŸ¥è¯¢å‘é‡
3. **æ³¨æ„åŠ›æ¨¡å‹**ï¼šè®¡ç®—æ³¨æ„åŠ›æƒé‡
4. **è¾“å‡ºæ¨¡å‹**ï¼šç”Ÿæˆæœ€ç»ˆé¢„æµ‹ç»“æœ

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¼šä» _è¾“å…¥_ çš„è§’åº¦æ¥çœ‹**ç‰¹å¾æ¨¡å‹**å’Œ**æŸ¥è¯¢æ¨¡å‹**ï¼Œä» _è¾“å‡º_ çš„è§’åº¦æ¥çœ‹**æ³¨æ„åŠ›æ¨¡å‹**å’Œ**è¾“å‡ºæ¨¡å‹**

#### è¾“å…¥å¤„ç†æœºåˆ¶

1. **ç‰¹å¾æ¨¡å‹**ï¼Œå³å°†ä»»åŠ¡çš„è¾“å…¥è¿›è¡Œ embedding

   å¯¹äºè¾“å…¥çŸ©é˜µ $ X \in \mathbb{R}^{d_x \times n_x} $ ï¼Œç‰¹å¾æ¨¡å‹æå–ç‰¹å¾å‘é‡ï¼š $\boldsymbol{F} = [f_1, \ldots, f_{n_f}] \in \mathbb{R}^{d_f \times n_f}$

2. **æŸ¥è¯¢æ¨¡å‹**ï¼ŒæŸ¥è¯¢æ¨¡å‹äº§ç”ŸæŸ¥è¯¢å‘é‡$ \boldsymbol{q} \in \mathbb{R}^{d_q} $ï¼Œç”¨ä»¥å‘Šè¯‰æ³¨æ„åŠ›æ¨¡å‹å“ªä¸€ä¸ªç‰¹å¾æ˜¯é‡è¦çš„

ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œè¿™ä¸¤ä¸ªæ¨¡å‹å¯ä»¥ç”¨ CNN æˆ– RNN

#### è¾“å‡ºè®¡ç®—æœºåˆ¶

<img src="/img/Attention/GeneralAttentionModule.png" alt="GeneralAttentionModule" width="50%" height="auto">

ä¸Šå›¾æ˜¯ Attention æ¨¡å‹æ€»ä½“ç»“æ„çš„è¯´æ˜ï¼Œä¸‹é¢å¯¹è¿™å¼ å›¾è¿›è¡Œè¯¦ç»†çš„è¯´æ˜

1. ç‰¹å¾çŸ©é˜µ$\boldsymbol{F} = [\boldsymbol{f}\_1, \ldots, \boldsymbol{f}\_{n\_f}] \in \mathbb{R}^{d\_f \times n\_f}$ï¼Œé€šè¿‡*æŸäº›æ–¹æ³•*å°†å…¶åˆ†ä¸º Keys çŸ©é˜µ$\boldsymbol{K} = [\boldsymbol{k}\_1, \ldots, \boldsymbol{k}\_{n\_f}] \in \mathbb{R}^{d\_k \times n\_f}$å’Œ Values çŸ©é˜µ$\boldsymbol{V} = [\boldsymbol{v}_1, \ldots, \boldsymbol{v}\_{n\_f}] \in \mathbb{R}^{d\_v \times n\_f}$ï¼Œè¿™é‡Œçš„*æŸäº›æ–¹æ³•*ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ï¼ŒæŒ‰ä»¥ä¸‹çš„æ–¹å¼é€šè¿‡**çº¿æ€§å˜æ¢**å¾—åˆ°ï¼š

$$
\underset{d\_{k} \times n\_{f}}{\boldsymbol{K}}=\underset{d\_{k} \times d\_{f}}{\boldsymbol{W}\_{K}} \times \underset{d\_{f} \times n\_{f}}{\boldsymbol{F}}, \quad \underset{d\_{v} \times n\_{f}}{\boldsymbol{V}}=\underset{d\_{v} \times d\_{f}}{\boldsymbol{W}\_{V}} \times \underset{d\_{f} \times n\_{f}}{\boldsymbol{F}} .
$$

2. `Attention Scores`æ¨¡å—æ ¹æ® $\boldsymbol{q}$ è®¡ç®—æ¯ä¸€ä¸ª key å‘é‡å¯¹åº”çš„åˆ†æ•°$\boldsymbol{e} = [e_1, \ldots, e_{n_f}] \in \mathbb{R}^{n_f}$ï¼š

   $$
   \underset{1\times 1}{e\_l} = \text{score}(\underset{d\_q \times 1}{\boldsymbol{q}}, \underset{d\_k \times 1}{\boldsymbol{k}\_l})
   $$

   å¦‚å‰æ‰€è¿°ï¼ŒæŸ¥è¯¢è±¡å¾ç€å¯¹ä¿¡æ¯çš„è¯·æ±‚ã€‚æ³¨æ„åŠ›åˆ†æ•°$e_l$è¡¨ç¤ºæ ¹æ®æŸ¥è¯¢ï¼Œå…³é”®å‘é‡$\boldsymbol{k}_l$ä¸­åŒ…å«çš„ä¿¡æ¯çš„é‡è¦æ€§ã€‚å¦‚æœæŸ¥è¯¢å’Œå…³é”®å‘é‡çš„ç»´åº¦ç›¸åŒï¼Œåˆ™å¾—åˆ†å‡½æ•°çš„ä¸€ä¸ªä¾‹å­æ˜¯å–å‘é‡çš„ç‚¹ç§¯ã€‚

3. ç”±äºç»è¿‡è¿™ä¹ˆä¸€å †æ“ä½œä¹‹åï¼Œåˆ†æ•°æœ‰å¾ˆå¤§çš„å¯èƒ½å·²ç»é£èµ·æ¥äº†æï¼Œè¿™ä¸ªæ—¶å€™å°±éœ€è¦`Attention Alignment`æ¨¡å—å¯¹å…¶è¿›è¡Œ**å½’ä¸€åŒ–**ä¹‹ç±»çš„æ“ä½œäº†æ

   $$
   \underset{1\times 1}{a\_l} = \text{align}(\underset{d\_q \times 1}{\boldsymbol{e\_l}}, \underset{n\_f \times 1}{\boldsymbol{e}})
   $$

æ³¨æ„åŠ›æƒé‡$\boldsymbol{a} = [a_1, \ldots, a_{n_f}] \in \mathbb{R}^{n_f}$ä¸ºæ³¨æ„åŠ›æ¨¡å—æä¾›äº†ä¸€ä¸ªç›¸å½“ç›´è§‚çš„è§£é‡Šã€‚æ¯ä¸ªæƒé‡ç›´æ¥è¡¨æ˜äº†æ¯ä¸ªç‰¹å¾å‘é‡ç›¸å¯¹äºå…¶ä»–ç‰¹å¾å‘é‡å¯¹äºè¿™ä¸ªé—®é¢˜çš„é‡è¦æ€§ã€‚

4. åœ¨`Weight Average`æ¨¡å—å®Œæˆ**ä¸Šä¸‹æ–‡ç”Ÿæˆ**ï¼š

   $$
   \underset{d\_v \times 1}{\boldsymbol{c}} = \sum\_{l = 1}^{n\_f} \underset{1 \times 1}{a\_l}\times \underset{d\_v \times 1}{\boldsymbol{v}\_l}
   $$

5. è¾“å‡ºå¤„ç†å°±æƒ³æ€ä¹ˆæå°±æ€ä¹ˆæäº†æï¼Œä¾‹å¦‚ ç”¨äºåˆ†ç±»

   $$
   \underset{d\_y \times 1}{\hat{\boldsymbol{y}}} = \text{softmax}( \underset{d\_y \times d\_v}{\boldsymbol{W}\_c}\times \underset{d\_v \times 1}{\boldsymbol{c}} + \underset{d\_y \times 1}{\boldsymbol{b}\_c})
   $$

### Attention åˆ†ç±»

<img src="/img/Attention/Taxonomy.png" style="max-width: 100%; height: auto;">

è®ºæ–‡æŒ‰ç…§ä¸Šå›¾çš„æ–¹å¼ç»™ Attention è¿›è¡Œäº†åˆ†ç±»

ç”±äºç¯‡å¹…é™åˆ¶ï¼Œè¿™é‡Œå†³å®šé‡å¼€å‡ ä¸ªåšæ–‡æ¥åˆ†åˆ«ä»‹ç»è¿™äº› Attentionï¼Œé“¾æ¥å¦‚ä¸‹ï¼š

{% post_link 'Feature-Related-Attention' %}
<br/>
{% post_link 'General-Attention' %}
<br/>
{% post_link 'Query-Related-Attention' %}

### æ€æ ·è¯„ä»· Attention

#### å¤–åœ¨æ€§èƒ½è¯„ä¼°

1. **é¢†åŸŸç‰¹å®šçš„è¯„ä¼°æŒ‡æ ‡**

ä¸åŒé¢†åŸŸç”¨äºè¯„ä¼°æ³¨æ„åŠ›æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ï¼š

| é¢†åŸŸ | å¸¸ç”¨è¯„ä¼°æŒ‡æ ‡ | å…¸å‹åº”ç”¨ |
|------|------------|---------|
| è‡ªç„¶è¯­è¨€å¤„ç† | BLEU, METEOR, Perplexity | æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆ |
| è¯­éŸ³å¤„ç† | è¯é”™è¯¯ç‡(WER)ã€éŸ³ç´ é”™è¯¯ç‡(PER) | è¯­éŸ³è¯†åˆ« |
| è®¡ç®—æœºè§†è§‰ | PSNR, SSIM, IoU | å›¾åƒç”Ÿæˆã€åˆ†å‰² |
| é€šç”¨åˆ†ç±» | å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1 | æƒ…æ„Ÿåˆ†æã€æ–‡æ¡£åˆ†ç±» |

2. **æ¶ˆèç ”ç©¶**

    è®ºæ–‡å¼ºè°ƒäº†æ¶ˆèç ”ç©¶(ablation study)åœ¨è¯„ä¼°æ³¨æ„åŠ›æœºåˆ¶é‡è¦æ€§æ–¹é¢çš„ä»·å€¼ã€‚å…¸å‹åšæ³•åŒ…æ‹¬ï¼š
    1. ç§»é™¤æˆ–æ›¿æ¢æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¦‚ç”¨å¹³å‡æ± åŒ–ä»£æ›¿æ³¨æ„åŠ›æ± åŒ–ï¼‰
    2. æ¯”è¾ƒæ¨¡å‹åœ¨æœ‰æ— æ³¨æ„åŠ›æœºåˆ¶æ—¶çš„æ€§èƒ½å·®å¼‚
    3. åˆ†æä¸åŒæ³¨æ„åŠ›å˜ä½“å¯¹æœ€ç»ˆæ€§èƒ½çš„å½±å“

    è¿™ç§è¯„ä¼°æ–¹æ³•å¯ä»¥æ˜ç¡®æ³¨æ„åŠ›æœºåˆ¶å¯¹æ¨¡å‹æ€§èƒ½çš„å®é™…è´¡çŒ®ï¼Œè€Œä¸ä»…ä»…æ˜¯å±•ç¤ºæœ€ç»ˆç»“æœã€‚

#### å†…åœ¨ç‰¹æ€§è¯„ä¼°

1. **æ³¨æ„åŠ›æƒé‡åˆ†æ**

   1. **å¯¹é½é”™è¯¯ç‡(AER)**ï¼šæ¯”è¾ƒæ¨¡å‹ç”Ÿæˆçš„æ³¨æ„åŠ›æƒé‡ä¸äººå·¥æ ‡æ³¨çš„"é»„é‡‘"æ³¨æ„åŠ›æƒé‡ä¹‹é—´çš„å·®å¼‚
   2. **ç›‘ç£æ³¨æ„åŠ›è®­ç»ƒ**ï¼šå°†äººå·¥æ ‡æ³¨çš„æ³¨æ„åŠ›æƒé‡ä½œä¸ºé¢å¤–ç›‘ç£ä¿¡å·ï¼Œä¸ä»»åŠ¡æŸå¤±è”åˆè®­ç»ƒ
   3. **æ³¨æ„åŠ›å¯è§†åŒ–**ï¼šé€šè¿‡çƒ­å›¾ç­‰æ–¹å¼ç›´è§‚å±•ç¤ºæ¨¡å‹å…³æ³¨åŒºåŸŸ

   è¿™äº›æ–¹æ³•å¯ä»¥è¯„ä¼°æ³¨æ„åŠ›æƒé‡æ˜¯å¦ç¬¦åˆäººç±»ç›´è§‰æˆ–é¢†åŸŸçŸ¥è¯†ã€‚

2. **åŸºäºäººç±»æ³¨æ„åŠ›çš„è¯„ä¼°**

    è®ºæ–‡æå‡ºäº†"æ³¨æ„åŠ›æ­£ç¡®æ€§"(Attention Correctness)çš„æ¦‚å¿µï¼Œå°†æ¨¡å‹çš„æ³¨æ„åŠ›æ¨¡å¼ä¸çœŸå®äººç±»æ³¨æ„åŠ›è¡Œä¸ºè¿›è¡Œæ¯”è¾ƒï¼š

    1. **æ•°æ®æ”¶é›†**ï¼šè®°å½•äººç±»åœ¨æ‰§è¡Œç›¸åŒä»»åŠ¡æ—¶çš„æ³¨æ„åŠ›æ¨¡å¼ï¼ˆå¦‚çœ¼åŠ¨è¿½è¸ªï¼‰
    2. **åº¦é‡è®¡ç®—**ï¼šå®šä¹‰æ¨¡å‹æ³¨æ„åŠ›ä¸äººç±»æ³¨æ„åŠ›çš„ç›¸ä¼¼åº¦æŒ‡æ ‡
    3. **è”åˆè®­ç»ƒ**ï¼šå°†äººç±»æ³¨æ„åŠ›æ•°æ®ä½œä¸ºç›‘ç£ä¿¡å·

    è¿™ç§è¯„ä¼°æ–¹æ³•åŸºäºè®¤çŸ¥ç§‘å­¦åŸç†ï¼Œè®¤ä¸ºå¥½çš„æ³¨æ„åŠ›æ¨¡å‹åº”è¯¥æ¨¡æ‹Ÿäººç±»çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚

#### æ³¨æ„åŠ›è§£é‡Šæ€§è¯„ä¼°ã€

è®ºæ–‡è®¨è®ºäº†å­¦æœ¯ç•Œå…³äº"æ³¨æ„åŠ›æ˜¯å¦æä¾›è§£é‡Š"çš„äº‰è®ºï¼š

1. **"Attention is not Explanation"è§‚ç‚¹**
   - æ³¨æ„åŠ›æƒé‡ä¸æ¨¡å‹å†³ç­–ä¹‹é—´ç¼ºä¹ç¨³å®šå…³è”
   - å¯ä»¥æ„é€ å¯¹æŠ—æ€§æ³¨æ„åŠ›åˆ†å¸ƒè€Œä¸æ”¹å˜æ¨¡å‹è¾“å‡º
   - æ³¨æ„åŠ›æƒé‡å¯èƒ½åæ˜ ç›¸å…³æ€§è€Œéå› æœæ€§

2. **"Attention is not not Explanation"åé©³**
   - å¯¹æŠ—æ€§æ³¨æ„åŠ›åˆ†å¸ƒé€šå¸¸æ€§èƒ½æ›´å·®
   - æ³¨æ„åŠ›æƒé‡ç¡®å®åæ˜ äº†è¾“å…¥çš„ç›¸å¯¹é‡è¦æ€§
   - åœ¨ç‰¹å®šæ¶æ„ä¸‹æ³¨æ„åŠ›å¯ä»¥æä¾›æœ‰æ„ä¹‰çš„è§£é‡Š

~~è¿™æ®µæ¯”è¾ƒéš¾ç»·ï¼Œå› æ­¤æŠŠ~~åŸæ–‡è´´åœ¨ä¸‹é¢äº†æ

> However, rather than checking if the model focuses on the most important parts of the data, some use the attention weights to determine which parts of the data are most important. This would imply that attention models provide a type of explanation, which is a subject of contention among researchers. Particularly, in [120], extensive experiments are conducted for various natural language processing tasks to investigate the relation between attention weights and important information to determine whether attention can actually provide meaningful explanations. In this paper titled â€œAttention is not Explanationâ€, it is found that attention weights do not tend to correlate with important features. Additionally, the authors are able to replace the produced attention weights with completely different values while keeping the model output the same. These so-called â€œadversarialâ€ attention distributions show that an attention model may focus on completely different information and still come to the same conclusions, which makes interpretation difficult. Yet, in another paper titled â€œAttention is not not Explanationâ€ [121], the claim that attention is not explanation is questioned by challenging the assumptions of the previous work. It is found that the adversarial attention distributions do not perform as reliably well as the learned attention weights, indicating that it was not proved that attention is not viable for explanation. In general, the conclusion regarding the interpretability of attention models is that researchers must be extremely careful when drawing conclusions based on attention patterns. For example, problems with an attention model can be diagnosed via the attention weights if the model is found to focus on the incorrect parts of the data, if such information is available. Yet, conversely, attention weights may only be used to obtain plausible explanations for why certain parts of the data are focused on, rather than concluding that those parts are significant to the problem [121]. However, one should still be cautious as the viability of such approaches can depend on the model architecture [122].

# ğŸ“š ğ’¥ğ‘’ğ’»ğ‘’ğ“‡ğ‘’ğ“ƒğ’¸ğ‘’

<a href="/paper/Brauwerså’ŒFrasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning.pdf" target="_blank">ğŸ“„ Brauwers å’Œ Frasincar - 2023 - A General Survey on Attention Mechanisms in Deep Learning</a>
